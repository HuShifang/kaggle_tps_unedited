{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e815d3-d755-4fa2-85a5-ea4df4948fcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# widedeep_20211120\n",
    "I want to start playing around with some NN architectures. Eventually, I want to try some straight PyTorch, but for starters, I'll use `widedeep`. As a scaler, I'll use RankGauss; I won't (yet) do any feature reduction or selection. I also won't (yet) use `cleanlab`, though I may try it in the future, either via the wrapper they suggest or via `skorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843e3531-f950-4701-9330-07960ae9a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "COLAB = False # will trigger manual installation of packages\n",
    "USE_GPU = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a61aa18-6ef3-41d1-bb75-08d1f766dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "import gc; gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d84988-5ddb-40d9-bc12-826012acb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"deeptrainer_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf1e65-6447-47b9-88f2-8d02bbc29af0",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee16c791-548b-4616-9c82-ea76001e4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler #StandardScaler #, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n",
    "\n",
    "# from BorutaShap import BorutaShap\n",
    "from gauss_rank_scaler import GaussRankScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e9bc11-cfbd-4526-9bbb-95308c52a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAINT import TabAttention # from the official SAINT implementation as of 20211118, https://github.com/somepago/saint/blob/main/models/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b139207-5ea6-464d-922e-e4d0a398062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "from pytorch_widedeep.metrics import Accuracy\n",
    "from torchmetrics import AUROC\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71932e1-2e32-4474-acbc-3cbe63ce993e",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880b9f33-b517-40ef-a3e6-049dcc52a4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/nov2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "#     datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')  \n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'studies'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath, studypath]:\n",
    "        pth.mkdir(exist_ok=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1905a9de-cd7c-4a73-a39e-b42a8297785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892c1dbb-a47d-4838-a1ed-074b96a362f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a962daf-3010-4d8a-ae1e-b5ecaaf547e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmodel_config = {'arch': 'widedeep-saint',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60cd4c5f-d848-4e4c-9fa6-097961ac8cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'X_orig.feather'),\n",
    "    'target_source': str(datapath/'y_corrected.joblib'),\n",
    "    'test_source': str(datapath/'X_test_orig-no_scaling.feather'),\n",
    "    'scaler': str(GaussRankScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "X = pd.read_feather(dataset_params['train_source'])# load(dataset_params['train_source'])\n",
    "y = load(dataset_params['target_source'])\n",
    "X_test = pd.read_feather(dataset_params['test_source']) #load(dataset_params['test_source'])\n",
    "\n",
    "# dataset_params['feature_count'] = X.shape[1]\n",
    "# dataset_params['instance_count'] = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df908c8-d40b-4882-b5b9-f64de46c128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 114.44 Mb (75.0% reduction)\n",
      "Mem. usage decreased to 103.00 Mb (75.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# decrease memory footprint\n",
    "X = reduce_memory_usage(X)\n",
    "X_test = reduce_memory_usage(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b9135-660e-4cbf-b769-7e75127f0cc3",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "Inspired a bit by Laurent Pourchot's Aug2021 Tabular Playground entry, I'm going to try to generate two versions of the dataset: a categorical one, using bins, and then (for now) a GaussRankScaled one. In the future, I might add further variations, e.g. with feature reduction via PCA and perhaps also UMAP and also denoising; I might also try other normalizations, e.g. Quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fee94d0-041a-4640-82c1-523738de6638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/umap/__init__.py:9: ImportWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "031ae88d-dcab-4edc-afd1-12b6d3f56653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "preprocessing_params = {\n",
    "    'binning': \"pd.qcut(X.iloc[:,i],X.shape[1],labels=False,duplicates = 'drop')\",\n",
    "    'scaling, normalization': str(GaussRankScaler(epsilon=0.005)),\n",
    "    # 'reduction': str(PCA(n_components='mle', random_state=42)),\n",
    "    'reduction': None,\n",
    "    'manifold': None,\n",
    "    # 'manifold': str(umap.UMAP(n_components=10, n_neighbors=15, random_state=42, transform_seed=42,)),\n",
    "    'clustering': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ed94a-3b27-412b-804e-587094e1d27f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Binning (Generating wide cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abaf19a7-159f-466f-a5c2-d3fa57443af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h/t Laurent Pourchot https://www.kaggle.com/pourchot/in-python-tabular-denoising-residual-network/\n",
    "if preprocessing_params['binning']:\n",
    "    # 100 bins for the bins head of the NN (i.e. percentiles):\n",
    "    X_bins = np.zeros((X.shape[0],X.shape[1])) # he used all available data for the first tuple entry, but I'll start like this\n",
    "    X_bins_test = np.zeros((X_test.shape[0], X_test.shape[1]))\n",
    "    for i in range(X.shape[1]): # assumes X is a pd.DataFrame\n",
    "        X_bins[:,i] = pd.qcut(X.iloc[:,i],X.shape[1],labels=False,duplicates = 'drop')\n",
    "        X_bins_test[:,i] = pd.qcut(X_test.iloc[:,i],X.shape[1],labels=False,duplicates = 'drop')\n",
    "    X_bins = X_bins.astype(np.int8)\n",
    "    X_bins_test = X_bins_test.astype(np.int8)\n",
    "    X_bins = pd.DataFrame(X_bins, index=X.index, columns=[f'rkd_f{col}' for col in range(100)])\n",
    "    X_bins_test = pd.DataFrame(X_bins_test, index=X_test.index, columns=[f'rkd_f{col}' for col in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8116078-18e3-4391-bc66-5cf3d00f71f7",
   "metadata": {},
   "source": [
    "## Normalizing (Preprocessing Deep Cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56993a51-da51-40d0-9e7f-756534220098",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocessing_params['scaling, normalization']:\n",
    "    scaler = GaussRankScaler(epsilon=0.005)\n",
    "    X_gauss = scaler.fit_transform(X)\n",
    "    X_gauss_test = scaler.transform(X_test)\n",
    "    X_gauss = pd.DataFrame(X_gauss, columns=X.columns, index=X.index)\n",
    "    X_gauss_test = pd.DataFrame(X_gauss_test, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5ef3f-392d-4210-a6e4-6e0375e80a13",
   "metadata": {},
   "source": [
    "## Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65711458-8fdc-4264-a6cd-33bbd81a99d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# if preprocessing_params['reduction']:\n",
    "#     from sklearn.decomposition import PCA\n",
    "#     pca = PCA(n_components='mle', random_state=42)\n",
    "#     X_pca = pca.fit_transform(X_gauss)\n",
    "#     # X_pca = pca.fit_transform(X)\n",
    "#     X_pca = pd.DataFrame(X_pca, index=X.index)\n",
    "#     import umap\n",
    "#     reducer = umap.UMAP(n_components=10, # low end of typical for feature reduction\n",
    "#                     n_neighbors=15, # default value\n",
    "#                     random_state=42,\n",
    "#                     transform_seed=42,\n",
    "#                    )\n",
    "#     umapper = reducer.fit(X_pca)\n",
    "#     embedding = reducer.transform(X_pca)\n",
    "#     embedding_df = pd.DataFrame(embedding,columns=[f'embed_{col}' for col in range(10)])\n",
    "#     X_gauss = X_gauss.join(embedding_df)\n",
    "#     # X = X.join(embedding_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333abc9c-a059-4b86-a856-cfb7ff94868b",
   "metadata": {},
   "source": [
    "## X_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9048c9b1-8bd1-427b-a06f-f951532bd5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X_pre = X_gauss.join(X_bins)\n",
    "X_pre_test = X_gauss_test.join(X_bins_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8eadcb29-4596-4afc-96e3-34e4b14b9962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43167b42-138b-42ec-9a74-0948903c773b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rkd_f0     100\n",
       "rkd_f1     100\n",
       "rkd_f2     100\n",
       "rkd_f3     100\n",
       "rkd_f4     100\n",
       "rkd_f5     100\n",
       "rkd_f6     100\n",
       "rkd_f7     100\n",
       "rkd_f8     100\n",
       "rkd_f9     100\n",
       "rkd_f10    100\n",
       "rkd_f11    100\n",
       "rkd_f12    100\n",
       "rkd_f13    100\n",
       "rkd_f14    100\n",
       "rkd_f15    100\n",
       "rkd_f16    100\n",
       "rkd_f17    100\n",
       "rkd_f18    100\n",
       "rkd_f19    100\n",
       "rkd_f20    100\n",
       "rkd_f21    100\n",
       "rkd_f22    100\n",
       "rkd_f23    100\n",
       "rkd_f24    100\n",
       "rkd_f25    100\n",
       "rkd_f26    100\n",
       "rkd_f27    100\n",
       "rkd_f28    100\n",
       "rkd_f29    100\n",
       "rkd_f30    100\n",
       "rkd_f31    100\n",
       "rkd_f32    100\n",
       "rkd_f33    100\n",
       "rkd_f34    100\n",
       "rkd_f35    100\n",
       "rkd_f36    100\n",
       "rkd_f37    100\n",
       "rkd_f38    100\n",
       "rkd_f39    100\n",
       "rkd_f40    100\n",
       "rkd_f41    100\n",
       "rkd_f42    100\n",
       "rkd_f43    100\n",
       "rkd_f44    100\n",
       "rkd_f45    100\n",
       "rkd_f46    100\n",
       "rkd_f47    100\n",
       "rkd_f48    100\n",
       "rkd_f49    100\n",
       "rkd_f50    100\n",
       "rkd_f51    100\n",
       "rkd_f52    100\n",
       "rkd_f53    100\n",
       "rkd_f54    100\n",
       "rkd_f55    100\n",
       "rkd_f56    100\n",
       "rkd_f57    100\n",
       "rkd_f58    100\n",
       "rkd_f59    100\n",
       "rkd_f60    100\n",
       "rkd_f61    100\n",
       "rkd_f62    100\n",
       "rkd_f63    100\n",
       "rkd_f64    100\n",
       "rkd_f65    100\n",
       "rkd_f66    100\n",
       "rkd_f67    100\n",
       "rkd_f68    100\n",
       "rkd_f69    100\n",
       "rkd_f70    100\n",
       "rkd_f71    100\n",
       "rkd_f72    100\n",
       "rkd_f73    100\n",
       "rkd_f74    100\n",
       "rkd_f75    100\n",
       "rkd_f76    100\n",
       "rkd_f77    100\n",
       "rkd_f78    100\n",
       "rkd_f79    100\n",
       "rkd_f80    100\n",
       "rkd_f81    100\n",
       "rkd_f82    100\n",
       "rkd_f83    100\n",
       "rkd_f84    100\n",
       "rkd_f85    100\n",
       "rkd_f86    100\n",
       "rkd_f87    100\n",
       "rkd_f88    100\n",
       "rkd_f89    100\n",
       "rkd_f90    100\n",
       "rkd_f91    100\n",
       "rkd_f92    100\n",
       "rkd_f93    100\n",
       "rkd_f94    100\n",
       "rkd_f95    100\n",
       "rkd_f96    100\n",
       "rkd_f97    100\n",
       "rkd_f98    100\n",
       "rkd_f99    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pre.iloc[:, 100:].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fe3688e-62ab-461b-ae25-2d201bf3ff44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rkd_f0     100\n",
       "rkd_f1     100\n",
       "rkd_f2     100\n",
       "rkd_f3     100\n",
       "rkd_f4     100\n",
       "rkd_f5     100\n",
       "rkd_f6     100\n",
       "rkd_f7     100\n",
       "rkd_f8     100\n",
       "rkd_f9     100\n",
       "rkd_f10    100\n",
       "rkd_f11    100\n",
       "rkd_f12    100\n",
       "rkd_f13    100\n",
       "rkd_f14    100\n",
       "rkd_f15    100\n",
       "rkd_f16    100\n",
       "rkd_f17    100\n",
       "rkd_f18    100\n",
       "rkd_f19    100\n",
       "rkd_f20    100\n",
       "rkd_f21    100\n",
       "rkd_f22    100\n",
       "rkd_f23    100\n",
       "rkd_f24    100\n",
       "rkd_f25    100\n",
       "rkd_f26    100\n",
       "rkd_f27    100\n",
       "rkd_f28    100\n",
       "rkd_f29    100\n",
       "rkd_f30    100\n",
       "rkd_f31    100\n",
       "rkd_f32    100\n",
       "rkd_f33    100\n",
       "rkd_f34    100\n",
       "rkd_f35    100\n",
       "rkd_f36    100\n",
       "rkd_f37    100\n",
       "rkd_f38    100\n",
       "rkd_f39    100\n",
       "rkd_f40    100\n",
       "rkd_f41    100\n",
       "rkd_f42    100\n",
       "rkd_f43    100\n",
       "rkd_f44    100\n",
       "rkd_f45    100\n",
       "rkd_f46    100\n",
       "rkd_f47    100\n",
       "rkd_f48    100\n",
       "rkd_f49    100\n",
       "rkd_f50    100\n",
       "rkd_f51    100\n",
       "rkd_f52    100\n",
       "rkd_f53    100\n",
       "rkd_f54    100\n",
       "rkd_f55    100\n",
       "rkd_f56    100\n",
       "rkd_f57    100\n",
       "rkd_f58    100\n",
       "rkd_f59    100\n",
       "rkd_f60    100\n",
       "rkd_f61    100\n",
       "rkd_f62    100\n",
       "rkd_f63    100\n",
       "rkd_f64    100\n",
       "rkd_f65    100\n",
       "rkd_f66    100\n",
       "rkd_f67    100\n",
       "rkd_f68    100\n",
       "rkd_f69    100\n",
       "rkd_f70    100\n",
       "rkd_f71    100\n",
       "rkd_f72    100\n",
       "rkd_f73    100\n",
       "rkd_f74    100\n",
       "rkd_f75    100\n",
       "rkd_f76    100\n",
       "rkd_f77    100\n",
       "rkd_f78    100\n",
       "rkd_f79    100\n",
       "rkd_f80    100\n",
       "rkd_f81    100\n",
       "rkd_f82    100\n",
       "rkd_f83    100\n",
       "rkd_f84    100\n",
       "rkd_f85    100\n",
       "rkd_f86    100\n",
       "rkd_f87    100\n",
       "rkd_f88    100\n",
       "rkd_f89    100\n",
       "rkd_f90    100\n",
       "rkd_f91    100\n",
       "rkd_f92    100\n",
       "rkd_f93    100\n",
       "rkd_f94    100\n",
       "rkd_f95    100\n",
       "rkd_f96    100\n",
       "rkd_f97    100\n",
       "rkd_f98    100\n",
       "rkd_f99    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pre_test.iloc[:, 100:].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "716e3e17-c0e8-4fc1-81d2-eb625a322f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pre = pd.read_feather(datapath/'X_bins+GaussRankScaled+PCA,UMAP.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d1d8c-ee99-4ef3-9dec-c1e2b6d4e54a",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15840d89-4860-442a-b687-e8d6c8854a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pre_cont = X_pre.iloc[:, :110].join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01fc9c31-0366-4e7e-9e0d-b84684e765f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corr = X_pre_cont.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ca6ea-9581-43c1-ba23-7cb1c5a1e430",
   "metadata": {},
   "source": [
    "## Preparing Data for WideDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "558dad99-775d-4ed4-96be-e74cf0a67b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params['feature_count'] = X_pre.shape[1]\n",
    "dataset_params['instance_count'] = X_pre.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8b5a198-4f55-48ad-ab67-52b3742a7121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:179: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n"
     ]
    }
   ],
   "source": [
    "if 'widedeep' in exmodel_config['arch']:\n",
    "    cont_cols = X_pre.iloc[:,:100].columns # 110 if using PCA-UMAP embedding\n",
    "    wide_cols = X_pre.iloc[:, 100:].columns # 110 if using PCA-UMAP embedding\n",
    "    # # if not preprocessing\n",
    "    # X_wide = X_pre[wide_cols]\n",
    "    # X_tab = X_pre[cont_cols]\n",
    "    \n",
    "    # if preprocessing\n",
    "    wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "    X_wide = wide_preprocessor.fit_transform(X_pre)\n",
    "    X_wide_test = wide_preprocessor.transform(X_pre_test)\n",
    "    # tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=False,embed_cols=wide_cols) # for TabMLP\n",
    "    tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=True,embed_cols=wide_cols) # for SAINT\n",
    "    X_tab = tab_preprocessor.fit_transform(X_pre)\n",
    "    X_tab_test = tab_preprocessor.transform(X_pre_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ccec002-593e-411e-9ca8-dd4387260f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "widedeep_preprocessing_params = {\n",
    "        'wide': str(wide_preprocessor),\n",
    "        'deeptabular': str(tab_preprocessor),\n",
    "    }\n",
    "    \n",
    "preprocessing_params.update(widedeep_preprocessing_params)\n",
    "# print(preprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90886657-9928-4280-bb9e-dcaa777fc2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/datasets/X_tab_test_FIXED.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(X_wide_test, datapath/'X_wide_test_FIXED.joblib')\n",
    "dump(X_tab_test, datapath/'X_tab_test_FIXED.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddf2e042-015b-4b99-909a-f1b9edbaf599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/datasets/cont_cols.joblib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_wide = load(datapath/'X_wide.joblib')\n",
    "# X_tab = load(datapath/'X_tab.joblib')\n",
    "# X_wide_test = load(datapath/'X_wide_test.joblib')\n",
    "# X_tab_test = load(datapath/'X_tab_test.joblib')\n",
    "# dump(cont_cols, datapath/'cont_cols.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6eac14-220b-4c0e-b40c-e48584d46daf",
   "metadata": {},
   "source": [
    "# Config Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d647e8af-e065-4ea8-aa6c-5d158791904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "exmodel_config.update({\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "#     'random_state': SEED,\n",
    "#     'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "#     'subsample': 1,\n",
    "    'cross_val_strategy': KFold(n_splits=5, shuffle=True, random_state=SEED), # None for holdout, or the relevant sklearn class\n",
    "    'kfolds': 5, # if 1, that means just doing holdout\n",
    "    'test_size': 0.2,\n",
    "    **dataset_params,\n",
    "    **preprocessing_params\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5402d98-7bcd-4e46-b85d-6b0129ca6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['widedeep', 'deeplearning'],\n",
    "    'notes': \"Attempt using SAINT, default model params.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b715656-27ac-4c63-9657-0a5c3602c3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc7a58e4-bb29-4267-9b6c-ef66f4beb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pre_np = np.array(X_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ca6d90a-41f2-4b5c-bd6d-3d6b53d2ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeptabular = TabMlp(continuous_cols=cont_cols, column_idx=tab_preprocessor.column_idx)\n",
    "# deeptabular = TabMlp(continuous_cols=list(range(110)), column_idx={str(x): x for x in range(len(cont_cols))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9111391-90a5-422e-928a-bd55598befcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba89bb41-a06a-4302-9dad-71da88b38066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcf52554-e17b-4937-a57a-4c38c1c5c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_widedeep(arch, X_wide=X_wide, X_tab=X_tab, y=y, X_wide_test=X_wide_test, X_tab_test=X_tab_test, folds=list(range(5)), \n",
    "                            prev_epochs=0, n_epochs=20, exmodel_config=exmodel_config, wandb_config=wandb_config, \n",
    "                            random_state=42, shuffle_kfolds=True, wandb_tracked=True):\n",
    "    \"\"\"\n",
    "    Modification of the `cross_validate_model` function used in my stacking notebooks, customized to the dataset and to deep learning approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare for k-fold cross-validation; random-state here is notebook-wide, not per-model\n",
    "    # shuffle on the initial sets, but not subsequently -- performing the same operation twice means a very different dataset\n",
    "    if shuffle_kfolds:\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)#exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=SEED)\n",
    "    else:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=False)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = 'widedeep-saint'\n",
    "        # exmodel_config[f'model_params'] = str(model.parameters())\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )   \n",
    "    \n",
    "    # initialize lists for out-of-fold preds and ground truth\n",
    "    oof_preds, oof_y = [], []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # initialize a numpy.ndarray containing the fold-model's preds for test set\n",
    "    test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "#     if start_fold == 4:\n",
    "#     # immediately extend to include predictions from the 0 fold, which had a code bug\n",
    "#         oof_preds.extend(load(predpath/'widedeep_saint-20211127-50epochs-fold0-oofpreds.joblib'))\n",
    "#         oof_preds.extend(load(predpath/'widedeep_saint-20211127-50epochs-fold1-oofpreds.joblib'))\n",
    "#         oof_preds.extend(load(predpath/'widedeep_saint-20211127-50epochs-fold2-oofpreds.joblib'))\n",
    "#         oof_preds.extend(load(predpath/'widedeep_saint-20211127-55epochs-fold3-oofpreds.joblib'))\n",
    "        \n",
    "#         oof_y.extend(load(datapath/'y_valid-fold0.joblib'))\n",
    "#         oof_y.extend(y[load(datapath/'kfold42-fold1-valid_ids.joblib')])\n",
    "#         oof_y.extend(y[load(datapath/'kfold42-fold2-valid_ids.joblib')])\n",
    "#         oof_y.extend(y[load(datapath/'kfold42-fold3-valid_ids.joblib')])\n",
    "        \n",
    "#         test_preds += load(predpath/'widedeep_saint-20211127-50epochs-fold0-testpreds.joblib')\n",
    "#         test_preds += load(predpath/'widedeep_saint-20211127-50epochs-fold1-testpreds.joblib')\n",
    "#         test_preds += load(predpath/'widedeep_saint-20211127-50epochs-fold2-testpreds.joblib')\n",
    "#         test_preds += load(predpath/'widedeep_saint-20211127-55epochs-fold3-testpreds.joblib')\n",
    "    \n",
    "    # print(f\"Before entering loop, oof_preds is length {len(oof_preds)}, oof_y is {len(oof_y)}, and test_preds is {test_preds.shape}\")\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "        torch.cuda.empty_cache()\n",
    "#         print(f\"type(train_ids) = {type(train_ids)} and train_ids.shape = {train_ids.shape}\")\n",
    "#         print(f\"type(valid_ids) = {type(valid_ids)} and train_ids.shape = {valid_ids.shape}\")\n",
    "        if fold not in folds: # skip folds that are already trained\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"FOLD {fold}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            dump(train_ids, datapath/f'kfold42-fold{fold}-train_ids.joblib')\n",
    "            dump(valid_ids, datapath/f'kfold42-fold{fold}-valid_ids.joblib')\n",
    "            y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "            print(f\"y_train shape is {y_train.shape}, y_valid shape is {y_valid.shape}\")\n",
    "            # dump(y_train, datapath/f'y_train-fold{fold}.joblib')\n",
    "            # dump(y_valid, datapath/f'y_valid-fold{fold}.joblib')\n",
    "            # if isinstance(X, np.ndarray):\n",
    "                # X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "            X_train_wide, X_train_tab = X_wide[train_ids], X_tab[train_ids]\n",
    "            X_valid_wide, X_valid_tab = X_wide[valid_ids], X_tab[valid_ids]\n",
    "                \n",
    "                # X_train = pd.DataFrame(X_train, columns=\n",
    "            # else:\n",
    "            #     X_train_wide, X_train_tab = X_wide.iloc[train_ids,:], X_tab[train_ids,:]\n",
    "            #     X_valid_wide, X_valid_tab = X_wide[valid_ids,:], X_tab[valid_ids,:]\n",
    "            \n",
    "            # print(f\"X_train shape is {X_train.shape}\")\n",
    "            # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "            # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "            # scaling\n",
    "            # scaler = GaussRankScaler()\n",
    "            # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "            # X_valid = pd.DataFrame(scaler.transform(X_valid), columns=X.columns)\n",
    "            # X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "            \n",
    "            # print(\"Scaling complete\")\n",
    "            # print(f\"X_train shape is {X_train.shape}\")\n",
    "            # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "            # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "            # embedding & library-specific preprocessing\n",
    "#             tab_preprocessor = TabPreprocessor(\n",
    "#                 scale=False, # because GaussRank scaling already occurred\n",
    "#                 # scale=True\n",
    "#                 for_transformer=False, # change if using a Transformer-based model\n",
    "#                 continuous_cols=X.columns,\n",
    "#                 # continuous_cols=range(X.shape[1]), # since it'll be working on a numpy.ndarray\n",
    "#                 auto_embed_dim=True, # uses fastai's rule of thumb\n",
    "#             )#, embed_cols=embed_cols, )\n",
    "#             X_train = tab_preprocessor.fit_transform(X_train)   \n",
    "#             X_valid = tab_preprocessor.transform(X_valid)\n",
    "#             X_test = tab_preprocessor.transform(X_test)\n",
    "            \n",
    "#             print(\"Tab preprocessing complete.\")\n",
    "#             print(f\"Type of X_train is {type(X_train)}\")\n",
    "#             # print(f\"X_train shape is {X_train.shape}\")\n",
    "#             # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "#             # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "#             # define model\n",
    "#             deeptabular = TabMlp(\n",
    "#                 mlp_hidden_dims=[64,32],\n",
    "#                 column_idx=tab_preprocessor.column_idx,\n",
    "#             #     embed_input=tab_preprocessor.embeddings_input,\n",
    "#                 # continuous_cols=range(X.shape[1]), # since it'll be working on a numpy.ndarray\n",
    "#                 continuous_cols=X.columns,\n",
    "#             )\n",
    "\n",
    "            if 'saint' in arch:\n",
    "                wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "                deeptabular = SAINT(continuous_cols=cont_cols, column_idx=tab_preprocessor.column_idx,)\n",
    "                model = WideDeep(wide=wide, deeptabular=deeptabular)\n",
    "                if prev_epochs > 0:\n",
    "                    model.load_state_dict(torch.load(datapath/f\"{arch}-20211127-weights-{prev_epochs}epochs-fold{fold}/wd_model.pt\"))\n",
    "        \n",
    "                # n_epochs = 55\n",
    "\n",
    "                # model = WideDeep(wide=None, deeptabular=deeptabular)\n",
    "\n",
    "                # pytorch hyperparams\n",
    "                wide_opt = AdamW(model.wide.parameters(),)\n",
    "                deep_opt = SGD(model.deeptabular.parameters(),  lr=0.01, momentum=0.75)\n",
    "\n",
    "                wide_sch = CosineAnnealingWarmRestarts(optimizer=wide_opt, T_0=5) \n",
    "                deep_sch = ReduceLROnPlateau(optimizer=deep_opt, )\n",
    "\n",
    "                # deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_train_tab.shape[0], epochs=n_epochs)\n",
    "\n",
    "                # optimizers = {'deeptabular': deep_opt }\n",
    "                # lr_schedulers = {'deeptabular': deep_sch }\n",
    "\n",
    "                optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "                lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "\n",
    "                callbacks = [\n",
    "                    LRHistory(n_epochs=n_epochs), \n",
    "                ]\n",
    "\n",
    "                # trainer\n",
    "                trainer = Trainer(model=model, \n",
    "                                  objective='binary', \n",
    "                                  metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                                  seed=random_state, \n",
    "                                  optimizers=optimizers,\n",
    "                                  callbacks=callbacks\n",
    "                                 )\n",
    "                \n",
    "            else:\n",
    "                wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "                deeptabular = TabMlp(continuous_cols=cont_cols, column_idx=tab_preprocessor.column_idx)\n",
    "                model = WideDeep(wide=wide, deeptabular=deeptabular)\n",
    "                \n",
    "                wide_opt = AdamW(model.wide.parameters(), lr=0.1)\n",
    "                deep_opt = AdamW(model.deeptabular.parameters(), lr=0.1)\n",
    "\n",
    "                wide_sch = OneCycleLR(optimizer=wide_opt, max_lr=0.01, steps_per_epoch=X_wide_train.shape[0], epochs=n_epochs)\n",
    "                deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_tab_train.shape[0], epochs=n_epochs)\n",
    "\n",
    "                optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "                lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "\n",
    "\n",
    "                callbacks = [\n",
    "                    LRHistory(n_epochs=n_epochs), \n",
    "                ]\n",
    "\n",
    "                # trainer\n",
    "                trainer = Trainer(model=model, \n",
    "                                  objective='binary', \n",
    "                                  metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                                  seed=42, \n",
    "                                  optimizers=optimizers,\n",
    "                                  callbacks=callbacks\n",
    "                                 )\n",
    "    #             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "            trainer.fit( \n",
    "                X_wide=X_train_wide,\n",
    "                X_tab=X_train_tab,# np.array(X_train),\n",
    "                target=np.array(y_train),\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=1048, # default value is 32\n",
    "    #                 val_split=0.2, # no need for this\n",
    "            )\n",
    "        \n",
    "            trainer.save(path=datapath/f'{arch}-20211127-weights-{prev_epochs + n_epochs}epochs-fold{fold}', save_state_dict=True)\n",
    "\n",
    "            y_valid_preds = trainer.predict_proba(X_wide=np.array(X_valid_wide), X_tab=np.array(X_valid_tab), batch_size=1048)[:,1]\n",
    "            dump(y_valid_preds, predpath/f'{arch}-20211127-{prev_epochs + n_epochs}epochs-fold{fold}-oofpreds.joblib')\n",
    "\n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "\n",
    "\n",
    "            # test set inference\n",
    "            fold_test_preds = trainer.predict_proba(X_wide=np.array(X_wide_test), X_tab=np.array(X_tab_test), batch_size=1048)[:,1]\n",
    "            dump(fold_test_preds, predpath/f'{arch}-20211127-{prev_epochs + n_epochs}epochs-fold{fold}-testpreds.joblib')\n",
    "            test_preds += fold_test_preds\n",
    "            \n",
    "            # print(f\"NaNs in y_valid_preds: {np.isnan(y_valid_preds).any()}\")\n",
    "            # print(f\"NaNs in y_valid: {np.isnan(y_valid).any()}\")\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    #         valid_loss = log_loss(y_valid, y_pred)\n",
    "            # give the valid AUC score, for edification\n",
    "            fold_valid_auc = roc_auc_score(y_valid, y_valid_preds)\n",
    "            if wandb_tracked:\n",
    "                wandb.log({f'fold{fold}_valid_roc_auc': fold_valid_auc})\n",
    "            print(f\"Valid AUC for fold {fold} is {fold_valid_auc}\")   \n",
    "        # dump(model, Path(runpath/f\"{arch}_fold{fold}_rs{random_state}_model.joblib\"))\n",
    "\n",
    "    if len(folds) == 5:\n",
    "        model_valid_auc = roc_auc_score(oof_y, oof_preds)\n",
    "        print(f\"Valid AUC score for {arch} model is {model_valid_auc}\")\n",
    "        if wandb_tracked:\n",
    "            wandb.log({'overall_valid_auc': model_valid_auc,\n",
    "                       'model_params': str(model.parameters()), #if 'widedeep' in arch else str(model.get_params()),\n",
    "                       'model_seed': random_state,\n",
    "                      })\n",
    "            wandb.finish()\n",
    "        # finalize test preds\n",
    "        test_preds /= exmodel_config['kfolds']\n",
    "        \n",
    "    else:\n",
    "        if wandb_tracked:\n",
    "                wandb.log({#'overall_valid_auc': model_valid_auc,\n",
    "                           'model_params': str(model.parameters()), #if 'widedeep' in arch else str(model.get_params()),\n",
    "                           'model_seed': random_state,\n",
    "                          })\n",
    "                wandb.finish()\n",
    "    \n",
    "    \n",
    "    # save OOF preds and test-set preds\n",
    "#     if 'widedeep' in arch:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "#     else:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "    if not (datapath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\").is_file():\n",
    "        dump(oof_y, predpath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\")\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         if 'widedeep' in arch:\n",
    "#         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "#                    'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()), \n",
    "#         #                    'model_params': str(model.get_params()),\n",
    "#         })\n",
    "# #         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "# # #                    'model_params': str(model.get_params()),\n",
    "# #                   })\n",
    "#         wandb.finish()\n",
    "    return oof_preds, test_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16776bcf-98e3-4534-a92d-937fd355fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(y_valid, datapath/'y_valid-fold0.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e2f0245-26e2-495f-a108-c16f140bc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(X_wide, datapath/'X_wide.joblib')\n",
    "# dump(X_tab, datapath/'X_tab.joblib')\n",
    "# dump(X_wide_test, datapath/'X_wide_test.joblib')\n",
    "# dump(X_tab_test, datapath/'X_tab_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de43c25e-ec29-41bc-b022-d344e64dc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_gauss, X_bins, X_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe178966-df55-4c8a-a1c5-b306cea61175",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find deeptrainer_20211130.ipynb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/202111_Kaggle_tabular_playground/runs/2daf6q5l\" target=\"_blank\">deeptrainer_20211130_105857</a></strong> to <a href=\"https://wandb.ai/hushifang/202111_Kaggle_tabular_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 4\n",
      "---------------------------------------------------\n",
      "y_train shape is (480000,), y_valid shape is (120000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|| 3750/3750 [06:20<00:00,  9.85it/s, loss=0.00411, metrics={'acc': 0.9987}]\n",
      "epoch 2: 100%|| 3750/3750 [06:16<00:00,  9.95it/s, loss=0.00375, metrics={'acc': 0.9988}]\n",
      "epoch 3: 100%|| 3750/3750 [06:31<00:00,  9.59it/s, loss=0.00451, metrics={'acc': 0.9986}]\n",
      "epoch 4: 100%|| 3750/3750 [06:41<00:00,  9.35it/s, loss=0.00424, metrics={'acc': 0.9987}]\n",
      "epoch 5: 100%|| 3750/3750 [06:29<00:00,  9.63it/s, loss=0.00275, metrics={'acc': 0.9992}]\n",
      "predict: 100%|| 938/938 [00:19<00:00, 49.36it/s]\n",
      "predict: 100%|| 4219/4219 [01:24<00:00, 49.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid AUC for fold 4 is 0.9971492369325716\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13368... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold4_valid_roc_auc</td><td></td></tr><tr><td>model_seed</td><td></td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold4_valid_roc_auc</td><td>0.99715</td></tr><tr><td>model_params</td><td><generator object Mo...</td></tr><tr><td>model_seed</td><td>42</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deeptrainer_20211130_105857</strong>: <a href=\"https://wandb.ai/hushifang/202111_Kaggle_tabular_playground/runs/2daf6q5l\" target=\"_blank\">https://wandb.ai/hushifang/202111_Kaggle_tabular_playground/runs/2daf6q5l</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211130_105858-2daf6q5l/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([1.0,\n",
       "  5.493406715341154e-22,\n",
       "  1.3696180849734248e-25,\n",
       "  1.0,\n",
       "  1.6518714614518933e-27,\n",
       "  8.472907840834445e-14,\n",
       "  2.155132662176514e-16,\n",
       "  5.163647773938263e-23,\n",
       "  0.0022446175571531057,\n",
       "  4.415031003792974e-32,\n",
       "  3.5804975633165254e-29,\n",
       "  4.0473842091159895e-06,\n",
       "  3.695994778968641e-25,\n",
       "  1.0,\n",
       "  4.963985302185849e-19,\n",
       "  1.8329861931841934e-21,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  6.835721929430574e-28,\n",
       "  8.988390704951494e-16,\n",
       "  1.0,\n",
       "  6.141231621577192e-30,\n",
       "  0.9999992847442627,\n",
       "  0.8389108777046204,\n",
       "  8.855578109795315e-26,\n",
       "  5.903971755927273e-11,\n",
       "  1.0,\n",
       "  7.770483621769711e-20,\n",
       "  0.9999983310699463,\n",
       "  2.0249052591004628e-18,\n",
       "  1.7225357851202716e-07,\n",
       "  5.4914977226872e-05,\n",
       "  7.193731166310613e-10,\n",
       "  1.8958172593347637e-27,\n",
       "  0.9846873879432678,\n",
       "  1.0,\n",
       "  0.9998026490211487,\n",
       "  1.3233522350858395e-12,\n",
       "  4.9884170927986754e-26,\n",
       "  7.926402603427211e-35,\n",
       "  7.590585606371034e-23,\n",
       "  3.0834619579866995e-11,\n",
       "  4.8079591863236466e-18,\n",
       "  3.1220332122637107e-16,\n",
       "  1.0,\n",
       "  1.5841191093079557e-32,\n",
       "  3.5302198079488045e-30,\n",
       "  8.817424568263732e-10,\n",
       "  5.954167810142861e-16,\n",
       "  4.428138471101253e-31,\n",
       "  3.5632087309470042e-28,\n",
       "  0.9999982118606567,\n",
       "  1.0,\n",
       "  0.9999998807907104,\n",
       "  1.1052868689742645e-12,\n",
       "  0.9999935626983643,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0274275889091848e-23,\n",
       "  2.204896774786978e-21,\n",
       "  0.020405951887369156,\n",
       "  0.0006506434874609113,\n",
       "  3.9595862064770103e-17,\n",
       "  8.71347074280493e-05,\n",
       "  0.9363069534301758,\n",
       "  0.9999996423721313,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.00011420121154515073,\n",
       "  5.226283862293923e-25,\n",
       "  2.2173365574196247e-11,\n",
       "  1.0,\n",
       "  4.9969282184259e-12,\n",
       "  1.2670634799603824e-30,\n",
       "  0.9999217987060547,\n",
       "  7.533508323831484e-05,\n",
       "  1.0,\n",
       "  0.9999642372131348,\n",
       "  1.568730151977185e-25,\n",
       "  1.0,\n",
       "  3.352206207930808e-13,\n",
       "  0.9999995231628418,\n",
       "  8.851346847311981e-13,\n",
       "  1.0214433838410652e-16,\n",
       "  1.2330682466199505e-06,\n",
       "  5.437283237793641e-22,\n",
       "  1.0,\n",
       "  6.898368595104065e-13,\n",
       "  1.0,\n",
       "  1.4514031973933094e-17,\n",
       "  1.8906172200711113e-12,\n",
       "  0.0002321064966963604,\n",
       "  1.7052634395211573e-27,\n",
       "  1.1772402030427936e-24,\n",
       "  6.612089895691721e-32,\n",
       "  0.999991774559021,\n",
       "  0.0032352961134165525,\n",
       "  1.53523773012941e-13,\n",
       "  1.0,\n",
       "  1.4480920550152644e-30,\n",
       "  3.827872685491229e-09,\n",
       "  0.06348073482513428,\n",
       "  3.423832128773166e-19,\n",
       "  7.233160431496799e-06,\n",
       "  3.981374691736095e-13,\n",
       "  1.0,\n",
       "  6.843839820442663e-27,\n",
       "  1.0126773304364178e-05,\n",
       "  0.9999667406082153,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.6339568587042663e-12,\n",
       "  1.0,\n",
       "  6.860031960144301e-16,\n",
       "  1.0,\n",
       "  2.805035304570367e-26,\n",
       "  8.640408902325297e-27,\n",
       "  3.901668171899674e-23,\n",
       "  2.9205761165940203e-05,\n",
       "  1.0,\n",
       "  0.09956097602844238,\n",
       "  0.9997456669807434,\n",
       "  2.367281695114798e-06,\n",
       "  2.638766253885539e-20,\n",
       "  0.9904237389564514,\n",
       "  1.300352055766172e-21,\n",
       "  0.9999992847442627,\n",
       "  5.919366663534121e-21,\n",
       "  0.03627161681652069,\n",
       "  4.315374835159708e-24,\n",
       "  0.11841914802789688,\n",
       "  4.0793241673604527e-19,\n",
       "  5.792164357969032e-26,\n",
       "  0.9999542236328125,\n",
       "  1.6020090675351508e-18,\n",
       "  1.453062781209269e-09,\n",
       "  8.083315151452553e-06,\n",
       "  3.0218438060695324e-19,\n",
       "  0.9998014569282532,\n",
       "  3.9843008411677677e-16,\n",
       "  4.0496860975736126e-08,\n",
       "  2.764293886836893e-14,\n",
       "  7.905942068543725e-13,\n",
       "  6.5223017591877596e-18,\n",
       "  0.9999752044677734,\n",
       "  1.496419149212951e-32,\n",
       "  6.421339324078702e-30,\n",
       "  1.0,\n",
       "  0.0064156982116401196,\n",
       "  8.912630517539119e-30,\n",
       "  1.0,\n",
       "  0.9559732675552368,\n",
       "  1.2312523267610231e-07,\n",
       "  2.2372223865141372e-11,\n",
       "  2.0295691342604915e-25,\n",
       "  0.9020354747772217,\n",
       "  2.628010973134246e-25,\n",
       "  0.9999998807907104,\n",
       "  4.5579464869576385e-23,\n",
       "  0.9999943971633911,\n",
       "  1.0,\n",
       "  5.204788600555075e-28,\n",
       "  7.414460005170349e-32,\n",
       "  1.0,\n",
       "  0.3892960846424103,\n",
       "  1.469319598588963e-16,\n",
       "  1.0,\n",
       "  1.4530711078819536e-09,\n",
       "  0.0003317267692182213,\n",
       "  1.0,\n",
       "  1.0149683593952106e-24,\n",
       "  2.5266524983358352e-11,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.042469535022974014,\n",
       "  3.790844117020249e-14,\n",
       "  1.0,\n",
       "  7.864998224249575e-06,\n",
       "  6.307782074443935e-10,\n",
       "  5.4344144970733314e-17,\n",
       "  2.9953684429987008e-18,\n",
       "  1.0,\n",
       "  2.1120644567041574e-14,\n",
       "  7.243465854264143e-28,\n",
       "  2.131775099158606e-21,\n",
       "  1.6177608154650654e-26,\n",
       "  1.3795473823626642e-13,\n",
       "  0.9999929666519165,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999971389770508,\n",
       "  2.127059361978212e-10,\n",
       "  3.695470228623396e-19,\n",
       "  2.0271613720979076e-07,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.1351553070786889e-16,\n",
       "  6.703672192710219e-17,\n",
       "  1.314161181653617e-06,\n",
       "  1.0127043278268543e-27,\n",
       "  1.0,\n",
       "  0.9867936968803406,\n",
       "  4.1386020626310194e-20,\n",
       "  0.9999997615814209,\n",
       "  3.7157585081869994e-17,\n",
       "  1.2083437468390912e-05,\n",
       "  6.012550483533441e-13,\n",
       "  0.9999978542327881,\n",
       "  1.9301336184903685e-22,\n",
       "  0.0021982097532600164,\n",
       "  1.9240060282754712e-05,\n",
       "  0.0018447942566126585,\n",
       "  3.6745419531701886e-25,\n",
       "  4.136460325741137e-14,\n",
       "  0.9465308785438538,\n",
       "  1.5888385862298178e-26,\n",
       "  4.511851813998678e-27,\n",
       "  3.734574021538475e-24,\n",
       "  1.6160724805645694e-24,\n",
       "  5.412877007605717e-32,\n",
       "  2.229236656381417e-10,\n",
       "  0.057900551706552505,\n",
       "  0.9064026474952698,\n",
       "  2.998398092831704e-33,\n",
       "  0.8825644254684448,\n",
       "  3.345149631504374e-16,\n",
       "  1.0093220747034448e-12,\n",
       "  8.222414888309734e-32,\n",
       "  0.03983200713992119,\n",
       "  8.727689419174567e-06,\n",
       "  5.30458282336579e-23,\n",
       "  0.999998927116394,\n",
       "  6.631547054344724e-30,\n",
       "  1.0851665275528865e-23,\n",
       "  0.9996277093887329,\n",
       "  1.3053720847757143e-28,\n",
       "  2.760089624679897e-29,\n",
       "  4.00747701689852e-09,\n",
       "  0.41359612345695496,\n",
       "  0.012014145962893963,\n",
       "  1.9474130112939747e-06,\n",
       "  1.5932736947429505e-28,\n",
       "  1.0241457175652613e-06,\n",
       "  0.9999994039535522,\n",
       "  1.0,\n",
       "  1.0179856069214566e-08,\n",
       "  9.071165822766694e-23,\n",
       "  0.9993870258331299,\n",
       "  3.742504698462187e-16,\n",
       "  1.1544620370784742e-08,\n",
       "  1.2157907323082723e-26,\n",
       "  2.6347536307583862e-27,\n",
       "  0.7946151494979858,\n",
       "  1.0,\n",
       "  1.1287712595731136e-06,\n",
       "  0.8594677448272705,\n",
       "  5.848907805623744e-29,\n",
       "  8.326527677342354e-21,\n",
       "  1.9654824650510315e-18,\n",
       "  3.806484329176548e-19,\n",
       "  2.2068068026760227e-21,\n",
       "  0.9128878712654114,\n",
       "  1.147544993057581e-11,\n",
       "  0.9999810457229614,\n",
       "  0.9999972581863403,\n",
       "  0.9999997615814209,\n",
       "  0.42300945520401,\n",
       "  1.0306154123477726e-13,\n",
       "  1.0,\n",
       "  4.307320523366798e-06,\n",
       "  7.570164223638743e-32,\n",
       "  1.0,\n",
       "  2.6926204321764402e-11,\n",
       "  2.8707807837335525e-31,\n",
       "  2.1550806328815268e-26,\n",
       "  2.4212302380414635e-18,\n",
       "  1.0,\n",
       "  6.673037816762317e-11,\n",
       "  1.0,\n",
       "  3.017073017358359e-28,\n",
       "  1.0,\n",
       "  9.068743704243303e-23,\n",
       "  1.0,\n",
       "  2.9894136438839913e-23,\n",
       "  1.7962354923273387e-23,\n",
       "  1.0,\n",
       "  1.3581564290586292e-25,\n",
       "  1.0,\n",
       "  3.432445610886956e-29,\n",
       "  0.999997615814209,\n",
       "  1.3456292254390822e-26,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.784271026130094e-30,\n",
       "  0.4936438202857971,\n",
       "  1.1191698404470227e-22,\n",
       "  6.206451283121266e-28,\n",
       "  0.0020486300345510244,\n",
       "  1.0,\n",
       "  2.653290369435373e-28,\n",
       "  1.5648157288259275e-32,\n",
       "  1.0,\n",
       "  1.0132236948957012e-24,\n",
       "  4.177419826130789e-25,\n",
       "  0.04836710914969444,\n",
       "  1.0,\n",
       "  3.226943761092116e-07,\n",
       "  0.9999988079071045,\n",
       "  1.1016756951143236e-11,\n",
       "  1.1042162375401435e-16,\n",
       "  1.0,\n",
       "  7.479782038969512e-14,\n",
       "  5.566457711436712e-11,\n",
       "  2.4397091692662798e-05,\n",
       "  1.7730946710869944e-09,\n",
       "  0.9999994039535522,\n",
       "  0.99935382604599,\n",
       "  2.0021791583304767e-16,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  6.248508065687743e-25,\n",
       "  0.9999905824661255,\n",
       "  1.1536136280270853e-21,\n",
       "  5.680747122829813e-24,\n",
       "  1.0,\n",
       "  1.1131603971925585e-10,\n",
       "  0.0011043535778298974,\n",
       "  1.2027893672642003e-29,\n",
       "  0.9999902248382568,\n",
       "  1.3463991688661782e-19,\n",
       "  5.234591192714586e-15,\n",
       "  0.015724508091807365,\n",
       "  1.0485953534080817e-29,\n",
       "  7.448977878965479e-09,\n",
       "  1.0,\n",
       "  3.994054433523275e-24,\n",
       "  0.9999947547912598,\n",
       "  0.9999727010726929,\n",
       "  0.19983574748039246,\n",
       "  5.2597573812818155e-05,\n",
       "  0.9999924898147583,\n",
       "  7.227044700133125e-20,\n",
       "  9.650136801719598e-29,\n",
       "  1.0,\n",
       "  2.7361835989692447e-17,\n",
       "  1.2708814511793293e-17,\n",
       "  0.9998922348022461,\n",
       "  2.4768365979038018e-23,\n",
       "  9.932663953713745e-32,\n",
       "  2.253832530492699e-14,\n",
       "  5.371649893349186e-19,\n",
       "  1.0,\n",
       "  1.820405159930765e-17,\n",
       "  1.0,\n",
       "  1.1514772779719351e-07,\n",
       "  1.0,\n",
       "  0.9999998807907104,\n",
       "  4.4008009330927685e-19,\n",
       "  1.1581356804558213e-13,\n",
       "  0.034915659576654434,\n",
       "  6.254258861686804e-24,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0008947630994953215,\n",
       "  1.4150483447738078e-16,\n",
       "  0.9947910308837891,\n",
       "  3.0895212148607243e-06,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999990463256836,\n",
       "  4.801000215091911e-16,\n",
       "  0.9737892150878906,\n",
       "  6.379254384416439e-28,\n",
       "  1.740762568813725e-11,\n",
       "  1.5252076530119694e-15,\n",
       "  2.5066866742522746e-20,\n",
       "  1.0945186659228057e-05,\n",
       "  3.5866383339237355e-20,\n",
       "  1.1737390607244411e-15,\n",
       "  1.3311272223370807e-11,\n",
       "  3.4042874759630087e-32,\n",
       "  3.8707939382638805e-20,\n",
       "  0.0005224904743954539,\n",
       "  3.333033571362165e-10,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  9.265645764411508e-17,\n",
       "  1.8152819077951758e-23,\n",
       "  6.1182650057389765e-09,\n",
       "  1.0,\n",
       "  4.254241155952541e-33,\n",
       "  1.0,\n",
       "  9.889935185445284e-18,\n",
       "  3.8323266782214205e-10,\n",
       "  0.1363631933927536,\n",
       "  1.0,\n",
       "  2.2211148462305593e-20,\n",
       "  0.9998962879180908,\n",
       "  0.9999980926513672,\n",
       "  0.9978213310241699,\n",
       "  4.017878427291402e-24,\n",
       "  0.009628526866436005,\n",
       "  1.0,\n",
       "  5.489857361567173e-14,\n",
       "  0.06440649926662445,\n",
       "  1.0172967026149185e-19,\n",
       "  7.822799564339422e-20,\n",
       "  8.328841089033645e-15,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  3.860366632579826e-06,\n",
       "  0.9999525547027588,\n",
       "  1.0,\n",
       "  7.251203083076183e-14,\n",
       "  0.9997171759605408,\n",
       "  3.6981587226130735e-14,\n",
       "  4.1349427577727526e-19,\n",
       "  1.0,\n",
       "  0.9999997615814209,\n",
       "  7.47939123829644e-12,\n",
       "  1.0,\n",
       "  3.4307161586821745e-15,\n",
       "  0.036628447473049164,\n",
       "  2.421800495063288e-20,\n",
       "  4.796010451341371e-28,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999964237213135,\n",
       "  9.318009866341211e-22,\n",
       "  3.8315827353549907e-28,\n",
       "  1.3498004797218854e-12,\n",
       "  1.0,\n",
       "  0.9998672008514404,\n",
       "  3.8616264835882787e-32,\n",
       "  2.320614256312581e-10,\n",
       "  4.756791849302999e-09,\n",
       "  2.764373222691717e-25,\n",
       "  4.40943337398636e-10,\n",
       "  0.9654076099395752,\n",
       "  0.9999269247055054,\n",
       "  1.0,\n",
       "  1.4796416651144981e-30,\n",
       "  1.0,\n",
       "  3.359710660788551e-07,\n",
       "  0.9999972581863403,\n",
       "  0.0018219739431515336,\n",
       "  1.0,\n",
       "  2.8856778432017493e-25,\n",
       "  1.0,\n",
       "  0.9994640946388245,\n",
       "  0.028667090460658073,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  8.44875422095548e-28,\n",
       "  1.0817352316275725e-32,\n",
       "  1.552389578043385e-32,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  9.539319854867667e-12,\n",
       "  0.006991956382989883,\n",
       "  0.9952791929244995,\n",
       "  0.9556798338890076,\n",
       "  1.2493027698601431e-19,\n",
       "  1.0,\n",
       "  0.015851058065891266,\n",
       "  3.0128973388348242e-21,\n",
       "  0.9999995231628418,\n",
       "  7.757319901554791e-16,\n",
       "  0.05318388342857361,\n",
       "  7.362283818900658e-19,\n",
       "  1.6542576871122451e-24,\n",
       "  1.0,\n",
       "  7.114894777201361e-19,\n",
       "  1.7937037978687692e-21,\n",
       "  0.9999988079071045,\n",
       "  1.447559134248877e-05,\n",
       "  1.4597405457550768e-28,\n",
       "  0.9999995231628418,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999990463256836,\n",
       "  6.337333645843448e-23,\n",
       "  2.8104267357775825e-08,\n",
       "  0.4983680546283722,\n",
       "  3.3513328895615726e-30,\n",
       "  1.0,\n",
       "  1.3618540489218775e-27,\n",
       "  1.0,\n",
       "  7.769118464881486e-13,\n",
       "  1.6999448684239353e-26,\n",
       "  2.1562555657033997e-27,\n",
       "  1.0,\n",
       "  1.8757697645359803e-18,\n",
       "  1.0,\n",
       "  0.9999945163726807,\n",
       "  1.383371781087239e-30,\n",
       "  4.382950493425135e-30,\n",
       "  1.0,\n",
       "  0.9999982118606567,\n",
       "  0.9967169165611267,\n",
       "  2.7288257349395514e-31,\n",
       "  0.9999986886978149,\n",
       "  0.7766513228416443,\n",
       "  1.1847937742004433e-08,\n",
       "  0.9997013211250305,\n",
       "  1.0190811545349608e-16,\n",
       "  1.8919649846793618e-06,\n",
       "  2.705188403646776e-28,\n",
       "  7.78159137126977e-09,\n",
       "  9.906519160551852e-09,\n",
       "  2.386062843888568e-14,\n",
       "  0.946246325969696,\n",
       "  1.0,\n",
       "  8.992203496726712e-22,\n",
       "  1.0070373042620455e-20,\n",
       "  2.2004516264662044e-10,\n",
       "  9.12185530417019e-17,\n",
       "  1.6742870197925116e-32,\n",
       "  5.3088104297796044e-17,\n",
       "  1.984175135305577e-08,\n",
       "  0.0004080219368916005,\n",
       "  4.2049239743509077e-23,\n",
       "  2.845680234422193e-13,\n",
       "  0.9999996423721313,\n",
       "  5.0018474267739323e-20,\n",
       "  3.276188351719611e-07,\n",
       "  1.0,\n",
       "  6.936401180591016e-22,\n",
       "  3.836672840407118e-05,\n",
       "  1.0,\n",
       "  3.848125273296715e-18,\n",
       "  0.9998923540115356,\n",
       "  6.613357548799648e-30,\n",
       "  1.0,\n",
       "  1.8762036688713124e-06,\n",
       "  0.9797967076301575,\n",
       "  1.0,\n",
       "  7.948757456688327e-07,\n",
       "  7.965670507458443e-24,\n",
       "  0.0005076598026789725,\n",
       "  0.9999992847442627,\n",
       "  1.8743630142029793e-23,\n",
       "  6.969146307672979e-25,\n",
       "  3.884043126010864e-22,\n",
       "  4.2655855736484227e-07,\n",
       "  1.0,\n",
       "  1.729700794213509e-27,\n",
       "  0.02118040807545185,\n",
       "  1.0,\n",
       "  3.165893671623309e-30,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.4984061042232497e-08,\n",
       "  0.9999974966049194,\n",
       "  1.0144837715443553e-18,\n",
       "  0.07717182487249374,\n",
       "  1.0,\n",
       "  1.2304360725575157e-19,\n",
       "  1.9175088542240016e-24,\n",
       "  1.1649532306800458e-27,\n",
       "  1.4715011965159486e-24,\n",
       "  1.8474383620261737e-15,\n",
       "  1.1566503663833066e-20,\n",
       "  1.0,\n",
       "  7.251265560226372e-12,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.712813862437713e-10,\n",
       "  6.58691433318016e-32,\n",
       "  0.43599244952201843,\n",
       "  1.3008686006560654e-35,\n",
       "  0.00021580574684776366,\n",
       "  0.9983283877372742,\n",
       "  1.993450737121519e-26,\n",
       "  1.7145979390288447e-19,\n",
       "  2.819684763455399e-11,\n",
       "  1.0,\n",
       "  4.722200264743499e-26,\n",
       "  2.895570356168447e-24,\n",
       "  6.9617120082816505e-31,\n",
       "  1.0,\n",
       "  4.366762525621091e-30,\n",
       "  4.740236718433848e-16,\n",
       "  7.507070370035612e-17,\n",
       "  2.3559753117297078e-06,\n",
       "  1.033328076809307e-20,\n",
       "  2.0192386665859744e-15,\n",
       "  1.8358197381436285e-14,\n",
       "  5.424534972829997e-32,\n",
       "  8.769079613557551e-06,\n",
       "  2.0640186448872555e-08,\n",
       "  5.228627635096511e-21,\n",
       "  1.0,\n",
       "  2.7025279791814683e-09,\n",
       "  1.0,\n",
       "  6.141936010686532e-37,\n",
       "  7.866601221349645e-22,\n",
       "  4.84210294206289e-33,\n",
       "  4.736905198885954e-18,\n",
       "  1.0,\n",
       "  0.9999833106994629,\n",
       "  2.8236023291223393e-15,\n",
       "  7.787460857607508e-19,\n",
       "  0.9999685287475586,\n",
       "  0.8956625461578369,\n",
       "  3.948545321003777e-13,\n",
       "  1.180300380980864e-20,\n",
       "  1.1341745413933485e-13,\n",
       "  3.4318122193159216e-17,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999992847442627,\n",
       "  1.1505621746447545e-16,\n",
       "  1.3468970843807718e-27,\n",
       "  2.3906546094956335e-14,\n",
       "  0.007776892744004726,\n",
       "  0.9999973773956299,\n",
       "  8.432793670706572e-18,\n",
       "  0.9983152151107788,\n",
       "  0.9999998807907104,\n",
       "  3.124400095370854e-26,\n",
       "  8.7151218878439e-27,\n",
       "  4.512084282592976e-12,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0026368016842752695,\n",
       "  2.9226137308625686e-17,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  3.519175809609383e-18,\n",
       "  7.237482835265041e-10,\n",
       "  1.8280038602824789e-06,\n",
       "  7.217878774052125e-14,\n",
       "  1.659268267294873e-18,\n",
       "  1.0769291869496319e-19,\n",
       "  2.90018194260889e-15,\n",
       "  1.9316523637344964e-28,\n",
       "  2.48381041956236e-07,\n",
       "  0.9999858140945435,\n",
       "  4.119229809605952e-27,\n",
       "  3.5838260714626813e-07,\n",
       "  8.484443370093686e-09,\n",
       "  0.9999994039535522,\n",
       "  7.94277511886321e-05,\n",
       "  1.0,\n",
       "  2.990510097433376e-22,\n",
       "  0.9997885823249817,\n",
       "  0.9999995231628418,\n",
       "  7.654154239546052e-21,\n",
       "  1.8666981574761725e-17,\n",
       "  1.6971657538541431e-18,\n",
       "  0.9999967813491821,\n",
       "  5.543939209928794e-07,\n",
       "  1.0,\n",
       "  5.884787767431277e-20,\n",
       "  2.690168035565971e-25,\n",
       "  1.125618820374541e-09,\n",
       "  0.9997345805168152,\n",
       "  0.9997963309288025,\n",
       "  0.9999926090240479,\n",
       "  1.0,\n",
       "  1.5303137582004314e-14,\n",
       "  1.0988064841691014e-12,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.022710364311933517,\n",
       "  1.0,\n",
       "  1.7656816453381907e-07,\n",
       "  4.207541975422002e-14,\n",
       "  1.0,\n",
       "  1.92162896788865e-19,\n",
       "  0.9999995231628418,\n",
       "  8.096173182172608e-17,\n",
       "  9.51949329996469e-30,\n",
       "  4.147377694607712e-06,\n",
       "  0.9999901056289673,\n",
       "  2.084837972708796e-31,\n",
       "  0.9999935626983643,\n",
       "  4.816733817358809e-11,\n",
       "  3.2213658153779274e-34,\n",
       "  0.02713971771299839,\n",
       "  6.833389262341232e-24,\n",
       "  0.9999972581863403,\n",
       "  9.270325962946315e-29,\n",
       "  1.7128137782379352e-26,\n",
       "  4.022822679417004e-07,\n",
       "  0.9986811280250549,\n",
       "  1.0,\n",
       "  4.352214428018806e-21,\n",
       "  3.077481976735252e-29,\n",
       "  1.2386984674356267e-16,\n",
       "  5.799981539610579e-22,\n",
       "  1.0,\n",
       "  1.6050761895535903e-15,\n",
       "  0.9999978542327881,\n",
       "  1.3760457965987993e-22,\n",
       "  0.0025528958067297935,\n",
       "  4.1741415710755457e-16,\n",
       "  1.0,\n",
       "  1.1093329855614927e-24,\n",
       "  1.0,\n",
       "  2.217216739224388e-31,\n",
       "  0.9999998807907104,\n",
       "  1.7875345292850398e-07,\n",
       "  0.9999114274978638,\n",
       "  0.006989798508584499,\n",
       "  7.442580887877054e-37,\n",
       "  1.2604537083896205e-23,\n",
       "  4.1591704302845756e-08,\n",
       "  1.0,\n",
       "  5.949316821719006e-21,\n",
       "  1.1533311101341438e-30,\n",
       "  1.226937388493221e-15,\n",
       "  8.569091733079404e-06,\n",
       "  5.612059705825715e-31,\n",
       "  1.537240750447447e-22,\n",
       "  1.0,\n",
       "  1.9202479961677454e-07,\n",
       "  0.00022664193238597363,\n",
       "  1.7904862247462813e-26,\n",
       "  1.0,\n",
       "  4.385105973906351e-31,\n",
       "  2.2307899758975132e-17,\n",
       "  0.9997442364692688,\n",
       "  0.1657087504863739,\n",
       "  8.260167211817536e-33,\n",
       "  1.775987766450271e-05,\n",
       "  8.666751772104626e-27,\n",
       "  0.9998570680618286,\n",
       "  3.972381534822489e-07,\n",
       "  1.2783059762034215e-17,\n",
       "  3.5280884470017105e-25,\n",
       "  7.714983578828692e-28,\n",
       "  1.6931577988552773e-24,\n",
       "  0.9996684789657593,\n",
       "  3.8862890162841026e-33,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.1519429854445207e-09,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0993772292522408e-07,\n",
       "  2.9505031307053287e-06,\n",
       "  1.7271584511036053e-05,\n",
       "  3.4336819495776934e-26,\n",
       "  0.9999990463256836,\n",
       "  1.0,\n",
       "  9.228247332959683e-11,\n",
       "  1.0,\n",
       "  4.5361227252703473e-13,\n",
       "  2.478893305901675e-11,\n",
       "  1.2941047950309393e-17,\n",
       "  1.973412447721975e-14,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9998290538787842,\n",
       "  2.7418279202606755e-08,\n",
       "  2.146195778787684e-26,\n",
       "  8.622130286409107e-28,\n",
       "  0.9999899864196777,\n",
       "  6.443470920203254e-05,\n",
       "  1.327944381659208e-19,\n",
       "  4.210339301254681e-20,\n",
       "  1.0,\n",
       "  0.9999998807907104,\n",
       "  0.9985424280166626,\n",
       "  5.61007027499727e-07,\n",
       "  4.7788447035084645e-20,\n",
       "  4.958374595647906e-15,\n",
       "  5.14125895278994e-06,\n",
       "  1.2184910300530854e-14,\n",
       "  2.827161162038466e-21,\n",
       "  1.0,\n",
       "  1.898344143838012e-18,\n",
       "  0.00013867804955225438,\n",
       "  4.741698710535699e-17,\n",
       "  0.9999957084655762,\n",
       "  4.593185446043397e-16,\n",
       "  0.00010326288611395285,\n",
       "  7.868470675020944e-07,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999998807907104,\n",
       "  1.3970609066306372e-13,\n",
       "  3.9174449955736414e-24,\n",
       "  1.5110583150477103e-32,\n",
       "  1.0,\n",
       "  1.084538818450576e-28,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  4.254519013429717e-32,\n",
       "  3.205322556364073e-25,\n",
       "  1.0,\n",
       "  3.404655509351676e-22,\n",
       "  6.517715110545813e-21,\n",
       "  2.6850125337275123e-24,\n",
       "  1.0377083299317746e-06,\n",
       "  3.8559024138469496e-31,\n",
       "  1.4134807377343585e-20,\n",
       "  1.2817490484346403e-31,\n",
       "  2.8266069525426685e-24,\n",
       "  1.7715440250773214e-19,\n",
       "  2.524707665152448e-10,\n",
       "  1.7947532349182893e-11,\n",
       "  1.2588212680313487e-21,\n",
       "  0.999997615814209,\n",
       "  6.226763114835326e-24,\n",
       "  3.4435466659488156e-05,\n",
       "  4.709830037086249e-08,\n",
       "  2.3195950368815055e-11,\n",
       "  5.431711865367573e-20,\n",
       "  2.444780919475784e-22,\n",
       "  2.3332028830529047e-27,\n",
       "  2.0633180611184798e-05,\n",
       "  1.0,\n",
       "  1.658779579569416e-27,\n",
       "  0.9390969276428223,\n",
       "  1.2283032680453629e-25,\n",
       "  3.9629896491533145e-05,\n",
       "  4.183312131276961e-29,\n",
       "  0.7854772806167603,\n",
       "  0.9999994039535522,\n",
       "  0.07556230574846268,\n",
       "  0.9999992847442627,\n",
       "  0.9925897121429443,\n",
       "  1.0307543538730969e-33,\n",
       "  1.6967139838160136e-24,\n",
       "  1.0,\n",
       "  1.9615051738707234e-18,\n",
       "  0.0027196540031582117,\n",
       "  0.010950081050395966,\n",
       "  9.085505299539409e-30,\n",
       "  1.2888336499867158e-24,\n",
       "  4.9233431398812483e-20,\n",
       "  0.9999990463256836,\n",
       "  3.2155991203097756e-09,\n",
       "  2.4192246029370095e-20,\n",
       "  1.8959355298827502e-13,\n",
       "  0.00045384844997897744,\n",
       "  4.2866189384985903e-14,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999998807907104,\n",
       "  9.446687917963347e-27,\n",
       "  3.786780382597499e-07,\n",
       "  0.00023186211183201522,\n",
       "  1.0,\n",
       "  4.2827776860345014e-29,\n",
       "  9.31857835106395e-22,\n",
       "  1.0297877771011761e-25,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0020253921393305063,\n",
       "  6.354752500404811e-19,\n",
       "  1.0,\n",
       "  2.625847130878489e-13,\n",
       "  7.833832205506042e-05,\n",
       "  0.9999964237213135,\n",
       "  0.9999759197235107,\n",
       "  1.0,\n",
       "  0.009010259062051773,\n",
       "  3.2329170451816935e-09,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0774294050380278e-21,\n",
       "  1.5214460758372704e-25,\n",
       "  1.4521736665030658e-16,\n",
       "  1.2775066106970184e-13,\n",
       "  5.648997555678775e-18,\n",
       "  2.899545870604925e-05,\n",
       "  3.5668382136755383e-10,\n",
       "  1.7319051437425514e-07,\n",
       "  0.9999995231628418,\n",
       "  1.0,\n",
       "  0.0022932596039026976,\n",
       "  2.9731713407740547e-17,\n",
       "  3.122984952862597e-24,\n",
       "  8.131472995906095e-21,\n",
       "  5.252438024447303e-13,\n",
       "  4.690461352652164e-10,\n",
       "  0.9999998807907104,\n",
       "  3.777416068828643e-08,\n",
       "  0.19846148788928986,\n",
       "  5.622975095320726e-06,\n",
       "  0.9999998807907104,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  5.766067215748132e-32,\n",
       "  0.9999974966049194,\n",
       "  3.827678361171139e-15,\n",
       "  9.343476686796137e-11,\n",
       "  1.0,\n",
       "  1.0531388610825749e-26,\n",
       "  1.4988705284073653e-24,\n",
       "  1.0,\n",
       "  9.984731264012225e-09,\n",
       "  5.878112232707058e-24,\n",
       "  1.3802733955792235e-10,\n",
       "  0.000559186446480453,\n",
       "  0.9999924898147583,\n",
       "  2.982437945320271e-05,\n",
       "  0.9698516726493835,\n",
       "  1.0,\n",
       "  5.037828490950282e-20,\n",
       "  1.7670692614046857e-05,\n",
       "  1.3462457418677714e-33,\n",
       "  5.840656974932202e-22,\n",
       "  1.0,\n",
       "  0.3900424540042877,\n",
       "  1.0705048403281126e-08,\n",
       "  5.833814320510555e-09,\n",
       "  0.0002949217159766704,\n",
       "  1.0,\n",
       "  5.972744193713142e-34,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  5.27363738456188e-07,\n",
       "  9.80011784382237e-30,\n",
       "  3.1690075796685245e-32,\n",
       "  0.9892206788063049,\n",
       "  1.0,\n",
       "  0.9999361038208008,\n",
       "  1.0,\n",
       "  0.9701822400093079,\n",
       "  3.420618984457795e-20,\n",
       "  3.51178380142644e-27,\n",
       "  0.9999945163726807,\n",
       "  1.2970760237118723e-23,\n",
       "  6.929393066526777e-22,\n",
       "  2.483712402412408e-10,\n",
       "  2.345212132952687e-21,\n",
       "  1.0,\n",
       "  0.9907236695289612,\n",
       "  1.0,\n",
       "  2.6502073448152824e-34,\n",
       "  1.7697822854689483e-18,\n",
       "  2.2723274525443494e-15,\n",
       "  2.161178675975491e-28,\n",
       "  1.0320564715338154e-11,\n",
       "  1.5268968272912076e-14,\n",
       "  8.377878887201191e-23,\n",
       "  1.1542025746260694e-30,\n",
       "  0.9999995231628418,\n",
       "  4.198646974049273e-20,\n",
       "  0.997931718826294,\n",
       "  1.0,\n",
       "  1.9324189329868874e-13,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.4666919799629786e-25,\n",
       "  4.441798230794531e-28,\n",
       "  2.2204952685647115e-21,\n",
       "  1.5493688114685516e-29,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  7.072801366803105e-08,\n",
       "  2.4889360820649843e-27,\n",
       "  3.091279449790407e-24,\n",
       "  1.8805524770217652e-29,\n",
       "  0.9999876022338867,\n",
       "  2.3138579014647014e-16,\n",
       "  4.597189701760711e-20,\n",
       "  1.9299503661862616e-10,\n",
       "  1.286849601456197e-05,\n",
       "  9.121770440021599e-28,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  9.245886513805148e-29,\n",
       "  1.1392780185170766e-21,\n",
       "  1.4659768954317087e-09,\n",
       "  1.489181888914004e-09,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.4555294650338235e-23,\n",
       "  0.7501032948493958,\n",
       "  0.0003847504558507353,\n",
       "  1.0,\n",
       "  1.8043604654849332e-07,\n",
       "  1.0,\n",
       "  1.7241809050638125e-34,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9463872313499451,\n",
       "  0.9999997615814209,\n",
       "  0.999992847442627,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  6.710228815736788e-30,\n",
       "  ...],\n",
       " array([1.00000000e+00, 1.00000000e+00, 9.99755561e-01, ...,\n",
       "        1.88240268e-08, 1.00000000e+00, 1.00000000e+00]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_widedeep('widedeep-tabmlp', prev_epochs=0, n_epochs=50)\n",
    "# oof_preds, test_preds = cross_validate_widedeep('widedeep-saint', )\n",
    "# dump(oof_preds, predpath/f'widedeep_saint-20211127-{n_epochs}epochs-mean-oofpreds.joblib')\n",
    "# dump(test_preds, predpath/f'widedeep_saint-20211127-{n_epochs}epochs-mean-testpreds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "094cdc73-64ad-4ddf-a730-11d1c66da671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/datasets/tab_preprocessor.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(tab_preprocessor, datapath/'tab_preprocessor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae6eb1e4-2ff7-4c56-a2ca-3aa2023a78c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_valid_wide' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-bd4e1a3d3229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0my_valid_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_wide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_wide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_tab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'widedeep_saint-20211127-{n_epochs}epochs-fold{fold}-oofpreds.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_valid_wide' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/datasets/widedeep_saint-20211127-weights-50epochs-fold0/wd_model.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f7550ae-882a-4ea8-a687-9b55917067ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef48a347-e95a-45d3-b1b6-911b49f0ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "    if fold == 0:\n",
    "        y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "        print(f\"y_train shape is {y_train.shape}, y_valid shape is {y_valid.shape}\")\n",
    "        # if isinstance(X, np.ndarray):\n",
    "            # X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "        X_train_wide, X_train_tab = X_wide[train_ids], X_tab[train_ids]\n",
    "        X_valid_wide, X_valid_tab = X_wide[valid_ids], X_tab[valid_ids]\n",
    "        \n",
    "        n_epochs = 50\n",
    "\n",
    "    # model = WideDeep(wide=None, deeptabular=deeptabular)\n",
    "\n",
    "    # pytorch hyperparams\n",
    "    wide_opt = AdamW(model.wide.parameters(),)\n",
    "    deep_opt = SGD(model.deeptabular.parameters(),  lr=0.01, momentum=0.75)\n",
    "\n",
    "    wide_sch = CosineAnnealingWarmRestarts(optimizer=wide_opt, T_0=5) \n",
    "    deep_sch = ReduceLROnPlateau(optimizer=deep_opt, )\n",
    "\n",
    "    # deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_train_tab.shape[0], epochs=n_epochs)\n",
    "\n",
    "    # optimizers = {'deeptabular': deep_opt }\n",
    "    # lr_schedulers = {'deeptabular': deep_sch }\n",
    "\n",
    "    optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "    lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "\n",
    "    callbacks = [\n",
    "        LRHistory(n_epochs=n_epochs), \n",
    "    ]\n",
    "\n",
    "    # 2. Instantiate the trainer\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        objective=\"binary\",\n",
    "        metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "        seed=42, \n",
    "        optimizers=optimizers,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    y_valid_preds = trainer.predict_proba(X_wide=np.array(X_valid_wide), X_tab=np.array(X_valid_tab), batch_size=128)[:,1]\n",
    "    dump(y_valid_preds, predpath/f'widedeep_saint-20211127-{n_epochs}epochs-fold{fold}-oofpreds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1ba9e4a-8110-474e-a8c8-00a75ad6ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # test set inference\n",
    "    fold_test_preds = trainer.predict_proba(X_wide=np.array(X_wide_test), X_tab=np.array(X_tab_test), batch_size=128)[:,1]\n",
    "    dump(fold_test_preds, predpath/f'widedeep_saint-20211127-{n_epochs}epochs-fold{fold}-testpreds.joblib')\n",
    "\n",
    "    #         valid_loss = log_loss(y_valid, y_pred)\n",
    "    # give the valid AUC score, for edification\n",
    "    fold_valid_auc = roc_auc_score(y_valid, y_valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e29034ea-e577-41f1-b870-a43d10f08b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid AUC for fold 0 is 0.997027397963446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "wandb.log({f'fold{fold}_valid_roc_auc': fold_valid_auc})\n",
    "print(f\"Valid AUC for fold {fold} is {fold_valid_auc}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e8052-8d00-4dc2-b9ef-2397fb9d8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "oof_preds.extend(y_valid_preds)\n",
    "oof_y.extend(y_valid)\n",
    "\n",
    "test_preds += fold_test_preds\n",
    "\n",
    "# print(f\"NaNs in y_valid_preds: {np.isnan(y_valid_preds).any()}\")\n",
    "# print(f\"NaNs in y_valid: {np.isnan(y_valid).any()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a42b7-fcee-4de8-a031-85bc33882f22",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd77e6-24e3-4d6c-9699-0448960c82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_deep_train, X_deep_valid\n",
    "X_wide_train, X_wide_valid, y_train, y_valid = train_test_split(X_wide, y, test_size=0.2, random_state=42)\n",
    "X_tab_train, X_tab_valid, _, _ = train_test_split(X_tab, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ac1e4-6ce8-4a7b-9d20-76f276a401fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8142a8-1b63-4755-a50a-df34115a9ae5",
   "metadata": {},
   "source": [
    "From (shaky) earlier sweep, I got these as best params for the TabMLP model:\n",
    "\n",
    "```python\n",
    "{'learning_rate': 0.08763568442121664,\n",
    " 'weight_decay': 4.414536876494478e-05,\n",
    " 'wide_optimization': 'AdamW',\n",
    " 'tab_optimization': 'SGD',\n",
    " 'wide_momentum': 0.2862031274746775,\n",
    " 'tab_momentum': 0.7220103849055353,\n",
    " 'wide_scheduler': 'CosineAnnealingWarmRestarts',\n",
    " 'tab_scheduler': 'ReduceLROnPlateau'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f0424-b7f9-4b94-a430-aeef9fa336c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmodel_config['training_params'] = str({'learning_rate': 0.08763568442121664, 'weight_decay': 4.414536876494478e-05, 'wide_optimization': 'AdamW', 'tab_optimization': 'SGD', 'wide_momentum': 0.2862031274746775, 'tab_momentum': 0.7220103849055353, 'wide_scheduler': 'CosineAnnealingWarmRestarts', 'tab_scheduler': 'ReduceLROnPlateau'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7dfa2-6dbf-487e-91cb-73be4d0f1d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5f177-fc14-491a-a70e-a5e730c65363",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "lr = 0.08763568442121664\n",
    "wd = 4.414536876494478e-05\n",
    "\n",
    "wide_opt = AdamW(model.wide.parameters(), lr=lr)\n",
    "deep_opt = SGD(model.deeptabular.parameters(), lr=lr, weight_decay=wd, momentum=0.7220103849055353)\n",
    "\n",
    "# wide_sch = OneCycleLR(optimizer=wide_opt, max_lr=0.01, steps_per_epoch=X_wide_train.shape[0], epochs=n_epochs)\n",
    "# deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_tab_train.shape[0], epochs=n_epochs)\n",
    "wide_sch = CosineAnnealingWarmRestarts(optimizer=wide_opt, T_0=5) \n",
    "deep_sch = ReduceLROnPlateau(optimizer=deep_opt, )\n",
    "\n",
    "optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "\n",
    "callbacks = [\n",
    "    LRHistory(n_epochs=n_epochs), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780039b4-feec-48a8-b2ac-152a227cda05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad090b-fef0-4248-b37e-54b897157526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer\n",
    "trainer = Trainer(model=model, \n",
    "                  objective='binary', \n",
    "                  metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                  seed=42, \n",
    "                  optimizers=optimizers,\n",
    "                  callbacks=callbacks\n",
    "                 )\n",
    "\n",
    "#             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "trainer.fit( \n",
    "    X_wide=X_wide_train,\n",
    "    X_tab=X_tab_train,\n",
    "    target=y_train,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=1024, # default value is 32; 1024 works for TabMLP\n",
    "#                 val_split=0.2, # no need for this\n",
    ")\n",
    "\n",
    "y_valid_preds = trainer.predict_proba(X_wide=X_wide_valid, X_tab=X_tab_valid, batch_size=1024)[:,1]\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8c11f-36ac-44a6-b20a-4a2f061cbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(path=datapath/'widedeep_tabmlp-202111271032-weights-20epochs', save_state_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e23500-fbd8-4ee7-9dd5-5c857d0e6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_auc = roc_auc_score(y_score=y_valid_preds, y_true=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacfd0ad-0ab5-4c38-8c60-6dc9fb8197f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f5555-872d-41bb-8260-c5b3cdaaba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'overall_valid_auc': valid_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56ce4353-65c8-4117-aa19-5c4b08a04e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 128572... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.08MB of 0.08MB uploaded (0.06MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>overall_valid_auc</td><td></td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>overall_valid_auc</td><td>0.98312</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deeptrainer_20211127_124857</strong>: <a href=\"https://wandb.ai/hushifang/202111_Kaggle_tabular_playground/runs/ey9qb8gw\" target=\"_blank\">https://wandb.ai/hushifang/202111_Kaggle_tabular_playground/runs/ey9qb8gw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211127_124901-ey9qb8gw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5693d1e-0d02-47cf-b05e-3219e144cbb7",
   "metadata": {},
   "source": [
    "# WideDeep Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcec515-3fee-4578-8d28-d0909a5502c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-ef23d42afe3c>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ef23d42afe3c>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    random_state=42, wandb_tracked=True):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def trainer(model, X_wide = X_bins, X_deep=X_gauss, y=y, exmodel_config=exmodel_config, wandb_config=wandb_config, random_state=42, wandb_tracked=True):\n",
    "    \"\"\"\n",
    "    Simple trainer wrapper for widedeep models, with holdout\n",
    "    \"\"\"\n",
    "    # concatenate together wide and deep data\n",
    "    X = X_wide.join(X_deep)\n",
    "    \n",
    "    wide = Wide()\n",
    "    \n",
    "    deeptabular = TabMlp(\n",
    "        mlp_hidden_dims=[64,32],\n",
    "        continuous_cols=X_deep.columns,\n",
    "    # scaling with GaussRankScaler, before doing holdout split\n",
    "    # scaler = GaussRankScaler(X)\n",
    "    # scaler.fit_transform(X)\n",
    "    \n",
    "    # skipping denoising for now\n",
    "    \n",
    "    # holdout split\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)    \n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(params)\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88368160-d477-4b54-8835-b782c8e272df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b759437-c961-4903-8fb2-a7a92b5ed3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required keyword-only arguments: 'categories', 'num_continuous', 'dim', 'depth', and 'heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9f3e64378f09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaint_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 5 required keyword-only arguments: 'categories', 'num_continuous', 'dim', 'depth', and 'heads'"
     ]
    }
   ],
   "source": [
    "saint_model = TabAttention(categories=None, num_continuous=X.shape[1], dim=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0eda44-9aa3-41b0-9679-13f7d3203886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3ab351d-2c44-454d-a8b6-171f470a1685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-5c6737837e8e>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-5c6737837e8e>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    for_transformer=False, # change if using a Transformer-based model\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def cross_validate_pytorch_model(arch:str, X, y, X_test, params:dict={}, start_fold=0, \n",
    "                         exmodel_config=exmodel_config, wandb_config=wandb_config, \n",
    "                         random_state=42, shuffle_kfolds=True, wandb_tracked=True, encode_cats=False):\n",
    "    \"\"\"\n",
    "    Modification of the `cross_validate_model` function used in my stacking notebooks, customized to the dataset and to deep learning approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare for k-fold cross-validation; random-state here is notebook-wide, not per-model\n",
    "    # shuffle on the initial sets, but not subsequently -- performing the same operation twice means a very different dataset\n",
    "    if shuffle_kfolds:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=SEED)\n",
    "    else:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=False)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(params)\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )   \n",
    "    \n",
    "    # initialize lists for out-of-fold preds and ground truth\n",
    "    oof_preds, oof_y = [], []\n",
    "    \n",
    "    # initialize a numpy.ndarray containing the fold-model's preds for test set\n",
    "    test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "#         print(f\"type(train_ids) = {type(train_ids)} and train_ids.shape = {train_ids.shape}\")\n",
    "#         print(f\"type(valid_ids) = {type(valid_ids)} and train_ids.shape = {valid_ids.shape}\")\n",
    "        if fold < start_fold: # skip folds that are already trained\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"FOLD {fold}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "            print(f\"y_train shape is {y_train.shape}, y_valid shape is {y_valid.shape}\")\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                # X_train = pd.DataFrame(X_train, columns=\n",
    "            else:\n",
    "                X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:] # bc need pandas.DataFrames for ce\n",
    "            \n",
    "            # print(f\"X_train shape is {X_train.shape}\")\n",
    "            # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "            # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "            # scaling\n",
    "            # scaler = GaussRankScaler()\n",
    "            # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "            # X_valid = pd.DataFrame(scaler.transform(X_valid), columns=X.columns)\n",
    "            # X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "            \n",
    "            # print(\"Scaling complete\")\n",
    "            # print(f\"X_train shape is {X_train.shape}\")\n",
    "            # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "            # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "            # embedding & library-specific preprocessing\n",
    "            tab_preprocessor = TabPreprocessor(\n",
    "                # scale=False, # because GaussRank scaling already occurred\n",
    "                scale=True\n",
    "                for_transformer=False, # change if using a Transformer-based model\n",
    "                continuous_cols=X.columns,\n",
    "                # continuous_cols=range(X.shape[1]), # since it'll be working on a numpy.ndarray\n",
    "                auto_embed_dim=True, # uses fastai's rule of thumb\n",
    "            )#, embed_cols=embed_cols, )\n",
    "            X_train = tab_preprocessor.fit_transform(X_train)   \n",
    "            X_valid = tab_preprocessor.transform(X_valid)\n",
    "            X_test = tab_preprocessor.transform(X_test)\n",
    "            \n",
    "            print(\"Tab preprocessing complete.\")\n",
    "            print(f\"Type of X_train is {type(X_train)}\")\n",
    "            # print(f\"X_train shape is {X_train.shape}\")\n",
    "            # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "            # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "            # define model\n",
    "            deeptabular = TabMlp(\n",
    "                mlp_hidden_dims=[64,32],\n",
    "                column_idx=tab_preprocessor.column_idx,\n",
    "            #     embed_input=tab_preprocessor.embeddings_input,\n",
    "                # continuous_cols=range(X.shape[1]), # since it'll be working on a numpy.ndarray\n",
    "                continuous_cols=X.columns,\n",
    "            )\n",
    "\n",
    "            n_epochs = 30\n",
    "\n",
    "            model = WideDeep(wide=None, deeptabular=deeptabular)\n",
    "\n",
    "            # pytorch hyperparams\n",
    "            deep_opt = AdamW(model.parameters(), lr=0.1)\n",
    "\n",
    "            # deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_train_tab.shape[0], epochs=n_epochs)\n",
    "\n",
    "            # optimizers = {'deeptabular': deep_opt }\n",
    "            # lr_schedulers = {'deeptabular': deep_sch }\n",
    "\n",
    "\n",
    "            callbacks = [\n",
    "                LRHistory(n_epochs=n_epochs), \n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "            # trainer\n",
    "            trainer = Trainer(model=model, \n",
    "                              objective='binary', \n",
    "                              metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                              seed=random_state, \n",
    "                              optimizers=deep_opt,\n",
    "                              callbacks=callbacks\n",
    "                             )\n",
    "\n",
    "    #             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "            trainer.fit( # this is where problem is beginning\n",
    "                # X_wide=X_train_wide,\n",
    "                X_tab=np.array(X_train),\n",
    "                target=np.array(y_train),\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=1024, # default value is 32\n",
    "    #                 val_split=0.2, # no need for this\n",
    "            )\n",
    "\n",
    "            y_valid_preds = trainer.predict_proba(X_tab=np.array(X_valid), batch_size=1024)[:,1]\n",
    "\n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "\n",
    "\n",
    "            # test set inference\n",
    "            fold_test_preds = trainer.predict_proba(X_tab=np.array(X_test), batch_size=1024)[:,1]\n",
    "            test_preds += fold_test_preds\n",
    "            \n",
    "            print(f\"NaNs in y_valid_preds: {np.isnan(y_valid_preds).any()}\")\n",
    "            print(f\"NaNs in y_valid: {np.isnan(y_valid).any()}\")\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    #         valid_loss = log_loss(y_valid, y_pred)\n",
    "            # give the valid AUC score, for edification\n",
    "            fold_valid_auc = roc_auc_score(y_valid, y_valid_preds)\n",
    "            if wandb_tracked:\n",
    "                wandb.log({f'fold{fold}_valid_roc_auc': fold_valid_auc})\n",
    "            print(f\"Valid AUC for fold {fold} is {fold_valid_auc}\")   \n",
    "        # dump(model, Path(runpath/f\"{arch}_fold{fold}_rs{random_state}_model.joblib\"))\n",
    "\n",
    "    model_valid_auc = roc_auc_score(oof_y, oof_preds)\n",
    "    print(f\"Valid AUC score for {arch} model is {model_valid_auc}\")\n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_valid_auc': model_valid_auc,\n",
    "                   'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()),\n",
    "                   'model_seed': random_state,\n",
    "                  })\n",
    "        wandb.finish()\n",
    "    \n",
    "    # finalize test preds\n",
    "    test_preds /= exmodel_config['kfolds']\n",
    "    \n",
    "    # save OOF preds and test-set preds\n",
    "#     if 'widedeep' in arch:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "#     else:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "    if not (datapath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\").is_file():\n",
    "        dump(oof_y, predpath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\")\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         if 'widedeep' in arch:\n",
    "#         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "#                    'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()), \n",
    "#         #                    'model_params': str(model.get_params()),\n",
    "#         })\n",
    "# #         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "# # #                    'model_params': str(model.get_params()),\n",
    "# #                   })\n",
    "#         wandb.finish()\n",
    "    return oof_preds, test_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b49138e1-077c-41e2-be5d-bd7fcf2b8c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "---------------------------------------------------\n",
      "y_train shape is (480000,), y_valid shape is (120000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:179: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1:   2%|         | 8/469 [00:00<00:06, 72.72it/s, loss=0.852, metrics={'acc': 0.498}] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tab preprocessing complete.\n",
      "Type of X_train is <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|| 469/469 [00:05<00:00, 78.36it/s, loss=0.647, metrics={'acc': 0.6435}]\n",
      "epoch 2: 100%|| 469/469 [00:06<00:00, 75.90it/s, loss=0.64, metrics={'acc': 0.6527}] \n",
      "epoch 3: 100%|| 469/469 [00:06<00:00, 68.60it/s, loss=0.645, metrics={'acc': 0.6499}]\n",
      "epoch 4: 100%|| 469/469 [00:06<00:00, 76.83it/s, loss=0.648, metrics={'acc': 0.6462}]\n",
      "epoch 5: 100%|| 469/469 [00:06<00:00, 76.18it/s, loss=0.648, metrics={'acc': 0.6472}]\n",
      "epoch 6: 100%|| 469/469 [00:06<00:00, 74.80it/s, loss=0.645, metrics={'acc': 0.65}]  \n",
      "epoch 7: 100%|| 469/469 [00:05<00:00, 78.54it/s, loss=0.656, metrics={'acc': 0.634}] \n",
      "epoch 8: 100%|| 469/469 [00:06<00:00, 72.74it/s, loss=0.693, metrics={'acc': 0.5112}]\n",
      "epoch 9: 100%|| 469/469 [00:06<00:00, 74.71it/s, loss=0.691, metrics={'acc': 0.5116}]\n",
      "epoch 10: 100%|| 469/469 [00:06<00:00, 76.26it/s, loss=0.699, metrics={'acc': 0.5246}]\n",
      "epoch 11: 100%|| 469/469 [00:05<00:00, 79.13it/s, loss=0.693, metrics={'acc': 0.503}] \n",
      "epoch 12: 100%|| 469/469 [00:06<00:00, 77.79it/s, loss=0.693, metrics={'acc': 0.5022}]\n",
      "epoch 13: 100%|| 469/469 [00:06<00:00, 77.95it/s, loss=0.693, metrics={'acc': 0.5014}]\n",
      "epoch 14: 100%|| 469/469 [00:05<00:00, 80.07it/s, loss=0.693, metrics={'acc': 0.5026}]\n",
      "epoch 15: 100%|| 469/469 [00:06<00:00, 74.53it/s, loss=0.693, metrics={'acc': 0.5023}]\n",
      "epoch 16: 100%|| 469/469 [00:05<00:00, 79.15it/s, loss=0.693, metrics={'acc': 0.5028}]\n",
      "epoch 17: 100%|| 469/469 [00:05<00:00, 79.72it/s, loss=0.693, metrics={'acc': 0.502}] \n",
      "epoch 18: 100%|| 469/469 [00:06<00:00, 77.65it/s, loss=0.693, metrics={'acc': 0.5016}]\n",
      "epoch 19: 100%|| 469/469 [00:05<00:00, 81.81it/s, loss=0.693, metrics={'acc': 0.5019}]\n",
      "epoch 20: 100%|| 469/469 [00:06<00:00, 76.76it/s, loss=0.693, metrics={'acc': 0.5015}]\n",
      "epoch 21: 100%|| 469/469 [00:06<00:00, 74.97it/s, loss=0.693, metrics={'acc': 0.5017}]\n",
      "epoch 22: 100%|| 469/469 [00:06<00:00, 78.01it/s, loss=0.693, metrics={'acc': 0.5032}]\n",
      "epoch 23: 100%|| 469/469 [00:06<00:00, 74.61it/s, loss=0.693, metrics={'acc': 0.502}] \n",
      "epoch 24: 100%|| 469/469 [00:06<00:00, 73.76it/s, loss=0.693, metrics={'acc': 0.5027}]\n",
      "epoch 25: 100%|| 469/469 [00:06<00:00, 71.35it/s, loss=0.693, metrics={'acc': 0.5013}]\n",
      "epoch 26: 100%|| 469/469 [00:06<00:00, 72.63it/s, loss=0.693, metrics={'acc': 0.5011}]\n",
      "epoch 27: 100%|| 469/469 [00:06<00:00, 76.28it/s, loss=0.693, metrics={'acc': 0.5009}]\n",
      "epoch 28: 100%|| 469/469 [00:06<00:00, 77.03it/s, loss=0.693, metrics={'acc': 0.5015}]\n",
      "epoch 29: 100%|| 469/469 [00:05<00:00, 78.65it/s, loss=0.694, metrics={'acc': 0.5018}]\n",
      "epoch 30: 100%|| 469/469 [00:05<00:00, 85.00it/s, loss=0.693, metrics={'acc': 0.502}] \n",
      "predict: 100%|| 118/118 [00:00<00:00, 130.64it/s]\n",
      "predict: 100%|| 528/528 [00:01<00:00, 271.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in y_valid_preds: True\n",
      "NaNs in y_valid: False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-75824e55d591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moof_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_pytorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'widedeep-TabMLP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_tracked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-b9f964242d10>\u001b[0m in \u001b[0;36mcross_validate_pytorch_model\u001b[0;34m(arch, X, y, X_test, params, start_fold, exmodel_config, wandb_config, random_state, shuffle_kfolds, wandb_tracked, encode_cats)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m#         valid_loss = log_loss(y_valid, y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# give the valid AUC score, for edification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mfold_valid_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwandb_tracked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf'fold{fold}_valid_roc_auc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfold_valid_auc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     if y_type == \"multiclass\" or (\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "oof_preds, test_preds = cross_validate_pytorch_model('widedeep-TabMLP', X, y, X_test, wandb_tracked=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
