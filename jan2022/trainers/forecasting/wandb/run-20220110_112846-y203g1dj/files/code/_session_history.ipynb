{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36652a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "# if '/sf/' in pwd:\n",
    "#     COLAB, SAGE = False, False\n",
    "# elif 'google.colab' in str(get_ipython()):\n",
    "#     COLAB, SAGE = True, False # do colab-specific installs later\n",
    "# else:\n",
    "#     COLAB, SAGE = False, True\n",
    "    \n",
    "CONTEXT = 'local' # or 'colab', 'sage', 'kaggle'\n",
    "USE_GPU = True \n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70dead53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import requests # for telegram notifications\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b95027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series\n",
    "import tsfresh\n",
    "\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "# from darts.models import ExponentialSmoothing, AutoARIMA, ARIMA, Prophet, RandomForest, RegressionEnsembleModel, RegressionModel, TFTModel, TCNModel, TransformerModel, NBEATSModel\n",
    "from darts.metrics import smape\n",
    "\n",
    "import holidays\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"nb_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151b8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import (\n",
    "    NaiveSeasonal,\n",
    "    NaiveDrift,\n",
    "#     Prophet, # on 20220108 postponing this due to df vs ts object wrinkles\n",
    "    ExponentialSmoothing,\n",
    "    ARIMA,\n",
    "    AutoARIMA,\n",
    "    RegressionEnsembleModel,\n",
    "    RegressionModel,\n",
    "    Theta,\n",
    "    FFT\n",
    ")\n",
    "\n",
    "from prophet import Prophet # for now, just imporing the native API\n",
    "from neuralprophet import NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2fd77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "import torch\n",
    "# from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "\n",
    "# widedeep\n",
    "# from pytorch_widedeep import Trainer\n",
    "# from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "# from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT#, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "# from pytorch_widedeep.metrics import Accuracy\n",
    "# from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884d6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTEXT == 'colab':\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    # datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/dec2021/')\n",
    "    root = Path('') # TODO\n",
    "\n",
    "elif CONTEXT == 'sage':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "elif CONTEXT == 'kaggle':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "else: # if on local machine\n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'studies'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath, studypath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1d51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything but the models\n",
    "def seed_everything(seed, pytorch=True, reproducible=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if pytorch:\n",
    "        torch.manual_seed(seed) # set torch CPU seed\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed) # set torch GPU(s) seed(s)\n",
    "        if reproducible and torch.backends.cudnn.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7249447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Function to reduce memory usage by downcasting datatypes in a Pandas DataFrame when possible.\n",
    "    \n",
    "    h/t to Bryan Arnold (https://www.kaggle.com/puremath86/label-correction-experiments-tps-nov-21)\n",
    "    \"\"\"\n",
    "    \n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba29e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_api_token = 'your_api_token' # for Galileo (jupyter_watcher_bot) on Telegram\n",
    "tg_chat_id = 'your_chat_id'\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_tg_message(text='Cell execution completed.'):  \n",
    "    \"\"\"\n",
    "    h/t Ivan Dembicki Jr. for the base version \n",
    "    (https://medium.com/@ivan.dembicki.jr/notifications-in-jupyter-notebook-with-telegram-f2e892c55173)\n",
    "    \"\"\"\n",
    "    requests.post('https://api.telegram.org/' +  'bot{}/sendMessage'.format(tg_api_token),\n",
    "                  params=dict(chat_id=tg_chat_id, text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9c4e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(y_true, y_pred):\n",
    "    '''\n",
    "    h/t Jean-FranÃ§ois Puget (@CPMP) -- see https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\n",
    "    '''\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf89cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'train.csv'),\n",
    "#     'target_source': str(datapath/'train.csv'),\n",
    "    'test_source': str(datapath/'test.csv'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "df_train = pd.read_csv(datapath/'train.csv')\n",
    "df_test = pd.read_csv(datapath/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15378f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "\n",
    "countries = ['Sweden', 'Finland', 'Norway']\n",
    "stores = ['KaggleMart', 'KaggleRama']\n",
    "products = ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e3c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if date is a holiday    \n",
    "def isHoliday(country, date):\n",
    "    # h/t @sumeetbohra for following (https://www.kaggle.com/sumeetbohra/eda-dataviz-fe-lightgbm)\n",
    "    country_holidays = holidays.CountryHoliday(country, years = date.year)\n",
    "    return int(date in country_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "528d4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['isHoliday'] = df_train.apply(lambda x: isHoliday(x['country'], x['date'].date()), axis = 1)\n",
    "# df_test['isHoliday'] = df_test.apply(lambda x: isHoliday(x['country'], x['date'].date()), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e08b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norway_2018 = holidays.CountryHoliday('Norway', years=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7343497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norway_2018.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d7ad779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in native Prophet API, holidays must be a DataFrame with \"ds\" and \"holiday\" columns.\n",
    "# holidays_train = pd.DataFrame({\n",
    "#     'ds': df_train['date'],\n",
    "#     'holiday': df_train['isHoliday']\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dcd76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id       date  country       store         product  num_sold\n",
      "26278   26278 2018-12-30   Sweden  KaggleRama      Kaggle Hat      2138\n",
      "26279   26279 2018-12-30   Sweden  KaggleRama  Kaggle Sticker       587\n",
      "26280   26280 2018-12-31  Finland  KaggleMart      Kaggle Mug       469\n",
      "26281   26281 2018-12-31  Finland  KaggleMart      Kaggle Hat       822\n",
      "26282   26282 2018-12-31  Finland  KaggleMart  Kaggle Sticker       238\n",
      "26283   26283 2018-12-31  Finland  KaggleRama      Kaggle Mug       831\n",
      "26284   26284 2018-12-31  Finland  KaggleRama      Kaggle Hat      1231\n",
      "26285   26285 2018-12-31  Finland  KaggleRama  Kaggle Sticker       360\n",
      "26286   26286 2018-12-31   Norway  KaggleMart      Kaggle Mug       728\n",
      "26287   26287 2018-12-31   Norway  KaggleMart      Kaggle Hat      1124\n",
      "26288   26288 2018-12-31   Norway  KaggleMart  Kaggle Sticker       351\n",
      "26289   26289 2018-12-31   Norway  KaggleRama      Kaggle Mug      1383\n",
      "26290   26290 2018-12-31   Norway  KaggleRama      Kaggle Hat      2128\n",
      "26291   26291 2018-12-31   Norway  KaggleRama  Kaggle Sticker       561\n",
      "26292   26292 2018-12-31   Sweden  KaggleMart      Kaggle Mug       570\n",
      "26293   26293 2018-12-31   Sweden  KaggleMart      Kaggle Hat       823\n",
      "26294   26294 2018-12-31   Sweden  KaggleMart  Kaggle Sticker       250\n",
      "26295   26295 2018-12-31   Sweden  KaggleRama      Kaggle Mug      1004\n",
      "26296   26296 2018-12-31   Sweden  KaggleRama      Kaggle Hat      1441\n",
      "26297   26297 2018-12-31   Sweden  KaggleRama  Kaggle Sticker       388"
     ]
    }
   ],
   "source": [
    "df_train.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "019c4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'general_random_state': SEED,\n",
    "    'model_goal': 'forecasting', # or 'residual', in boosted hybrids\n",
    "#     'cross_validation_type': 'holdout',\n",
    "#     'validation_set_size': 0.25,\n",
    "}\n",
    "\n",
    "# following are only applicable for residual models in time series context\n",
    "# folds = 5\n",
    "# training_params['cross_val_strategy'] = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3dcee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will add kwargs later\n",
    "model_params = {\n",
    "    'architecture': 'NeuralProphet',\n",
    "    'library': 'neuralprophet',\n",
    "    'hyperparams': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3247f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['baseline', 'forecasting', 'neuralprophet'],\n",
    "    'notes': \"Baseline run for Neural Prophet via native API as forecaster, following @gunesevitan's notebook.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fafd013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [\n",
    "    ('2015-01-01', '2018-01-01'),\n",
    "    ('2018-01-01', '2019-01-01'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdf79ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "#     'holidays':holidays_train, # will add this in-function\n",
    "    'n_changepoints':10,\n",
    "    'changepoint_range':0.4,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_prior_scale':25,\n",
    "    'holidays_prior_scale':100,\n",
    "    'changepoint_prior_scale':0.01,\n",
    "    'interval_width':0.5,\n",
    "    'uncertainty_samples':False\n",
    "}\n",
    "\n",
    "neuralprophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "    'n_changepoints':10,\n",
    "    'changepoints_range':0.4,\n",
    "    'trend_reg':1,\n",
    "    'trend_reg_threshold':False,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_reg':1,\n",
    "    'n_forecasts':365,\n",
    "    'normalize':'off'\n",
    "}\n",
    "\n",
    "model_params['hyperparams'] = str(neuralprophet_kwargs)\n",
    "model_params['holiday_source'] = 'Prophet builtin for each country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8456a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline -- alter as needed later\n",
    "exmodel_config = {\n",
    "    **dataset_params,\n",
    "    **training_params,\n",
    "    **model_params \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76c04af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id       date  country       store         product  num_sold\n",
      "0           0 2015-01-01  Finland  KaggleMart      Kaggle Mug       329\n",
      "1           1 2015-01-01  Finland  KaggleMart      Kaggle Hat       520\n",
      "2           2 2015-01-01  Finland  KaggleMart  Kaggle Sticker       146\n",
      "3           3 2015-01-01  Finland  KaggleRama      Kaggle Mug       572\n",
      "4           4 2015-01-01  Finland  KaggleRama      Kaggle Hat       911\n",
      "...       ...        ...      ...         ...             ...       ...\n",
      "26293   26293 2018-12-31   Sweden  KaggleMart      Kaggle Hat       823\n",
      "26294   26294 2018-12-31   Sweden  KaggleMart  Kaggle Sticker       250\n",
      "26295   26295 2018-12-31   Sweden  KaggleRama      Kaggle Mug      1004\n",
      "26296   26296 2018-12-31   Sweden  KaggleRama      Kaggle Hat      1441\n",
      "26297   26297 2018-12-31   Sweden  KaggleRama  Kaggle Sticker       388\n",
      "\n",
      "[26298 rows x 6 columns]"
     ]
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "668bbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralProphet(**neuralprophet_kwargs)\n",
    "model.add_country_holidays(country_name=country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fe0d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralProphet(**neuralprophet_kwargs)\n",
    "model.add_country_holidays(country_name='finland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fb85ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralProphet(**neuralprophet_kwargs)\n",
    "model = model.add_country_holidays(country_name='finland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08b5ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralProphet(**neuralprophet_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "554f619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.add_country_holidays(country_name='finland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d179de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.add_country_holidays(country_name='Finland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "322ccf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<neuralprophet.forecaster.NeuralProphet at 0x7f008beee430>"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cd298c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0720f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df_train, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "455557f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.rename(columns={'date': 'ds', 'num_sold': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c68a7e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id         ds  country       store         product     y\n",
      "0           0 2015-01-01  Finland  KaggleMart      Kaggle Mug   329\n",
      "1           1 2015-01-01  Finland  KaggleMart      Kaggle Hat   520\n",
      "2           2 2015-01-01  Finland  KaggleMart  Kaggle Sticker   146\n",
      "3           3 2015-01-01  Finland  KaggleRama      Kaggle Mug   572\n",
      "4           4 2015-01-01  Finland  KaggleRama      Kaggle Hat   911\n",
      "...       ...        ...      ...         ...             ...   ...\n",
      "26293   26293 2018-12-31   Sweden  KaggleMart      Kaggle Hat   823\n",
      "26294   26294 2018-12-31   Sweden  KaggleMart  Kaggle Sticker   250\n",
      "26295   26295 2018-12-31   Sweden  KaggleRama      Kaggle Mug  1004\n",
      "26296   26296 2018-12-31   Sweden  KaggleRama      Kaggle Hat  1441\n",
      "26297   26297 2018-12-31   Sweden  KaggleRama  Kaggle Sticker   388\n",
      "\n",
      "[26298 rows x 6 columns]"
     ]
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f7882cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99ec0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "country, store, product = 'Finland', 'KaggleRama', 'Kaggle Mug'\n",
    "train_idx = (df_train['country'] == country) &\\\n",
    "            (df_train['store'] == store) &\\\n",
    "            (df_train['product'] == product)\n",
    "\n",
    "train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38f0fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ds     y\n",
      "0    2015-01-01   572\n",
      "1    2015-01-02   544\n",
      "2    2015-01-03   579\n",
      "3    2015-01-04   582\n",
      "4    2015-01-05   423\n",
      "...         ...   ...\n",
      "1456 2018-12-27   652\n",
      "1457 2018-12-28   895\n",
      "1458 2018-12-29  1398\n",
      "1459 2018-12-30  1241\n",
      "1460 2018-12-31   831\n",
      "\n",
      "[1461 rows x 2 columns]"
     ]
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fa9a3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SmoothL1Loss         MAE        RMSE   RegLoss\n",
      "0      351.387625  351.887628  363.100771  0.000000\n",
      "1      343.869949  344.369951  355.944993  0.000000\n",
      "2      334.414336  334.914334  346.301075  0.000000\n",
      "3      321.596716  322.096714  333.332584  0.000000\n",
      "4      304.170013  304.670015  316.462346  0.000000\n",
      "..            ...         ...         ...       ...\n",
      "159     25.333783   25.825522   50.915792  0.533805\n",
      "160     25.329484   25.820796   50.420712  0.535413\n",
      "161     25.328154   25.819373   51.387434  0.536585\n",
      "162     25.325885   25.817041   49.360127  0.537391\n",
      "163     25.325257   25.816413   50.275099  0.537793\n",
      "\n",
      "[164 rows x 4 columns]"
     ]
    }
   ],
   "source": [
    "model.fit(train, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "184ef45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country, store, product = 'Finland', 'KaggleRama', 'Kaggle Mug'\n",
    "store, product = 'KaggleRama', 'Kaggle Mug'\n",
    "for country in ['Sweden', 'Finland']:\n",
    "    \n",
    "    train_idx = (df_train['country'] == country) &\\\n",
    "                (df_train['store'] == store) &\\\n",
    "                (df_train['product'] == product)\n",
    "\n",
    "    train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "    train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "    \n",
    "    model = NeuralProphet(**neuralprophet_kwargs)\n",
    "    model = model.add_country_holidays(country_name=country)\n",
    "    model.fit(train, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f04c9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model_kwargs=neuralprophet_kwargs, countries=countries, stores=stores, products=products, folds=folds, \n",
    "            df_train=df_train, df_test=df_test, wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "\n",
    "#                     model = Prophet(**prophet_kwargs)\n",
    "                    model = NeuralProphet(**model_kwargs)\n",
    "\n",
    "                    model = model.add_country_holidays(country_name=country) # uses FacebookProphet or NeuralProphet API to add holidays\n",
    "                    print(train.columns)\n",
    "                    model.fit(train, freq='D') # neuralprophet\n",
    "                    # prophet\n",
    "#                     train_predictions = model.predict(train[['ds']])['yhat']\n",
    "#                     val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    # neuralprophet\n",
    "                    train_predictions = model.predict(train)['yhat1']\n",
    "                    val_predictions = model.predict(val)['yhat1']\n",
    "                    df_train.loc[train_idx, 'prophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test_predictions = model.predict(test[['ds']])['yhat']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'neuralprophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train, df_test, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1d0acd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/202201_Kaggle_tabular_playground/runs/y203g1dj\" target=\"_blank\">nb_20220110_112051</a></strong> to <a href=\"https://wandb.ai/hushifang/202201_Kaggle_tabular_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961171cab76a46d2ba67076f8bcd017d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7271151658104c75b4e24dd400b24868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b286ce573774b359a9ff00866fe7d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_train_prophet, df_test_prophet = exdarts_trainer(model)\n",
    "# df_train_prophet, df_test_prophet, train_smape_prophet, val_smape_prophet = prophet_trainer(wandb_tracked=True)\n",
    "df_train_preds, df_test_preds, train_smape, val_smape = trainer(model_kwargs=neuralprophet_kwargs, wandb_tracked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af8d8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model_kwargs=neuralprophet_kwargs, countries=countries, stores=stores, products=products, folds=folds, \n",
    "            df_train=df_train, df_test=df_test, wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "\n",
    "#                     model = Prophet(**prophet_kwargs)\n",
    "                    model = NeuralProphet(**model_kwargs)\n",
    "\n",
    "                    model = model.add_country_holidays(country_name=country) # uses FacebookProphet or NeuralProphet API to add holidays\n",
    "                    print(train.columns)\n",
    "                    model.fit(train, freq='D') # neuralprophet\n",
    "                    # prophet\n",
    "#                     train_predictions = model.predict(train[['ds']])['yhat']\n",
    "#                     val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    # neuralprophet\n",
    "                    train_predictions = model.predict(train)['yhat1']\n",
    "                    val_predictions = model.predict(val)['yhat1']\n",
    "                    df_train.loc[train_idx, 'prophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test_predictions = model.predict(test)['yhat']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'neuralprophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train, df_test, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cea1aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_prophet, df_test_prophet = exdarts_trainer(model)\n",
    "# df_train_prophet, df_test_prophet, train_smape_prophet, val_smape_prophet = prophet_trainer(wandb_tracked=True)\n",
    "df_train_preds, df_test_preds, train_smape, val_smape = trainer(model_kwargs=neuralprophet_kwargs, wandb_tracked=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
