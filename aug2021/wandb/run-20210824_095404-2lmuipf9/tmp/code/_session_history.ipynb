{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d03412",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse = 7.8619006924521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169164cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold#, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "# import timm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "# feature engineering tools\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92c59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6230f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torchinfo import summary\n",
    "# # from fastai.callback.wandb import *\n",
    "# from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598e28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.vision.all import *\n",
    "from fastai.tabular.all import *\n",
    "# from fastai.callback.wandb import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52130784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# # from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b722f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env \"WANDB_NOTEBOOK_NAME\" \"202108090846_XGBoostRegressor_tree_sweep\"\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '20210823_XGBClassifier_feature_selection.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba40742",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_run = {\n",
    "    'name': os.environ['WANDB_NOTEBOOK_NAME'][:-6], # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['XGBoost', 'experimental', 'feature_selection'],\n",
    "    'notes': \"Going to try doing some feature selection now using techniques from Abishek Thakur's book.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cefa7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92bf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(42, reproducible=True) # fastai only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ea50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/202108_august/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1760e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')\n",
    "df = pd.read_feather(path='dataset_df.feather')\n",
    "df.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d07abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum().any() # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a6dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ffdb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a68a0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f9208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in df.columns if x != 'loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37624361",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29227267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in df.columns:\n",
    "#     print(f + '\\n-----------')\n",
    "#     print(f\"{f} max is {max(df[f])}\")\n",
    "#     print(f\"{f} min is {min(df[f])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a047191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in df.columns[:5]:\n",
    "#     sns.scatterplot(data=df, x=f, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "873d8a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000"
     ]
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "104941a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          f0   f1        f2        f3          f4        f5        f6  \\\n",
      "id                                                                      \n",
      "0  -0.002350   59  0.766739 -1.350460     42.2727  16.68570   30.3599   \n",
      "1   0.784462  145 -0.463845 -0.530421  27324.9000   3.47545  160.4980   \n",
      "2   0.317816   19 -0.432571 -0.382644   1383.2600  19.71290   31.1026   \n",
      "3   0.210753   17 -0.616454  0.946362   -119.2530   4.08235  185.2570   \n",
      "4   0.439671   20  0.968126 -0.092546     74.3020  12.30650   72.1860   \n",
      "\n",
      "          f7         f8       f9  ...        f91        f92      f93  \\\n",
      "id                                ...                                  \n",
      "0   1.267300   0.392007  1.09101  ...  -42.43990  26.854000  1.45751   \n",
      "1   0.828007   3.735860  1.28138  ... -184.13200   7.901370  1.70644   \n",
      "2  -0.515354  34.430800  1.24210  ...    7.43721  37.218100  3.25339   \n",
      "3   1.383310 -47.521400  1.09130  ...    9.66778   0.626942  1.49425   \n",
      "4  -0.233964  24.399100  1.10151  ...  290.65700  15.604300  1.73557   \n",
      "\n",
      "         f94        f95       f96       f97      f98       f99  loss  \n",
      "id                                                                    \n",
      "0   0.696161   0.941764  1.828470  0.924090  2.29658  10.48980    15  \n",
      "1  -0.494699  -2.058300  0.819184  0.439152  2.36470   1.14383     3  \n",
      "2   0.337934   0.615037  2.216760  0.745268  1.69679  12.30550     6  \n",
      "3   0.517513 -10.222100  2.627310  0.617270  1.45645  10.02880     2  \n",
      "4  -0.476668   1.390190  2.195740  0.826987  1.78485   7.07197     1  \n",
      "\n",
      "[5 rows x 101 columns]"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f8c6e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 43)"
     ]
    }
   ],
   "source": [
    "len(y), len(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c12f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train['f1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76520526",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_defaults = {\n",
    "    \"library\": \"xgboost\",\n",
    "    \"tree_method\": \"auto\", # set to 'gpu_hist' to try GPU if available\n",
    "    \"booster\": 'gbtree', # dart may be marginally better, but will opt for this quicker approach as a default\n",
    "    \"n_estimators\": 100, # a very low number -- optimal is probably 300ish -- but this will be quicker\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"test_size\": 0.2,\n",
    "    \"scaler\": MaxAbsScaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42e73469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "# # from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9af88efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env \"WANDB_NOTEBOOK_NAME\" \"202108090846_XGBoostRegressor_tree_sweep\"\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '20210824_XGBClassifier_feature_selection.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4c7d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_defaults = {\n",
    "    \"library\": \"xgboost\",\n",
    "    \"tree_method\": \"auto\", # set to 'gpu_hist' to try GPU if available\n",
    "    \"booster\": 'gbtree', # dart may be marginally better, but will opt for this quicker approach as a default\n",
    "    \"n_estimators\": 100, # a very low number -- optimal is probably 300ish -- but this will be quicker\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"test_size\": 0.2,\n",
    "    \"scaler\": MaxAbsScaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "862c0be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(wandb_config):#, scaler): # passed in via config dict for now\n",
    "    wandb.init(\n",
    "        project=\"202108_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=wandb_config)\n",
    "    \n",
    "    config = wandb.config\n",
    "        \n",
    "    # applying hold-out before scaling\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=config['test_size'], random_state=42)\n",
    "    \n",
    "    wandb.log({'scaler': MaxAbsScaler})\n",
    "    s = MaxAbsScaler()\n",
    "    X_train = s.fit_transform(X_train)\n",
    "    X_valid = s.fit_transform(X_valid)\n",
    "    \n",
    "#     # instantiating the scaler and fitting it\n",
    "#     if scaler:\n",
    "#         s = scaler()\n",
    "#         X_train = s.fit_transform(X_train)\n",
    "#         X_valid = s.fit_transform(X_valid)\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        tree_method=config.tree_method,\n",
    "        booster=config.booster,\n",
    "        n_estimators=config.n_estimators, \n",
    "        max_depth=config.max_depth,\n",
    "        learning_rate=config.learning_rate, \n",
    "        test_size=config.test_size,\n",
    "        subsample=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1, \n",
    "        verbosity=1, \n",
    "    )\n",
    "    wandb.log({'params': model.get_params()})\n",
    "    model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "    y_preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_preds)\n",
    "    rmse = math.sqrt(abs(mse))\n",
    "    wandb.log({'mse':mse, 'rmse':rmse})\n",
    "    print(f\"MSE is {mse}\\nRMSE is {rmse}\")   \n",
    "    wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28e81cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=config['test_size'], random_state=config['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9aad1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=config_defaults['test_size'], random_state=config_defaults['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91a0b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_defaults = {\n",
    "    \"library\": \"xgboost\",\n",
    "    \"tree_method\": \"auto\", # set to 'gpu_hist' to try GPU if available\n",
    "    \"booster\": 'gbtree', # dart may be marginally better, but will opt for this quicker approach as a default\n",
    "    \"n_estimators\": 100, # a very low number -- optimal is probably 300ish -- but this will be quicker\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"test_size\": 0.2,\n",
    "    \"scaler\": MaxAbsScaler,\n",
    "    'random_state': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "239a4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=config_defaults['test_size'], random_state=config_defaults['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5d16421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56fb9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, f_classif, f_regression, mutual_info_regression, SelectKBest, SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65ab5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"202108_Kaggle_tabular_playground\",\n",
    "    save_code=True,\n",
    "    tags=config_run['tags'],\n",
    "    name=config_run['name'],\n",
    "    notes=config_run['notes'],\n",
    "    config=model_config)\n",
    "\n",
    "\n",
    "# applying hold-out before scaling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=model_config['test_size'], random_state=42)\n",
    "\n",
    "#     s = model_config['scaler']\n",
    "#     wandb.log({'scaler':s)\n",
    "s = MaxAbsScaler()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_valid_s = s.fit_transform(X_valid)\n",
    "#     X_train_s = s.fit_transform(x=X_train, y=y_train)\n",
    "#     X_valid_s = s.fit_transform(x=X_valid, y=y_valid)\n",
    "\n",
    "# removing features with post-normalization variance <0.01\n",
    "#     var_thresh = VarianceThreshold(threshold=0.017)\n",
    "#     X_train_v = var_thresh.fit_transform(X_train_s)\n",
    "#     X_valid_v = X_valid_s[:, var_thresh.get_support()]\n",
    "# X_valid_v = var_thresh.fit_transform(X_valid_s)\n",
    "\n",
    "#     # instantiating the scaler and fitting it\n",
    "#     if scaler:\n",
    "#         s = scaler()\n",
    "#         X_train = s.fit_transform(X_train)\n",
    "#         X_valid = s.fit_transform(X_valid)\n",
    "\n",
    "selector = SelectKBest(score_func='f_regression', k=90)\n",
    "X_train_fs = selector.fit_tarnsform(X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49da4256",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config_defaults \n",
    "\n",
    "wandb.init(\n",
    "    project=\"202108_Kaggle_tabular_playground\",\n",
    "    save_code=True,\n",
    "    tags=config_run['tags'],\n",
    "    name=config_run['name'],\n",
    "    notes=config_run['notes'],\n",
    "    config=model_config)\n",
    "\n",
    "\n",
    "# applying hold-out before scaling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=model_config['test_size'], random_state=42)\n",
    "\n",
    "#     s = model_config['scaler']\n",
    "#     wandb.log({'scaler':s)\n",
    "s = MaxAbsScaler()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_valid_s = s.fit_transform(X_valid)\n",
    "#     X_train_s = s.fit_transform(x=X_train, y=y_train)\n",
    "#     X_valid_s = s.fit_transform(x=X_valid, y=y_valid)\n",
    "\n",
    "# removing features with post-normalization variance <0.01\n",
    "#     var_thresh = VarianceThreshold(threshold=0.017)\n",
    "#     X_train_v = var_thresh.fit_transform(X_train_s)\n",
    "#     X_valid_v = X_valid_s[:, var_thresh.get_support()]\n",
    "# X_valid_v = var_thresh.fit_transform(X_valid_s)\n",
    "\n",
    "#     # instantiating the scaler and fitting it\n",
    "#     if scaler:\n",
    "#         s = scaler()\n",
    "#         X_train = s.fit_transform(X_train)\n",
    "#         X_valid = s.fit_transform(X_valid)\n",
    "\n",
    "selector = SelectKBest(score_func='f_regression', k=90)\n",
    "X_train_fs = selector.fit_tarnsform(X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5019b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config_defaults \n",
    "\n",
    "wandb.init(\n",
    "    project=\"202108_Kaggle_tabular_playground\",\n",
    "    save_code=True,\n",
    "    tags=config_run['tags'],\n",
    "    name=config_run['name'],\n",
    "    notes=config_run['notes'],\n",
    "    config=model_config)\n",
    "\n",
    "\n",
    "# applying hold-out before scaling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=model_config['test_size'], random_state=42)\n",
    "\n",
    "#     s = model_config['scaler']\n",
    "#     wandb.log({'scaler':s)\n",
    "s = MaxAbsScaler()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_valid_s = s.fit_transform(X_valid)\n",
    "#     X_train_s = s.fit_transform(x=X_train, y=y_train)\n",
    "#     X_valid_s = s.fit_transform(x=X_valid, y=y_valid)\n",
    "\n",
    "# removing features with post-normalization variance <0.01\n",
    "#     var_thresh = VarianceThreshold(threshold=0.017)\n",
    "#     X_train_v = var_thresh.fit_transform(X_train_s)\n",
    "#     X_valid_v = X_valid_s[:, var_thresh.get_support()]\n",
    "# X_valid_v = var_thresh.fit_transform(X_valid_s)\n",
    "\n",
    "#     # instantiating the scaler and fitting it\n",
    "#     if scaler:\n",
    "#         s = scaler()\n",
    "#         X_train = s.fit_transform(X_train)\n",
    "#         X_valid = s.fit_transform(X_valid)\n",
    "\n",
    "selector = SelectKBest(score_func='f_regression', k=90)\n",
    "X_train_fs = selector.fit_transform(X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d3b9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config_defaults \n",
    "\n",
    "wandb.init(\n",
    "    project=\"202108_Kaggle_tabular_playground\",\n",
    "    save_code=True,\n",
    "    tags=config_run['tags'],\n",
    "    name=config_run['name'],\n",
    "    notes=config_run['notes'],\n",
    "    config=model_config)\n",
    "\n",
    "\n",
    "# applying hold-out before scaling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=model_config['test_size'], random_state=42)\n",
    "\n",
    "#     s = model_config['scaler']\n",
    "#     wandb.log({'scaler':s)\n",
    "s = MaxAbsScaler()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_valid_s = s.fit_transform(X_valid)\n",
    "#     X_train_s = s.fit_transform(x=X_train, y=y_train)\n",
    "#     X_valid_s = s.fit_transform(x=X_valid, y=y_valid)\n",
    "\n",
    "# removing features with post-normalization variance <0.01\n",
    "#     var_thresh = VarianceThreshold(threshold=0.017)\n",
    "#     X_train_v = var_thresh.fit_transform(X_train_s)\n",
    "#     X_valid_v = X_valid_s[:, var_thresh.get_support()]\n",
    "# X_valid_v = var_thresh.fit_transform(X_valid_s)\n",
    "\n",
    "#     # instantiating the scaler and fitting it\n",
    "#     if scaler:\n",
    "#         s = scaler()\n",
    "#         X_train = s.fit_transform(X_train)\n",
    "#         X_valid = s.fit_transform(X_valid)\n",
    "\n",
    "selector = SelectKBest(score_func='f_regression', k=90)\n",
    "X_train_fs = selector.fit_transform(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bffa7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config_defaults \n",
    "\n",
    "wandb.init(\n",
    "    project=\"202108_Kaggle_tabular_playground\",\n",
    "    save_code=True,\n",
    "    tags=config_run['tags'],\n",
    "    name=config_run['name'],\n",
    "    notes=config_run['notes'],\n",
    "    config=model_config)\n",
    "\n",
    "\n",
    "# applying hold-out before scaling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=model_config['test_size'], random_state=42)\n",
    "\n",
    "#     s = model_config['scaler']\n",
    "#     wandb.log({'scaler':s)\n",
    "s = MaxAbsScaler()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_valid_s = s.fit_transform(X_valid)\n",
    "#     X_train_s = s.fit_transform(x=X_train, y=y_train)\n",
    "#     X_valid_s = s.fit_transform(x=X_valid, y=y_valid)\n",
    "\n",
    "# removing features with post-normalization variance <0.01\n",
    "#     var_thresh = VarianceThreshold(threshold=0.017)\n",
    "#     X_train_v = var_thresh.fit_transform(X_train_s)\n",
    "#     X_valid_v = X_valid_s[:, var_thresh.get_support()]\n",
    "# X_valid_v = var_thresh.fit_transform(X_valid_s)\n",
    "\n",
    "#     # instantiating the scaler and fitting it\n",
    "#     if scaler:\n",
    "#         s = scaler()\n",
    "#         X_train = s.fit_transform(X_train)\n",
    "#         X_valid = s.fit_transform(X_valid)\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=90)\n",
    "X_train_fs = selector.fit_transform(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53e9ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b786245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_fs = X_valid_s[:, selector.get_support()]\n",
    "\n",
    "model = XGBRegressor(\n",
    "    tree_method=config_defaults.tree_method,\n",
    "    booster=config_defaults.booster,\n",
    "    n_estimators=config_defaults.n_estimators, \n",
    "    max_depth=config_defaults.max_depth,\n",
    "    learning_rate=config_defaults.learning_rate, \n",
    "    test_size=config_defaults.test_size,\n",
    "    subsample=1,\n",
    "    random_state=config_defaults['random_state'],\n",
    "    n_jobs=-1, \n",
    "    verbosity=1, \n",
    ")\n",
    "wandb.log({'params': model.get_params()})\n",
    "model.fit(X_train_fs, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "y_preds = model.predict(X_valid_fs)\n",
    "mse = mean_squared_error(y_valid, y_preds)\n",
    "rmse = math.sqrt(abs(mse))\n",
    "wandb.log({'mse':mse, 'rmse':rmse})\n",
    "print(f\"MSE is {mse}\\nRMSE is {rmse}\")   \n",
    "wandb.finish()\n",
    "#     if rmse < baseline_rmse:\n",
    "#         print(\"RMSE is improved\")\n",
    "#     else:\n",
    "#         print(\"RMSE is not improved\")    \n",
    "    \n",
    "# selector = SelectKBest(score_func='f_regression', k=90)\n",
    "# X_train_fs = selector.fit_tarnsform(X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8ce9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_fs = X_valid_s[:, selector.get_support()]\n",
    "\n",
    "model = XGBRegressor(\n",
    "    tree_method=config_defaults['tree_method'],\n",
    "    booster=config_defaults['booster'],\n",
    "    n_estimators=config_defaults['n_estimators'], \n",
    "    max_depth=config_defaults['max_depth'],\n",
    "    learning_rate=config_defaults['learning_rate'], \n",
    "    test_size=config_defaults['test_size'],\n",
    "    subsample=1,\n",
    "    random_state=config_defaults['random_state'],\n",
    "    n_jobs=-1, \n",
    "    verbosity=1, \n",
    ")\n",
    "wandb.log({'params': model.get_params()})\n",
    "model.fit(X_train_fs, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "y_preds = model.predict(X_valid_fs)\n",
    "mse = mean_squared_error(y_valid, y_preds)\n",
    "rmse = math.sqrt(abs(mse))\n",
    "wandb.log({'mse':mse, 'rmse':rmse})\n",
    "print(f\"MSE is {mse}\\nRMSE is {rmse}\")   \n",
    "wandb.finish()\n",
    "#     if rmse < baseline_rmse:\n",
    "#         print(\"RMSE is improved\")\n",
    "#     else:\n",
    "#         print(\"RMSE is not improved\")    \n",
    "    \n",
    "# selector = SelectKBest(score_func='f_regression', k=90)\n",
    "# X_train_fs = selector.fit_tarnsform(X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cea71579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "rmse < baseline_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb39820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fs(k, model_config=config_defaults):\n",
    "    wandb.init(\n",
    "        project=\"202108_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=model_config)\n",
    "\n",
    "\n",
    "    # applying hold-out before scaling\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=model_config['test_size'], random_state=42)\n",
    "\n",
    "    #     s = model_config['scaler']\n",
    "    #     wandb.log({'scaler':s)\n",
    "    s = MaxAbsScaler()\n",
    "    X_train_s = s.fit_transform(X_train)\n",
    "    X_valid_s = s.fit_transform(X_valid)\n",
    "    #     X_train_s = s.fit_transform(x=X_train, y=y_train)\n",
    "    #     X_valid_s = s.fit_transform(x=X_valid, y=y_valid)\n",
    "\n",
    "    # removing features with post-normalization variance <0.01\n",
    "    #     var_thresh = VarianceThreshold(threshold=0.017)\n",
    "    #     X_train_v = var_thresh.fit_transform(X_train_s)\n",
    "    #     X_valid_v = X_valid_s[:, var_thresh.get_support()]\n",
    "    # X_valid_v = var_thresh.fit_transform(X_valid_s)\n",
    "\n",
    "    #     # instantiating the scaler and fitting it\n",
    "    #     if scaler:\n",
    "    #         s = scaler()\n",
    "    #         X_train = s.fit_transform(X_train)\n",
    "    #         X_valid = s.fit_transform(X_valid)\n",
    "\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    X_train_fs = selector.fit_transform(X_train_s, y_train)\n",
    "    X_valid_fs = X_valid_s[:, selector.get_support()]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        tree_method=config_defaults['tree_method'],\n",
    "        booster=config_defaults['booster'],\n",
    "        n_estimators=config_defaults['n_estimators'], \n",
    "        max_depth=config_defaults['max_depth'],\n",
    "        learning_rate=config_defaults['learning_rate'], \n",
    "        test_size=config_defaults['test_size'],\n",
    "        subsample=1,\n",
    "        random_state=config_defaults['random_state'],\n",
    "        n_jobs=-1, \n",
    "        verbosity=1, \n",
    "    )\n",
    "    wandb.log({'params': model.get_params()})\n",
    "    model.fit(X_train_fs, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "    y_preds = model.predict(X_valid_fs)\n",
    "    mse = mean_squared_error(y_valid, y_preds)\n",
    "    rmse = math.sqrt(abs(mse))\n",
    "    wandb.log({'mse':mse, 'rmse':rmse, 'kbest':k})\n",
    "    print(f\"MSE is {mse}\\nRMSE is {rmse} with {k}-best features.\")\n",
    "    wandb.finish()\n",
    "    #     if rmse < baseline_rmse:\n",
    "    #         print(\"RMSE is improved\")\n",
    "    #     else:\n",
    "    #         print(\"RMSE is not improved\")    \n",
    "\n",
    "    # selector = SelectKBest(score_func='f_regression', k=90)\n",
    "    # X_train_fs = selector.fit_tarnsform(X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c31144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(80,101):\n",
    "    train_fs(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a101f39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">20210823_XGBClassifier_feature_selection</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hushifang/202108_Kaggle_tabular_playground\" target=\"_blank\">https://wandb.ai/hushifang/202108_Kaggle_tabular_playground</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hushifang/202108_Kaggle_tabular_playground/runs/2lmuipf9\" target=\"_blank\">https://wandb.ai/hushifang/202108_Kaggle_tabular_playground/runs/2lmuipf9</a><br/>\n",
       "                Run data is saved locally in <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/aug2021/wandb/run-20210824_095404-2lmuipf9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(60,80):\n",
    "    train_fs(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
