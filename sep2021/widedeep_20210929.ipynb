{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "Setting up a more robust baseline notebook, suitable for use with all of the \"Big Three\" (XGBoost, CatBoost, LightGBM) libraries and on either Google Colab or the local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae2ff1e-bd1f-4cc9-8357-5a88d1746ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two manual flags (ex-config)\n",
    "colab = False\n",
    "gpu_available = True\n",
    "# libraries = ['xgboost', 'lightgbm', 'catboost']\n",
    "libraries = ['fastai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16849bd2-428c-497b-ba3b-675002f8d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d2654b-3bc6-49b5-ade8-cc82112b60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"widedeep_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416d6118-e543-4df4-9219-2d4a63743c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle Google Colab-specific library installation/updating\n",
    "if colab:\n",
    "    # much of the below inspired by or cribbed from the May 2021 Kaggle Tabular Playground winner, at \n",
    "    # https://colab.research.google.com/gist/academicsuspect/0aac7bd6e506f5f70295bfc9a3dc2250/tabular-may-baseline.ipynb?authuser=1#scrollTo=LJoVKJb5wN0L\n",
    "    \n",
    "    # Kaggle API for downloading the datasets\n",
    "    !pip install --upgrade -q kaggle\n",
    "\n",
    "    # weights and biases\n",
    "    !pip install -qqqU wandb\n",
    "    \n",
    "    # Optuna for parameter search\n",
    "    !pip install -q optuna\n",
    "\n",
    "    # upgrade sklearn\n",
    "    !pip install --upgrade scikit-learn\n",
    "\n",
    "    !pip install category_encoders\n",
    "    \n",
    "    if 'catboost' in libraries:\n",
    "        !pip install catboost\n",
    "    \n",
    "    if 'xgboost' in libraries:\n",
    "        if gpu_available: \n",
    "            # this part is from https://github.com/rapidsai/gputreeshap/issues/24\n",
    "            !pip install cmake --upgrade\n",
    "            # !pip install sklearn --upgrade\n",
    "            !git clone --recursive https://github.com/dmlc/xgboost\n",
    "            %cd /content/xgboost\n",
    "            !mkdir build\n",
    "            %cd build\n",
    "            !cmake .. -DUSE_CUDA=ON\n",
    "            !make -j4\n",
    "            %cd /content/xgboost/python-package\n",
    "            !python setup.py install --use-cuda --use-nccl\n",
    "            !/opt/bin/nvidia-smi\n",
    "            !pip install shap\n",
    "        else:\n",
    "            !pip install --upgrade xgboost\n",
    "    if 'lightgbm' in libraries:\n",
    "        if gpu_available:\n",
    "            # lighgbm gpu compatible\n",
    "            !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "            ! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;\n",
    "        else:\n",
    "            !pip install --upgrade lightgbm\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40df194-4474-4bcf-ac5a-98efe24b91fd",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01e85f7-d602-4dde-bef9-611683cd74c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /tmp/pip-req-build-1_ic8ial/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "# from wandb.xgboost import wandb_callback\n",
    "# from wandb.lightgbm import wandb_callback\n",
    "from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "# import timm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler #, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce765a83-270a-4762-915b-17ff3319fbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6166c2-ca44-4b7c-a4dc-3db47c2624fe",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c18a787-2193-43cb-87ee-51c6ae7b6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is the code for reading the train.csv and converting it to a .feather file\n",
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a41cd7e-accb-41c4-ad8b-0eaa3e2b0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/sep2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "    datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e47b3-43bd-4d35-b463-9d76100c6ed5",
   "metadata": {},
   "source": [
    "## Ex-Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb288275-a858-4806-9dc0-0b316c334536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "exmodel_config = {\n",
    "    # model config\n",
    "#     \"model\": XGBClassifier,\n",
    "#     \"n_estimators\": 100, \n",
    "#     \"max_depth\": 3,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"test_size\": 0.2,\n",
    "#     \"reg_lambda\": None, \n",
    "    \"library\": 'widedeep',\n",
    "    \"scaler\": StandardScaler, # TODO: experiment with others (but imputation may be slow)\n",
    "    \"scale_b4_impute\": False,\n",
    "    \"imputer\": SimpleImputer(strategy='median', add_indicator=True),\n",
    "#     \"knn_imputer_n_neighbors\": None, # None if a different imputer is used\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "    'random_state': 42,\n",
    "    'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "    'features_categorized': True,\n",
    "#     'subsample': 1,\n",
    "#     'cross_val_strategy': KFold, # None for holdout, or the relevant sklearn class\n",
    "#     'kfolds': 5, # if 1, that means just doing holdout\n",
    "#     'test_size': 0.2,\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d9012-34f1-435a-ba16-4416e0d4a286",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912a62f-970a-48b4-b428-d886f2612fc2",
   "metadata": {},
   "source": [
    "Due to the importance of identifying categorical variables for deep learning on tabular data (namely, the generation of embeddings containing meaningful information about them), I'm going to try using `fastai`'s `cont_cat_split` on the original dataset (post-imputation and generation of features based on summary statistics) and then proceeding with the other transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a3ac2c-37e8-4372-b64e-225f6dc3c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here's how to load the original, unaltered dataset and separate features from targets\n",
    "# df = pd.read_feather(path=datapath/'dataset_df.feather') # this is the unaltered original dataset\n",
    "# features = [x for x in df.columns if x != 'claim']\n",
    "# X = df[features]\n",
    "# y = df.claim\n",
    "\n",
    "\n",
    "\n",
    "# load the version of the dataset with imputations; X and y were stored separately, as feather and joblib respectively\n",
    "X = pd.read_feather(datapath/'X_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather') \n",
    "y = load(datapath/'y.joblib')    \n",
    "X.index.name = 'id'\n",
    "y.index.name = 'id'\n",
    "\n",
    "exmodel_config['feature_count'] = len(X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf791b86-a049-439b-a92a-fbbad524682e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f73f73e-393d-4ea0-ac2b-f5805a5b5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "113d4b09-4800-406b-b79f-cd36fce94c47",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      147965\n",
       "1      117427\n",
       "2      274624\n",
       "3      265618\n",
       "4      125414\n",
       "5      286473\n",
       "6      201975\n",
       "7      209765\n",
       "8      368026\n",
       "9      370266\n",
       "10     157265\n",
       "11     325095\n",
       "12     117373\n",
       "13     231413\n",
       "14     308572\n",
       "15     313089\n",
       "16      77994\n",
       "17     172559\n",
       "18     160073\n",
       "19     318317\n",
       "20     313905\n",
       "21      83438\n",
       "22     148460\n",
       "23     176988\n",
       "24     208329\n",
       "25     333767\n",
       "26     290999\n",
       "27     269916\n",
       "28     233936\n",
       "29     135072\n",
       "30     263378\n",
       "31     297891\n",
       "32     281103\n",
       "33     245721\n",
       "34     357008\n",
       "35     316292\n",
       "36     210948\n",
       "37      38267\n",
       "38     229893\n",
       "39     223437\n",
       "40     307516\n",
       "41     264543\n",
       "42      81540\n",
       "43     213949\n",
       "44     245352\n",
       "45      84044\n",
       "46     212671\n",
       "47      87459\n",
       "48     245196\n",
       "49     127050\n",
       "50     305728\n",
       "51     212595\n",
       "52     342517\n",
       "53       7459\n",
       "54     117101\n",
       "55     240290\n",
       "56     246959\n",
       "57     163484\n",
       "58      76378\n",
       "59     158582\n",
       "60     245796\n",
       "61     266661\n",
       "62     194153\n",
       "63     267355\n",
       "64     142378\n",
       "65     149027\n",
       "66     213419\n",
       "67     209319\n",
       "68      14049\n",
       "69     210462\n",
       "70      30980\n",
       "71     203071\n",
       "72     357058\n",
       "73     346323\n",
       "74     227088\n",
       "75      78697\n",
       "76     292411\n",
       "77     183952\n",
       "78     292606\n",
       "79     212278\n",
       "80      33486\n",
       "81     214663\n",
       "82     306521\n",
       "83     339635\n",
       "84     218333\n",
       "85     285557\n",
       "86     213305\n",
       "87     193832\n",
       "88     205000\n",
       "89     107019\n",
       "90     117596\n",
       "91     362662\n",
       "92     199921\n",
       "93     203994\n",
       "94     222502\n",
       "95     306483\n",
       "96        429\n",
       "97     335221\n",
       "98      41905\n",
       "99     190383\n",
       "100    223050\n",
       "101    261000\n",
       "102    312859\n",
       "103    318262\n",
       "104    155296\n",
       "105    223838\n",
       "106    252276\n",
       "107    305686\n",
       "108    207140\n",
       "109    206037\n",
       "110     44838\n",
       "111    328337\n",
       "112    291842\n",
       "113    316128\n",
       "114     12970\n",
       "115    336136\n",
       "116    200080\n",
       "117    164263\n",
       "118        15\n",
       "119       135\n",
       "120    957919\n",
       "121    183098\n",
       "122    957919\n",
       "123    957919\n",
       "124    957919\n",
       "125    957919\n",
       "126    218219\n",
       "127    191646\n",
       "128         2\n",
       "129         2\n",
       "130         2\n",
       "131         2\n",
       "132         2\n",
       "133         2\n",
       "134         2\n",
       "135         2\n",
       "136         2\n",
       "137         2\n",
       "138         2\n",
       "139         2\n",
       "140         2\n",
       "141         2\n",
       "142         2\n",
       "143         2\n",
       "144         2\n",
       "145         2\n",
       "146         2\n",
       "147         2\n",
       "148         2\n",
       "149         2\n",
       "150         2\n",
       "151         2\n",
       "152         2\n",
       "153         2\n",
       "154         2\n",
       "155         2\n",
       "156         2\n",
       "157         2\n",
       "158         2\n",
       "159         2\n",
       "160         2\n",
       "161         2\n",
       "162         2\n",
       "163         2\n",
       "164         2\n",
       "165         2\n",
       "166         2\n",
       "167         2\n",
       "168         2\n",
       "169         2\n",
       "170         2\n",
       "171         2\n",
       "172         2\n",
       "173         2\n",
       "174         2\n",
       "175         2\n",
       "176         2\n",
       "177         2\n",
       "178         2\n",
       "179         2\n",
       "180         2\n",
       "181         2\n",
       "182         2\n",
       "183         2\n",
       "184         2\n",
       "185         2\n",
       "186         2\n",
       "187         2\n",
       "188         2\n",
       "189         2\n",
       "190         2\n",
       "191         2\n",
       "192         2\n",
       "193         2\n",
       "194         2\n",
       "195         2\n",
       "196         2\n",
       "197         2\n",
       "198         2\n",
       "199         2\n",
       "200         2\n",
       "201         2\n",
       "202         2\n",
       "203         2\n",
       "204         2\n",
       "205         2\n",
       "206         2\n",
       "207         2\n",
       "208         2\n",
       "209         2\n",
       "210         2\n",
       "211         2\n",
       "212         2\n",
       "213         2\n",
       "214         2\n",
       "215         2\n",
       "216         2\n",
       "217         2\n",
       "218         2\n",
       "219         2\n",
       "220         2\n",
       "221         2\n",
       "222         2\n",
       "223         2\n",
       "224         2\n",
       "225         2\n",
       "226         2\n",
       "227         2\n",
       "228         2\n",
       "229         2\n",
       "230         2\n",
       "231         2\n",
       "232         2\n",
       "233         2\n",
       "234         2\n",
       "235         2\n",
       "236         2\n",
       "237         2\n",
       "238         2\n",
       "239         2\n",
       "240         2\n",
       "241         2\n",
       "242         2\n",
       "243         2\n",
       "244         2\n",
       "245         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eae57f-6a1e-4d5f-8355-62ff4c2a2bcf",
   "metadata": {},
   "source": [
    "I'll follow JH's advice here and set 10k as the ceiling for the `max_card=`; this might be something to experiment with later using Optuna. (Esp. as JH says that numbers in the 5000s and higher make him very nervous.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b2ffcd-49c8-4cf2-885e-a9dd0e640869",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_card_embed = 10000\n",
    "max_card_cat = 100000\n",
    "exmodel_config['max_card_for_embedding'] = max_card_embed\n",
    "exmodel_config['max_card_for_categorical'] = max_card_cat\n",
    "\n",
    "\n",
    "# X_orig = X.iloc[:, :118] # excluding summary, meta-statistics\n",
    "# X_meta = X.iloc[:, 118:] # including summary, meta-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53f9d86-dce7-46db-83c1-929dc1ec7ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exmodel_config['max_card_for_categorical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86b704d8-ec62-4d98-8410-1e636e492178",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_card_features = [f for f in X.columns if X[f].nunique() <= 50000]\n",
    "high_card_features = [f for f in X.columns if X[f].nunique() > 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0b2e8b-50da-4656-b14b-2af467387223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(low_card_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9d9f790-8b15-4011-ab42-2b342a455e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_card_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbd183-5501-4dcd-870e-b5f2eae70749",
   "metadata": {},
   "source": [
    "# WideDeep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5a9cb-9524-4d16-9a98-d00ea8f99f03",
   "metadata": {},
   "source": [
    "## (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f712681f-f201-4c6b-9983-bd95f02ecb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(datapath/\"adult.csv.zip\")\n",
    "# df[\"income_label\"] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "# df.drop(\"income\", axis=1, inplace=True)\n",
    "# df_train, df_test = train_test_split(df, test_size=0.2, stratify=df.income_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb4e450b-08f7-4e02-9a6d-233b7225466a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for f in df.columns:\n",
    "#     print(f\"{f}: {df[f].nunique()}\")\n",
    "#     print(f\"NaNs: {df[f].isna().sum()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c3ae8b-e0a2-4948-9dee-1dc6a62765c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3256738c-6bf9-4e63-8e69-82645bbe11f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 957919 entries, 0 to 957918\n",
      "Columns: 246 entries, 0 to 245\n",
      "dtypes: float64(246)\n",
      "memory usage: 1.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f230f28-cf7a-4f1f-ab9c-3e0075cbebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cols_pre = [f for f in X.columns if X[f].nunique() <= max_card_cat and X[f].nunique() > 2]\n",
    "wide_cols_onehot = [f for f in X.columns if X[f].nunique() == 2]\n",
    "cont_cols = high_card_features\n",
    "embed_cols = [f for f in X.columns if X[f].nunique() <= max_card_embed and X[f].nunique() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d37360c-a8c0-4393-b616-f787ef46f7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 118, 117, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wide_cols_pre), len(wide_cols_onehot), len(cont_cols), len(embed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06ebc1ab-0e32-4ae7-ab18-62fb345481cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols_pre)\n",
    "X_wide_pre = wide_preprocessor.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e4f4953-f2bc-4d58-86ec-f7228b7e00cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 18)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9700db34-7b28-4e8c-82d2-63d009295fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,  77995, 161433, 199700, 281240, 365284, 452743, 460202,\n",
       "        536580, 550629, 581609, 660306, 693792, 694221, 736126, 780964,\n",
       "        793934, 793949],\n",
       "       [     2,  77996, 161434, 199701, 281241, 365285, 452744, 460203,\n",
       "        536581, 550630, 581610, 660307, 693793, 694222, 736127, 780965,\n",
       "        793935, 793950],\n",
       "       [     3,  77997, 161435, 199702, 281242, 365286, 452745, 460204,\n",
       "        536582, 550631, 581611, 660308, 693794, 694223, 736128, 780966,\n",
       "        793936, 793951],\n",
       "       [     4,  77998, 161436, 199703, 281243, 365287, 452746, 460205,\n",
       "        536583, 550632, 581612, 660309, 693795, 694224, 736129, 780967,\n",
       "        793937, 793952],\n",
       "       [     5,  77999, 161437, 199704, 281244, 365288, 452747, 460206,\n",
       "        536584, 550633, 581613, 660310, 693794, 694225, 736130, 780968,\n",
       "        793938, 793953],\n",
       "       [     6,  78000, 161438, 199705, 281245, 365289, 452748, 460207,\n",
       "        536585, 550634, 581614, 660311, 693796, 694226, 736131, 780969,\n",
       "        793934, 793954],\n",
       "       [     7,  78001, 161439, 199706, 281246, 365290, 452749, 460208,\n",
       "        536586, 550635, 581615, 660312, 693797, 694227, 736132, 780970,\n",
       "        793939, 793955],\n",
       "       [     8,  78002, 161440, 199707, 281247, 365291, 452750, 460209,\n",
       "        536587, 550636, 581616, 660313, 693798, 694228, 736133, 780971,\n",
       "        793934, 793956],\n",
       "       [     9,  78003, 161441, 199708, 281248, 365292, 452751, 460210,\n",
       "        536588, 550637, 581617, 660314, 693799, 694229, 736134, 780972,\n",
       "        793935, 793950],\n",
       "       [    10,  78004, 161442, 199709, 281249, 365293, 452752, 460211,\n",
       "        536589, 550638, 581618, 660315, 693800, 694230, 736135, 780973,\n",
       "        793935, 793950]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_pre[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b40fbb61-9229-40b8-bc29-a4045521a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wide_pre_df = pd.DataFrame(X_wide_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf8db2e8-2be9-4615-a2ce-1899d1ea965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>77995</td>\n",
       "      <td>161433</td>\n",
       "      <td>199700</td>\n",
       "      <td>281240</td>\n",
       "      <td>365284</td>\n",
       "      <td>452743</td>\n",
       "      <td>460202</td>\n",
       "      <td>536580</td>\n",
       "      <td>550629</td>\n",
       "      <td>581609</td>\n",
       "      <td>660306</td>\n",
       "      <td>693792</td>\n",
       "      <td>694221</td>\n",
       "      <td>736126</td>\n",
       "      <td>780964</td>\n",
       "      <td>793934</td>\n",
       "      <td>793949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>77996</td>\n",
       "      <td>161434</td>\n",
       "      <td>199701</td>\n",
       "      <td>281241</td>\n",
       "      <td>365285</td>\n",
       "      <td>452744</td>\n",
       "      <td>460203</td>\n",
       "      <td>536581</td>\n",
       "      <td>550630</td>\n",
       "      <td>581610</td>\n",
       "      <td>660307</td>\n",
       "      <td>693793</td>\n",
       "      <td>694222</td>\n",
       "      <td>736127</td>\n",
       "      <td>780965</td>\n",
       "      <td>793935</td>\n",
       "      <td>793950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>77997</td>\n",
       "      <td>161435</td>\n",
       "      <td>199702</td>\n",
       "      <td>281242</td>\n",
       "      <td>365286</td>\n",
       "      <td>452745</td>\n",
       "      <td>460204</td>\n",
       "      <td>536582</td>\n",
       "      <td>550631</td>\n",
       "      <td>581611</td>\n",
       "      <td>660308</td>\n",
       "      <td>693794</td>\n",
       "      <td>694223</td>\n",
       "      <td>736128</td>\n",
       "      <td>780966</td>\n",
       "      <td>793936</td>\n",
       "      <td>793951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>77998</td>\n",
       "      <td>161436</td>\n",
       "      <td>199703</td>\n",
       "      <td>281243</td>\n",
       "      <td>365287</td>\n",
       "      <td>452746</td>\n",
       "      <td>460205</td>\n",
       "      <td>536583</td>\n",
       "      <td>550632</td>\n",
       "      <td>581612</td>\n",
       "      <td>660309</td>\n",
       "      <td>693795</td>\n",
       "      <td>694224</td>\n",
       "      <td>736129</td>\n",
       "      <td>780967</td>\n",
       "      <td>793937</td>\n",
       "      <td>793952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>77999</td>\n",
       "      <td>161437</td>\n",
       "      <td>199704</td>\n",
       "      <td>281244</td>\n",
       "      <td>365288</td>\n",
       "      <td>452747</td>\n",
       "      <td>460206</td>\n",
       "      <td>536584</td>\n",
       "      <td>550633</td>\n",
       "      <td>581613</td>\n",
       "      <td>660310</td>\n",
       "      <td>693794</td>\n",
       "      <td>694225</td>\n",
       "      <td>736130</td>\n",
       "      <td>780968</td>\n",
       "      <td>793938</td>\n",
       "      <td>793953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1       2       3       4       5       6       7       8       9   \\\n",
       "0   1  77995  161433  199700  281240  365284  452743  460202  536580  550629   \n",
       "1   2  77996  161434  199701  281241  365285  452744  460203  536581  550630   \n",
       "2   3  77997  161435  199702  281242  365286  452745  460204  536582  550631   \n",
       "3   4  77998  161436  199703  281243  365287  452746  460205  536583  550632   \n",
       "4   5  77999  161437  199704  281244  365288  452747  460206  536584  550633   \n",
       "\n",
       "       10      11      12      13      14      15      16      17  \n",
       "0  581609  660306  693792  694221  736126  780964  793934  793949  \n",
       "1  581610  660307  693793  694222  736127  780965  793935  793950  \n",
       "2  581611  660308  693794  694223  736128  780966  793936  793951  \n",
       "3  581612  660309  693795  694224  736129  780967  793937  793952  \n",
       "4  581613  660310  693794  694225  736130  780968  793938  793953  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_pre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "619e112c-4969-45eb-bc93-50fb2c079bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>7.814957</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>7.795214</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>7.846019</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>7.784252</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>7.80212</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>7.858285</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>7.799560</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>7.814957</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>7.795214</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>7.804170</td>\n",
       "      <td>7.809044</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>7.806991</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>7.801096</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>7.828890</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>7.858285</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         128       129       130       131       132       133       134  \\\n",
       "id                                                                         \n",
       "0  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "1  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "2  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "3  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "4  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "\n",
       "         135       136       137      138       139       140       141  \\\n",
       "id                                                                        \n",
       "0  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "1  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "2  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "3  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "4  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "\n",
       "         142      143       144       145       146       147       148  \\\n",
       "id                                                                        \n",
       "0  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "1  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "2   7.795214 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "3  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "4   7.795214 -0.12801 -0.127939 -0.127508  7.804170  7.809044 -0.128052   \n",
       "\n",
       "         149       150       151       152       153      154       155  \\\n",
       "id                                                                        \n",
       "0  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "1  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "2  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "3  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "4  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "\n",
       "         156       157       158       159       160       161       162  \\\n",
       "id                                                                         \n",
       "0  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "1  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "2  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "3  -0.127888 -0.127791 -0.128992 -0.128368  7.799560 -0.127182 -0.127546   \n",
       "4  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "\n",
       "         163       164       165       166      167       168       169  \\\n",
       "id                                                                        \n",
       "0  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "1  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "2  -0.127669  7.846019 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "3  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "4  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "\n",
       "         170       171       172       173       174       175      176  \\\n",
       "id                                                                        \n",
       "0  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "1  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "2  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "3  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "4  -0.128057  7.806991 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "\n",
       "         177       178       179       180      181       182       183  \\\n",
       "id                                                                        \n",
       "0  -0.128506  7.814957 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "1  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "2  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "3  -0.128506  7.814957 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "4  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "\n",
       "         184       185       186       187       188       189       190  \\\n",
       "id                                                                         \n",
       "0  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "1  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "2  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "3  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "4  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "\n",
       "         191       192       193       194       195       196       197  \\\n",
       "id                                                                         \n",
       "0  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "1  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "2  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745  7.784252 -0.127242   \n",
       "3  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "4  -0.128574 -0.127884 -0.127884  7.801096 -0.128745 -0.128464 -0.127242   \n",
       "\n",
       "        198       199       200      201       202       203       204  \\\n",
       "id                                                                       \n",
       "0  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "1  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "2  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "3  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "4  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "\n",
       "         205       206       207       208       209       210       211  \\\n",
       "id                                                                         \n",
       "0  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "1  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "2  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "3  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "4  -0.128031  7.828890 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "\n",
       "         212       213      214       215       216      217       218  \\\n",
       "id                                                                       \n",
       "0  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "1  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "2  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015  7.80212 -0.128275   \n",
       "3  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "4  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "\n",
       "         219       220       221       222       223       224       225  \\\n",
       "id                                                                         \n",
       "0  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "1  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "2  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339  7.858285 -0.127352   \n",
       "3  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "4  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339  7.858285 -0.127352   \n",
       "\n",
       "         226       227      228       229       230      231       232  \\\n",
       "id                                                                       \n",
       "0  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "1  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "2  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "3  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "4  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "\n",
       "         233       234       235       236       237       238       239  \\\n",
       "id                                                                         \n",
       "0  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "1  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "2  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "3  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "4  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "\n",
       "         240       241       242      243       244      245  \n",
       "id                                                            \n",
       "0  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "1  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "2  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "3  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "4  -0.127119 -0.127985 -0.128494 -0.12862  7.821398 -0.12703  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[:, wide_cols_onehot].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4ad554a-9bff-402c-a202-b9059e9d7c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X_wide = X_wide_pre_df.join(X.loc[:,wide_cols_onehot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b23289b-824b-4384-84ad-783671d680d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 957919 entries, 0 to 957918\n",
      "Columns: 136 entries, 0 to 245\n",
      "dtypes: float64(118), int64(18)\n",
      "memory usage: 993.9 MB\n"
     ]
    }
   ],
   "source": [
    "X_wide.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4e3a43e-4052-4461-bd36-ba8400145eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eaac21f-fbb0-4d37-a70f-16dcb116192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_preprocessor = TabPreprocessor(embed_cols=embed_cols, continuous_cols=cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7a2446c-4182-45be-a5c2-06d450f578c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 957919 entries, 0 to 957918\n",
      "Columns: 246 entries, 0 to 245\n",
      "dtypes: float64(246)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64cf66a6-7382-4358-9617-1f10cda0af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tab = tab_preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9825519e-8aa8-498b-aca6-1ba1a98592c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(957919, 121)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14d819db-4b99-4f60-98be-4dc6a323c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptabular = TabMlp(\n",
    "    mlp_hidden_dims=[64,32],\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    embed_input=tab_preprocessor.embeddings_input,\n",
    "    continuous_cols=cont_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7306cc9-5e5e-49a1-b6cc-2e8d789f9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3a253cf-51d3-4b59-97ae-a6b7e060265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wide = np.array(X_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de8d01a0-3f3b-4e8e-861b-c9da6a8db682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(957919, 136)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7800573b-d576-4687-9917-cf4f6a2d19f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 121)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dade972-be94-44b6-8b48-1b53c57c6b19",
   "metadata": {},
   "source": [
    "39774, 758737, 552968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af89e657-f704-43fd-b76f-1055ccb229ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6fe7e338-d8c6-44ed-8e51-32f5c35e622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2abd8b0-994f-4cc1-aa4e-6ea2e20049ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 749/749 [00:32<00:00, 22.92it/s, loss=5.85, metrics={'acc': 0.6669}]\n",
      "valid: 100%|██████████| 188/188 [00:01<00:00, 106.25it/s, loss=1.46, metrics={'acc': 0.7442}]\n",
      "epoch 2: 100%|██████████| 749/749 [00:32<00:00, 23.20it/s, loss=1.78, metrics={'acc': 0.6758}]\n",
      "valid: 100%|██████████| 188/188 [00:01<00:00, 110.47it/s, loss=1.19, metrics={'acc': 0.7414}]\n",
      "epoch 3: 100%|██████████| 749/749 [00:31<00:00, 23.57it/s, loss=1.45, metrics={'acc': 0.6799}]\n",
      "valid: 100%|██████████| 188/188 [00:01<00:00, 111.22it/s, loss=1.11, metrics={'acc': 0.746}] \n",
      "epoch 4: 100%|██████████| 749/749 [00:31<00:00, 23.55it/s, loss=1.27, metrics={'acc': 0.6854}]\n",
      "valid: 100%|██████████| 188/188 [00:02<00:00, 85.10it/s, loss=0.957, metrics={'acc': 0.7423}] \n",
      "epoch 5: 100%|██████████| 749/749 [00:31<00:00, 23.56it/s, loss=1.15, metrics={'acc': 0.6912}]\n",
      "valid: 100%|██████████| 188/188 [00:01<00:00, 112.74it/s, loss=0.924, metrics={'acc': 0.7447}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    X_wide=X_wide,\n",
    "    X_tab=X_tab,\n",
    "    target=y,\n",
    "    n_epochs=5,\n",
    "    batch_size=1024,\n",
    "    val_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea409dfb-98a2-4b05-8a14-6b055eafdf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_feather(datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ca5cde74-1623-40b7-879e-87cb24b397b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py:136: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return df.copy()[self.wide_cols]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-623462ede1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_wide_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwide_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_tab_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtab_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;34mr\"\"\"Returns the processed dataframe\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mdf_wide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_wide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_wide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_crossed_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_crossed_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py\u001b[0m in \u001b[0;36m_prepare_wide\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_cc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "X_test = X_test.to_numpy()\n",
    "X_wide_te = wide_preprocessor.transform(X_test)\n",
    "X_tab_te = tab_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c904b869-0b6f-443c-895f-b93ca4f80e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493474, 246)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f17cd1d-c053-4fee-b08f-8f6e3ee0a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_feather(datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')\n",
    "\n",
    "low_card_features = [f for f in X_test.columns if X_test[f].nunique() <= 50000]\n",
    "high_card_features = [f for f in X_test.columns if X_test[f].nunique() > 50000]\n",
    "\n",
    "wide_cols_pre = [f for f in X_test.columns if X_test[f].nunique() <= max_card_cat and X_test[f].nunique() > 2]\n",
    "wide_cols_onehot = [f for f in X_test.columns if X_test[f].nunique() == 2]\n",
    "cont_cols = high_card_features\n",
    "embed_cols = [f for f in X_test.columns if X_test[f].nunique() <= max_card_embed and X_test[f].nunique() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c06afef5-79e2-4751-8e79-57037f1cf393",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-68a894bef700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# X_test = X_test.to_numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_wide_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwide_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_tab_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtab_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;34mr\"\"\"Returns the processed dataframe\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mdf_wide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_wide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_wide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_crossed_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_crossed_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py\u001b[0m in \u001b[0;36m_prepare_wide\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_cc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# X_test = X_test.to_numpy()\n",
    "X_wide_te = wide_preprocessor.transform(X_test)\n",
    "X_tab_te = tab_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9d22a-cce9-4370-8192-13a432bdd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wide_te = wide_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb9552-d4b9-4e83-9cb4-fd22c4d3f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(X_wide=X_wide_te, X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078f3c6-b2af-412c-8f3d-a5866f13d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5b122-a5e4-4ae8-ad5d-bae73d59e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_proba = trainer.predict_proba(X_wide=X_wide_te, X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba64d2b-96a1-4dc9-a837-c27ce7ea5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_proba[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ea2fe-0f38-4b5b-b9e9-913d8d33e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(preds_proba, datapath/'preds/widedeep_5epochs_bs1024_64x32tabmlp_20210929_probas.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5f215-b585-4f7b-afac-36e49ee28c8f",
   "metadata": {},
   "source": [
    "## Weights and Biases Run Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a928e1-0a18-4c91-b9bb-a32846e39e5b",
   "metadata": {},
   "source": [
    "Below is the configuration for a Weights and Biases (`wandb`) run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5402d98-7bcd-4e46-b85d-6b0129ca6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "config_run = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['stacking-sklearn', 'attempt'],\n",
    "    'notes': \"Trying a fastai tabular MLP model, for ensembling with the GBMs\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638002ad-9266-44d6-8302-ebce2a6f7b06",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30d6f8-d893-43d3-8a4d-41c5756b6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379b0bf-1c3b-4095-b382-ec5a16e3474d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24391812-dce3-4513-bd38-ee95694730e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_valid, y_train, y_valid, model_config, \n",
    "                                              random_state=42,\n",
    "                                              exmodel_config=exmodel_config, \n",
    "                                              config_run=config_run):#, scaler): # passed in via config dict for now\n",
    "    \"\"\"\n",
    "    Basic training function. Note that some of the options passed via the argument are\n",
    "    in fact hard-coded in, to avoid inconveniences.\n",
    "    :param X_train: the training set features\n",
    "    :param X_valid: the validation set features\n",
    "    :param y_train: the training set targets\n",
    "    :param y_valid: the validation set targets\n",
    "    :param random_staKFold: for reproducibility\n",
    "    :param exmodel_config: dict containing configuration details including the library \n",
    "                            (thus model) used, preprocessing, and cross-validation\n",
    "    :param model_config: dict containing hyperparameter specifications for the model\n",
    "    :param config_run: dict containing wandb run configuration (name, etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    # As of 20210920, best CatBoost config is:\n",
    "    best_20210920_catboost_params = {\n",
    "        'iterations': 3493,\n",
    "        'depth': 5,\n",
    "        'learning_rate': 0.09397459954141321,\n",
    "        'random_strength': 43,\n",
    "        'l2_leaf_reg': 26,\n",
    "        'border_count': 239,\n",
    "        'bagging_temperature': 12.532400413798356,\n",
    "        'od_type': 'Iter'\n",
    "    }\n",
    "    \n",
    "    # catboost 20210921 on colab (only 15 trials though)\n",
    "    best_catboost_params = {\n",
    "        'iterations': 3302,\n",
    "        'depth': 5,\n",
    "        'learning_rate': 0.017183208677599107,\n",
    "        'random_strength': 41,\n",
    "        'l2_leaf_reg': 30,\n",
    "        'border_count': 251,\n",
    "        'bagging_temperature': 9.898390369028036, \n",
    "        'od_type': 'IncToDec'\n",
    "    }\n",
    "    \n",
    "    # optuna 20210921\n",
    "    best_xgboost_params = {\n",
    "        'n_estimators': 1119,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.04123392555159452,\n",
    "        'reg_alpha': 4.511876752318655,\n",
    "        'reg_lambda': 4.074347238862406,\n",
    "        'subsample': 0.8408586950521992\n",
    "    }\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"202109_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=exmodel_config)   \n",
    "        \n",
    "    if exmodel_config['library'] == 'xgboost':\n",
    "        model = XGBClassifier(\n",
    "            tree_method=model_config['tree_method'],\n",
    "            random_state=random_state,\n",
    "            n_jobs=model_config['n_jobs'], \n",
    "            verbosity=model_config['verbosity'], \n",
    "            objective=model_config['objective'],\n",
    "            **best_xgboost_params\n",
    "            # #             eval_metric=model_config['eval_metric'],\n",
    "\n",
    "            # comment out the below for a fairly default model\n",
    "#             booster=model_config['booster'],\n",
    "#             max_depth=model_config['max_depth'],\n",
    "#             learning_rate=model_config['learning_rate'], \n",
    "#             subsample=model_config['subsample'],\n",
    "#             reg_alpha=model_config['reg_alpha'],\n",
    "#             reg_lambda=model_config['reg_lambda'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()],\n",
    "#                                     eval_metric=model_config['eval_metric'],\n",
    "                 )\n",
    "\n",
    "\n",
    "    elif exmodel_config['library'] == 'lightgbm':\n",
    "        model = LGBMClassifier(\n",
    "#             boosting_type=model_config['boosting_type'],\n",
    "#             max_depth=model_config['max_depth']\n",
    "            # TODO\n",
    "            random_state=random_state,\n",
    "            n_jobs=model_config['n_jobs'],\n",
    "            objective=model_config['objective'],\n",
    "#             eval_metric=model_config['eval_metric'],\n",
    "            boosting_type=model_config['boosting_type'],\n",
    "            device_type=model_config['device_type'],\n",
    "            \n",
    "            # comment out the below for a basically default model\n",
    "            n_estimators=model_config['n_estimators'],\n",
    "            learning_rate=model_config['learning_rate'],\n",
    "            max_depth=model_config['max_depth'],\n",
    "            reg_alpha=model_config['reg_alpha'],\n",
    "            reg_lambda=model_config['reg_lambda'],\n",
    "            subsample=model_config['subsample'],\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],\n",
    "#                                     eval_metric=model_config['eval_metric'],\n",
    "                 )\n",
    "        \n",
    "    elif exmodel_config['library'] == 'catboost':\n",
    "        print(\"CatBoost, therefore no WandB callback.\")\n",
    "        model = CatBoostClassifier(\n",
    "#             n_estimators=config['n_estimators'],\n",
    "#             learning_rate=config['learning_rate'],\n",
    "#             max_depth=config['max_depth'],\n",
    "            task_type=model_config['task_type'],\n",
    "    #         n_jobs=config['n_jobs'],\n",
    "    #         verbosity=config['verbosity'],\n",
    "    #         subsample=config['subsample'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "            random_state=random_state,\n",
    "            # objective='Logloss', # default, accepts only one\n",
    "#             custom_metrics=model_config['custom_metrics'],\n",
    "    #         bootstrap_type=config['bootstrap_type'],\n",
    "    #         device:config['device']\n",
    "            **best_catboost_params\n",
    "        ) \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "#     y_train_pred = model.predict(X_train)\n",
    "    y_train_pred = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    train_loss = log_loss(y_train, y_train_pred)\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    wandb.log({'train_loss': train_loss, 'train_auc': train_auc})\n",
    "\n",
    "    if exmodel_config['library'] == 'catboost':\n",
    "        print(model.get_all_params())\n",
    "        wandb.log(model.get_all_params())\n",
    "    else:\n",
    "        wandb.log(model.get_params()) # logging model parameters, trying bare-invocation rather than params: model.get_params()\n",
    "    \n",
    "    # trying with predict_proba\n",
    "    y_pred = model.predict_proba(X_valid)[:,1]\n",
    "#     y_pred = model.predict(X_valid)\n",
    "\n",
    "    valid_loss = log_loss(y_valid, y_pred)\n",
    "    valid_auc = roc_auc_score(y_valid, y_pred)\n",
    "    wandb.log({'valid_loss':valid_loss, 'valid_auc':valid_auc})\n",
    "    print(f\"Valid log-loss is {valid_loss}\\nValid AUC is {valid_auc}\")   \n",
    "#     wandb.finish()   \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfa66428-3fb9-410f-9ea5-50785a4bd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model_config, X=X, y=y, start_fold=0, exmodel_config=exmodel_config, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to handle model training process in the context of cross-validation -- via hold-out or via k-fold.\n",
    "    If exmodel_config['cross_val_strategy'] == None, then any kfolds= input is ignored; otherwise, the number specified is used.\n",
    "    \n",
    "    :param kfolds: int specifying number of k-folds to use in cross-validation\n",
    "    :param exmodel_config: dict containing general config including for cross-validation -- `kfold=1` implies hold-out\n",
    "    \"\"\"\n",
    "    if exmodel_config['kfolds'] == 1:\n",
    "        print(\"Proceeding with holdout\")\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=exmodel_config['test_size'], \n",
    "                                                      random_state=random_state,\n",
    "                                                     )\n",
    "        model = train(X_train, X_valid, y_train, y_valid, exmodel_config=exmodel_config, \n",
    "                                                    model_config=model_config,\n",
    "                                                    config_run=config_run)\n",
    "        wandb.finish()\n",
    "        \n",
    "    else:\n",
    "        X, y = X.to_numpy(), y.to_numpy()\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=random_state)\n",
    "        models = {}\n",
    "        model_path = Path(datapath/f\"models/{config_run['name']}_{exmodel_config['kfolds']}folds/\")\n",
    "        (model_path).mkdir(exist_ok=True)\n",
    "        for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "            if fold < start_fold:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"FOLD {fold}\")\n",
    "                print(\"---------------------------------------------------\")\n",
    "                X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "                model = train(X_train, X_valid, y_train, y_valid, exmodel_config=exmodel_config, \n",
    "                                                    model_config=model_config,\n",
    "                                                    config_run=config_run)\n",
    "                wandb.log({'fold': fold})\n",
    "                models[fold] = model\n",
    "                dump(model, Path(model_path/f\"{exmodel_config['library']}_fold{fold}_model.joblib\"))\n",
    "                wandb.finish()\n",
    "        return models\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a437b-f880-4284-8e02-66a398ba6454",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58234814-68d1-4ee4-bedd-d722c18e4fa6",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4085e60e-13cf-4da7-90b9-30306480b4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library = 'xgboost'\n",
    "# exmodel_config['library'] = library\n",
    "# model_config = model_configurator(library)\n",
    "# xgboost_models = cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b780292-f23c-4a3f-be1e-53038c5ae7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for scaler in [StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler]:\n",
    "#     exmodel_config['scaler'] = scaler\n",
    "#     scaler = scaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "#     exmodel_config['library'] = 'lightgbm'\n",
    "#     model_config = model_configurator('lightgbm')\n",
    "#     cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1106b86c-70c1-4722-a86f-6c53a2e32503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library = 'lightgbm'\n",
    "# exmodel_config['library'] = library\n",
    "# model_config = model_configurator(library)\n",
    "# lightgbm_models = cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542539d-323d-4a4a-b574-d0bf883179b2",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776a94d-01f3-46c0-9c9c-0d162de17e37",
   "metadata": {},
   "source": [
    "## Via `sklearn.ensemble.StackingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca87f5f-e214-4968-a789-74f83dc7a656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e68e3adf-fd9d-4185-87c4-330ecd768679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost_estimators = [(f'xgboost_fold{fold}', xgboost_models[fold]) for fold in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed3aa865-fec7-417b-9369-677d3b840760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaving this default for first try\n",
    "# final_estimator = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ac5214d-a3ab-4219-8047-822c4385e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacker(estimators:dict, library:str, X=X, y=y): #, load_models:bool=False, load_path:Path=None):\n",
    "    \"\"\"\n",
    "    A wrapper that will take a dict of the form {fold:int : model} and a string representing the library (for file-naming), \n",
    "    then run `sklearn.ensemble.StackingClassifier` with it, and save the stacked model afterward\n",
    "    \"\"\"\n",
    "    estimators_list = [(f'{library}_fold{fold}', estimators[fold]) for fold in range(5)]\n",
    "    blender = StackingClassifier(estimators=estimators_list,\n",
    "                                 cv=5,\n",
    "                                 stack_method='predict_proba',\n",
    "                                 n_jobs=2,\n",
    "                                 passthrough=False,\n",
    "                                 verbose=1\n",
    "                                )\n",
    "    print(f\"Starting fitting at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    blender.fit(X,y)\n",
    "    print(f\"Fitting complete at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    dump(blender, filename=datapath/f\"models/{config_run['name']}_{exmodel_config['kfolds']}folds/{library}_stack.joblib\")\n",
    "    print(f\"Blender model saved at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    return blender\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39f08073-4061-4f03-be9c-1c1fa601b624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">catboost_20210922_112905</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/drhfu0df\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/drhfu0df</a><br/>\n",
       "                Run data is saved locally in <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_112905-drhfu0df</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# might encapsulate this in a new version of the above train function later\n",
    "exmodel_config['ensemble'] = 'stacking'\n",
    "\n",
    "wandb.init(\n",
    "        project=\"202109_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=exmodel_config)   \n",
    "\n",
    "random_state = exmodel_config['random_state'] # 42\n",
    "\n",
    "\n",
    "# # optuna 20210921\n",
    "# best_xgboost_params = {\n",
    "#     'n_estimators': 1119,\n",
    "#     'max_depth': 6,\n",
    "#     'learning_rate': 0.04123392555159452,\n",
    "#     'reg_alpha': 4.511876752318655,\n",
    "#     'reg_lambda': 4.074347238862406,\n",
    "#     'subsample': 0.8408586950521992\n",
    "# }\n",
    "\n",
    "# model_config = model_configurator('xgboost')\n",
    "# xgboost_model = XGBClassifier(\n",
    "#             tree_method=model_config['tree_method'],\n",
    "#             random_state=random_state,\n",
    "# #             n_jobs=model_config['n_jobs'], \n",
    "#             verbosity=model_config['verbosity'], \n",
    "#             objective=model_config['objective'],\n",
    "#             **best_xgboost_params\n",
    "#             # #             eval_metric=model_config['eval_metric'],\n",
    "\n",
    "#             # comment out the below for a fairly default model\n",
    "# #             booster=model_config['booster'],\n",
    "# #             max_depth=model_config['max_depth'],\n",
    "# #             learning_rate=model_config['learning_rate'], \n",
    "# #             subsample=model_config['subsample'],\n",
    "# #             reg_alpha=model_config['reg_alpha'],\n",
    "# #             reg_lambda=model_config['reg_lambda'],\n",
    "# #             n_estimators=model_config['n_estimators'],\n",
    "#         )\n",
    "\n",
    "# model_config = model_configurator('lightgbm')\n",
    "# lightgbm_model = LGBMClassifier(\n",
    "#             random_state=random_state,\n",
    "# #             n_jobs=model_config['n_jobs'],\n",
    "#             objective=model_config['objective'],\n",
    "#             boosting_type=model_config['boosting_type'],\n",
    "#             device_type=model_config['device_type'],\n",
    "            \n",
    "#             # comment out the below for a basically default model\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "#             learning_rate=model_config['learning_rate'],\n",
    "#             max_depth=model_config['max_depth'],\n",
    "#             reg_alpha=model_config['reg_alpha'],\n",
    "#             reg_lambda=model_config['reg_lambda'],\n",
    "#             subsample=model_config['subsample'],\n",
    "#         )\n",
    "\n",
    "model_config = model_configurator('catboost', gpu_available=False) # set GPU false to avoid parallel threads blocking GPU\n",
    "\n",
    "# # As of 20210920, best CatBoost config is:\n",
    "# best_20210920_catboost_params = {\n",
    "#     'iterations': 3493,\n",
    "#     'depth': 5,\n",
    "#     'learning_rate': 0.09397459954141321,\n",
    "#     'random_strength': 43,\n",
    "#     'l2_leaf_reg': 26,\n",
    "#     'border_count': 239,\n",
    "#     'bagging_temperature': 12.532400413798356,\n",
    "#     'od_type': 'Iter'\n",
    "# }\n",
    "\n",
    "# catboost 20210921 on colab (only 15 trials though)\n",
    "best_catboost_params = {\n",
    "    'iterations': 3302,\n",
    "    'depth': 5,\n",
    "    'learning_rate': 0.017183208677599107,\n",
    "    'random_strength': 41,\n",
    "    'l2_leaf_reg': 30,\n",
    "    'border_count': 251,\n",
    "    'bagging_temperature': 9.898390369028036, \n",
    "    'od_type': 'IncToDec'\n",
    "}\n",
    "    \n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "            task_type=model_config['task_type'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "            random_state=random_state,\n",
    "            \n",
    "            **best_catboost_params\n",
    "        ) \n",
    "\n",
    "\n",
    "\n",
    "estimators_list = [\n",
    "#     ('xgboost', xgboost_model),\n",
    "#     ('lightgbm', lightgbm_model),\n",
    "    ('catboost', catboost_model)\n",
    "]\n",
    "\n",
    "# wandb.log({'estimators': estimators_list})\n",
    "\n",
    "final_estimator = LogisticRegression(max_iter=1000)\n",
    "exmodel_config['blender_final_estimator'] = str(final_estimator)\n",
    "exmodel_config['blender-passthrough'] = False\n",
    "\n",
    "blender = StackingClassifier(estimators=estimators_list,\n",
    "                             final_estimator=final_estimator,\n",
    "                             cv=5,\n",
    "                             stack_method='predict_proba',\n",
    "                             n_jobs=-1, # 4 is max allowable for CPU\n",
    "                             passthrough=exmodel_config['blender-passthrough'],\n",
    "                             verbose=1\n",
    "                            )\n",
    "\n",
    "\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aca21cef-74f4-4b23-a82e-8c35419dd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'blender-final_estimator': str(blender.final_estimator),\n",
    "#            'blender-final_estimator_params': str(blender.final_estimator.get_params()),\n",
    "           'blender-stack_mdethod': 'predict_proba',\n",
    "           'blender-cv': 5\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e82b11-2324-449a-8b85-58b4cbe9921c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting at 20210922_112912\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting fitting at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "blender.fit(X,y) # unsure of this -- given kwarg cv=5, is it producing the splits? Or do I have to somehow?\n",
    "print(f\"Fitting complete at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1b180-22cf-494b-8987-aec8918d9d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb.log({'xgboost_params':str(blender.estimators[0][1].get_params()),\n",
    "#            'lightgbm_params':str(blender.estimators[1][1].get_params()),\n",
    "# #            'catboost_params':str(blender.estimators[2][1].get_all_params()),\n",
    "#           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8076f3-d1dc-4bd1-80ad-0eb29cc5bd00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = Path(datapath/f\"models/{config_run['name']}/\")\n",
    "(model_path).mkdir(exist_ok=True)\n",
    "dump(blender, filename=model_path/f\"{config_run['name']}_stack.joblib\")\n",
    "print(f\"Blender model saved at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b57e6-e4d7-4cb2-aa5d-7757a8d6d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = blender.predict_proba(X)[:,1]\n",
    "train_loss = log_loss(y_pred=train_preds, y_true=y)\n",
    "train_auc = roc_auc_score(y, train_preds)\n",
    "wandb.log({'train_loss': train_loss, 'train_auc': train_auc})\n",
    "print(f\"train_loss is {train_loss}, train_auc is {train_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec8f4d-ef7e-4382-b5c0-a960099a95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c226e-1ef7-4e03-91a1-06fbb73139f0",
   "metadata": {},
   "source": [
    "# Test set preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ec520-259f-44d2-ad33-7b8c22621132",
   "metadata": {},
   "source": [
    "(Here's where encapsulating the transformations in a pipeline would come in handy. But I'll do it manually for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec74e4-ccb8-43b4-b910-3df1542aaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [x for x in test_df.columns if x != 'claim']\n",
    "# X_test = test_df[features] # this is just for naming consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725cd3e-f883-4d20-837a-9f557b2122a9",
   "metadata": {},
   "source": [
    "Now, let's get the features the model was trained on and subset the test set's features accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66e579-7f01-46e5-a47c-580c8f5d678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation polynomial features\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)\n",
    "# X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1817e3a-7d90-4bc2-8c47-e97806f7dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_poly_names = poly.get_feature_names(X_test.columns)\n",
    "# X_poly_names[100:150]\n",
    "# features = pd.read_csv('X_candidates_20210827.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6bc49-d478-4f59-84d2-5e23e3e236db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks = [feature in X_test_poly_names for feature in features]\n",
    "# checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68187e-271a-4df1-ae02-a2bb5d62c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_final = pd.DataFrame(X_test_poly, columns=X_test_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020ad9b-1b05-49b8-b89b-c90362c256d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_final = X_test_final[features[1:]]\n",
    "# X_test_final = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07699efa-37df-4ed9-aaf2-1b77a73f9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['nan_count'] = X_test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd0c2d-7f9a-4fb6-8b4d-c3c51a9ef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = SimpleImputer(strategy='median', add_indicator=True)\n",
    "# X_test_imputed_np = imputer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ef600-e3cd-44ef-ae66-e1d1663ec4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_imputed = pd.DataFrame(X_test_imputed, columns=[str(x) for x in range(X_test_imputed.shape[1])])\n",
    "# X_test_imputed.to_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da840a-caa6-4d76-a542-c1315a593346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = exmodel_config['scaler']()\n",
    "# X_test_imputed_scaled_np = scaler.fit_transform(X_test_imputed)\n",
    "# X_test_imputed_scaled = pd.DataFrame(X_test_imputed_scaled_np, columns=X_test_imputed.columns)\n",
    "# X_test_imputed_scaled.to_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators_StandardScaled.feather')\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=X_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c062597-20aa-4054-accf-4da20a1400bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path = str(datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')\n",
    "wandb.log({'test_set': test_set_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99cf5b-3477-47f4-9391-73e2ff93c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_imputed_scaled = pd.read_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators_StandardScaled.feather')\n",
    "X_test_imputed_scaled = pd.read_feather(path=datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f347f-872f-4011-9f13-78fad542f36a",
   "metadata": {},
   "source": [
    "## Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773d286-257d-4776-add5-0f724944befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = Path(datapath/\"preds/\")\n",
    "\n",
    "blender_preds = blender.predict_proba(X_test_imputed_scaled)[:,1]\n",
    "dump(blender_preds, preds_path/f\"{config_run['name']}_stack.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719290c6-a10f-4506-8eeb-6a4bbf281b3d",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a464a1c-9ca8-4a07-9cdb-18af399cf95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f4b8-2356-4916-a091-45793db784ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'claim'] = blender_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957ce26-bbf5-4aee-bccd-988f2471db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f6d45-6b2a-4fe8-be77-ecc70cec6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = datapath/'submissions'\n",
    "submission_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ae726f8-0c8e-46ab-bba1-cff095978d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(submission_path/f\"{config_run['name']}_blended.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9672937c-18a0-4eaf-a3f1-54d3a779d672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f227c7b81c0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str(blender.estimators[2][1].get_all_params())\n",
    "# blender.estimators[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4e34083-9f47-4583-b4f3-c10b4f3311d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'leaderboard_auc': ,\n",
    "           'catboost_params': str(best_catboost_params),\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b08b089-292e-43a1-87f1-9d46ac917ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2327090<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_055836-17fcrnlu/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_055836-17fcrnlu/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>blender-final_estimator</td><td>LogisticRegression(m...</td></tr><tr><td>blender-stack_mdethod</td><td>predict_proba</td></tr><tr><td>blender-cv</td><td>5</td></tr><tr><td>_runtime</td><td>16221</td></tr><tr><td>_timestamp</td><td>1632331738</td></tr><tr><td>_step</td><td>4</td></tr><tr><td>xgboost_params</td><td>{'objective': 'binar...</td></tr><tr><td>lightgbm_params</td><td>{'boosting_type': 'g...</td></tr><tr><td>train_loss</td><td>0.48961</td></tr><tr><td>train_auc</td><td>0.84839</td></tr><tr><td>test_set</td><td>/media/sf/easystore/...</td></tr><tr><td>leaderboard_auc</td><td>0.81671</td></tr><tr><td>catboost_params</td><td>{'iterations': 3302,...</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>blender-cv</td><td>▁</td></tr><tr><td>_runtime</td><td>▁████</td></tr><tr><td>_timestamp</td><td>▁████</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>train_auc</td><td>▁</td></tr><tr><td>leaderboard_auc</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">stacking_off-shelf_20210922_055836</strong>: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/17fcrnlu\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/17fcrnlu</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afb61b-8022-4f0f-8996-2adfb3ec640c",
   "metadata": {},
   "source": [
    "## Manual Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1524723c-87c3-410f-863b-9387ef5b59e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.425545</td>\n",
       "      <td>-2.357891</td>\n",
       "      <td>-0.637206</td>\n",
       "      <td>-0.866657</td>\n",
       "      <td>-0.111568</td>\n",
       "      <td>-4.829243</td>\n",
       "      <td>-1.171229</td>\n",
       "      <td>-0.603397</td>\n",
       "      <td>-0.596871</td>\n",
       "      <td>-0.516828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247600</td>\n",
       "      <td>-0.323982</td>\n",
       "      <td>1.223569</td>\n",
       "      <td>0.361863</td>\n",
       "      <td>1.071182</td>\n",
       "      <td>-0.361140</td>\n",
       "      <td>0.082051</td>\n",
       "      <td>-0.746590</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.469668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.032371</td>\n",
       "      <td>-2.435680</td>\n",
       "      <td>-0.488960</td>\n",
       "      <td>0.341193</td>\n",
       "      <td>1.069656</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.537069</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>-0.763516</td>\n",
       "      <td>1.056879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.438373</td>\n",
       "      <td>-2.337605</td>\n",
       "      <td>-0.508914</td>\n",
       "      <td>-0.829607</td>\n",
       "      <td>1.485682</td>\n",
       "      <td>3.592008</td>\n",
       "      <td>-1.189087</td>\n",
       "      <td>-0.339152</td>\n",
       "      <td>-0.735281</td>\n",
       "      <td>-0.529158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602333</td>\n",
       "      <td>1.076218</td>\n",
       "      <td>-0.648438</td>\n",
       "      <td>0.463365</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>-0.157989</td>\n",
       "      <td>0.727338</td>\n",
       "      <td>-0.905498</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>-0.511066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "id                                                                         \n",
       "0   0.425545 -2.357891 -0.637206 -0.866657 -0.111568 -4.829243 -1.171229   \n",
       "1   0.247600 -0.323982  1.223569  0.361863  1.071182 -0.361140  0.082051   \n",
       "2   2.032371 -2.435680 -0.488960  0.341193  1.069656  0.118532  0.537069   \n",
       "3   1.438373 -2.337605 -0.508914 -0.829607  1.485682  3.592008 -1.189087   \n",
       "4   0.602333  1.076218 -0.648438  0.463365  0.275053 -0.157989  0.727338   \n",
       "\n",
       "           7         8         9  ...       227       228       229       230  \\\n",
       "id                                ...                                           \n",
       "0  -0.603397 -0.596871 -0.516828  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "1  -0.746590  0.899454  0.469668  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "2  -0.044075 -0.763516  1.056879  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "3  -0.339152 -0.735281 -0.529158  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "4  -0.905498  0.052478 -0.511066  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "\n",
       "         231       232       233      234       235      236  \n",
       "id                                                            \n",
       "0  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "1  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "2  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "3  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "4  -0.127119 -0.127985 -0.128494 -0.12862  7.821398 -0.12703  \n",
       "\n",
       "[5 rows x 237 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fe341d69-270a-445e-9a05-287cd9f12c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0a114476-71cb-4ef2-8f1f-2d8b2c837d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 237)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8dd525a7-3254-4e0c-8295-8871ffadd39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# generate probability predictions for the XGBoost model's folds\n",
    "for fold in xgboost_models.keys():\n",
    "#     X1[f\"xgboost_fold{fold}_pred\"] = xgboost_models[fold].predict(X)\n",
    "    X1[f\"xgboost_fold{fold}_pred\"] = xgboost_models[fold].predict_proba(X)[:,1]\n",
    "#     xgboost_preds[fold] = xgboost_models[fold].predict(X_test_imputed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eff33be8-e342-42e8-843c-75e1acd96120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>xgboost_fold0_pred</th>\n",
       "      <th>xgboost_fold1_pred</th>\n",
       "      <th>xgboost_fold2_pred</th>\n",
       "      <th>xgboost_fold3_pred</th>\n",
       "      <th>xgboost_fold4_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.425545</td>\n",
       "      <td>-2.357891</td>\n",
       "      <td>-0.637206</td>\n",
       "      <td>-0.866657</td>\n",
       "      <td>-0.111568</td>\n",
       "      <td>-4.829243</td>\n",
       "      <td>-1.171229</td>\n",
       "      <td>-0.603397</td>\n",
       "      <td>-0.596871</td>\n",
       "      <td>-0.516828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.582566</td>\n",
       "      <td>0.580950</td>\n",
       "      <td>0.576743</td>\n",
       "      <td>0.569523</td>\n",
       "      <td>0.595877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247600</td>\n",
       "      <td>-0.323982</td>\n",
       "      <td>1.223569</td>\n",
       "      <td>0.361863</td>\n",
       "      <td>1.071182</td>\n",
       "      <td>-0.361140</td>\n",
       "      <td>0.082051</td>\n",
       "      <td>-0.746590</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.469668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.152252</td>\n",
       "      <td>0.150803</td>\n",
       "      <td>0.148316</td>\n",
       "      <td>0.155218</td>\n",
       "      <td>0.147297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.032371</td>\n",
       "      <td>-2.435680</td>\n",
       "      <td>-0.488960</td>\n",
       "      <td>0.341193</td>\n",
       "      <td>1.069656</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.537069</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>-0.763516</td>\n",
       "      <td>1.056879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.794083</td>\n",
       "      <td>0.789945</td>\n",
       "      <td>0.788326</td>\n",
       "      <td>0.787177</td>\n",
       "      <td>0.797979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.438373</td>\n",
       "      <td>-2.337605</td>\n",
       "      <td>-0.508914</td>\n",
       "      <td>-0.829607</td>\n",
       "      <td>1.485682</td>\n",
       "      <td>3.592008</td>\n",
       "      <td>-1.189087</td>\n",
       "      <td>-0.339152</td>\n",
       "      <td>-0.735281</td>\n",
       "      <td>-0.529158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.774001</td>\n",
       "      <td>0.768510</td>\n",
       "      <td>0.774555</td>\n",
       "      <td>0.782187</td>\n",
       "      <td>0.773245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602333</td>\n",
       "      <td>1.076218</td>\n",
       "      <td>-0.648438</td>\n",
       "      <td>0.463365</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>-0.157989</td>\n",
       "      <td>0.727338</td>\n",
       "      <td>-0.905498</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>-0.511066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.759366</td>\n",
       "      <td>0.755764</td>\n",
       "      <td>0.763769</td>\n",
       "      <td>0.758034</td>\n",
       "      <td>0.758038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "id                                                                         \n",
       "0   0.425545 -2.357891 -0.637206 -0.866657 -0.111568 -4.829243 -1.171229   \n",
       "1   0.247600 -0.323982  1.223569  0.361863  1.071182 -0.361140  0.082051   \n",
       "2   2.032371 -2.435680 -0.488960  0.341193  1.069656  0.118532  0.537069   \n",
       "3   1.438373 -2.337605 -0.508914 -0.829607  1.485682  3.592008 -1.189087   \n",
       "4   0.602333  1.076218 -0.648438  0.463365  0.275053 -0.157989  0.727338   \n",
       "\n",
       "           7         8         9  ...       232       233      234       235  \\\n",
       "id                                ...                                          \n",
       "0  -0.603397 -0.596871 -0.516828  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "1  -0.746590  0.899454  0.469668  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "2  -0.044075 -0.763516  1.056879  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "3  -0.339152 -0.735281 -0.529158  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "4  -0.905498  0.052478 -0.511066  ... -0.127985 -0.128494 -0.12862  7.821398   \n",
       "\n",
       "        236  xgboost_fold0_pred  xgboost_fold1_pred  xgboost_fold2_pred  \\\n",
       "id                                                                        \n",
       "0  -0.12703            0.582566            0.580950            0.576743   \n",
       "1  -0.12703            0.152252            0.150803            0.148316   \n",
       "2  -0.12703            0.794083            0.789945            0.788326   \n",
       "3  -0.12703            0.774001            0.768510            0.774555   \n",
       "4  -0.12703            0.759366            0.755764            0.763769   \n",
       "\n",
       "    xgboost_fold3_pred  xgboost_fold4_pred  \n",
       "id                                          \n",
       "0             0.569523            0.595877  \n",
       "1             0.155218            0.147297  \n",
       "2             0.787177            0.797979  \n",
       "3             0.782187            0.773245  \n",
       "4             0.758034            0.758038  \n",
       "\n",
       "[5 rows x 242 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac245be1-3a2d-4c84-b83e-7224e4b13194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
