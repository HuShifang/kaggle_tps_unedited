{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e815d3-d755-4fa2-85a5-ea4df4948fcd",
   "metadata": {},
   "source": [
    "# 20220114\n",
    "Trying to follow Yam Peleg's Time Series Transformer + Time2Vec implementation (which is in Keras -- see [here](https://www.kaggle.com/yamqwe/tutorial-time-series-transformer-time2vec)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55dfc634-98f0-45b8-bf65-931d28ddb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "# if '/sf/' in pwd:\n",
    "#     COLAB, SAGE = False, False\n",
    "# elif 'google.colab' in str(get_ipython()):\n",
    "#     COLAB, SAGE = True, False # do colab-specific installs later\n",
    "# else:\n",
    "#     COLAB, SAGE = False, True\n",
    "    \n",
    "CONTEXT = 'local' # or 'colab', 'sage', 'kaggle'\n",
    "USE_GPU = True \n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a1fae-dd27-45ff-a494-d3572d5893dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c99bcc4-0451-4896-ba41-78477c3a890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests # for telegram notifications\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720163c-934a-472a-bdef-40de5b849b3e",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf5b0c8-8b46-41d2-8d6e-e017bfcc5540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model selection\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "# metrics\n",
    "# from sklearn.metrics import accuracy_score#, log_loss, roc_auc_score\n",
    "\n",
    "# eda\n",
    "import missingno\n",
    "# import doubtlab \n",
    "\n",
    "# data cleaning\n",
    "# from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "# import cleanlab\n",
    "\n",
    "# normalization\n",
    "# from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\n",
    "# from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "# feature generation\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# import category_encoders as ce\n",
    "\n",
    "# models\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "\n",
    "# feature reduction\n",
    "# from sklearn.decomposition import PCA\n",
    "# from umap import UMAP\n",
    "\n",
    "# clustering\n",
    "# from sklearn.cluster import DBSCAN, KMeans\n",
    "# import hdbscan\n",
    "\n",
    "# feature selection\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "# import featuretools as ft\n",
    "# from BorutaShap import BorutaShap\n",
    "# from boruta import BorutaPy\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"nb_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58ef079-2b2c-4433-a3c0-f62fdbbcac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "# widedeep\n",
    "# from pytorch_widedeep import Trainer\n",
    "# from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "# from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT#, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "# from pytorch_widedeep.metrics import Accuracy\n",
    "# from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee770b73-8343-4336-889e-f54d6b1e35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series\n",
    "# import tsfresh\n",
    "\n",
    "# import darts\n",
    "# from darts import TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968f484e-7cde-4dcf-910b-3038b6f7163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from darts.models import ExponentialSmoothing, AutoARIMA, ARIMA, Prophet, RandomForest, RegressionEnsembleModel, RegressionModel, TFTModel, TCNModel, TransformerModel, NBEATSModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188812f6-26a6-4b9c-b018-094861c5c077",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5b480-e54e-4e88-8826-8b453d3b9cd4",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81678474-39ce-4416-9de5-4ca973454e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTEXT == 'colab':\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    # datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/dec2021/')\n",
    "    root = Path('') # TODO\n",
    "\n",
    "elif CONTEXT == 'sage':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "elif CONTEXT == 'kaggle':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "else: # if on local machine\n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'studies'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath, studypath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1c918-7f76-4f7b-b194-7b9e19e4b87e",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9443f858-8b66-4a01-a9ab-aaaf32c54186",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything but the models\n",
    "def seed_everything(seed, pytorch=True, reproducible=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if pytorch:\n",
    "        torch.manual_seed(seed) # set torch CPU seed\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed) # set torch GPU(s) seed(s)\n",
    "        if reproducible and torch.backends.cudnn.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb8923c-bf74-4de6-bffc-e041b8dab680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Function to reduce memory usage by downcasting datatypes in a Pandas DataFrame when possible.\n",
    "    \n",
    "    h/t to Bryan Arnold (https://www.kaggle.com/puremath86/label-correction-experiments-tps-nov-21)\n",
    "    \"\"\"\n",
    "    \n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d92853-fc1b-48bc-9236-2777b2209570",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_api_token = 'your_api_token' # for Galileo (jupyter_watcher_bot) on Telegram\n",
    "tg_chat_id = 'your_chat_id'\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_tg_message(text='Cell execution completed.'):  \n",
    "    \"\"\"\n",
    "    h/t Ivan Dembicki Jr. for the base version \n",
    "    (https://medium.com/@ivan.dembicki.jr/notifications-in-jupyter-notebook-with-telegram-f2e892c55173)\n",
    "    \"\"\"\n",
    "    requests.post('https://api.telegram.org/' +  'bot{}/sendMessage'.format(tg_api_token),\n",
    "                  params=dict(chat_id=tg_chat_id, text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ac144e-1440-40cf-b172-bff04134d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(y_true, y_pred):\n",
    "    '''\n",
    "    h/t Jean-François Puget (@CPMP) -- see https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\n",
    "    '''\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9054b5-493b-41e4-8918-50d155c7b859",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9479abf7-0a80-4a3a-8fff-ba4504813236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'train.csv'),\n",
    "    'target_source': str(datapath/'train.csv'),\n",
    "    'test_source': str(datapath/'test.csv'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2e1b1-a413-4b47-9548-2e4f14ae4b24",
   "metadata": {},
   "source": [
    "Following Yam Peleg, I'll combine all the data together for ease of transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca45fe59-ca24-42fa-a2a0-d3080905fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80230b23-135b-4b96-9315-56e6ab47ed11",
   "metadata": {},
   "source": [
    "Now, I'll manually create time features -- no holidays (at least for now), however. Perhaps later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "403bf64d-19df-4cd1-97ef-726c2d51bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['date'] = pd.to_datetime(all_data['date'])\n",
    "all_data['year'] = all_data['date'].dt.year\n",
    "all_data['month'] = all_data['date'].dt.month\n",
    "all_data['day'] = all_data['date'].dt.day\n",
    "all_data['dayofweek'] = all_data['date'].dt.dayofweek\n",
    "all_data['dayofmonth'] = all_data['date'].dt.days_in_month\n",
    "all_data['dayofyear'] = all_data['date'].dt.dayofyear\n",
    "all_data['weekday'] = all_data['date'].dt.weekday\n",
    "all_data['weekofyear'] = all_data['date'].dt.weekofyear\n",
    "all_data.drop(columns = ['num_sold', 'date', 'row_id'], inplace = True)\n",
    "all_data = pd.get_dummies(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b23c226-51a2-4548-a390-c040bf0d2bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear',\n",
       "       'weekday', 'weekofyear', 'country_Finland', 'country_Norway',\n",
       "       'country_Sweden', 'store_KaggleMart', 'store_KaggleRama',\n",
       "       'product_Kaggle Hat', 'product_Kaggle Mug', 'product_Kaggle Sticker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecd3c8-aecc-4e95-a3f0-8be3fa977766",
   "metadata": {},
   "source": [
    "- Note that the call to `pd.get_dummies` eliminates the object/string categorical columns, replacing them with one-hots -- it doesn't simply add the one-hots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13df9dbc-875e-48d2-91dd-2f7b1beefeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['num_sold'].values\n",
    "y_orig = train_df['num_sold'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aae116-dbf1-4575-8056-a68fccab6d21",
   "metadata": {},
   "source": [
    "Now, we break the dataset back apart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9df13e14-463c-4e7c-84de-5de34c3a9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_data[:len(train_df)]\n",
    "test_df = all_data[len(train_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a902b83-afd1-4a7e-94e4-a58030f62c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26298, 6570)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384426de-b9ff-4a90-933a-d643850998d0",
   "metadata": {},
   "source": [
    "Now, we define the rolling window helper function, which will help construct a vector suitable for the DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43fa0902-08ad-4e4e-97d2-1662b7b8b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(df, y = None, window_size = 10):\n",
    "    all_features, all_targets = [], []\n",
    "    for i in range(0, len(df) - window_size):\n",
    "        all_features.append(np.expand_dims(df[i: i + window_size].values, axis = 0))\n",
    "        if y is not None: all_targets.append(np.expand_dims(y[i + window_size], axis = 0))\n",
    "    if y is not None: return np.concatenate(all_features, axis = 0), np.concatenate(all_targets, axis = 0)\n",
    "    else: return np.concatenate(all_features, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73274436-5747-43bf-a4df-4dcc9338e98f",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc7bff-b510-463d-b40d-bd9e826bea96",
   "metadata": {},
   "source": [
    "Now, let's define the Transformer Block. Some notes...\n",
    "- The `nn.MultiheadAttention` layer expects an `embed_dim=` argument; the `kdim=` argument defaults to `None`, which is to say it sets `kdim=` to the value `embed_dim`. So, I think my code below is equivalent to the Keras `self.att = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)`.\n",
    "- `nn.Linear` is equivalent to Keras's `layers.Dense`, with two difference: 1) you have to explicitly give an input dimension to PyTorch, whereas in Keras you only have to supply an output dimension, and 2) you separate out the activation function in PyTorch. I think, then that my `nn.Linear(in_features=embed_dim, out_features=ff_dim)` followed by `nn.GeLU` is equivalent to Yam Peleg's `layers.Dense(ff_dim, activation = \"gelu\")`.\n",
    "- Not sure of the `nn.BatchNorm2d`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58679647-edc9-42f7-871b-5041946bd3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D\n",
       "inputs with optional additional channel dimension) as described in the paper\n",
       "`Batch Normalization: Accelerating Deep Network Training by Reducing\n",
       "Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n",
       "\n",
       ".. math::\n",
       "\n",
       "    y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
       "\n",
       "The mean and standard-deviation are calculated per-dimension over\n",
       "the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n",
       "of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\n",
       "to 1 and the elements of :math:`\\beta` are set to 0. The standard-deviation is calculated\n",
       "via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.\n",
       "\n",
       "Also by default, during training this layer keeps running estimates of its\n",
       "computed mean and variance, which are then used for normalization during\n",
       "evaluation. The running estimates are kept with a default :attr:`momentum`\n",
       "of 0.1.\n",
       "\n",
       "If :attr:`track_running_stats` is set to ``False``, this layer then does not\n",
       "keep running estimates, and batch statistics are instead used during\n",
       "evaluation time as well.\n",
       "\n",
       ".. note::\n",
       "    This :attr:`momentum` argument is different from one used in optimizer\n",
       "    classes and the conventional notion of momentum. Mathematically, the\n",
       "    update rule for running statistics here is\n",
       "    :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n",
       "    where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n",
       "    new observed value.\n",
       "\n",
       "Because the Batch Normalization is done over the `C` dimension, computing statistics\n",
       "on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.\n",
       "\n",
       "Args:\n",
       "    num_features: :math:`C` from an expected input of size\n",
       "        :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`\n",
       "    eps: a value added to the denominator for numerical stability.\n",
       "        Default: 1e-5\n",
       "    momentum: the value used for the running_mean and running_var\n",
       "        computation. Can be set to ``None`` for cumulative moving average\n",
       "        (i.e. simple average). Default: 0.1\n",
       "    affine: a boolean value that when set to ``True``, this module has\n",
       "        learnable affine parameters. Default: ``True``\n",
       "    track_running_stats: a boolean value that when set to ``True``, this\n",
       "        module tracks the running mean and variance, and when set to ``False``,\n",
       "        this module does not track such statistics, and initializes statistics\n",
       "        buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n",
       "        When these buffers are ``None``, this module always uses batch statistics.\n",
       "        in both training and eval modes. Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
       "    - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # With Learnable Parameters\n",
       "    >>> m = nn.BatchNorm1d(100)\n",
       "    >>> # Without Learnable Parameters\n",
       "    >>> m = nn.BatchNorm1d(100, affine=False)\n",
       "    >>> input = torch.randn(20, 100)\n",
       "    >>> output = m(input)\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/time/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.BatchNorm1d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3e87473-1ff4-49f9-a38e-9ddc5fd5ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, feat_dim, num_heads=8, ff_dim=256, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim, out_features=ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=ff_dim, out_features=feat_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.BatchNorm1d() # or 2d?\n",
    "        self.layernorm2 = nn.BatchNorm1d() # or 2d?\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.dropout2 = nn.Dropout()\n",
    "        \n",
    "    def forward(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        # TODO:finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87c153-3811-44a0-8889-5695fd7531cb",
   "metadata": {},
   "source": [
    "I'm adapting `Time2Vec` from [this @ojus1 repo](https://github.com/ojus1/Time2Vec-PyTorch) rather than trying to clone Yam Peleg's. He implements a function that expresses the linear combination of $\\omega_i \\tau + \\varphi_i$ for when $i=0$ and then wraps it, if appropriate, in a periodic activation function $\\mathcal{F}$ -- which can be either sine or cosine. Recall that the full equation is $$t2v(\\tau)[i] = \\left\\{\\begin{array}{ll} \\omega_i\\tau + \\varphi_i & \\text{if }i=0 \\\\ \\mathcal{F}(\\omega_i \\tau + \\varphi_i) & \\text{if } 1 \\leq i \\leq k\\end{array}\\right.$$where $k$ is the time2vec dimension (that is, the dimensionality of the embedding in latent space), $\\tau$ is a raw [[time series]], $\\mathcal{F}$ is a periodic [[activation function]], and $\\omega$ and $\\varphi$ are learnable params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad6fa613-4be6-47df-8e52-899b66fba3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a672ca3-ff70-4d8a-8f49-d29d2f418079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sin(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c940dfeb-3603-4523-8697-0a45a261f2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.cos(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "196ea061-84ec-4551-9263-7c457d095fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2v(tau, f, out_features, w, b, w0, b0, arg=None):\n",
    "    # https://github.com/ojus1/Time2Vec-PyTorch/blob/master/periodic_activations.py\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        #print(w.shape, t1.shape, b.shape)\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "    #print(v1.shape)\n",
    "    return torch.cat([v1, v2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3194c9aa-8fd5-44bd-9ecf-6fde88b29a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineActivation(nn.Module):\n",
    "    # https://github.com/ojus1/Time2Vec-PyTorch/blob/master/periodic_activations.py\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "859d5a35-c84d-4f67-9062-bdb8e2505906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineActivation(nn.Module):\n",
    "    # https://github.com/ojus1/Time2Vec-PyTorch/blob/master/periodic_activations.py\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CosineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be356a58-e2c3-4edb-b34e-eb1935343937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     sineact = SineActivation(1, 64)\n",
    "#     cosact = CosineActivation(1, 64)\n",
    "\n",
    "#     print(sineact(torch.Tensor([[7]])).shape)\n",
    "#     print(cosact(torch.Tensor([[7]])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2a5c6-4df7-4c3f-886f-1195c91fbe60",
   "metadata": {},
   "source": [
    "The wrapper then looks like so (from [here](https://github.com/ojus1/Time2Vec-PyTorch/blob/master/Model.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d96101d-44cb-4faa-89ec-51f1ab562773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(nn.Module):\n",
    "    # https://github.com/ojus1/Time2Vec-PyTorch/blob/master/Model.py\n",
    "    def __init__(self, activation, hidden_dim):\n",
    "        super(Time2Vec, self).__init__()\n",
    "        if activation == 'sin':\n",
    "            self.l1 = SineActivation(1, hidden_dim)\n",
    "        elif activation == 'cos':\n",
    "            self.l1 = CosineActivation(1, hidden_dim)\n",
    "        # note that if no activation function is supplied, the linear combo is used\n",
    "        self.fc1 = nn.Linear(hidden_dim, 2) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x = x.unsqueeze(1) # for testing without batch numbers\n",
    "        x = self.l1(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f59700-6ff7-4b43-a758-4f6981600208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_params['feature_count'] = X.shape[1]\n",
    "# dataset_params['instance_count'] = X.shape[0]\n",
    "\n",
    "# # might eventually shift from dict to tuple\n",
    "\n",
    "# # simplest approach: k-v where key is new feature, v is string with the operation to get it\n",
    "# # sacrifices sortability, but could recover that through regexes, and it's much quicker to input\n",
    "# dataset_params['feature_combinations'] = {\n",
    "#     'EHiElv': \"df['Horizontal_Distance_To_Roadways'] * df['Elevation']\",\n",
    "#     'EViElv': \"df['Vertical_Distance_To_Hydrology'] * df['Elevation']\",\n",
    "#     'EVDtH': \"df.Elevation - df.Vertical_Distance_To_Hydrology\",\n",
    "#     'EHDtH': \"df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\",\n",
    "#     'Euclidean_Distance_to_Hydrology': \"(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\",\n",
    "#     'Manhattan_Distance_to_Hydrology': \"df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\",\n",
    "#     'Hydro_Fire_1': \"df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\",\n",
    "#     'Hydro_Fire_2': \"abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\",\n",
    "#     'Hydro_Road_1': \"abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\",\n",
    "#     'Hydro_Road_2': \"abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\",\n",
    "#     'Fire_Road_1': \"abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\",\n",
    "#     'Fire_Road_2': \"abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\"\n",
    "# }\n",
    "\n",
    "# dataset_params['feature_clipping'] = [\n",
    "#     {\n",
    "#         'features': ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm'],\n",
    "#         'range': range(0,256)\n",
    "#     },\n",
    "#     {\n",
    "#         'features': ['Aspect'],\n",
    "#         'range': range(0,360)\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # the features that are just getting the one-hots counted\n",
    "# dataset_params['feature_counts'] = ['Soil_Type*', 'Wilderness_Area*']\n",
    "# dataset_params['feature_complements'] = [\n",
    "#     {\n",
    "#         'old': 'Aspect', \n",
    "#         'new': 'Aspect2',\n",
    "#         'operation': 'If x < 180 return x-180, else return x + 180'\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# dataset_params['feature_indicators'] = {\n",
    "#     'Hillshade_3pm_is_zero': \"(df.Hillshade_3pm == 0).astype(int)\",\n",
    "# }\n",
    "\n",
    "# dataset_params['feature_typecasting'] = {\n",
    "#     'Highwater': \"(df.Vertical_Distance_To_Hydrology < 0).astype(int)\"\n",
    "# }\n",
    "\n",
    "# dataset_params['feature_encodings'] = \"Soil_Type* features concatenated into single 40-bit integers and then five 8-bit integers, and finally to five decimals; see gbms_20211223.ipynb and the section 'Encoding the `Soil_Type` Features'.\"\n",
    "# dataset_params['feature_removals'] = \"Soil_Type* features removed after being encoded\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c4ac3-7332-439d-845d-75221d131842",
   "metadata": {},
   "source": [
    "## Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18755070-b567-48a6-bfb6-074c057316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params = {\n",
    "#     'general_random_state': SEED,\n",
    "# }\n",
    "\n",
    "# folds = 5\n",
    "# training_params['cross_val_strategy'] = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d2cd7-cd67-47b7-af1f-463bd1550cd9",
   "metadata": {},
   "source": [
    "## Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda3fa7-d8a5-48ac-8bfc-8aca81a11225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc67f393-3cfd-43ba-8d44-a07770ea3dec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58824f5-e4f0-496e-94dd-c4ad74a2d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # baseline -- alter as needed later\n",
    "# exmodel_config = {\n",
    "#     'general_random_state': SEED,\n",
    "# #     'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "#     **dataset_params,\n",
    "# #     **training_params,\n",
    "# #     **model_params # perhaps do later\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a1208-433e-4438-8f98-3de95e93af12",
   "metadata": {},
   "source": [
    "## WandB Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8155982-c840-4ef5-a78d-15a7f31ed59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wandb config:\n",
    "# wandb_config = {\n",
    "#     'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "#     'tags': ['EDA'],\n",
    "#     'notes': \"EDA\"\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
