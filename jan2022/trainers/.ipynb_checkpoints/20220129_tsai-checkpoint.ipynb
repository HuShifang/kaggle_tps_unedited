{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa68fe7-8591-4ea3-a5cf-d2547c9b5c49",
   "metadata": {},
   "source": [
    "# tsai\n",
    "Trying fastai-based library for some quick & dirty NN implementations, for variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853e0cad-277b-4672-8932-00043098c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "# if '/sf/' in pwd:\n",
    "#     COLAB, SAGE = False, False\n",
    "# elif 'google.colab' in str(get_ipython()):\n",
    "#     COLAB, SAGE = True, False # do colab-specific installs later\n",
    "# else:\n",
    "#     COLAB, SAGE = False, True\n",
    "    \n",
    "CONTEXT = 'local' # or 'colab', 'sage', 'kaggle'\n",
    "USE_GPU = True \n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139e8f0-6387-4fb5-a095-f6bbbb67ce28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f92b26f-9097-4dfc-b570-9501aab4e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests # for telegram notifications\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392bc7f-da5d-4ff0-866f-184ee342ad49",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3602ce7e-5d45-406e-85f4-587a749a5a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'google.protobuf.descriptor' has no attribute '_internal_create_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a9e8f3201e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtermsetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msdk\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwandb_sdk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/wandb/sdk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_history\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHistory\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_attach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_login\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogin\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_require\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_login\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipython\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtelemetry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunDisabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummaryDisabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/wandb/sdk/backend/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterfaceBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface_queue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterfaceQueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_internal_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_telemetry_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtpb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m from wandb.util import (\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/wandb/proto/wandb_internal_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimestamp_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgoogle_dot_protobuf_dot_timestamp__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_base_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwandb_dot_proto_dot_wandb__base__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_telemetry_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwandb_dot_proto_dot_wandb__telemetry__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/wandb/proto/wandb_base_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0msyntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'proto3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mserialized_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mcreate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_create_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mserialized_pb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb'\\n\\x1cwandb/proto/wandb_base.proto\\x12\\x0ewandb_internal\\\" \\n\\x0b_RecordInfo\\x12\\x11\\n\\tstream_id\\x18\\x01 \\x01(\\t\\\"!\\n\\x0c_RequestInfo\\x12\\x11\\n\\tstream_id\\x18\\x01 \\x01(\\tb\\x06proto3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'"
     ]
    }
   ],
   "source": [
    "# model selection\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# normalization\n",
    "# from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\n",
    "# from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "# feature generation\n",
    "# import category_encoders as ce\n",
    "\n",
    "# models\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "# from pytorch_widedeep import Trainer\n",
    "# from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "# from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT#, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "# from pytorch_widedeep.metrics import Accuracy\n",
    "# from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint\n",
    "\n",
    "# feature reduction\n",
    "# from sklearn.decomposition import PCA\n",
    "# from umap import UMAP\n",
    "\n",
    "# clustering\n",
    "# from sklearn.cluster import DBSCAN, KMeans\n",
    "# import hdbscan\n",
    "\n",
    "# feature selection\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "# import featuretools as ft\n",
    "# from BorutaShap import BorutaShap\n",
    "# from boruta import BorutaPy\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"nb_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d37f7-8c08-468c-b29e-e7df26bd1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time series\n",
    "# import tsfresh\n",
    "\n",
    "# import darts\n",
    "# from darts import TimeSeries\n",
    "# from darts.models import ExponentialSmoothing, AutoARIMA, ARIMA, Prophet, RandomForest, RegressionEnsembleModel, RegressionModel, TFTModel, TCNModel, TransformerModel, NBEATSModel\n",
    "import holidays\n",
    "import dateutil.easter as easter\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "from tsai.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465787a5-af5f-4871-9e53-a5854f4774fd",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb8882-dc8a-482e-a1fa-2c264932b220",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5deced-4235-46ee-a06e-d0e7a5a08064",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTEXT == 'colab':\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    # datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/dec2021/')\n",
    "    root = Path('') # TODO\n",
    "\n",
    "elif CONTEXT == 'sage':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "elif CONTEXT == 'kaggle':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "else: # if on local machine\n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    modelpath = root/'models'\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'studies'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath, studypath, modelpath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449a96a-d63e-4add-9577-987bf9911616",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb71d9-e4ea-4a2e-a3b5-686442df3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything but the models\n",
    "def seed_everything(seed, pytorch=True, reproducible=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if pytorch:\n",
    "        torch.manual_seed(seed) # set torch CPU seed\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed) # set torch GPU(s) seed(s)\n",
    "        if reproducible and torch.backends.cudnn.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a439da1-41c5-4094-8c90-74a0997b9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Function to reduce memory usage by downcasting datatypes in a Pandas DataFrame when possible.\n",
    "    \n",
    "    h/t to Bryan Arnold (https://www.kaggle.com/puremath86/label-correction-experiments-tps-nov-21)\n",
    "    \"\"\"\n",
    "    \n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc37fac-1397-4afb-b935-2c35cc58cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_api_token = 'your_api_token' # for Galileo (jupyter_watcher_bot) on Telegram\n",
    "tg_chat_id = 'your_chat_id'\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_tg_message(text='Cell execution completed.'):  \n",
    "    \"\"\"\n",
    "    h/t Ivan Dembicki Jr. for the base version \n",
    "    (https://medium.com/@ivan.dembicki.jr/notifications-in-jupyter-notebook-with-telegram-f2e892c55173)\n",
    "    \"\"\"\n",
    "    requests.post('https://api.telegram.org/' +  'bot{}/sendMessage'.format(tg_api_token),\n",
    "                  params=dict(chat_id=tg_chat_id, text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663de45-2da6-4db3-9f38-bc80baac37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(y_true, y_pred):\n",
    "    '''\n",
    "    h/t Jean-Fran√ßois Puget (@CPMP) -- see https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\n",
    "    '''\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372674e6-6280-4ab0-a5b6-f19b4444448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/282735\n",
    "def better_than_median(inputs, axis):\n",
    "    \"\"\"Compute the mean of the predictions if there are no outliers,\n",
    "    or the median if there are outliers.\n",
    "\n",
    "    Parameter: inputs = ndarray of shape (n_samples, n_folds)\"\"\"\n",
    "    spread = inputs.max(axis=axis) - inputs.min(axis=axis) \n",
    "    spread_lim = 0.45\n",
    "    print(f\"Inliers:  {(spread < spread_lim).sum():7} -> compute mean\")\n",
    "    print(f\"Outliers: {(spread >= spread_lim).sum():7} -> compute median\")\n",
    "    print(f\"Total:    {len(inputs):7}\")\n",
    "    return np.where(spread < spread_lim,\n",
    "                    np.mean(inputs, axis=axis),\n",
    "                    np.median(inputs, axis=axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f50d0f-6057-444e-ab5c-72fd91f72fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series\n",
    "def plot_periodogram(ts, detrend='linear', ax=None):\n",
    "    from scipy.signal import periodogram\n",
    "    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n",
    "    freqencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend=detrend,\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.step(freqencies, spectrum, color=\"purple\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Annual (1)\",\n",
    "            \"Semiannual (2)\",\n",
    "            \"Quarterly (4)\",\n",
    "            \"Bimonthly (6)\",\n",
    "            \"Monthly (12)\",\n",
    "            \"Biweekly (26)\",\n",
    "            \"Weekly (52)\",\n",
    "            \"Semiweekly (104)\",\n",
    "        ],\n",
    "        rotation=30,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Periodogram\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb6805-b4d5-41fc-a842-376c08fb1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series\n",
    "def fourier_features(index, freq, order):\n",
    "    time = np.arange(len(index), dtype=np.float32)\n",
    "    k = 2 * np.pi * (1 / freq) * time\n",
    "    features = {}\n",
    "    for i in range(1, order + 1):\n",
    "        features.update({\n",
    "            f\"sin_{freq}_{i}\": np.sin(i * k),\n",
    "            f\"cos_{freq}_{i}\": np.cos(i * k),\n",
    "        })\n",
    "    return pd.DataFrame(features, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da19663-3814-44d6-ab45-2929d022e370",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac483f-9be5-4ba7-bf3d-f7fb9a215355",
   "metadata": {},
   "source": [
    "### Original Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a3fe1-e98f-49fb-baa7-2fedc384160f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'train.csv'),\n",
    "    'target_source': str(datapath/'train.csv'),\n",
    "    'test_source': str(datapath/'test.csv'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')\n",
    "orig_train_df = train_df.copy()\n",
    "orig_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0abf4a-32dd-435a-9551-172f1c110b64",
   "metadata": {},
   "source": [
    "Since the dates are natively `Object` dtype (i.e. strings), we have to convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd956b7-823b-4f3d-ae43-81d73349508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model\n",
    "for df in [train_df, test_df]:\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "# for convenience later\n",
    "countries = ['Sweden', 'Finland', 'Norway']\n",
    "stores = ['KaggleMart', 'KaggleRama']\n",
    "products = ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ae630-74dd-40fe-87dc-179f1c4ae6ea",
   "metadata": {},
   "source": [
    "Provisionally, I'm going to concatenate together the `train_df` and `test_df` for preprocessing, to avoid having to constantly apply transforms twice (since I don't anticipate doing any transforms that might allow data leakage to occur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88789a-e9dc-479a-bdc5-3404e0b82385",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "# all_df.columns\n",
    "print(len(all_df) == len(train_df) + len(test_df))\n",
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83e050-8476-4b03-9141-76547a5b2ab8",
   "metadata": {},
   "source": [
    "### GDP Data\n",
    "Here's data from Carl McBride Ellis ([notebook](https://www.kaggle.com/carlmcbrideellis/gdp-of-finland-norway-and-sweden-2015-2019) and [dataset](https://www.kaggle.com/carlmcbrideellis/gdp-20152019-finland-norway-and-sweden) for doing GDP comparisons. They're frequently used in other entries. I've created a function to add them on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecb693-a4f9-4834-8733-1124a1b2110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gdp_data(df):\n",
    "    gdp_df = pd.read_csv(datapath/'GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n",
    "    gdp_df.set_index('year', inplace=True)\n",
    "    def get_gdp(row):\n",
    "        country = 'GDP_' + row.country\n",
    "        return gdp_df.loc[row.date.year, country]\n",
    "\n",
    "    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea0d4e-1a00-4598-9d66-f7e5d09f034a",
   "metadata": {},
   "source": [
    "I'll also define here (but perhaps move later) the GDP exponent, which will be used to transform the targets before inference (dividing num_sold by the $GDP^{1.212}$ and then taking the logarithm (after @ambrosm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a8d45-49c0-41d7-9dd0-641bee6ba01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_exponent = 1.2121103201489674 # see https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model for an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cea17-4e3d-4b0f-ac9b-ed2bc261fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = add_gdp_data(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce44724-6d3e-4a11-9dd7-d30a6d1197b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac430c-1f44-4cab-9f60-591938496422",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001db50-b826-438e-b2a4-2e978348fb28",
   "metadata": {},
   "source": [
    "### Time Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcaf542-5bd7-4c06-8e6b-d07255b27976",
   "metadata": {},
   "source": [
    "The goal of this function is to create features that will capture seasonalities -- but **not** trends. The trends will (hopefully) be captured by the deployment of linear forecasting algorithms on raw time series data (consisting exclusively of dates and targets); we want to have seasonalities that the residual models can learn, however -- holidays, weekly patterns, climactic season patterns, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc192ce-81e6-4a63-a284-4d454e66cb6a",
   "metadata": {},
   "source": [
    "The cell below will generate the `holidays` library's entries for the three countries. I may want to follow the template of @teckmengwong's code below, and add more holidays -- then, do some feature importance checking, and perhaps whittle down the features accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b86a61-63ec-42c9-bbdd-6d762df291b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in [holidays.Finland, holidays.Sweden, holidays.Norway]:\n",
    "#     print(c)\n",
    "    for h in c(years = [2019], observed=True).items():\n",
    "#         print(h)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a1b6e-1d18-4d03-8484-da80e51746a3",
   "metadata": {},
   "source": [
    "Here are the new FE techniques and helper techniques proposed by Teck Meng Wong (added as alt on 20220129, from [here](https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series#Data/Feature-Engineering))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b12695-6f92-4524-b9e3-0fd64900282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor, sqrt\n",
    "# from https://www.kaggle.com/fergusfindley/ensembling-and-rounding-techniques-comparison\n",
    "def geometric_round(arr):\n",
    "    result_array = arr\n",
    "    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n",
    "    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n",
    "\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4d157-dbd0-4e6f-8a98-14a2ed464978",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = \"date\"\n",
    "YEAR = \"year\"\n",
    "QUARTER = \"quarter\"\n",
    "MONTH = \"month\"\n",
    "WEEK = \"week\"\n",
    "DAY = \"day\"\n",
    "DAYOFYEAR = \"dayofyear\"\n",
    "WEEKOFYEAR = \"weekofyear\"\n",
    "DAYOFMONTH = \"dayofMonth\"\n",
    "DAYOFWEEK = \"dayofweek\"\n",
    "WEEKDAY = \"weekday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c7650-509f-4deb-a4af-0eb1ade65fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "\n",
    "def periodic_spline_transformer(period, n_splines=None, degree=3):\n",
    "    if n_splines is None:\n",
    "        n_splines = period\n",
    "    n_knots = n_splines + 1  # periodic and include_bias is True\n",
    "    return SplineTransformer(\n",
    "        degree=degree,\n",
    "        n_knots=n_knots,\n",
    "        knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\n",
    "        extrapolation=\"periodic\",\n",
    "        include_bias=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272bde0-c9aa-4a9d-bba2-405b762bac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year_df = pd.DataFrame(\n",
    "    np.linspace(0, 365, 1000).reshape(-1, 1),\n",
    "    columns=[DAYOFYEAR],\n",
    ")\n",
    "splines = periodic_spline_transformer(365, n_splines=12, degree=2).fit_transform(year_df)\n",
    "splines_df = pd.DataFrame(\n",
    "    splines,\n",
    "    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\n",
    ")\n",
    "pd.concat([year_df, splines_df], axis=\"columns\").plot(x=DAYOFYEAR, cmap=plt.cm.tab20b)\n",
    "_ = plt.title(f\"Periodic spline-based encoding for the {DAYOFYEAR} feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91ecc4-7810-4d21-a455-beb78a322e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/samuelcortinhas/tps-jan-22-quick-eda-hybrid-model/notebook\n",
    "def unofficial_holiday(df):\n",
    "    countries = {'Finland': 1, 'Norway': 2, 'Sweden': 3}\n",
    "    stores = {'KaggleMart': 1, 'KaggleRama': 2}\n",
    "    products = {'Kaggle Mug': 1,'Kaggle Hat': 2, 'Kaggle Sticker': 3}\n",
    "    \n",
    "    # load holiday info.\n",
    "#     hol_path = '../input/public-and-unofficial-holidays-nor-fin-swe-201519/holidays.csv'\n",
    "    hol_path = datapath/'holidays.csv'\n",
    "    holiday = pd.read_csv(hol_path)\n",
    "    \n",
    "    fin_holiday = holiday.loc[holiday.country == 'Finland']\n",
    "    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n",
    "    nor_holiday = holiday.loc[holiday.country == 'Norway']\n",
    "    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n",
    "    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n",
    "    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n",
    "    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n",
    "    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n",
    "    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n",
    "    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n",
    "    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405a5ee-2f98-47c4-b9c5-d8815bfa3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUID calendar columns\n",
    "MONTH_COLUMNS = []\n",
    "WEEKOFYEAR_COLUMNS = []\n",
    "DAYOFYEAR_COLUMNS = []\n",
    "WEEKDAY_COLUMNS = []\n",
    "\n",
    "for x in [MONTH,WEEKOFYEAR,DAYOFYEAR,WEEKDAY]:\n",
    "    for y in [f'mug_{x}', f'hat_{x}', f'stick_{x}']:\n",
    "        if x == MONTH:\n",
    "            MONTH_COLUMNS.append(y)\n",
    "        if x == WEEKOFYEAR:\n",
    "            WEEKOFYEAR_COLUMNS.append(y)\n",
    "        if x == DAYOFYEAR:\n",
    "            DAYOFYEAR_COLUMNS.append(y)\n",
    "        if x == WEEKDAY:\n",
    "            WEEKDAY_COLUMNS.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e0d2f-aecb-448f-abb4-20f3862656ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_features(index, freq, order):\n",
    "    time = np.arange(len(index), dtype=np.float32)\n",
    "    k = 2 * np.pi * (1 / freq) * time\n",
    "    features = {}\n",
    "    for i in range(1, order + 1):\n",
    "        features.update({\n",
    "            f\"sin_{freq}_{i}\": np.sin(i * k),\n",
    "            f\"cos_{freq}_{i}\": np.cos(i * k),\n",
    "        })\n",
    "    return pd.DataFrame(features, index=index)\n",
    "\n",
    "def get_basic_ts_features(df):\n",
    "#     gdp_df = pd.read_csv('../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n",
    "    gdp_df = pd.read_csv(datapath/'GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n",
    "    gdp_df.set_index('year', inplace=True)\n",
    "#     gdp_exponent = 1.2121103201489674 # see https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model for an explanation\n",
    "    def get_gdp(row):\n",
    "        country = 'GDP_' + row.country\n",
    "        return gdp_df.loc[row.date.year, country] #**gdp_exponent\n",
    "\n",
    "    # Apply GDP log\n",
    "    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n",
    "    \n",
    "#     # Split GDP by country (for linear model)\n",
    "#     df['fin_gdp']=np.where(df['country'] == 'Finland', df['gdp'], 0)\n",
    "#     df['nor_gdp']=np.where(df['country'] == 'Norway', df['gdp'], 0)\n",
    "#     df['swe_gdp']=np.where(df['country'] == 'Sweden', df['gdp'], 0)\n",
    "    \n",
    "#     # Drop column\n",
    "#     df=df.drop(['gdp'],axis=1)\n",
    "    \n",
    "    # one-hot encoding should be used. linear model should not learn this as numeric value\n",
    "#     df[YEAR] = df[DATE].dt.year\n",
    "#     df[MONTH] = df[DATE].dt.month\n",
    "#     df[WEEKOFYEAR] = df[DATE].dt.isocalendar().week\n",
    "#     df[DAYOFYEAR] = df[DATE].dt.dayofyear\n",
    "#     df[WEEKDAY] = df[DATE].dt.weekday\n",
    "#     df[DAY] = df[DATE].dt.day # day in month\n",
    "#     df[DAYOFMONTH] = df[DATE].dt.days_in_month\n",
    "#     df[DAYOFWEEK] = df[DATE].dt.dayofweek\n",
    "#     df[MONTH] = df[DATE].dt.month # Min SMAPE: 4.005319478790032\n",
    "#     df[QUARTER] = df.date.dt.quarter\n",
    "\n",
    "    df['wd0'] = df[DATE].dt.weekday == 0 # + Monday\n",
    "    df['wd1'] = df[DATE].dt.weekday == 1 # Tuesday\n",
    "    df['wd2'] = df[DATE].dt.weekday == 2\n",
    "    df['wd3'] = df[DATE].dt.weekday == 3\n",
    "    df['wd4'] = df[DATE].dt.weekday == 4 # + Friday\n",
    "    df['wd56'] = df[DATE].dt.weekday >= 5 # + Weekend\n",
    "\n",
    "#     df[f'mug_wd4'] = np.where(df['product'] == 'Kaggle Mug', df[f'wd4'], False)\n",
    "#     df[f'mug_wd56'] = np.where(df['product'] == 'Kaggle Mug', df[f'wd56'], False)\n",
    "#     df[f'hat_wd4'] = np.where(df['product'] == 'Kaggle Hat', df[f'wd4'], False)\n",
    "#     df[f'hat_wd56'] = np.where(df['product'] == 'Kaggle Hat', df[f'wd56'], False)\n",
    "#     df[f'stick_wd4'] = np.where(df['product'] == 'Kaggle Sticker', df[f'wd4'], False)\n",
    "#     df[f'stick_wd56'] = np.where(df['product'] == 'Kaggle Sticker', df[f'wd56'], False)\n",
    "#     df = df.drop(columns=[f'wd4', f'wd56'])\n",
    "    # 4 seasons\n",
    "#     df['season'] = ((df[DATE].dt.month % 12 + 3) // 3).map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_splines(df):\n",
    "    # one-hot encoding should be used. linear model should not learn this as numeric value\n",
    "#     df[MONTH] = df[DATE].dt.month\n",
    "#     df[WEEKOFYEAR] = df[DATE].dt.isocalendar().week\n",
    "    df[WEEKDAY] = df[DATE].dt.weekday\n",
    "#     df[DAYOFYEAR] = df[DATE].dt.dayofyear\n",
    "    \n",
    "    dayofyear_splines = periodic_spline_transformer(365, n_splines=9, degree=2).fit_transform(df[DATE].dt.dayofyear.values.reshape(-1, 1))\n",
    "    splines_df = pd.DataFrame(\n",
    "        dayofyear_splines,\n",
    "        columns=[f\"spline_{i}\" for i in range(dayofyear_splines.shape[1])],\n",
    "    )\n",
    "    for i in range(dayofyear_splines.shape[1]):\n",
    "        df[f'mug_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Mug', splines_df[f\"spline_{i}\"], 0.)\n",
    "        df[f'hat_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Hat', splines_df[f\"spline_{i}\"], 0.)\n",
    "        df[f'stick_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Sticker', splines_df[f\"spline_{i}\"], 0.)\n",
    "#         df[f'fin_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Finland', splines_df[f\"spline_{i}\"], 0.)\n",
    "#         df[f'nor_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Norway', splines_df[f\"spline_{i}\"], 0.)\n",
    "#         df[f'swe_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Sweden', splines_df[f\"spline_{i}\"], 0.)\n",
    "\n",
    "#     weekofyear_splines = periodic_spline_transformer(52, n_splines=2, degree=2).fit_transform(df[DATE].dt.isocalendar().week.values.astype(np.float64).reshape(-1,1))\n",
    "#     splines_df = pd.DataFrame(\n",
    "#         weekofyear_splines,\n",
    "#         columns=[f\"spline_{i}\" for i in range(weekofyear_splines.shape[1])],\n",
    "#     )\n",
    "#     for i in range(weekofyear_splines.shape[1]):\n",
    "#         df[f'weekofyear_{WEEKOFYEAR}{i}'] = splines_df[f\"spline_{i}\"]\n",
    "#         df[f'hat_{WEEKOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Hat', splines_df[f\"spline_{i}\"], 0)\n",
    "#         df[f'stick_{WEEKOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Sticker', splines_df[f\"spline_{i}\"], 0)\n",
    "#     df[f'mug_{MONTH}'] = np.where(df['product'] == 'Kaggle Mug', df[MONTH], 0)\n",
    "#     df[f'mug_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Mug', df[WEEKOFYEAR], 0)\n",
    "#     df[f'mug_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Mug', df[DAYOFYEAR], 0)\n",
    "#     df[f'mug_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Mug', df[WEEKDAY], 0)\n",
    "#     df[f'hat_{MONTH}'] = np.where(df['product'] == 'Kaggle Hat', df[MONTH], 0)\n",
    "#     df[f'hat_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Hat', df[WEEKOFYEAR], 0)\n",
    "#     df[f'hat_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Hat', df[DAYOFYEAR], 0)\n",
    "#     df[f'hat_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Hat', df[WEEKDAY], 0)\n",
    "#     df[f'stick_{MONTH}'] = np.where(df['product'] == 'Kaggle Sticker', df[MONTH], 0)\n",
    "#     df[f'stick_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Sticker', df[WEEKOFYEAR], 0)\n",
    "#     df[f'stick_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Sticker', df[DAYOFYEAR], 0)\n",
    "#     df[f'stick_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Sticker', df[WEEKDAY], 0)\n",
    "\n",
    "#     df = df.drop(columns=[DAYOFYEAR]) #MONTH, WEEKOFYEAR, WEEKDAY\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_periodic(df):\n",
    "    # 21 days cyclic for lunar\n",
    "    # 21 4.244872419046287 31 4.23870 37 4.2359085545955875 47 4.24590382934362 39 4.236812122257115 \n",
    "    # 35 4.2358561209794665 33 4.237682217183017 36 4.230652791910613 3 4.241000488616227 4.23833321067532\n",
    "    #[7, 14, 21, 28, 30, 31, 91] range(1, 32, 4) range(1,3,1)[1,2,4]\n",
    "    # Long term periodic\n",
    "    dayofyear = df.date.dt.dayofyear\n",
    "    j=-36\n",
    "    for k in [2]:\n",
    "        df = pd.concat([df,\n",
    "                        pd.DataFrame({\n",
    "                            f\"sin{k}\": np.sin((dayofyear+j) / 365 * 1 * math.pi * k),\n",
    "                            f\"cos{k}\": np.cos((dayofyear+j) / 365 * 1 * math.pi * k),\n",
    "                                     })], axis=1)\n",
    "        # Products\n",
    "        df[f'mug_sin{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'sin{k}'], 0)\n",
    "        df[f'mug_cos{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'cos{k}'], 0)\n",
    "        df[f'hat_sin{k}'] = np.where(df['product'] == 'Kaggle Hat', df[f'sin{k}'], 0)\n",
    "        df[f'hat_cos{k}'] = np.where(df['product'] == 'Kaggle Hat', df[f'cos{k}'], 0)\n",
    "        df[f'stick_sin{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'sin{k}'], 0)\n",
    "        df[f'stick_cos{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'cos{k}'], 0)\n",
    "        df = df.drop(columns=[f'sin{k}', f'cos{k}'])\n",
    "\n",
    "    # Short term Periodic\n",
    "    weekday = df.date.dt.weekday\n",
    "    df[f'weekly_sin'] = np.sin((1 / 7) * 2 * math.pi*(weekday+1)) #+\n",
    "    df[f'weekly_cos'] = np.cos((1 / 7) * 2 * math.pi*(weekday+1)) #+\n",
    "    df[f'semiweekly_sin'] = np.sin((1 / 7) * 4 * math.pi*(dayofyear-1.5)) #+ ‚ÅÖsin(1/7 ùúã‚ãÖ4(ùë•‚àí2))‚ÅÜ\n",
    "    df[f'semiweekly_cos'] = np.cos((1 / 7) * 4 * math.pi*(dayofyear-1.5)) #+ ‚ÅÖcos(1/7 ùúã‚ãÖ4ùë•)‚ÅÜ\n",
    "    \n",
    "    df[f'fin_weekly_sin'] = np.where(df['country'] == 'Finland', df[f'weekly_sin'], 0)\n",
    "    df[f'fin_weekly_cos'] = np.where(df['country'] == 'Finland', df[f'weekly_cos'], 0)\n",
    "    df[f'nor_weekly_sin'] = np.where(df['country'] == 'Norway', df[f'weekly_sin'], 0)\n",
    "    df[f'nor_weekly_cos'] = np.where(df['country'] == 'Norway', df[f'weekly_cos'], 0)\n",
    "    df[f'swe_weekly_sin'] = np.where(df['country'] == 'Sweden', df[f'weekly_sin'], 0)\n",
    "    df[f'swe_weekly_cos'] = np.where(df['country'] == 'Sweden', df[f'weekly_cos'], 0)\n",
    "    \n",
    "    df[f'mug_weekly_sin'] = np.where(df['product'] == 'Kaggle Mug', df[f'weekly_sin'], 0)\n",
    "    df[f'mug_weekly_cos'] = np.where(df['product'] == 'Kaggle Mug', df[f'weekly_cos'], 0)\n",
    "    df[f'hat_weekly_sin'] = np.where(df['product'] == 'Kaggle Hat', df[f'weekly_sin'], 0)\n",
    "    df[f'hat_weekly_cos'] = np.where(df['product'] == 'Kaggle Hat', df[f'weekly_cos'], 0)\n",
    "    df[f'stick_weekly_sin'] = np.where(df['product'] == 'Kaggle Sticker', df[f'weekly_sin'], 0)\n",
    "    df[f'stick_weekly_cos'] = np.where(df['product'] == 'Kaggle Sticker', df[f'weekly_cos'], 0)\n",
    "    \n",
    "    df[f'mug_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Mug', df[f'semiweekly_sin'], 0)\n",
    "    df[f'mug_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Mug', df[f'semiweekly_cos'], 0)\n",
    "    df[f'hat_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Hat', df[f'semiweekly_sin'], 0)\n",
    "    df[f'hat_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Hat', df[f'semiweekly_cos'], 0)\n",
    "    df[f'stick_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Sticker', df[f'semiweekly_sin'], 0)\n",
    "    df[f'stick_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Sticker', df[f'semiweekly_cos'], 0)\n",
    "    \n",
    "    df = df.drop(columns=['weekly_sin', 'weekly_cos', 'semiweekly_sin', 'semiweekly_cos'])\n",
    "    \n",
    "#     df[f'semiannual_sin'] = np.sin(dayofyear / 182.5 * 2 * math.pi)\n",
    "#     df[f'semiannual_cos'] = np.cos(dayofyear / 182.5 * 2 * math.pi)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_holiday(df):\n",
    "# Dec Jan\n",
    "    # End of year\n",
    "    df = pd.concat([df,\n",
    "                        pd.DataFrame({f\"f-dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "                                      for d in range(24, 32)}),\n",
    "                        pd.DataFrame({f\"n-dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "                                      for d in range(24, 32)}),\n",
    "                        pd.DataFrame({f\"s-dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "                                      for d in range(24, 32)}),\n",
    "                        pd.DataFrame({f\"f-jan{d}\":\n",
    "                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "                                      for d in range(1, 14)}),\n",
    "                        pd.DataFrame({f\"n-jan{d}\":\n",
    "                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "                                      for d in range(1, 10)}),\n",
    "                        pd.DataFrame({f\"s-jan{d}\":\n",
    "                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "                                      for d in range(1, 15)})\n",
    "                       ], axis=1)\n",
    "        \n",
    "    # May\n",
    "    df = pd.concat([df,\n",
    "                        pd.DataFrame({f\"may{d}\":\n",
    "                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n",
    "                                      for d in list(range(1, 10))}),\n",
    "                        pd.DataFrame({f\"may{d}\":\n",
    "                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & \n",
    "                                      (df.country == 'Norway')\n",
    "                                      for d in list(range(18, 28))})\n",
    "                        ], axis=1)\n",
    "    \n",
    "    # June and July 8, 14\n",
    "    df = pd.concat([df,\n",
    "                        pd.DataFrame({f\"june{d}\":\n",
    "                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & \n",
    "                                      (df.country == 'Sweden')\n",
    "                                      for d in list(range(8, 14))}),\n",
    "                       ], axis=1)\n",
    "    # Last Wednesday of June\n",
    "    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n",
    "                                         2016: pd.Timestamp(('2016-06-29')),\n",
    "                                         2017: pd.Timestamp(('2017-06-28')),\n",
    "                                         2018: pd.Timestamp(('2018-06-27')),\n",
    "                                         2019: pd.Timestamp(('2019-06-26'))})\n",
    "    df = pd.concat([df, pd.DataFrame({f\"wed_june{d}\": \n",
    "                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & \n",
    "                                      (df.country != 'Norway')\n",
    "                                      for d in list(range(-4, 6))})], axis=1)\n",
    "\n",
    "    # First Sunday of November\n",
    "    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n",
    "                                         2016: pd.Timestamp(('2016-11-6')),\n",
    "                                         2017: pd.Timestamp(('2017-11-5')),\n",
    "                                         2018: pd.Timestamp(('2018-11-4')),\n",
    "                                         2019: pd.Timestamp(('2019-11-3'))})\n",
    "    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n",
    "                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n",
    "                                      for d in list(range(0, 9))})], axis=1)\n",
    "    # First half of December (Independence Day of Finland, 6th of December)\n",
    "    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "                                      for d in list(range(6, 14))})], axis=1)\n",
    "    # Easter April\n",
    "    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n",
    "    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n",
    "                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n",
    "                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c76c6-85fd-42ce-99d5-32f0aa5a7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_engineering(df):\n",
    "    df = get_basic_ts_features(df)\n",
    "    df = feature_splines(df)\n",
    "    df = feature_periodic(df)\n",
    "    df = feature_holiday(df)\n",
    "    df = unofficial_holiday(df)\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e4912-fd39-4f15-a440-47ed10fd80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old feature engineering function\n",
    "\n",
    "# def temporal_engineering(df):\n",
    "#     '''\n",
    "#     Function inspired by / borrowing from @teckmengwong and @ambrosm to create time features that will\n",
    "#     capture seasonality.\n",
    "#     '''\n",
    "    \n",
    "# #     df[YEAR] = df[DATE].dt.year\n",
    "#     df['month'] = df['date'].dt.month\n",
    "# #     df['week'] = df['date'].dt.week # not used by Teck Meng Wong\n",
    "# #     df['day'] = df['date'].dt.day # not used by Teck Meng Wong\n",
    "# #     df['day_of_year'] = df['date'].dt.dayofyear # not used by Teck Meng Wong\n",
    "# #     df['day_of_month'] = df['date'].dt.days_in_month # not used by Teck Meng Wong\n",
    "# #     df['day_of_week'] = df['date'].dt.dayofweek # not used by Teck Meng Wong\n",
    "# #    df['weekday'] = df['date'].dt.weekday # not used by Teck Meng Wong\n",
    "#     # Teck Meng Wong mapped the integers to first-letters in triplets\n",
    "#     # I'm leaving it as integers, where winter=1, spring=2, summer=3, fall=4\n",
    "#     df['season'] = ((df['date'].dt.month % 12 + 3) // 3) #.map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n",
    "# #     df['month'] = df['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "\n",
    "#     df['wd4'] = df['date'].dt.weekday == 4\n",
    "#     df['wd56'] = df['date'].dt.weekday >= 5\n",
    "# #     df['wd6'] = df['date'].dt.weekday >= 6\n",
    "# #     df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), 'day_of_year'] += 1 # fix for leap years\n",
    "    \n",
    "#     # 21 days cyclic for lunar\n",
    "#     dayofyear = df.date.dt.dayofyear # for convenience\n",
    "    \n",
    "#     # here he's creating Fourier features\n",
    "#     for k in range(1, 32, 4):\n",
    "#         df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
    "#         df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
    "#         df[f'finland_sin{k}'] = np.where(df['country'] == 'Finland', df[f'sin{k}'], 0)\n",
    "#         df[f'finland_cos{k}'] = np.where(df['country'] == 'Finland', df[f'cos{k}'], 0)\n",
    "#         df[f'norway_sin{k}'] = np.where(df['country'] == 'Norway', df[f'sin{k}'], 0)\n",
    "#         df[f'norway_cos{k}'] = np.where(df['country'] == 'Norway', df[f'cos{k}'], 0)\n",
    "#         df[f'store_sin{k}'] = np.where(df['store'] == 'KaggleMart', df[f'sin{k}'], 0)\n",
    "#         df[f'store_cos{k}'] = np.where(df['store'] == 'KaggleMart', df[f'cos{k}'], 0)\n",
    "#         df[f'mug_sin{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'sin{k}'], 0)\n",
    "#         df[f'mug_cos{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'cos{k}'], 0)\n",
    "#         df[f'sticker_sin{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'sin{k}'], 0)\n",
    "#         df[f'sticker_cos{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'cos{k}'], 0)\n",
    "    \n",
    "# #     df[f'semiweekly_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 14)\n",
    "# #     df[f'semiweekly_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 14)\n",
    "# #     df[f'lunar_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 21)\n",
    "# #     df[f'lunar_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 21)\n",
    "#     df[f'season_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 91.5)\n",
    "#     df[f'season_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 91.5)\n",
    "# #     df = pd.concat([df, pd.DataFrame({f'fin{ptr[1]}':\n",
    "# #                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Finland')\n",
    "# #                                       for ptr in holidays.Finland(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "# #     df = pd.concat([df, pd.DataFrame({f'nor{ptr[1]}':\n",
    "# #                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Norway')\n",
    "# #                                       for ptr in holidays.Norway(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "# #     df = pd.concat([df, pd.DataFrame({f'swe{ptr[1]}':\n",
    "# #                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Sweden')\n",
    "# #                                       for ptr in holidays.Sweden(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "\n",
    "#     # End of year\n",
    "#     # Dec - teckmengwong\n",
    "#     for d in range(24, 32):\n",
    "#         df[f\"dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d)\n",
    "#     # I'm unsure of the logic of only doing this for Norway\n",
    "#     for d in range(24, 32):\n",
    "#         df[f\"n-dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "    \n",
    "#     # not sure why he's using different date ranges for each country here\n",
    "#     # Jan - teckmengwong\n",
    "#     for d in range(1, 14):\n",
    "#         df[f\"f-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "#     for d in range(1, 10):\n",
    "#         df[f\"n-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "#     for d in range(1, 15):\n",
    "#         df[f\"s-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "    \n",
    "    \n",
    "#     # May - tekcmengwong\n",
    "#     for d in list(range(1, 10)): # May Day and after, I guess\n",
    "#         df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d)\n",
    "#     for d in list(range(19, 26)):\n",
    "#         df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "#     # June \n",
    "#     for d in list(range(8, 14)):\n",
    "#         df[f\"june{d}\"] = (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "    \n",
    "#     #Swedish Rock Concert - teckmengwong\n",
    "#     #Jun 3, 2015 ‚Äì Jun 6, 2015\n",
    "#     #Jun 8, 2016 ‚Äì Jun 11, 2016\n",
    "#     #Jun 7, 2017 ‚Äì Jun 10, 2017\n",
    "#     #Jun 6, 2018 ‚Äì Jun 10, 2018\n",
    "#     #Jun 5, 2019 ‚Äì Jun 8, 2019\n",
    "#     swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),\n",
    "#                                          2016: pd.Timestamp(('2016-06-11')),\n",
    "#                                          2017: pd.Timestamp(('2017-06-10')),\n",
    "#                                          2018: pd.Timestamp(('2018-06-10')),\n",
    "#                                          2019: pd.Timestamp(('2019-06-8'))})\n",
    "\n",
    "#     df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n",
    "#                                       (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n",
    "#                                       for d in list(range(-3, 3))})], axis=1)\n",
    "\n",
    "    \n",
    "#     # Last Wednesday of June - teckmengwong\n",
    "#     wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n",
    "#                                          2016: pd.Timestamp(('2016-06-29')),\n",
    "#                                          2017: pd.Timestamp(('2017-06-28')),\n",
    "#                                          2018: pd.Timestamp(('2018-06-27')),\n",
    "#                                          2019: pd.Timestamp(('2019-06-26'))})\n",
    "#     for d in list(range(-4, 6)):\n",
    "#         df[f\"wed_june{d}\"] = (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n",
    "        \n",
    "#     # First Sunday of November - teckmengwong\n",
    "#     sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n",
    "#                                          2016: pd.Timestamp(('2016-11-6')),\n",
    "#                                          2017: pd.Timestamp(('2017-11-5')),\n",
    "#                                          2018: pd.Timestamp(('2018-11-4')),\n",
    "#                                          2019: pd.Timestamp(('2019-11-3'))})\n",
    "#     df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n",
    "#                                       (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n",
    "#                                       for d in list(range(0, 9))})], axis=1)\n",
    "    \n",
    "#     # First half of December (Independence Day of Finland, 6th of December) -teckmengwong\n",
    "#     df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n",
    "#                                       (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "#                                       for d in list(range(6, 14))})], axis=1)\n",
    "    \n",
    "#     # Easter -teckmengwong\n",
    "#     easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n",
    "#     df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n",
    "#                                       (df.date - easter_date == np.timedelta64(d, \"D\"))\n",
    "#                                       for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb99a9-079a-4248-9d21-958813626510",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_all_df = temporal_engineering(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43b0b9-6388-4237-a7f4-7ae8db65d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4077ce-11b3-4536-947b-cb464653dde3",
   "metadata": {},
   "source": [
    "At this point, the `temporal_all_df` DataFrame contains all the time features for both the training and testing sets.\n",
    "* **Todo**: consider not only adding in holidays from `holidays`, but also borrowing ideas from the AmbrosM Linear notebook too (which creates fewer features, populating them instead with temporal distances from the selected holidays)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d96649-8a40-44d4-a449-2cd1ec31121f",
   "metadata": {},
   "source": [
    "### Target Transformation\n",
    "Now, I'll do the target transformation proposed by @AmbrosM. (I'll do it to the non-encoded DataFrame too, for testing with Prophet and NeuralProphet later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4deae1-898f-4bcb-a35c-60b55eeb9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [temporal_all_df]:\n",
    "    df['target'] = np.log(df['num_sold'] / df['gdp']**gdp_exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60614b6-d87b-4630-960c-f469ac4b000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_all_df['target'] = np.log(encoded_all_df['num_sold'] / (encoded_all_df['gdp']**gdp_exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f623aa-d9bb-4cf4-8975-5598721a32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eadb9e-0f04-4243-80b7-3f710e5542e2",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474675c-3b84-4aed-a5f7-319a312d9954",
   "metadata": {},
   "source": [
    "I'm going to encapsulate this in a function so that it can be invoked just-in-time, in the hopes of avoiding confusions with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cec107-6ed6-4013-bd86-83287b73e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    features = ['country', 'product', 'store']\n",
    "    le_dict = {feature: LabelEncoder().fit(orig_train_df[feature]) for feature in features}\n",
    "    enc_df = df.copy()\n",
    "    for feature in features:\n",
    "        enc_df[feature] = le_dict[feature].transform(df[feature])\n",
    "    return le_dict, enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057ff46-10a1-4a74-a049-49b1705d02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in le_dict.keys():\n",
    "#     print(f\"Values for key {key} are {le_dict[key].inverse_transform(range(len(le_dict[key].values())))}\")#\"\n",
    "# print(le_dict['country'].inverse_transform([0,1,2]))\n",
    "# print(le_dict['product'].inverse_transform([0,1,2]))\n",
    "# print(le_dict['store'].inverse_transform([0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65156ef4-a429-46b2-8ff0-f98ca651611a",
   "metadata": {},
   "source": [
    "```\n",
    "['Finland' 'Norway' 'Sweden']\n",
    "['Kaggle Hat' 'Kaggle Mug' 'Kaggle Sticker']\n",
    "['KaggleMart' 'KaggleRama']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ca0e8-1ee8-462d-945b-c00f9958409b",
   "metadata": {},
   "source": [
    "Now, we'll do the encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72b123-f8a8-4f90-820e-0da7772fe4a2",
   "metadata": {},
   "source": [
    "At this point, the `encoded_all_df` can be used -- perhaps with a call to `LabelEncoder.inverse_transform` -- to recover the \"original\" data when necessary (e.g. for feeding it into Prophet and NeuralProphet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57733c-7b91-4925-85dd-7b967cb56dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_all_df = label_encoder(temporal_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f4e48-b234-4cf5-8421-f2af145426e9",
   "metadata": {},
   "source": [
    "### Pseudolabeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca8eb6-a11a-4a33-9ac2-e10c1df48845",
   "metadata": {},
   "source": [
    "I'm not going to try this right now, but I may return to it later -- I note that Teck Meng Wong had some good results with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce23a5-1c47-4a1d-9c6b-0684f90c5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here's teck meng wong's implementation -- see the notebook for the constants\n",
    "# df_pseudolabels = pd.read_csv(PSEUDO_DIR, index_col=ID)\n",
    "# df_pseudolabels[DATE] = pd.to_datetime(test_df[DATE])\n",
    "# df_pseudolabels.to_csv(\"pseudo_labels_v0.csv\", index=True)\n",
    "# # if PSEUDO_LABEL:\n",
    "#     # df_pseudolabels = df_pseudolabels.set_index([DATE]).sort_index()\n",
    "# test_df[column_y] = df_pseudolabels[column_y].astype(np.float64)\n",
    "# train_df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61061e99-d56e-4290-ba18-c89e1a21fdf3",
   "metadata": {},
   "source": [
    "### Data Splitting, Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268dbc9-cf56-49d8-886e-ba34eb1b4499",
   "metadata": {},
   "source": [
    "Now that the preprocessing is done, I'm going to split the data back into the train and test sets; then, I'll create a view on the dataframes that omits the year. The year-less dataframes will be suitable for residual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447607ba-81cf-4125-9a1d-2d551da05fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = encoded_all_df.drop(columns=['num_sold', 'row_id'])\n",
    "all_df = temporal_all_df.drop(columns=['row_id']) # writing over the previous version of `all_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b3b51-4cfa-46f5-a32e-3e5c7497d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_df = all_df[:len(orig_train_df)] # training and validation sets -- still not encoded\n",
    "test_df = all_df[len(orig_train_df):] # still not encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120d77c-00bb-4345-b897-4d5d1105fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = encoded_all_df.iloc[np.where(encoded_all_df['date'] < '2019-01-01'), :]\n",
    "# test_df = encoded_all_df[[np.where(encoded_all_df['date'] > '2018-12-31')]]\n",
    "\n",
    "# encoded_tv_df = encoded_all_df.drop(columns=['row_id'])[:len(orig_train_df)]\n",
    "# encoded_test_df = encoded_all_df.drop(columns=['row_id'])[len(orig_train_df):]\n",
    "\n",
    "# valid_df = tv_df[tv_df['date'] > '2017-12-31']\n",
    "# train_df = tv_df[tv_df['date'] <= '2017-12-31']\n",
    "\n",
    "# train_and_valid_residual_df = train_and_valid_df.drop(columns=['date'])\n",
    "# test_residual_df = test_df.drop(columns=['date'])\n",
    "\n",
    "# len(valid_df) + len(train_df) == len(tv_df)\n",
    "\n",
    "# encoded_tv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59f277-f5c5-4e78-ae1e-74c6cb485017",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed153-18af-4f8d-8008-0e1133c7934d",
   "metadata": {},
   "source": [
    "### Forecasting Models Prep\n",
    "First, we'll set up functions to handle the training of forecasting models which will discern trends, and which may -- or may not -- yield insights concerning seasonality. While the Scikit-Learn models will be able to share a single trainer function, the Prophet and NeuralProphet models have subtly different expectations of their data, and as such will require separate handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2943f6c-0ea8-4686-8915-ea0e3148898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, HuberRegressor, LinearRegression, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet\n",
    "# earth? wouldn't install via pip on my machine at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7ac75-d9e8-4cb4-849e-351d651b4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc7c88-1d32-4731-b159-34d35aef250c",
   "metadata": {},
   "source": [
    "#### (Preprepared Preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb719de-dd9d-4a44-8a46-59ce92091555",
   "metadata": {},
   "source": [
    "The next cell contains code to import already-existing predictions -- but I think it's better to centralize the code that produces them here, and will comment out the import code for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07e728-55cd-4526-bd02-14fedb302f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_trainset = load(predpath/'20220121_prophet_baseline_trainset.joblib')\n",
    "\n",
    "# neural_trainset = load(predpath/'20220121_neuralprophet_baseline_trainset.joblib')\n",
    "# neural_test_preds = load(predpath/'20220121_neuralprophet_baseline_testset.joblib')\n",
    "\n",
    "# ridge_tv_preds = load(predpath/'20210121_ridge_baseline_trainset_preds.joblib')\n",
    "# ridge_test_preds = load(predpath/'20220121_ridge_testset_preds.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb101a-dc0a-4aef-8846-a49bf40221e8",
   "metadata": {},
   "source": [
    "And this cell would handle the parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9d647-4838-4336-9958-898c0a55eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_tv_preds = neural_trainset['prophet_forecast']\n",
    "# prophet_tv_preds = prophet_trainset['prophet_forecast']\n",
    "\n",
    "# neural_train_preds = neural_tv_preds[:train_length]\n",
    "# neural_valid_preds = neural_tv_preds[train_length:]\n",
    "\n",
    "# prophet_train_preds = prophet_tv_preds[:train_length]\n",
    "# prophet_valid_preds = prophet_tv_preds[train_length:]\n",
    "\n",
    "# train_length = len(neural_trainset[neural_trainset['date'] <= '2017-12-31'])\n",
    "\n",
    "# ridge_train_preds = ridge_tv_preds[:train_length]\n",
    "# ridge_valid_preds = ridge_tv_preds[train_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ce80e-1d5b-4f07-8381-8fe6339bbb1d",
   "metadata": {},
   "source": [
    "#### Scikit-Learn Linear Models Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f765ff-e4f9-487d-b271-b852add5c0c6",
   "metadata": {},
   "source": [
    "Linear models from Scikit-Learn seemingly require that datetime data be converted to numerics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df2215-385f-44c7-a18d-1368f81ff6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70752c1d-06cd-4b7c-b2e0-5f097bd1563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_linear_df = train_df.copy()\n",
    "# valid_linear_df = valid_df.copy()\n",
    "# test_linear_df = test_df.copy()\n",
    "# tv_linear_df = tv_df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba467720-1d42-496b-b3da-5de3bb8a49ce",
   "metadata": {},
   "source": [
    "### Forecasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43861d3a-3563-44c3-beda-b5845e37a2b2",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "I'll hard-code them for now, but in the future may Optuna them. May want to create a dict of all the kwargs to be used for all the models, with the model names as keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09c068-f227-4f7b-bdf3-1ea5250c4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "#     'holidays':holidays_train, # will add this in-function\n",
    "    'n_changepoints':10,\n",
    "    'changepoint_range':0.4,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_prior_scale':25,\n",
    "    'holidays_prior_scale':100,\n",
    "    'changepoint_prior_scale':0.01,\n",
    "    'interval_width':0.5,\n",
    "    'uncertainty_samples':False\n",
    "}\n",
    "\n",
    "neuralprophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "    'n_changepoints':10,\n",
    "    'changepoints_range':0.4,\n",
    "    'trend_reg':1,\n",
    "    'trend_reg_threshold':False,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_reg':1,\n",
    "    'n_forecasts':365,\n",
    "    'normalize':'off'\n",
    "}\n",
    "\n",
    "# for pytorch / skorch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tcn_kwargs = {\n",
    "#     'module': estimator, # will be handled at-call\n",
    "#     'criterion': nn.MSELoss, # consider enhancement here\n",
    "#     \"lr\": 0.01, # default is 0.01\n",
    "#     'optimizer':Adam,\n",
    "#     'max_epochs':10, # default is 10\n",
    "#     'device': 'cpu'#device,\n",
    "    \n",
    "}\n",
    "\n",
    "tcn_skorch_kwargs = {\n",
    "    'module__num_inputs':1,\n",
    "    'module__num_channels':[10] * 11,\n",
    "    'module__output_sz':1, #2 * samples_per_hour,\n",
    "    'module__kernel_size':5,\n",
    "    'module__dropout':0.0,\n",
    "    'max_epochs':60, # 60,\n",
    "    'batch_size':256,\n",
    "    'lr':2e-3,\n",
    "    'optimizer':torch.optim.Adam,\n",
    "    'train_split':None,\n",
    "}\n",
    "\n",
    "mlp_skorch_kwargs = {\n",
    "    'module__n_inputs': tv_df.shape[1],\n",
    "    'module__hidden_units': 200, \n",
    "    'module__dropout': 0.2,\n",
    "    'max_epochs':25, # 60,\n",
    "    'batch_size':256,\n",
    "    'lr':2e-3,\n",
    "    'optimizer':torch.optim.Adam,\n",
    "    'train_split':None,\n",
    "}\n",
    "\n",
    "\n",
    "# model_params['hyperparams'] = str(neuralprophet_kwargs)\n",
    "# model_params['holiday_source'] = 'Prophet builtin for each country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2eed0b-6f99-4ab0-8856-7de608f31e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee65f32-a819-4d1e-95f8-b723186618de",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgboost_params = load(studypath/'optuna_xgboost_study-20220126213551.joblib').best_trial.params\n",
    "best_xgboost_params['max_depth'] = best_xgboost_params['depth']\n",
    "del best_xgboost_params['depth']\n",
    "best_xgboost_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31271cf-143b-4757-8846-332755b60bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_catboost_params = load(studypath/'optuna_catboost_study-20220127082356.joblib').best_trial.params\n",
    "best_catboost_params['max_depth'] = best_catboost_params['depth']\n",
    "del best_catboost_params['depth']\n",
    "best_catboost_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01a025-a733-47ec-91a8-cad11f790c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lightgbm_params = load(studypath/'optuna_lightgbm_study-20220127171126.joblib').best_trial.params\n",
    "best_lightgbm_params['max_depth'] = best_lightgbm_params['depth']\n",
    "del best_lightgbm_params['depth']\n",
    "best_lightgbm_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b0aa9-021e-4a6f-ac7b-c984ec272d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {\n",
    "    # universal\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "#     'eval_metric': ['mae', 'mape', 'rmse'],\n",
    "#     'sampling_method': 'gradient_based',\n",
    "#     'grow_policy': 'lossguide',\n",
    "    \n",
    "    # best of 500 trials on Optuna\n",
    "    **best_xgboost_params\n",
    "}\n",
    "\n",
    "\n",
    "lightgbm_params = {\n",
    "    # universal\n",
    "    'objective': 'mse',\n",
    "#     'random_state': 42,\n",
    "    'device_type': 'cpu',\n",
    "    'n_jobs': -1,\n",
    "#                 eval_metric='auc',\n",
    "#     'device_type': 'gpu',\n",
    "#     'max_bin': 63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     'gpu_use_dp': False,\n",
    "#     'max_depth': 0,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'subsample': .15,\n",
    "#     'n_estimators': 1500,\n",
    "    **best_lightgbm_params\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    # universal\n",
    "#     'task_type':'GPU',\n",
    "#     'silent':True,\n",
    "#     'random_state':42,\n",
    "    \n",
    "    # from trial 4 (of 5) via Optuna\n",
    "    **best_catboost_params\n",
    "}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a1217-bc47-416d-b73e-95aceae3f973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b011357f-a7bd-4317-8a57-d06434cce590",
   "metadata": {},
   "source": [
    "#### Temporal Convolutional Network\n",
    "\n",
    "Implementation from https://www.kaggle.com/ceshine/pytorch-temporal-convolutional-networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8569c-05f5-418e-a269-27c5d8e6ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TemporalBlock(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "#         super(TemporalBlock, self).__init__()\n",
    "        \n",
    "#         # this is the first convolutional layer; note that it foregoes padding irrespective of argument\n",
    "#         self.conv1 = weight_norm(nn.Conv2d(n_inputs, n_outputs, (1, kernel_size),\n",
    "#                                            stride=stride, padding=0, dilation=dilation))\n",
    "#         # the padding is then added after the first conv layer\n",
    "#         self.pad = torch.nn.ZeroPad2d((padding, 0, 0, 0))\n",
    "#         # this is a very standard choice\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         # the second convolutional layer in the block is identical to the first, but now padding has been added to the input\n",
    "#         self.conv2 = weight_norm(nn.Conv2d(n_outputs, n_outputs, (1, kernel_size),\n",
    "#                                            stride=stride, padding=0, dilation=dilation))\n",
    "        \n",
    "#         # this simply strings together the above architectural elements, for convenience I guess\n",
    "#         self.net = nn.Sequential(self.pad, self.conv1, self.relu, self.dropout,\n",
    "#                                  self.pad, self.conv2, self.relu, self.dropout)\n",
    "        \n",
    "#         # if the n_outputs is nonzero, this adds on a final convlutional layer to ensure that we get the desired number of outputs\n",
    "#         self.downsample = nn.Conv1d(\n",
    "#             n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         # this initializes the weights as specified in the separate weight initialization method, below\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         # this method initializes the weights for the Conv1D and Conv2D layers, plus the Downsample layer (if it's used)\n",
    "#         self.conv1.weight.data.normal_(0, 0.01)\n",
    "#         self.conv2.weight.data.normal_(0, 0.01)\n",
    "#         if self.downsample is not None:\n",
    "#             self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # note the nice one-liner here, to add in the requisite number of dimensions both inbound to the NN and outbound\n",
    "#         out = self.net(x.unsqueeze(2)).squeeze(2) # original\n",
    "# #         out = self.net(x.unsqueeze(3)).squeeze(3) # my revision to address RuntimeError: Expected 4-dimensional input for 4-dimensional weight [32, 128, 1, 2], but got 3-dimensional input of size [128, 244, 2] instead\n",
    "# #         out = self.net(x.unsqueeze(3)).squeeze(2) # further revision to address IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\n",
    "#         # is this a residual, then?\n",
    "#         res = x if self.downsample is None else self.downsample(x)\n",
    "#         return self.relu(out + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30fb0e2-e39f-4ce8-90d2-7bf89dc20636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TemporalConvNet(nn.Module):\n",
    "#     def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "#         '''\n",
    "#         What does num_channels mean? See Obsidian 202201270954... It seems that it should be a \n",
    "#         list, with the number of hidden channels (i.e. activation units in each hidden layer), \n",
    "#         repeated the number of hidden layers there are. E.g. [25,25,25,25]. An alternate idea:\n",
    "#         it's [hidden_size]*(level_size-1) + [embedding_size]\n",
    "        \n",
    "#         I think that \n",
    "#         '''\n",
    "        \n",
    "#         super(TemporalConvNet, self).__init__()\n",
    "#         layers = []\n",
    "#         num_levels = len(num_channels)\n",
    "#         for i in range(num_levels):\n",
    "#             dilation_size = 2 ** i\n",
    "#             in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "#             out_channels = num_channels[i]\n",
    "#             layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "#                                      padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "#         self.network = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae9ef8-7b82-4211-acd1-68ddf7c46369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TCNModel(nn.Module):\n",
    "#     def __init__(self, num_channels, kernel_size=2, dropout=0.2):\n",
    "#         super(TCNModel, self).__init__()\n",
    "#         self.tcn = TemporalConvNet(\n",
    "#             128, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.decoder = nn.Linear(num_channels[-1], 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.decoder(self.dropout(self.tcn(x)[:, :, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bcdd6-241e-4a58-93ed-5f1b6537ceec",
   "metadata": {},
   "source": [
    "Going to use the [original implementation](https://github.com/locuslab/TCN/blob/master/TCN/tcn.py) (via the discussion [here](https://www.ethanrosenthal.com/2019/02/18/time-series-for-scikit-learn-people-part3/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a868a59-9181-4b1a-8456-f545ae191d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, hidden_units, dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense0 = nn.Linear(n_inputs, hidden_units)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        self.dropout0 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units // 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.dense2 = nn.Linear(hidden_units // 2, (hidden_units // 2) // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.dense3 = nn.Linear((hidden_units // 2) // 2, ((hidden_units // 2) // 2) // 2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.head = nn.Linear(((hidden_units // 2) // 2) // 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout0(self.relu0(self.dense0(x)))\n",
    "        x = self.dropout1(self.relu1(self.dense1(x)))\n",
    "        x = self.dropout2(self.relu2(self.dense2(x)))\n",
    "        x = self.dropout3(self.relu3(self.dense3(x)))\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cde14-0da1-4b88-8188-9798cc893805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, output_sz,\n",
    "                 kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
    "                                     dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size,\n",
    "                                     dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_sz)\n",
    "        self.last_activation = nn.ReLU()\n",
    "        self.output_sz = output_sz\n",
    "        # self.float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_sz = x.shape[0]\n",
    "        out = self.network(x.unsqueeze(1))\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.linear(out).mean(dim=1)\n",
    "        out = out.to(dtype=torch.float32) # my addition\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65016b8f-c72f-4ba7-bc12-0d5d642107af",
   "metadata": {},
   "source": [
    "#### Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5ffca-0447-4184-ac30-5a2c1eaa07c9",
   "metadata": {},
   "source": [
    "##### NeuralProphet\n",
    "I'm leaving the folds as they are. ~~Label encoding shouldn't matter -- the values are just being iterated over anyway.~~ It does matter because the Prophets use the strings to identify countries' holidays to add. Not sure about doing the target transform -- if you try it, just have the trainer call pass `target='target'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7142a2b-316b-49c7-b6ad-732d79681d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_folds = [\n",
    "    ('2015-01-01', '2018-01-01'),\n",
    "    ('2018-01-01', '2019-01-01'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd9ca4-04db-4176-b787-317eb91f544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_tv_df = tv_df_encoded.copy() # encoded_tv_df.copy()\n",
    "# prophet_test_df = test_df_encoded.copy() # encoded_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d618ce1-5b3f-470f-8c46-878f30aa7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in ['country', 'product', 'store']:\n",
    "#     prophet_tv_df[feature] = orig_train_df[feature]\n",
    "#     prophet_test_df[feature] = orig_test_df[feature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c69c6-7581-4401-9c83-16c1d4ece031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_tv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e48c6-ca35-425d-acdb-493ed48e2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_enc = le_dict['country'].transform(countries)\n",
    "# stores_enc = le_dict['store'].transform(stores)\n",
    "# products_enc = le_dict['product'].transform(products)\n",
    "\n",
    "# countries, countries_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839064d9-a771-4785-a99b-c951404801aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def neuralprophet_trainer(model_kwargs=neuralprophet_kwargs, countries=countries, stores=stores, products=products, folds=prophet_folds, \n",
    "                          tv_df=tv_df, test_df=test_df,\n",
    "#                           df_train=tv_df, df_test=test_df, \n",
    "                          target='num_sold', wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    # no label encoding here -- but test it with too\n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', target: 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', target: 'y'})\n",
    "\n",
    "#                     model = Prophet(**prophet_kwargs)\n",
    "                    model = NeuralProphet(**model_kwargs)\n",
    "\n",
    "                    model = model.add_country_holidays(country_name=country) # uses FacebookProphet or NeuralProphet API to add holidays\n",
    "                    print(train.columns)\n",
    "                    model.fit(train, freq='D') # neuralprophet\n",
    "                    # prophet\n",
    "#                     train_predictions = model.predict(train[['ds']])['yhat']\n",
    "#                     val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    # neuralprophet\n",
    "                    train_predictions = model.predict(train)['yhat1']\n",
    "                    val_predictions = model.predict(val)['yhat1']\n",
    "                    df_train.loc[train_idx, 'neuralprophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'neuralprophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test['y'] = np.nan\n",
    "                    test_predictions = model.predict(test)['yhat1']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'neuralprophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "#     train_\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train['neuralprophet_forecast'], df_test['neuralprophet_forecast']#, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd79c6c-8b25-41fe-bb2e-37b2ba1516ef",
   "metadata": {},
   "source": [
    "##### Prophet Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a508af-0494-4762-b9bf-f0909a402ad5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prophet_trainer(prophet_kwargs=prophet_kwargs, countries=countries, stores=stores, products=products, folds=prophet_folds, \n",
    "                    tv_df=tv_df, test_df=test_df,\n",
    "#                           df_train=tv_df, df_test=test_df, \n",
    "                    target='num_sold', wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "                    \n",
    "#                     print(train_idx)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', target]].reset_index(drop=True)\n",
    "#                     print(train.shape)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', target: 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', target: 'y'})\n",
    "\n",
    "                    model = Prophet(**prophet_kwargs)\n",
    "\n",
    "                    model.add_country_holidays(country_name=country) # uses FacebookProphet API to add holidays\n",
    "                    model.fit(train)\n",
    "        \n",
    "                    train_predictions = model.predict(train[['ds']])['yhat']\n",
    "                    val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    df_train.loc[train_idx, 'prophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test_predictions = model.predict(test[['ds']])['yhat']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'prophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train['prophet_forecast'], df_test['prophet_forecast']#, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d6d95-57f2-4634-89a0-5289ed4ca6a2",
   "metadata": {},
   "source": [
    "##### Scikit-Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce0522-8e44-4618-ad57-75c4ea4900cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sklearn_trainer(estimator, model_kwargs={}, tv_df=tv_df, test_df=test_df, #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                    folds=prophet_folds, countries=countries, stores=stores, products=products, target='target',\n",
    "#                     by_combo=True, \n",
    "#                     model_type=None, # None -> fully scikit-learn compatible; alternatives are 'skorch' or 'gbm'\n",
    "                    wandb_tracked=False):\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, tv_df = label_encoder(df_train) # should leave broader scope's tv_df alone\n",
    "    _, test_df = label_encoder(df_test) # should leave broader scope's test_df alone\n",
    "    del df_train, df_test\n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    # drop whichever version of the dependent variable is not being used\n",
    "#     for df in [tv_df, test_df]:\n",
    "    if target == 'num_sold': \n",
    "        tv_df = tv_df.drop(columns=['target'])\n",
    "        test_df = test_df.drop(columns=['target'])\n",
    "    else:\n",
    "        tv_df = tv_df.drop(columns=['num_sold'])\n",
    "        test_df = test_df.drop(columns=['num_sold'])\n",
    "            \n",
    "#     print(\"'num_sold' in test_df.columns == \", 'num_sold' in test_df.columns)\n",
    "    \n",
    "    # handling each combination of country, store, and product separately\n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (tv_df['date'] >= start) &\\\n",
    "                                (tv_df['date'] < end) &\\\n",
    "                                (tv_df['country'] == country) &\\\n",
    "                                (tv_df['store'] == store) &\\\n",
    "                                (tv_df['product'] == product)\n",
    "\n",
    "#                     print(train_idx)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = tv_df.loc[train_idx, :].reset_index(drop=True)\n",
    "#                         print(train.shape)\n",
    "\n",
    "                    val_idx = (tv_df['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (tv_df['date'] < folds[fold + 1][1]) &\\\n",
    "                              (tv_df['country'] == country) &\\\n",
    "                              (tv_df['store'] == store) &\\\n",
    "                              (tv_df['product'] == product)\n",
    "\n",
    "                    val = tv_df.loc[val_idx, :].reset_index(drop=True)\n",
    "\n",
    "                    test_idx = (test_df['country'] == country) &\\\n",
    "                               (test_df['store'] == store) &\\\n",
    "                               (test_df['product'] == product)\n",
    "                    test = test_df.loc[test_idx, :].reset_index(drop=True)\n",
    "\n",
    "                    # with the training and validation sets sorted out, make them integers for model fitting\n",
    "                    for df in [train, val, test]:\n",
    "                        df['date'] = df['date'].map(dt.datetime.toordinal)\n",
    "                    if 'model_forecast' in train.columns:\n",
    "                        X = train.drop(columns=[target, 'model_forecast'])\n",
    "                        X_valid = val.drop(columns=[target, 'model_forecast'])\n",
    "                        X_test = test.drop(columns=[target, 'model_forecast'])\n",
    "                    else:\n",
    "                        X = train.drop(columns=[target])\n",
    "                        X_valid = val.drop(columns=[target])\n",
    "                        X_test = test.drop(columns=[target])\n",
    "\n",
    "                    y = train[target]\n",
    "                    y_valid = val[target]\n",
    "\n",
    "\n",
    "#                         print(type(X), type(y))\n",
    "#                         print(f\"X has {X.isna().any().sum()} NaNs\")\n",
    "#                         print(f\"y has {y.isna().sum()} NaNs\")\n",
    "#                     print(X_test.info())\n",
    "#                     print(y_valid.dtype)\n",
    "    \n",
    "#                     if model_type == 'skorch':\n",
    "# #                         for df in [X, X_valid, X_test]:\n",
    "# # #                             df['date'] = df['date'].apply(dt.datetime.toordinal)\n",
    "# #                             df = torch.tensor(df.to_numpy(dtype=np.float32))\n",
    "# #                         for target in [y, y_valid]:\n",
    "# #                             target = torch.tensor(np.array(target))\n",
    "# # #                             target = target.reshape(-1,1)\n",
    "# #                             target = target.unsqueeze(0)\n",
    "#                         X = torch.tensor(X.to_numpy(dtype=np.float32))\n",
    "#                         X_valid = torch.tensor(X_valid.to_numpy(dtype=np.float32))\n",
    "#                         X_test = torch.tensor(X_test.to_numpy(dtype=np.float32))\n",
    "            \n",
    "#                         y = torch.tensor(np.array(y)).reshape(-1,1)\n",
    "#                         y_valid = torch.tensor(np.array(y)).reshape(-1,1)\n",
    "    \n",
    "#                         tcn_kwargs = {\n",
    "#                             'num_channels': [32,32,32,32],\n",
    "#                         }\n",
    "#                         print(type(y), type(y_valid))\n",
    "# #                         y = y.reshape(-1,1)\n",
    "# #                         y_valid = y_valid.reshape(-1,1)\n",
    "#                         # create the Datasets\n",
    "                \n",
    "#                         # create the DataLoaders\n",
    "\n",
    "#                         # instantiate the wrapper\n",
    "#                         model = NeuralNetRegressor(\n",
    "#                             module=estimator(**tcn_kwargs),\n",
    "#                             **model_kwargs\n",
    "#                         )\n",
    "#                     elif model_type=='gbm':\n",
    "                        \n",
    "#                     else:\n",
    "                    model = estimator(**model_kwargs)\n",
    "\n",
    "                    model.fit(X,y)\n",
    "\n",
    "                    model_train_preds = model.predict(X)\n",
    "                    model_valid_preds = model.predict(X_valid)\n",
    "                    model_test_preds = model.predict(X_test)\n",
    "\n",
    "                    tv_df.loc[train_idx, 'model_forecast'] = model_train_preds#.values\n",
    "                    tv_df.loc[val_idx, 'model_forecast'] =  model_valid_preds#.values\n",
    "                    test_df.loc[test_idx, 'model_forecast'] = model_test_preds#.values\n",
    "\n",
    "\n",
    "    # reverse the dependent variable transform if appropriate\n",
    "    if target == 'target':\n",
    "#             model_tv_preds = np.multiply(np.exp(model_tv_preds), tv_df['gdp']**gdp_exponent)\n",
    "        tv_df['model_forecast'] = np.exp(tv_df['model_forecast']) * tv_df['gdp']**gdp_exponent\n",
    "#             output_tv_df['model_forecast'] = np.exp(output_tv_df['model_forecast']) * output_tv_df['gdp']**gdp_exponent\n",
    "\n",
    "#             model_test_preds = np.multiply(np.exp(model_test_preds), test_df['gdp']**gdp_exponent)\n",
    "        test_df['model_forecast'] = np.exp(test_df['model_forecast']) * test_df['gdp']**gdp_exponent\n",
    "#             output_test_df['model_forecast'] = np.exp(output_test_df['model_forecast']) * output_test_df['gdp']**gdp_exponent\n",
    "#             model_test_preds = np.exp(model_test_preds) * test_df['gdp']**gdp_exponent\n",
    "        \n",
    "#         tv_df['model_forecast'] = model_tv_preds\n",
    "#         test_df['model_forecast'] = model_test_preds\n",
    "#     return output_tv_df, output_test_df\n",
    "    return tv_df['model_forecast'], test_df['model_forecast']\n",
    "#     return tv_df['model_forecast'], test_df['model_forecast']\n",
    "#     return model_tv_preds, model_test_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17cdb83-a789-4a40-a45e-550eda42f4f7",
   "metadata": {},
   "source": [
    "##### Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9af474-5d20-4906-bde3-5cfaa214daff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def skorch_trainer(model=TemporalConvNet, model_kwargs={}, tv_df=tv_df, test_df=test_df, #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "#                 countries=countries, stores=stores, products=products, random_seed=SEED,\n",
    "#                 target='target', wandb_tracked=False, forecasting=True):\n",
    "    \n",
    "#     # preprocessing\n",
    "    \n",
    "#     if USE_GPU and torch.cuda.is_available():\n",
    "#         device = 'cuda' \n",
    "#     else:\n",
    "#         device = 'cpu'\n",
    "    \n",
    "#     # start by creating working copies of dataframes to avoid mutation\n",
    "#     working_tv_df = tv_df.copy()\n",
    "#     working_test_df = test_df.copy()\n",
    "    \n",
    "#     # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "#     le_dict, working_tv_df = label_encoder(working_tv_df) # should leave broader scope's tv_df alone\n",
    "#     _, working_test_df = label_encoder(working_test_df) # should leave broader scope's test_df alone\n",
    "# #     del df_train, df_test\n",
    "    \n",
    "#     # encode the lists of countries, stores, and products\n",
    "#     countries = le_dict['country'].transform(countries)\n",
    "#     stores = le_dict['store'].transform(stores)\n",
    "#     products = le_dict['product'].transform(products)\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         exmodel_config['arch'] = arch\n",
    "# #         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "#         wandb.init(\n",
    "#             project=\"202201_Kaggle_tabular_playground\",\n",
    "#             save_code=True,\n",
    "#             tags=wandb_config['tags'],\n",
    "#             name=wandb_config['name'],\n",
    "#             notes=wandb_config['notes'],\n",
    "#             config=exmodel_config\n",
    "#     )\n",
    "    \n",
    "#     if forecasting: # if not, implement GroupKFold\n",
    "#         train_df = working_tv_df[working_tv_df['date'] < '2018-01-01']\n",
    "#         valid_df = working_tv_df[working_tv_df['date'] >= '2018-01-01']\n",
    "    \n",
    "#     # convert the dates to ordinals\n",
    "#     train_df['date'] = train_df['date'].map(dt.datetime.toordinal)\n",
    "#     valid_df['date'] = valid_df['date'].map(dt.datetime.toordinal)\n",
    "#     working_test_df['date'] = working_test_df['date'].map(dt.datetime.toordinal)\n",
    "    \n",
    "#     # typecast to np.float32\n",
    "#     train_df = train_df.astype(np.float32)\n",
    "#     valid_df = valid_df.astype(np.float32)\n",
    "#     working_test_df = working_test_df.astype(np.float32)\n",
    "    \n",
    "#     # clean up features\n",
    "#     X = train_df.drop(columns=['num_sold', 'target'])\n",
    "#     y = train_df[target]\n",
    "    \n",
    "#     X_valid = valid_df.drop(columns=['num_sold', 'target'])\n",
    "#     y_valid = valid_df[target]\n",
    "    \n",
    "#     X_test = working_test_df.drop(columns=['num_sold', 'target'])\n",
    "    \n",
    "#     # tensorify\n",
    "#     X = torch.tensor(X.values, dtype=torch.float32)\n",
    "#     X_valid = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "#     X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "#     y = torch.tensor(np.array(y).reshape(-1,1), dtype=torch.float32)\n",
    "#     y_valid = torch.tensor(np.array(y_valid).reshape(-1,1), dtype=torch.float32)\n",
    "    \n",
    "#     print(X.shape, y.shape)\n",
    "#     print(X.dtype, y.dtype)\n",
    "    \n",
    "#     model = NeuralNetRegressor(\n",
    "#         module=model,\n",
    "#         module__num_inputs=1,\n",
    "#         module__num_channels=[10] * 11,\n",
    "#         module__output_sz=1, #2 * samples_per_hour,\n",
    "#         module__kernel_size=5,\n",
    "#         module__dropout=0.0,\n",
    "#         max_epochs=3, # 60,\n",
    "#         batch_size=256,\n",
    "#         lr=2e-3,\n",
    "#         optimizer=torch.optim.Adam,\n",
    "#         device=device,\n",
    "#     #     iterator_train__shuffle=True,\n",
    "#     #     callbacks=[GradientNormClipping(gradient_clip_value=1,\n",
    "#     #                                     gradient_clip_norm_type=2)],\n",
    "#         train_split=None,\n",
    "#     )\n",
    "    \n",
    "#     model.fit(X,y)\n",
    "    \n",
    "#     y_valid_preds = model.predict(X_valid)\n",
    "# #     tv_preds = model.predict()\n",
    "#     test_preds = model.predict(X_test)\n",
    "    \n",
    "# #     print(f\"SMAPE on validation set (2018) is: {SMAPE(y_pred=y_valid_preds, y_true=y_valid)}\")\n",
    "    \n",
    "#     return model, y_valid_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b250930-2bc7-48e7-b401-ddce2a74e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae685b4-143a-418a-b042-79d3bb7a1553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def skorch_trainer(model, model_kwargs={}, tv_df=tv_df, test_df=test_df, folds=prophet_folds,#X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                countries=countries, stores=stores, products=products, random_seed=SEED,\n",
    "                target='target', wandb_tracked=False, forecasting=True):\n",
    "    \n",
    "    # preprocessing\n",
    "    \n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        device = 'cuda' \n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    \n",
    "    # start by creating working copies of dataframes to avoid mutation\n",
    "#     working_tv_df = tv_df.copy()\n",
    "#     working_test_df = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, tv_df = label_encoder(tv_df) # should leave broader scope's tv_df alone\n",
    "    _, test_df = label_encoder(test_df) # should leave broader scope's test_df alone\n",
    "#     del df_train, df_test\n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "#     y_tv = tv_df['num_sold']\n",
    "    tv_preds = pd.Series(0, index=tv_df.index)\n",
    "    test_preds = pd.Series(0, index=test_df.index)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    # handling each combination of country, store, and product separately\n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                print(f\"Training {le_dict['country'].inverse_transform([country])}, {le_dict['store'].inverse_transform([store])}, {le_dict['product'].inverse_transform([product])}\")\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (tv_df['date'] >= start) &\\\n",
    "                                (tv_df['date'] < end) &\\\n",
    "                                (tv_df['country'] == country) &\\\n",
    "                                (tv_df['store'] == store) &\\\n",
    "                                (tv_df['product'] == product)\n",
    "\n",
    "#                     print(train_idx)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = tv_df.loc[train_idx, :].reset_index(drop=True)\n",
    "#                         print(train.shape)\n",
    "\n",
    "                    val_idx = (tv_df['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (tv_df['date'] < folds[fold + 1][1]) &\\\n",
    "                              (tv_df['country'] == country) &\\\n",
    "                              (tv_df['store'] == store) &\\\n",
    "                              (tv_df['product'] == product)\n",
    "\n",
    "                    val = tv_df.loc[val_idx, :].reset_index(drop=True)\n",
    "\n",
    "                    test_idx = (test_df['country'] == country) &\\\n",
    "                               (test_df['store'] == store) &\\\n",
    "                               (test_df['product'] == product)\n",
    "                    test = test_df.loc[test_idx, :].reset_index(drop=True)\n",
    "                    \n",
    "                    y = train[target]\n",
    "                    y_valid = val[target]\n",
    "                    \n",
    "                    # with the training and validation sets sorted out, make them integers for model fitting\n",
    "                    for df in [train, val, test]:\n",
    "                        df['date'] = df['date'].map(dt.datetime.toordinal)\n",
    "                        df = df.drop(columns=['num_sold', 'target'], inplace=True)\n",
    "#                         df = df.astype(np.float32)\n",
    "                    \n",
    "#                     print(train.columns)\n",
    "#                     print(train.dtypes)\n",
    "#                     train_df = train_df.astype(np.float32)\n",
    "                    X, X_valid, X_test = train.astype(np.float32), val.astype(np.float32), test.astype(np.float32)\n",
    "#                         for feature in ['num_sold', 'target', 'model_forecast']:\n",
    "#                             if feature in df.columns:\n",
    "#                                 df = df.drop(columns=feature)\n",
    "#                     if 'model_forecast' in train.columns:\n",
    "#                         X = train.drop(columns=['num_sold', 'target', 'model_forecast'])\n",
    "#                         X_valid = val.drop(columns=['num_sold', 'target', 'model_forecast'])\n",
    "#                         X_test = test.drop(columns=['num_sold', 'target', 'model_forecast'])\n",
    "#                     else:\n",
    "#                         X = train.drop(columns=['num_sold', 'target'])\n",
    "#                         X_valid = val.drop(columns=['num_sold', 'target'])\n",
    "#                         X_test = test.drop(columns=['num_sold', 'target'])\n",
    "\n",
    "                    \n",
    "                    \n",
    "#                     X = train_df.drop(columns=['num_sold', 'target'])\n",
    "#                     y = train_df[target]\n",
    "\n",
    "#                     X_valid = valid_df.drop(columns=['num_sold', 'target'])\n",
    "#                     y_valid = valid_df[target]\n",
    "\n",
    "#                     X_test = working_test_df.drop(columns=['num_sold', 'target'])\n",
    "\n",
    "                    # tensorify\n",
    "#                     print(X.dtypes)\n",
    "#                     print(type(X.values))\n",
    "                    X = torch.tensor(X.values, dtype=torch.float32)\n",
    "                    X_valid = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "                    X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "                    y = torch.tensor(np.array(y).reshape(-1,1), dtype=torch.float32)\n",
    "                    y_valid = torch.tensor(np.array(y_valid).reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "#                     print(X.shape, y.shape)\n",
    "#                     print(X.dtype, y.dtype)\n",
    "\n",
    "                    net = NeuralNetRegressor(\n",
    "                        module=model,\n",
    "                        device=device,\n",
    "                        **model_kwargs\n",
    "                    #     iterator_train__shuffle=True,\n",
    "#                         callbacks=[Checkpoint(dirname=modelpath/'20220128-TCN-country{country}-store{store}-product{product}/')],\n",
    "                    #     callbacks=[GradientNormClipping(gradient_clip_value=1,\n",
    "                    #                                     gradient_clip_norm_type=2)],\n",
    "                        \n",
    "                    )\n",
    "\n",
    "                    net.fit(X,y)\n",
    "                    \n",
    "                    net.save_params(f_params=modelpath/f'20220128-TCN-country{country}-store{store}-product{product}-model_params.pkl')\n",
    "            \n",
    "                    y_train_preds = np.squeeze(net.predict(X))\n",
    "                    y_valid_preds = np.squeeze(net.predict(X_valid))\n",
    "                    fold_test_preds = np.squeeze(net.predict(X_test))\n",
    "#                     print(f\"Shape of fold test preds is {fold_test_preds.shape}\")\n",
    "\n",
    "                    tv_preds[train_idx] = y_train_preds\n",
    "                    tv_preds[val_idx] = y_valid_preds\n",
    "                    test_preds[test_idx] = fold_test_preds\n",
    "            \n",
    "                    print(f\"Valid SMAPE for {le_dict['country'].inverse_transform([country])}, {le_dict['store'].inverse_transform([store])}, {le_dict['product'].inverse_transform([product])} is {SMAPE(y_true=tv_df.loc[val_idx, 'num_sold'], y_pred=y_valid_preds)}\")\n",
    "                    \n",
    "    # reverse the dependent variable transform if appropriate\n",
    "    if target == 'target':\n",
    "#             model_tv_preds = np.multiply(np.exp(model_tv_preds), tv_df['gdp']**gdp_exponent)\n",
    "#         tv_df['model_forecast'] = np.exp(tv_df['model_forecast']) * tv_df['gdp']**gdp_exponent\n",
    "        tv_preds = np.exp(tv_preds) * tv_df['gdp']**gdp_exponent\n",
    "        test_preds = np.exp(test_preds) * test_df['gdp']**gdp_exponent\n",
    "        \n",
    "    return tv_preds, test_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a1836-4b75-40c1-90e6-08878c6953d3",
   "metadata": {},
   "source": [
    "##### GBMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09550a2-eee0-4fde-a083-c7c644133e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb330ff5-a835-4e03-bd2b-efbc21169df0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gbm_trainer(arch:str, model_kwargs={}, tv_df=tv_df, test_df=test_df,  #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                countries=countries, stores=stores, products=products, random_seed=SEED,\n",
    "                target='target', wandb_tracked=False):\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    X = tv_df.copy()\n",
    "    X_test = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, X = label_encoder(X) # should leave broader scope's tv_df alone\n",
    "    _, X_test = label_encoder(X_test) # should leave broader scope's test_df alone\n",
    "#     del df_train, df_test\n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "#     train_smape = 0\n",
    "#     val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "        )\n",
    "    \n",
    "    # drop whichever version of the dependent variable is not being used\n",
    "#     for df in [tv_df, test_df]:\n",
    "    y = X[target]\n",
    "#     for df in [X, X_test]:\n",
    "#         df = df.drop(columns=['num_sold', 'target'])\n",
    "    X = X.drop(columns=['num_sold', 'target'])\n",
    "    X_test = X_test.drop(columns=['num_sold', 'target'])\n",
    "#     X = X.drop(columns)\n",
    "#     if target == 'num_sold': \n",
    "#         y = X['num_sold']\n",
    "#         X = X.drop(columns=['target'])\n",
    "#         X_test = X_test.drop(columns=['target'])\n",
    "#     else:\n",
    "#         X = X.drop(columns=['num_sold'])\n",
    "#         X_test = X_test.drop(columns=['num_sold'])\n",
    "    \n",
    "    kfold = GroupKFold(n_splits=4)\n",
    "    oof_preds = pd.Series(0, index=tv_df.index)\n",
    "#     oof_preds, oof_y = [], []\n",
    "#     test_preds = np.zeros((X_test.shape[0]))\n",
    "    test_preds = pd.Series(0, index=test_df.index)\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(tv_df, groups=tv_df.date.dt.year)):\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        # remove dates \n",
    "#         for df in [X, X_test]:\n",
    "#             df = df.drop(columns=['date'])\n",
    "        if 'date' in X.columns:\n",
    "            X = X.drop(columns=['date'])\n",
    "            X_test = X_test.drop(columns=['date'])#, 'num_sold'])\n",
    "        \n",
    "        y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "        X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:]\n",
    "        \n",
    "        if arch == 'xgboost':\n",
    "            model = XGBRegressor(\n",
    "                tree_method= 'gpu_hist',\n",
    "                predictor= 'gpu_predictor',\n",
    "                eval_metric= ['mae', 'mape'],\n",
    "                sampling_method= 'gradient_based',\n",
    "                grow_policy= 'lossguide',\n",
    "                seed=random_seed,\n",
    "                objective='reg:squarederror',\n",
    "                **model_kwargs)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        elif arch == 'lightgbm':\n",
    "            model = LGBMRegressor(\n",
    "                random_state=random_seed,\n",
    "                **model_kwargs)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        \n",
    "        elif arch == 'catboost':\n",
    "            model = CatBoostRegressor(\n",
    "                task_type='GPU',\n",
    "                silent=True,\n",
    "                random_state=random_seed,\n",
    "                **model_kwargs)\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_preds = model.predict(X_valid)\n",
    "        \n",
    "        oof_preds[valid_ids] = y_valid_preds\n",
    "#         oof_preds.extend(y_valid_preds)\n",
    "#         oof_y.extend(y_valid)\n",
    "        \n",
    "        if arch == 'catboost':\n",
    "            test_preds += model.predict(X_test).flatten()\n",
    "        else:\n",
    "            test_preds += model.predict(X_test)\n",
    "        \n",
    "#         fold_smape = SMAPE(y_true=y_valid, y_pred=y_valid_preds)\n",
    "#         print(f\"FOLD {fold} OOF SMAPE: {fold_smape}\")\n",
    "    test_preds /= 4 # taking the average of the test preds\n",
    "    \n",
    "    if target == 'target':\n",
    "        oof_preds = np.exp(oof_preds) * tv_df['gdp']**gdp_exponent\n",
    "        test_preds = np.exp(test_preds) * test_df['gdp']**gdp_exponent\n",
    "        \n",
    "    smape = SMAPE(y_pred=oof_preds, y_true=tv_df['num_sold'])\n",
    "#     print(\"Lengths of oof_preds and tv_df[target] are same? \", len(oof_preds) == len(tv_df[target]))\n",
    "#     print(oof_preds[:10])\n",
    "#     print(tv_df[target][:10])\n",
    "    print(f\"SMAPE: {smape}\")\n",
    "    if wandb_tracked:\n",
    "        wandb.log({\n",
    "            'arch': arch,\n",
    "            'SMAPE': smape,\n",
    "            'model_params': str(model_kwargs),\n",
    "            'model_seed': random_state\n",
    "        })\n",
    "        wandb.finish()\n",
    "    return oof_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a60ac-1593-4e94-be1e-732628d68de3",
   "metadata": {},
   "source": [
    "# TSAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10542a-9a1c-4dff-8058-4e2fbe942a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data(tv_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
