{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba48c47a-dd6b-4902-b333-86a223e82418",
   "metadata": {},
   "source": [
    "# Hybrid\n",
    "Going to attempt a hybrid model after the example of [this Teck Meng Wong notebook](https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series/notebook).\n",
    "\n",
    "- 20220122: Going to try to form ensembles, with more code architecture. Forecasting models will include Prophet, NeuralProphet, Ridge, and Linear (with more to come -- e.g. perhaps transformers and other DNNs); residual models will include GBMs, perhaps some tabular DNNs too.\n",
    "\n",
    "- 20220129: Trying Teck Meng Wong's new feature engineering techniques. Have been trying different combinations of forecasters (including/excluding Huber and Lasso, while keeping in Prophet, NeuralProphet, Linear, Ridge, and Earth) and different meta-architectural techniques (GBMs coequal with the forecasters, GBMs on residuals only, GBMs incorporating forecaster predictions, 1-3 seeds of GBMs, and topping off the ensemble with either Lasso or Ridge), but haven't made real progress past the (single-seed GBMs+main forecasters with Lasso at the end. Have a working implementation of the TCN, but its performance is not good (yet). Will also want to get that working better (maybe also a [[time series Transformer|TST]] and/or [[temporal fusion Transformer|TFT]] if I get greedy and have the time?) and throw in an [[multilayer perceptron|MLP]], perhaps wherever I've experimented with GBMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55dfc634-98f0-45b8-bf65-931d28ddb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "# if '/sf/' in pwd:\n",
    "#     COLAB, SAGE = False, False\n",
    "# elif 'google.colab' in str(get_ipython()):\n",
    "#     COLAB, SAGE = True, False # do colab-specific installs later\n",
    "# else:\n",
    "#     COLAB, SAGE = False, True\n",
    "    \n",
    "CONTEXT = 'local' # or 'colab', 'sage', 'kaggle'\n",
    "USE_GPU = True \n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a1fae-dd27-45ff-a494-d3572d5893dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c99bcc4-0451-4896-ba41-78477c3a890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests # for telegram notifications\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720163c-934a-472a-bdef-40de5b849b3e",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf5b0c8-8b46-41d2-8d6e-e017bfcc5540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model selection\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# normalization\n",
    "# from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\n",
    "# from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "# feature generation\n",
    "# import category_encoders as ce\n",
    "\n",
    "# models\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "# from pytorch_widedeep import Trainer\n",
    "# from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "# from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT#, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "# from pytorch_widedeep.metrics import Accuracy\n",
    "# from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint\n",
    "\n",
    "# feature reduction\n",
    "# from sklearn.decomposition import PCA\n",
    "# from umap import UMAP\n",
    "\n",
    "# clustering\n",
    "# from sklearn.cluster import DBSCAN, KMeans\n",
    "# import hdbscan\n",
    "\n",
    "# feature selection\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "# import featuretools as ft\n",
    "# from BorutaShap import BorutaShap\n",
    "# from boruta import BorutaPy\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"nb_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0254c48-91ac-46a6-afa0-617f892e9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03044bf-61e9-4bca-b3b6-f649bc8d8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee770b73-8343-4336-889e-f54d6b1e35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time series\n",
    "# import tsfresh\n",
    "\n",
    "# import darts\n",
    "# from darts import TimeSeries\n",
    "# from darts.models import ExponentialSmoothing, AutoARIMA, ARIMA, Prophet, RandomForest, RegressionEnsembleModel, RegressionModel, TFTModel, TCNModel, TransformerModel, NBEATSModel\n",
    "import holidays\n",
    "import dateutil.easter as easter\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188812f6-26a6-4b9c-b018-094861c5c077",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5b480-e54e-4e88-8826-8b453d3b9cd4",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81678474-39ce-4416-9de5-4ca973454e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTEXT == 'colab':\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    # datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/dec2021/')\n",
    "    root = Path('') # TODO\n",
    "\n",
    "elif CONTEXT == 'sage':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "elif CONTEXT == 'kaggle':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "else: # if on local machine\n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    modelpath = root/'models'\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'studies'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath, studypath, modelpath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1c918-7f76-4f7b-b194-7b9e19e4b87e",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9443f858-8b66-4a01-a9ab-aaaf32c54186",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything but the models\n",
    "def seed_everything(seed, pytorch=True, reproducible=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if pytorch:\n",
    "        torch.manual_seed(seed) # set torch CPU seed\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed) # set torch GPU(s) seed(s)\n",
    "        if reproducible and torch.backends.cudnn.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb8923c-bf74-4de6-bffc-e041b8dab680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Function to reduce memory usage by downcasting datatypes in a Pandas DataFrame when possible.\n",
    "    \n",
    "    h/t to Bryan Arnold (https://www.kaggle.com/puremath86/label-correction-experiments-tps-nov-21)\n",
    "    \"\"\"\n",
    "    \n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d92853-fc1b-48bc-9236-2777b2209570",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_api_token = 'your_api_token' # for Galileo (jupyter_watcher_bot) on Telegram\n",
    "tg_chat_id = 'your_chat_id'\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_tg_message(text='Cell execution completed.'):  \n",
    "    \"\"\"\n",
    "    h/t Ivan Dembicki Jr. for the base version \n",
    "    (https://medium.com/@ivan.dembicki.jr/notifications-in-jupyter-notebook-with-telegram-f2e892c55173)\n",
    "    \"\"\"\n",
    "    requests.post('https://api.telegram.org/' +  'bot{}/sendMessage'.format(tg_api_token),\n",
    "                  params=dict(chat_id=tg_chat_id, text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ac144e-1440-40cf-b172-bff04134d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(y_true, y_pred):\n",
    "    '''\n",
    "    h/t Jean-François Puget (@CPMP) -- see https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\n",
    "    '''\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deb77153-1063-44a4-953f-043a35bd01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/282735\n",
    "def better_than_median(inputs, axis):\n",
    "    \"\"\"Compute the mean of the predictions if there are no outliers,\n",
    "    or the median if there are outliers.\n",
    "\n",
    "    Parameter: inputs = ndarray of shape (n_samples, n_folds)\"\"\"\n",
    "    spread = inputs.max(axis=axis) - inputs.min(axis=axis) \n",
    "    spread_lim = 0.45\n",
    "    print(f\"Inliers:  {(spread < spread_lim).sum():7} -> compute mean\")\n",
    "    print(f\"Outliers: {(spread >= spread_lim).sum():7} -> compute median\")\n",
    "    print(f\"Total:    {len(inputs):7}\")\n",
    "    return np.where(spread < spread_lim,\n",
    "                    np.mean(inputs, axis=axis),\n",
    "                    np.median(inputs, axis=axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3bac0f6-8bb1-453c-b567-398523792a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series\n",
    "def plot_periodogram(ts, detrend='linear', ax=None):\n",
    "    from scipy.signal import periodogram\n",
    "    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n",
    "    freqencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend=detrend,\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.step(freqencies, spectrum, color=\"purple\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Annual (1)\",\n",
    "            \"Semiannual (2)\",\n",
    "            \"Quarterly (4)\",\n",
    "            \"Bimonthly (6)\",\n",
    "            \"Monthly (12)\",\n",
    "            \"Biweekly (26)\",\n",
    "            \"Weekly (52)\",\n",
    "            \"Semiweekly (104)\",\n",
    "        ],\n",
    "        rotation=30,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Periodogram\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19a312e2-0175-41b3-a757-a81fb1186fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series\n",
    "def fourier_features(index, freq, order):\n",
    "    time = np.arange(len(index), dtype=np.float32)\n",
    "    k = 2 * np.pi * (1 / freq) * time\n",
    "    features = {}\n",
    "    for i in range(1, order + 1):\n",
    "        features.update({\n",
    "            f\"sin_{freq}_{i}\": np.sin(i * k),\n",
    "            f\"cos_{freq}_{i}\": np.cos(i * k),\n",
    "        })\n",
    "    return pd.DataFrame(features, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e560bef-8d90-4f5d-b6b1-4eeb5c0f5b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f704be-62c1-4f86-aec6-3cf2975d64cd",
   "metadata": {},
   "source": [
    "### Original Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59ed69e0-b9c6-4017-956f-b19e25238e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'train.csv'),\n",
    "    'target_source': str(datapath/'train.csv'),\n",
    "    'test_source': str(datapath/'test.csv'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')\n",
    "orig_train_df = train_df.copy()\n",
    "orig_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa9603-5103-4261-bbf4-47ad71eb1042",
   "metadata": {},
   "source": [
    "Since the dates are natively `Object` dtype (i.e. strings), we have to convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c268a78-b3db-4dbd-beee-790012b3cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model\n",
    "for df in [train_df, test_df]:\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "# for convenience later\n",
    "countries = ['Sweden', 'Finland', 'Norway']\n",
    "stores = ['KaggleMart', 'KaggleRama']\n",
    "products = ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d9b59-f4f9-487b-806d-1f69b7e68450",
   "metadata": {},
   "source": [
    "Provisionally, I'm going to concatenate together the `train_df` and `test_df` for preprocessing, to avoid having to constantly apply transforms twice (since I don't anticipate doing any transforms that might allow data leakage to occur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e13b145-46f1-424b-86db-4a89fabcd0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "# all_df.columns\n",
    "print(len(all_df) == len(train_df) + len(test_df))\n",
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57249aa-2cec-49be-bbc6-c90773bf541c",
   "metadata": {},
   "source": [
    "### GDP Data\n",
    "Here's data from Carl McBride Ellis ([notebook](https://www.kaggle.com/carlmcbrideellis/gdp-of-finland-norway-and-sweden-2015-2019) and [dataset](https://www.kaggle.com/carlmcbrideellis/gdp-20152019-finland-norway-and-sweden) for doing GDP comparisons. They're frequently used in other entries. I've created a function to add them on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "194cf800-7a26-4e72-a71e-ad0e44887565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gdp_data(df):\n",
    "    gdp_df = pd.read_csv(datapath/'GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n",
    "    gdp_df.set_index('year', inplace=True)\n",
    "    def get_gdp(row):\n",
    "        country = 'GDP_' + row.country\n",
    "        return gdp_df.loc[row.date.year, country]\n",
    "\n",
    "    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f745cd8-5e1b-4e98-90c1-d68a55b7b3a4",
   "metadata": {},
   "source": [
    "I'll also define here (but perhaps move later) the GDP exponent, which will be used to transform the targets before inference (dividing num_sold by the $GDP^{1.212}$ and then taking the logarithm (after @ambrosm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e373efbe-04ec-4b49-bb89-ab2903e7b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_exponent = 1.2121103201489674 # see https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model for an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66cbab38-f2c9-4e1a-9b32-5a72e4756885",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = add_gdp_data(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7531064c-b43c-4ab2-b990-b8866aa1c0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>gdp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>329.0</td>\n",
       "      <td>5.461456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>520.0</td>\n",
       "      <td>5.461456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>146.0</td>\n",
       "      <td>5.461456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>572.0</td>\n",
       "      <td>5.461456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>911.0</td>\n",
       "      <td>5.461456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>32863</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>32864</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>32865</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>32866</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>32867</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32868 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id       date  country       store         product  num_sold  \\\n",
       "0          0 2015-01-01  Finland  KaggleMart      Kaggle Mug     329.0   \n",
       "1          1 2015-01-01  Finland  KaggleMart      Kaggle Hat     520.0   \n",
       "2          2 2015-01-01  Finland  KaggleMart  Kaggle Sticker     146.0   \n",
       "3          3 2015-01-01  Finland  KaggleRama      Kaggle Mug     572.0   \n",
       "4          4 2015-01-01  Finland  KaggleRama      Kaggle Hat     911.0   \n",
       "...      ...        ...      ...         ...             ...       ...   \n",
       "6565   32863 2019-12-31   Sweden  KaggleMart      Kaggle Hat       NaN   \n",
       "6566   32864 2019-12-31   Sweden  KaggleMart  Kaggle Sticker       NaN   \n",
       "6567   32865 2019-12-31   Sweden  KaggleRama      Kaggle Mug       NaN   \n",
       "6568   32866 2019-12-31   Sweden  KaggleRama      Kaggle Hat       NaN   \n",
       "6569   32867 2019-12-31   Sweden  KaggleRama  Kaggle Sticker       NaN   \n",
       "\n",
       "           gdp  \n",
       "0     5.461456  \n",
       "1     5.461456  \n",
       "2     5.461456  \n",
       "3     5.461456  \n",
       "4     5.461456  \n",
       "...        ...  \n",
       "6565  6.282042  \n",
       "6566  6.282042  \n",
       "6567  6.282042  \n",
       "6568  6.282042  \n",
       "6569  6.282042  \n",
       "\n",
       "[32868 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012426d-60f2-484d-b970-4f6e9e7b4501",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3c71c-2040-41b7-afa9-cdecfe882fed",
   "metadata": {},
   "source": [
    "### Time Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebfbaa-3575-4030-bcf2-6ba358035c6e",
   "metadata": {},
   "source": [
    "The goal of this function is to create features that will capture seasonalities -- but **not** trends. The trends will (hopefully) be captured by the deployment of linear forecasting algorithms on raw time series data (consisting exclusively of dates and targets); we want to have seasonalities that the residual models can learn, however -- holidays, weekly patterns, climactic season patterns, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20669349-b517-4f24-9c78-3bc3e3f98427",
   "metadata": {},
   "source": [
    "The cell below will generate the `holidays` library's entries for the three countries. I may want to follow the template of @teckmengwong's code below, and add more holidays -- then, do some feature importance checking, and perhaps whittle down the features accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a03496b7-70e7-4546-a855-899f37d89e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in [holidays.Finland, holidays.Sweden, holidays.Norway]:\n",
    "#     print(c)\n",
    "    for h in c(years = [2019], observed=True).items():\n",
    "#         print(h)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06915aa4-7552-4c65-83c3-b69d1922d3d9",
   "metadata": {},
   "source": [
    "Here are the new FE techniques and helper techniques proposed by Teck Meng Wong (added as alt on 20220129, from [here](https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series#Data/Feature-Engineering))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0af50312-8206-40c7-95b7-87801725bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor, sqrt\n",
    "# from https://www.kaggle.com/fergusfindley/ensembling-and-rounding-techniques-comparison\n",
    "def geometric_round(arr):\n",
    "    result_array = arr\n",
    "    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n",
    "    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n",
    "\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "371eed13-707c-411e-9635-0f46de7c49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = \"date\"\n",
    "YEAR = \"year\"\n",
    "QUARTER = \"quarter\"\n",
    "MONTH = \"month\"\n",
    "WEEK = \"week\"\n",
    "DAY = \"day\"\n",
    "DAYOFYEAR = \"dayofyear\"\n",
    "WEEKOFYEAR = \"weekofyear\"\n",
    "DAYOFMONTH = \"dayofMonth\"\n",
    "DAYOFWEEK = \"dayofweek\"\n",
    "WEEKDAY = \"weekday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5793e82c-8c15-4f80-83dd-f6f18dae2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "\n",
    "def periodic_spline_transformer(period, n_splines=None, degree=3):\n",
    "    if n_splines is None:\n",
    "        n_splines = period\n",
    "    n_knots = n_splines + 1  # periodic and include_bias is True\n",
    "    return SplineTransformer(\n",
    "        degree=degree,\n",
    "        n_knots=n_knots,\n",
    "        knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\n",
    "        extrapolation=\"periodic\",\n",
    "        include_bias=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5344b298-70b1-48fd-98d6-e7b9ee0a5f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "year_df = pd.DataFrame(\n",
    "    np.linspace(0, 365, 1000).reshape(-1, 1),\n",
    "    columns=[DAYOFYEAR],\n",
    ")\n",
    "splines = periodic_spline_transformer(365, n_splines=12, degree=2).fit_transform(year_df)\n",
    "splines_df = pd.DataFrame(\n",
    "    splines,\n",
    "    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\n",
    ")\n",
    "pd.concat([year_df, splines_df], axis=\"columns\").plot(x=DAYOFYEAR, cmap=plt.cm.tab20b)\n",
    "_ = plt.title(f\"Periodic spline-based encoding for the {DAYOFYEAR} feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50758811-45d9-4871-8cf8-d15a04c16116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/samuelcortinhas/tps-jan-22-quick-eda-hybrid-model/notebook\n",
    "def unofficial_holiday(df):\n",
    "    countries = {'Finland': 1, 'Norway': 2, 'Sweden': 3}\n",
    "    stores = {'KaggleMart': 1, 'KaggleRama': 2}\n",
    "    products = {'Kaggle Mug': 1,'Kaggle Hat': 2, 'Kaggle Sticker': 3}\n",
    "    \n",
    "    # load holiday info.\n",
    "#     hol_path = '../input/public-and-unofficial-holidays-nor-fin-swe-201519/holidays.csv'\n",
    "    hol_path = datapath/'holidays.csv'\n",
    "    holiday = pd.read_csv(hol_path)\n",
    "    \n",
    "    fin_holiday = holiday.loc[holiday.country == 'Finland']\n",
    "    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n",
    "    nor_holiday = holiday.loc[holiday.country == 'Norway']\n",
    "    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n",
    "    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n",
    "    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n",
    "    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n",
    "    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n",
    "    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n",
    "    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n",
    "    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bae16020-0634-4684-9d1a-17cbc16ec42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUID calendar columns\n",
    "MONTH_COLUMNS = []\n",
    "WEEKOFYEAR_COLUMNS = []\n",
    "DAYOFYEAR_COLUMNS = []\n",
    "WEEKDAY_COLUMNS = []\n",
    "\n",
    "for x in [MONTH,WEEKOFYEAR,DAYOFYEAR,WEEKDAY]:\n",
    "    for y in [f'mug_{x}', f'hat_{x}', f'stick_{x}']:\n",
    "        if x == MONTH:\n",
    "            MONTH_COLUMNS.append(y)\n",
    "        if x == WEEKOFYEAR:\n",
    "            WEEKOFYEAR_COLUMNS.append(y)\n",
    "        if x == DAYOFYEAR:\n",
    "            DAYOFYEAR_COLUMNS.append(y)\n",
    "        if x == WEEKDAY:\n",
    "            WEEKDAY_COLUMNS.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c16d64cd-7cb1-4884-a19a-5cd34a5ae8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_features(index, freq, order):\n",
    "    time = np.arange(len(index), dtype=np.float32)\n",
    "    k = 2 * np.pi * (1 / freq) * time\n",
    "    features = {}\n",
    "    for i in range(1, order + 1):\n",
    "        features.update({\n",
    "            f\"sin_{freq}_{i}\": np.sin(i * k),\n",
    "            f\"cos_{freq}_{i}\": np.cos(i * k),\n",
    "        })\n",
    "    return pd.DataFrame(features, index=index)\n",
    "\n",
    "def get_basic_ts_features(df):\n",
    "#     gdp_df = pd.read_csv('../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n",
    "    gdp_df = pd.read_csv(datapath/'GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n",
    "    gdp_df.set_index('year', inplace=True)\n",
    "#     gdp_exponent = 1.2121103201489674 # see https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model for an explanation\n",
    "    def get_gdp(row):\n",
    "        country = 'GDP_' + row.country\n",
    "        return gdp_df.loc[row.date.year, country] #**gdp_exponent\n",
    "\n",
    "    # Apply GDP log\n",
    "    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n",
    "    \n",
    "#     # Split GDP by country (for linear model)\n",
    "#     df['fin_gdp']=np.where(df['country'] == 'Finland', df['gdp'], 0)\n",
    "#     df['nor_gdp']=np.where(df['country'] == 'Norway', df['gdp'], 0)\n",
    "#     df['swe_gdp']=np.where(df['country'] == 'Sweden', df['gdp'], 0)\n",
    "    \n",
    "#     # Drop column\n",
    "#     df=df.drop(['gdp'],axis=1)\n",
    "    \n",
    "    # one-hot encoding should be used. linear model should not learn this as numeric value\n",
    "#     df[YEAR] = df[DATE].dt.year\n",
    "#     df[MONTH] = df[DATE].dt.month\n",
    "#     df[WEEKOFYEAR] = df[DATE].dt.isocalendar().week\n",
    "#     df[DAYOFYEAR] = df[DATE].dt.dayofyear\n",
    "#     df[WEEKDAY] = df[DATE].dt.weekday\n",
    "#     df[DAY] = df[DATE].dt.day # day in month\n",
    "#     df[DAYOFMONTH] = df[DATE].dt.days_in_month\n",
    "#     df[DAYOFWEEK] = df[DATE].dt.dayofweek\n",
    "#     df[MONTH] = df[DATE].dt.month # Min SMAPE: 4.005319478790032\n",
    "#     df[QUARTER] = df.date.dt.quarter\n",
    "\n",
    "    df['wd0'] = df[DATE].dt.weekday == 0 # + Monday\n",
    "    df['wd1'] = df[DATE].dt.weekday == 1 # Tuesday\n",
    "    df['wd2'] = df[DATE].dt.weekday == 2\n",
    "    df['wd3'] = df[DATE].dt.weekday == 3\n",
    "    df['wd4'] = df[DATE].dt.weekday == 4 # + Friday\n",
    "    df['wd56'] = df[DATE].dt.weekday >= 5 # + Weekend\n",
    "\n",
    "#     df[f'mug_wd4'] = np.where(df['product'] == 'Kaggle Mug', df[f'wd4'], False)\n",
    "#     df[f'mug_wd56'] = np.where(df['product'] == 'Kaggle Mug', df[f'wd56'], False)\n",
    "#     df[f'hat_wd4'] = np.where(df['product'] == 'Kaggle Hat', df[f'wd4'], False)\n",
    "#     df[f'hat_wd56'] = np.where(df['product'] == 'Kaggle Hat', df[f'wd56'], False)\n",
    "#     df[f'stick_wd4'] = np.where(df['product'] == 'Kaggle Sticker', df[f'wd4'], False)\n",
    "#     df[f'stick_wd56'] = np.where(df['product'] == 'Kaggle Sticker', df[f'wd56'], False)\n",
    "#     df = df.drop(columns=[f'wd4', f'wd56'])\n",
    "    # 4 seasons\n",
    "#     df['season'] = ((df[DATE].dt.month % 12 + 3) // 3).map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_splines(df):\n",
    "    # one-hot encoding should be used. linear model should not learn this as numeric value\n",
    "#     df[MONTH] = df[DATE].dt.month\n",
    "#     df[WEEKOFYEAR] = df[DATE].dt.isocalendar().week\n",
    "    df[WEEKDAY] = df[DATE].dt.weekday\n",
    "#     df[DAYOFYEAR] = df[DATE].dt.dayofyear\n",
    "    \n",
    "    dayofyear_splines = periodic_spline_transformer(365, n_splines=9, degree=2).fit_transform(df[DATE].dt.dayofyear.values.reshape(-1, 1))\n",
    "    splines_df = pd.DataFrame(\n",
    "        dayofyear_splines,\n",
    "        columns=[f\"spline_{i}\" for i in range(dayofyear_splines.shape[1])],\n",
    "    )\n",
    "    for i in range(dayofyear_splines.shape[1]):\n",
    "        df[f'mug_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Mug', splines_df[f\"spline_{i}\"], 0.)\n",
    "        df[f'hat_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Hat', splines_df[f\"spline_{i}\"], 0.)\n",
    "        df[f'stick_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Sticker', splines_df[f\"spline_{i}\"], 0.)\n",
    "#         df[f'fin_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Finland', splines_df[f\"spline_{i}\"], 0.)\n",
    "#         df[f'nor_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Norway', splines_df[f\"spline_{i}\"], 0.)\n",
    "#         df[f'swe_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Sweden', splines_df[f\"spline_{i}\"], 0.)\n",
    "\n",
    "#     weekofyear_splines = periodic_spline_transformer(52, n_splines=2, degree=2).fit_transform(df[DATE].dt.isocalendar().week.values.astype(np.float64).reshape(-1,1))\n",
    "#     splines_df = pd.DataFrame(\n",
    "#         weekofyear_splines,\n",
    "#         columns=[f\"spline_{i}\" for i in range(weekofyear_splines.shape[1])],\n",
    "#     )\n",
    "#     for i in range(weekofyear_splines.shape[1]):\n",
    "#         df[f'weekofyear_{WEEKOFYEAR}{i}'] = splines_df[f\"spline_{i}\"]\n",
    "#         df[f'hat_{WEEKOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Hat', splines_df[f\"spline_{i}\"], 0)\n",
    "#         df[f'stick_{WEEKOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Sticker', splines_df[f\"spline_{i}\"], 0)\n",
    "#     df[f'mug_{MONTH}'] = np.where(df['product'] == 'Kaggle Mug', df[MONTH], 0)\n",
    "#     df[f'mug_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Mug', df[WEEKOFYEAR], 0)\n",
    "#     df[f'mug_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Mug', df[DAYOFYEAR], 0)\n",
    "#     df[f'mug_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Mug', df[WEEKDAY], 0)\n",
    "#     df[f'hat_{MONTH}'] = np.where(df['product'] == 'Kaggle Hat', df[MONTH], 0)\n",
    "#     df[f'hat_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Hat', df[WEEKOFYEAR], 0)\n",
    "#     df[f'hat_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Hat', df[DAYOFYEAR], 0)\n",
    "#     df[f'hat_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Hat', df[WEEKDAY], 0)\n",
    "#     df[f'stick_{MONTH}'] = np.where(df['product'] == 'Kaggle Sticker', df[MONTH], 0)\n",
    "#     df[f'stick_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Sticker', df[WEEKOFYEAR], 0)\n",
    "#     df[f'stick_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Sticker', df[DAYOFYEAR], 0)\n",
    "#     df[f'stick_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Sticker', df[WEEKDAY], 0)\n",
    "\n",
    "#     df = df.drop(columns=[DAYOFYEAR]) #MONTH, WEEKOFYEAR, WEEKDAY\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_periodic(df):\n",
    "    # 21 days cyclic for lunar\n",
    "    # 21 4.244872419046287 31 4.23870 37 4.2359085545955875 47 4.24590382934362 39 4.236812122257115 \n",
    "    # 35 4.2358561209794665 33 4.237682217183017 36 4.230652791910613 3 4.241000488616227 4.23833321067532\n",
    "    #[7, 14, 21, 28, 30, 31, 91] range(1, 32, 4) range(1,3,1)[1,2,4]\n",
    "    # Long term periodic\n",
    "    dayofyear = df.date.dt.dayofyear\n",
    "    j=-36\n",
    "    for k in [2]:\n",
    "        df = pd.concat([df,\n",
    "                        pd.DataFrame({\n",
    "                            f\"sin{k}\": np.sin((dayofyear+j) / 365 * 1 * math.pi * k),\n",
    "                            f\"cos{k}\": np.cos((dayofyear+j) / 365 * 1 * math.pi * k),\n",
    "                                     })], axis=1)\n",
    "        # Products\n",
    "        df[f'mug_sin{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'sin{k}'], 0)\n",
    "        df[f'mug_cos{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'cos{k}'], 0)\n",
    "        df[f'hat_sin{k}'] = np.where(df['product'] == 'Kaggle Hat', df[f'sin{k}'], 0)\n",
    "        df[f'hat_cos{k}'] = np.where(df['product'] == 'Kaggle Hat', df[f'cos{k}'], 0)\n",
    "        df[f'stick_sin{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'sin{k}'], 0)\n",
    "        df[f'stick_cos{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'cos{k}'], 0)\n",
    "        df = df.drop(columns=[f'sin{k}', f'cos{k}'])\n",
    "\n",
    "    # Short term Periodic\n",
    "    weekday = df.date.dt.weekday\n",
    "    df[f'weekly_sin'] = np.sin((1 / 7) * 2 * math.pi*(weekday+1)) #+\n",
    "    df[f'weekly_cos'] = np.cos((1 / 7) * 2 * math.pi*(weekday+1)) #+\n",
    "    df[f'semiweekly_sin'] = np.sin((1 / 7) * 4 * math.pi*(dayofyear-1.5)) #+ ⁅sin(1/7 𝜋⋅4(𝑥−2))⁆\n",
    "    df[f'semiweekly_cos'] = np.cos((1 / 7) * 4 * math.pi*(dayofyear-1.5)) #+ ⁅cos(1/7 𝜋⋅4𝑥)⁆\n",
    "    \n",
    "    df[f'fin_weekly_sin'] = np.where(df['country'] == 'Finland', df[f'weekly_sin'], 0)\n",
    "    df[f'fin_weekly_cos'] = np.where(df['country'] == 'Finland', df[f'weekly_cos'], 0)\n",
    "    df[f'nor_weekly_sin'] = np.where(df['country'] == 'Norway', df[f'weekly_sin'], 0)\n",
    "    df[f'nor_weekly_cos'] = np.where(df['country'] == 'Norway', df[f'weekly_cos'], 0)\n",
    "    df[f'swe_weekly_sin'] = np.where(df['country'] == 'Sweden', df[f'weekly_sin'], 0)\n",
    "    df[f'swe_weekly_cos'] = np.where(df['country'] == 'Sweden', df[f'weekly_cos'], 0)\n",
    "    \n",
    "    df[f'mug_weekly_sin'] = np.where(df['product'] == 'Kaggle Mug', df[f'weekly_sin'], 0)\n",
    "    df[f'mug_weekly_cos'] = np.where(df['product'] == 'Kaggle Mug', df[f'weekly_cos'], 0)\n",
    "    df[f'hat_weekly_sin'] = np.where(df['product'] == 'Kaggle Hat', df[f'weekly_sin'], 0)\n",
    "    df[f'hat_weekly_cos'] = np.where(df['product'] == 'Kaggle Hat', df[f'weekly_cos'], 0)\n",
    "    df[f'stick_weekly_sin'] = np.where(df['product'] == 'Kaggle Sticker', df[f'weekly_sin'], 0)\n",
    "    df[f'stick_weekly_cos'] = np.where(df['product'] == 'Kaggle Sticker', df[f'weekly_cos'], 0)\n",
    "    \n",
    "    df[f'mug_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Mug', df[f'semiweekly_sin'], 0)\n",
    "    df[f'mug_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Mug', df[f'semiweekly_cos'], 0)\n",
    "    df[f'hat_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Hat', df[f'semiweekly_sin'], 0)\n",
    "    df[f'hat_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Hat', df[f'semiweekly_cos'], 0)\n",
    "    df[f'stick_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Sticker', df[f'semiweekly_sin'], 0)\n",
    "    df[f'stick_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Sticker', df[f'semiweekly_cos'], 0)\n",
    "    \n",
    "    df = df.drop(columns=['weekly_sin', 'weekly_cos', 'semiweekly_sin', 'semiweekly_cos'])\n",
    "    \n",
    "#     df[f'semiannual_sin'] = np.sin(dayofyear / 182.5 * 2 * math.pi)\n",
    "#     df[f'semiannual_cos'] = np.cos(dayofyear / 182.5 * 2 * math.pi)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_holiday(df):\n",
    "# Dec Jan\n",
    "    # End of year\n",
    "    df = pd.concat([df,\n",
    "                        pd.DataFrame({f\"f-dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "                                      for d in range(24, 32)}),\n",
    "                        pd.DataFrame({f\"n-dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "                                      for d in range(24, 32)}),\n",
    "                        pd.DataFrame({f\"s-dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "                                      for d in range(24, 32)}),\n",
    "                        pd.DataFrame({f\"f-jan{d}\":\n",
    "                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "                                      for d in range(1, 14)}),\n",
    "                        pd.DataFrame({f\"n-jan{d}\":\n",
    "                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "                                      for d in range(1, 10)}),\n",
    "                        pd.DataFrame({f\"s-jan{d}\":\n",
    "                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "                                      for d in range(1, 15)})\n",
    "                       ], axis=1)\n",
    "        \n",
    "    # May\n",
    "    df = pd.concat([df,\n",
    "                        pd.DataFrame({f\"may{d}\":\n",
    "                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n",
    "                                      for d in list(range(1, 10))}),\n",
    "                        pd.DataFrame({f\"may{d}\":\n",
    "                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & \n",
    "                                      (df.country == 'Norway')\n",
    "                                      for d in list(range(18, 28))})\n",
    "                        ], axis=1)\n",
    "    \n",
    "    # June and July 8, 14\n",
    "    df = pd.concat([df,\n",
    "                        pd.DataFrame({f\"june{d}\":\n",
    "                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & \n",
    "                                      (df.country == 'Sweden')\n",
    "                                      for d in list(range(8, 14))}),\n",
    "                       ], axis=1)\n",
    "    # Last Wednesday of June\n",
    "    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n",
    "                                         2016: pd.Timestamp(('2016-06-29')),\n",
    "                                         2017: pd.Timestamp(('2017-06-28')),\n",
    "                                         2018: pd.Timestamp(('2018-06-27')),\n",
    "                                         2019: pd.Timestamp(('2019-06-26'))})\n",
    "    df = pd.concat([df, pd.DataFrame({f\"wed_june{d}\": \n",
    "                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & \n",
    "                                      (df.country != 'Norway')\n",
    "                                      for d in list(range(-4, 6))})], axis=1)\n",
    "\n",
    "    # First Sunday of November\n",
    "    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n",
    "                                         2016: pd.Timestamp(('2016-11-6')),\n",
    "                                         2017: pd.Timestamp(('2017-11-5')),\n",
    "                                         2018: pd.Timestamp(('2018-11-4')),\n",
    "                                         2019: pd.Timestamp(('2019-11-3'))})\n",
    "    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n",
    "                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n",
    "                                      for d in list(range(0, 9))})], axis=1)\n",
    "    # First half of December (Independence Day of Finland, 6th of December)\n",
    "    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "                                      for d in list(range(6, 14))})], axis=1)\n",
    "    # Easter April\n",
    "    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n",
    "    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n",
    "                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n",
    "                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65629510-58cc-46da-b4cc-d99bb2e3c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_engineering(df):\n",
    "    df = get_basic_ts_features(df)\n",
    "    df = feature_splines(df)\n",
    "    df = feature_periodic(df)\n",
    "    df = feature_holiday(df)\n",
    "    df = unofficial_holiday(df)\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e13f9b94-6c79-4712-80da-e828ed887a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old feature engineering function\n",
    "\n",
    "# def temporal_engineering(df):\n",
    "#     '''\n",
    "#     Function inspired by / borrowing from @teckmengwong and @ambrosm to create time features that will\n",
    "#     capture seasonality.\n",
    "#     '''\n",
    "    \n",
    "# #     df[YEAR] = df[DATE].dt.year\n",
    "#     df['month'] = df['date'].dt.month\n",
    "# #     df['week'] = df['date'].dt.week # not used by Teck Meng Wong\n",
    "# #     df['day'] = df['date'].dt.day # not used by Teck Meng Wong\n",
    "# #     df['day_of_year'] = df['date'].dt.dayofyear # not used by Teck Meng Wong\n",
    "# #     df['day_of_month'] = df['date'].dt.days_in_month # not used by Teck Meng Wong\n",
    "# #     df['day_of_week'] = df['date'].dt.dayofweek # not used by Teck Meng Wong\n",
    "# #    df['weekday'] = df['date'].dt.weekday # not used by Teck Meng Wong\n",
    "#     # Teck Meng Wong mapped the integers to first-letters in triplets\n",
    "#     # I'm leaving it as integers, where winter=1, spring=2, summer=3, fall=4\n",
    "#     df['season'] = ((df['date'].dt.month % 12 + 3) // 3) #.map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n",
    "# #     df['month'] = df['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "\n",
    "#     df['wd4'] = df['date'].dt.weekday == 4\n",
    "#     df['wd56'] = df['date'].dt.weekday >= 5\n",
    "# #     df['wd6'] = df['date'].dt.weekday >= 6\n",
    "# #     df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), 'day_of_year'] += 1 # fix for leap years\n",
    "    \n",
    "#     # 21 days cyclic for lunar\n",
    "#     dayofyear = df.date.dt.dayofyear # for convenience\n",
    "    \n",
    "#     # here he's creating Fourier features\n",
    "#     for k in range(1, 32, 4):\n",
    "#         df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
    "#         df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
    "#         df[f'finland_sin{k}'] = np.where(df['country'] == 'Finland', df[f'sin{k}'], 0)\n",
    "#         df[f'finland_cos{k}'] = np.where(df['country'] == 'Finland', df[f'cos{k}'], 0)\n",
    "#         df[f'norway_sin{k}'] = np.where(df['country'] == 'Norway', df[f'sin{k}'], 0)\n",
    "#         df[f'norway_cos{k}'] = np.where(df['country'] == 'Norway', df[f'cos{k}'], 0)\n",
    "#         df[f'store_sin{k}'] = np.where(df['store'] == 'KaggleMart', df[f'sin{k}'], 0)\n",
    "#         df[f'store_cos{k}'] = np.where(df['store'] == 'KaggleMart', df[f'cos{k}'], 0)\n",
    "#         df[f'mug_sin{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'sin{k}'], 0)\n",
    "#         df[f'mug_cos{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'cos{k}'], 0)\n",
    "#         df[f'sticker_sin{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'sin{k}'], 0)\n",
    "#         df[f'sticker_cos{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'cos{k}'], 0)\n",
    "    \n",
    "# #     df[f'semiweekly_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 14)\n",
    "# #     df[f'semiweekly_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 14)\n",
    "# #     df[f'lunar_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 21)\n",
    "# #     df[f'lunar_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 21)\n",
    "#     df[f'season_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 91.5)\n",
    "#     df[f'season_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 91.5)\n",
    "# #     df = pd.concat([df, pd.DataFrame({f'fin{ptr[1]}':\n",
    "# #                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Finland')\n",
    "# #                                       for ptr in holidays.Finland(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "# #     df = pd.concat([df, pd.DataFrame({f'nor{ptr[1]}':\n",
    "# #                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Norway')\n",
    "# #                                       for ptr in holidays.Norway(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "# #     df = pd.concat([df, pd.DataFrame({f'swe{ptr[1]}':\n",
    "# #                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Sweden')\n",
    "# #                                       for ptr in holidays.Sweden(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "\n",
    "#     # End of year\n",
    "#     # Dec - teckmengwong\n",
    "#     for d in range(24, 32):\n",
    "#         df[f\"dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d)\n",
    "#     # I'm unsure of the logic of only doing this for Norway\n",
    "#     for d in range(24, 32):\n",
    "#         df[f\"n-dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "    \n",
    "#     # not sure why he's using different date ranges for each country here\n",
    "#     # Jan - teckmengwong\n",
    "#     for d in range(1, 14):\n",
    "#         df[f\"f-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "#     for d in range(1, 10):\n",
    "#         df[f\"n-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "#     for d in range(1, 15):\n",
    "#         df[f\"s-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "    \n",
    "    \n",
    "#     # May - tekcmengwong\n",
    "#     for d in list(range(1, 10)): # May Day and after, I guess\n",
    "#         df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d)\n",
    "#     for d in list(range(19, 26)):\n",
    "#         df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "#     # June \n",
    "#     for d in list(range(8, 14)):\n",
    "#         df[f\"june{d}\"] = (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "    \n",
    "#     #Swedish Rock Concert - teckmengwong\n",
    "#     #Jun 3, 2015 – Jun 6, 2015\n",
    "#     #Jun 8, 2016 – Jun 11, 2016\n",
    "#     #Jun 7, 2017 – Jun 10, 2017\n",
    "#     #Jun 6, 2018 – Jun 10, 2018\n",
    "#     #Jun 5, 2019 – Jun 8, 2019\n",
    "#     swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),\n",
    "#                                          2016: pd.Timestamp(('2016-06-11')),\n",
    "#                                          2017: pd.Timestamp(('2017-06-10')),\n",
    "#                                          2018: pd.Timestamp(('2018-06-10')),\n",
    "#                                          2019: pd.Timestamp(('2019-06-8'))})\n",
    "\n",
    "#     df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n",
    "#                                       (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n",
    "#                                       for d in list(range(-3, 3))})], axis=1)\n",
    "\n",
    "    \n",
    "#     # Last Wednesday of June - teckmengwong\n",
    "#     wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n",
    "#                                          2016: pd.Timestamp(('2016-06-29')),\n",
    "#                                          2017: pd.Timestamp(('2017-06-28')),\n",
    "#                                          2018: pd.Timestamp(('2018-06-27')),\n",
    "#                                          2019: pd.Timestamp(('2019-06-26'))})\n",
    "#     for d in list(range(-4, 6)):\n",
    "#         df[f\"wed_june{d}\"] = (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n",
    "        \n",
    "#     # First Sunday of November - teckmengwong\n",
    "#     sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n",
    "#                                          2016: pd.Timestamp(('2016-11-6')),\n",
    "#                                          2017: pd.Timestamp(('2017-11-5')),\n",
    "#                                          2018: pd.Timestamp(('2018-11-4')),\n",
    "#                                          2019: pd.Timestamp(('2019-11-3'))})\n",
    "#     df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n",
    "#                                       (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n",
    "#                                       for d in list(range(0, 9))})], axis=1)\n",
    "    \n",
    "#     # First half of December (Independence Day of Finland, 6th of December) -teckmengwong\n",
    "#     df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n",
    "#                                       (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "#                                       for d in list(range(6, 14))})], axis=1)\n",
    "    \n",
    "#     # Easter -teckmengwong\n",
    "#     easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n",
    "#     df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n",
    "#                                       (df.date - easter_date == np.timedelta64(d, \"D\"))\n",
    "#                                       for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7541874-d7b9-4477-9fb2-1af5506c1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_all_df = temporal_engineering(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "479e68aa-3417-4671-a9d7-ea2c5d248ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>gdp</th>\n",
       "      <th>wd0</th>\n",
       "      <th>wd1</th>\n",
       "      <th>wd2</th>\n",
       "      <th>...</th>\n",
       "      <th>easter50</th>\n",
       "      <th>easter51</th>\n",
       "      <th>easter52</th>\n",
       "      <th>easter53</th>\n",
       "      <th>easter54</th>\n",
       "      <th>easter55</th>\n",
       "      <th>easter56</th>\n",
       "      <th>easter57</th>\n",
       "      <th>easter58</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>329.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>520.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>146.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>572.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>911.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>32863</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>32864</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>32865</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>32866</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>32867</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32868 rows × 208 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id       date  country       store         product  num_sold  \\\n",
       "0          0 2015-01-01  Finland  KaggleMart      Kaggle Mug     329.0   \n",
       "1          1 2015-01-01  Finland  KaggleMart      Kaggle Hat     520.0   \n",
       "2          2 2015-01-01  Finland  KaggleMart  Kaggle Sticker     146.0   \n",
       "3          3 2015-01-01  Finland  KaggleRama      Kaggle Mug     572.0   \n",
       "4          4 2015-01-01  Finland  KaggleRama      Kaggle Hat     911.0   \n",
       "...      ...        ...      ...         ...             ...       ...   \n",
       "6565   32863 2019-12-31   Sweden  KaggleMart      Kaggle Hat       NaN   \n",
       "6566   32864 2019-12-31   Sweden  KaggleMart  Kaggle Sticker       NaN   \n",
       "6567   32865 2019-12-31   Sweden  KaggleRama      Kaggle Mug       NaN   \n",
       "6568   32866 2019-12-31   Sweden  KaggleRama      Kaggle Hat       NaN   \n",
       "6569   32867 2019-12-31   Sweden  KaggleRama  Kaggle Sticker       NaN   \n",
       "\n",
       "           gdp    wd0    wd1    wd2  ...  easter50  easter51  easter52  \\\n",
       "0     5.461456  False  False  False  ...     False     False     False   \n",
       "1     5.461456  False  False  False  ...     False     False     False   \n",
       "2     5.461456  False  False  False  ...     False     False     False   \n",
       "3     5.461456  False  False  False  ...     False     False     False   \n",
       "4     5.461456  False  False  False  ...     False     False     False   \n",
       "...        ...    ...    ...    ...  ...       ...       ...       ...   \n",
       "6565  6.282042  False   True  False  ...     False     False     False   \n",
       "6566  6.282042  False   True  False  ...     False     False     False   \n",
       "6567  6.282042  False   True  False  ...     False     False     False   \n",
       "6568  6.282042  False   True  False  ...     False     False     False   \n",
       "6569  6.282042  False   True  False  ...     False     False     False   \n",
       "\n",
       "      easter53  easter54  easter55  easter56  easter57  easter58  holiday  \n",
       "0        False     False     False     False     False     False        1  \n",
       "1        False     False     False     False     False     False        1  \n",
       "2        False     False     False     False     False     False        1  \n",
       "3        False     False     False     False     False     False        1  \n",
       "4        False     False     False     False     False     False        1  \n",
       "...        ...       ...       ...       ...       ...       ...      ...  \n",
       "6565     False     False     False     False     False     False        0  \n",
       "6566     False     False     False     False     False     False        0  \n",
       "6567     False     False     False     False     False     False        0  \n",
       "6568     False     False     False     False     False     False        0  \n",
       "6569     False     False     False     False     False     False        0  \n",
       "\n",
       "[32868 rows x 208 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09014037-96b4-4037-835b-f9f23b87a7bb",
   "metadata": {},
   "source": [
    "At this point, the `temporal_all_df` DataFrame contains all the time features for both the training and testing sets.\n",
    "* **Todo**: consider not only adding in holidays from `holidays`, but also borrowing ideas from the AmbrosM Linear notebook too (which creates fewer features, populating them instead with temporal distances from the selected holidays)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86376eb-1c0b-4a8b-814d-b77eff89b925",
   "metadata": {},
   "source": [
    "### Target Transformation\n",
    "Now, I'll do the target transformation proposed by @AmbrosM. (I'll do it to the non-encoded DataFrame too, for testing with Prophet and NeuralProphet later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5542ef40-2c0e-4061-8e04-3a64e20cf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [temporal_all_df]:\n",
    "    df['target'] = np.log(df['num_sold'] / df['gdp']**gdp_exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20683a88-cfe1-499f-9ad1-bf15b5fea392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_all_df['target'] = np.log(encoded_all_df['num_sold'] / (encoded_all_df['gdp']**gdp_exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24c415b1-2f0e-4636-9928-ae713a0d74a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>gdp</th>\n",
       "      <th>wd0</th>\n",
       "      <th>wd1</th>\n",
       "      <th>wd2</th>\n",
       "      <th>...</th>\n",
       "      <th>easter51</th>\n",
       "      <th>easter52</th>\n",
       "      <th>easter53</th>\n",
       "      <th>easter54</th>\n",
       "      <th>easter55</th>\n",
       "      <th>easter56</th>\n",
       "      <th>easter57</th>\n",
       "      <th>easter58</th>\n",
       "      <th>holiday</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>329.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>3.738239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>520.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4.196010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>146.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2.925788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>572.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4.291321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>911.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4.756724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>32863</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>32864</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>32865</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>32866</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>32867</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32868 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id       date  country       store         product  num_sold  \\\n",
       "0          0 2015-01-01  Finland  KaggleMart      Kaggle Mug     329.0   \n",
       "1          1 2015-01-01  Finland  KaggleMart      Kaggle Hat     520.0   \n",
       "2          2 2015-01-01  Finland  KaggleMart  Kaggle Sticker     146.0   \n",
       "3          3 2015-01-01  Finland  KaggleRama      Kaggle Mug     572.0   \n",
       "4          4 2015-01-01  Finland  KaggleRama      Kaggle Hat     911.0   \n",
       "...      ...        ...      ...         ...             ...       ...   \n",
       "6565   32863 2019-12-31   Sweden  KaggleMart      Kaggle Hat       NaN   \n",
       "6566   32864 2019-12-31   Sweden  KaggleMart  Kaggle Sticker       NaN   \n",
       "6567   32865 2019-12-31   Sweden  KaggleRama      Kaggle Mug       NaN   \n",
       "6568   32866 2019-12-31   Sweden  KaggleRama      Kaggle Hat       NaN   \n",
       "6569   32867 2019-12-31   Sweden  KaggleRama  Kaggle Sticker       NaN   \n",
       "\n",
       "           gdp    wd0    wd1    wd2  ...  easter51  easter52  easter53  \\\n",
       "0     5.461456  False  False  False  ...     False     False     False   \n",
       "1     5.461456  False  False  False  ...     False     False     False   \n",
       "2     5.461456  False  False  False  ...     False     False     False   \n",
       "3     5.461456  False  False  False  ...     False     False     False   \n",
       "4     5.461456  False  False  False  ...     False     False     False   \n",
       "...        ...    ...    ...    ...  ...       ...       ...       ...   \n",
       "6565  6.282042  False   True  False  ...     False     False     False   \n",
       "6566  6.282042  False   True  False  ...     False     False     False   \n",
       "6567  6.282042  False   True  False  ...     False     False     False   \n",
       "6568  6.282042  False   True  False  ...     False     False     False   \n",
       "6569  6.282042  False   True  False  ...     False     False     False   \n",
       "\n",
       "      easter54  easter55  easter56  easter57  easter58  holiday    target  \n",
       "0        False     False     False     False     False        1  3.738239  \n",
       "1        False     False     False     False     False        1  4.196010  \n",
       "2        False     False     False     False     False        1  2.925788  \n",
       "3        False     False     False     False     False        1  4.291321  \n",
       "4        False     False     False     False     False        1  4.756724  \n",
       "...        ...       ...       ...       ...       ...      ...       ...  \n",
       "6565     False     False     False     False     False        0       NaN  \n",
       "6566     False     False     False     False     False        0       NaN  \n",
       "6567     False     False     False     False     False        0       NaN  \n",
       "6568     False     False     False     False     False        0       NaN  \n",
       "6569     False     False     False     False     False        0       NaN  \n",
       "\n",
       "[32868 rows x 209 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d953a5-33b0-401d-aefa-cd78fb6a6840",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569cc7f-ad48-481e-bb7e-7ac16d4adfc3",
   "metadata": {},
   "source": [
    "I'm going to encapsulate this in a function so that it can be invoked just-in-time, in the hopes of avoiding confusions with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d55d72bb-efeb-4b21-afd4-2e848636fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    features = ['country', 'product', 'store']\n",
    "    le_dict = {feature: LabelEncoder().fit(orig_train_df[feature]) for feature in features}\n",
    "    enc_df = df.copy()\n",
    "    for feature in features:\n",
    "        enc_df[feature] = le_dict[feature].transform(df[feature])\n",
    "    return le_dict, enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a98c723-1369-4421-884c-c195b07bcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in le_dict.keys():\n",
    "#     print(f\"Values for key {key} are {le_dict[key].inverse_transform(range(len(le_dict[key].values())))}\")#\"\n",
    "# print(le_dict['country'].inverse_transform([0,1,2]))\n",
    "# print(le_dict['product'].inverse_transform([0,1,2]))\n",
    "# print(le_dict['store'].inverse_transform([0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8030cb25-4179-482f-81aa-d83779f0f278",
   "metadata": {},
   "source": [
    "```\n",
    "['Finland' 'Norway' 'Sweden']\n",
    "['Kaggle Hat' 'Kaggle Mug' 'Kaggle Sticker']\n",
    "['KaggleMart' 'KaggleRama']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39254279-4486-412e-8c57-ba2e01a5c702",
   "metadata": {},
   "source": [
    "Now, we'll do the encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9186208-c4b9-4974-9daa-78dc05d4e599",
   "metadata": {},
   "source": [
    "At this point, the `encoded_all_df` can be used -- perhaps with a call to `LabelEncoder.inverse_transform` -- to recover the \"original\" data when necessary (e.g. for feeding it into Prophet and NeuralProphet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1da434d5-b499-4d24-ae0e-6f58fc8a9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_all_df = label_encoder(temporal_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76eb5ed-d638-4879-941e-7b9754459340",
   "metadata": {},
   "source": [
    "### Pseudolabeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ef2c6-14c7-4eaf-af12-5cf752e76fef",
   "metadata": {},
   "source": [
    "I'm not going to try this right now, but I may return to it later -- I note that Teck Meng Wong had some good results with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4276176-dbb1-4ed9-bbf7-922112e6d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here's teck meng wong's implementation -- see the notebook for the constants\n",
    "# df_pseudolabels = pd.read_csv(PSEUDO_DIR, index_col=ID)\n",
    "# df_pseudolabels[DATE] = pd.to_datetime(test_df[DATE])\n",
    "# df_pseudolabels.to_csv(\"pseudo_labels_v0.csv\", index=True)\n",
    "# # if PSEUDO_LABEL:\n",
    "#     # df_pseudolabels = df_pseudolabels.set_index([DATE]).sort_index()\n",
    "# test_df[column_y] = df_pseudolabels[column_y].astype(np.float64)\n",
    "# train_df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946e675-76be-45ba-9137-a6642e0e2b55",
   "metadata": {},
   "source": [
    "### Data Splitting, Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff3176f-0deb-4477-8f08-2517b9a5cef3",
   "metadata": {},
   "source": [
    "Now that the preprocessing is done, I'm going to split the data back into the train and test sets; then, I'll create a view on the dataframes that omits the year. The year-less dataframes will be suitable for residual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ec51c66-81b6-4587-bed7-ede9ffef2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = encoded_all_df.drop(columns=['num_sold', 'row_id'])\n",
    "all_df = temporal_all_df.drop(columns=['row_id']) # writing over the previous version of `all_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8ac1517-0b40-4c76-b136-7255216d5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_df = all_df[:len(orig_train_df)] # training and validation sets -- still not encoded\n",
    "test_df = all_df[len(orig_train_df):] # still not encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca9532ef-4a78-4a8d-a474-03b6f270946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = encoded_all_df.iloc[np.where(encoded_all_df['date'] < '2019-01-01'), :]\n",
    "# test_df = encoded_all_df[[np.where(encoded_all_df['date'] > '2018-12-31')]]\n",
    "\n",
    "# encoded_tv_df = encoded_all_df.drop(columns=['row_id'])[:len(orig_train_df)]\n",
    "# encoded_test_df = encoded_all_df.drop(columns=['row_id'])[len(orig_train_df):]\n",
    "\n",
    "# valid_df = tv_df[tv_df['date'] > '2017-12-31']\n",
    "# train_df = tv_df[tv_df['date'] <= '2017-12-31']\n",
    "\n",
    "# train_and_valid_residual_df = train_and_valid_df.drop(columns=['date'])\n",
    "# test_residual_df = test_df.drop(columns=['date'])\n",
    "\n",
    "# len(valid_df) + len(train_df) == len(tv_df)\n",
    "\n",
    "# encoded_tv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b92520-09d6-4a6e-a37e-8f8a81e0147f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beecb19-335d-4a0d-ba86-5957165ed5d4",
   "metadata": {},
   "source": [
    "### Forecasting Models Prep\n",
    "First, we'll set up functions to handle the training of forecasting models which will discern trends, and which may -- or may not -- yield insights concerning seasonality. While the Scikit-Learn models will be able to share a single trainer function, the Prophet and NeuralProphet models have subtly different expectations of their data, and as such will require separate handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f86c3a25-87e2-4287-b2ac-933ef1aaa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, HuberRegressor, LinearRegression, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet\n",
    "# earth? wouldn't install via pip on my machine at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23c0c2dd-9e9d-4bda-96b1-49c61d667e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab7ae8-ee37-498c-85db-f02548c896d1",
   "metadata": {},
   "source": [
    "#### (Preprepared Preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1837ecbe-5be3-4e0c-bff0-a3576e6aed25",
   "metadata": {},
   "source": [
    "The next cell contains code to import already-existing predictions -- but I think it's better to centralize the code that produces them here, and will comment out the import code for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2157a8d2-c17f-451f-9835-20b9ad537fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_trainset = load(predpath/'20220121_prophet_baseline_trainset.joblib')\n",
    "\n",
    "# neural_trainset = load(predpath/'20220121_neuralprophet_baseline_trainset.joblib')\n",
    "# neural_test_preds = load(predpath/'20220121_neuralprophet_baseline_testset.joblib')\n",
    "\n",
    "# ridge_tv_preds = load(predpath/'20210121_ridge_baseline_trainset_preds.joblib')\n",
    "# ridge_test_preds = load(predpath/'20220121_ridge_testset_preds.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9114c3-5287-4862-b94d-dca7e55f43c8",
   "metadata": {},
   "source": [
    "And this cell would handle the parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6871927d-969a-4507-91ca-6e56d0196637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_tv_preds = neural_trainset['prophet_forecast']\n",
    "# prophet_tv_preds = prophet_trainset['prophet_forecast']\n",
    "\n",
    "# neural_train_preds = neural_tv_preds[:train_length]\n",
    "# neural_valid_preds = neural_tv_preds[train_length:]\n",
    "\n",
    "# prophet_train_preds = prophet_tv_preds[:train_length]\n",
    "# prophet_valid_preds = prophet_tv_preds[train_length:]\n",
    "\n",
    "# train_length = len(neural_trainset[neural_trainset['date'] <= '2017-12-31'])\n",
    "\n",
    "# ridge_train_preds = ridge_tv_preds[:train_length]\n",
    "# ridge_valid_preds = ridge_tv_preds[train_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb08ecf-a947-47cd-97ad-cf55179cf717",
   "metadata": {},
   "source": [
    "#### Scikit-Learn Linear Models Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46336d-e7f0-4b8c-bdd6-a24156e02557",
   "metadata": {},
   "source": [
    "Linear models from Scikit-Learn seemingly require that datetime data be converted to numerics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f248a-fd99-496f-93f7-953275129874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83b82602-e3ce-4476-bc3f-05bc6f2a8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_linear_df = train_df.copy()\n",
    "# valid_linear_df = valid_df.copy()\n",
    "# test_linear_df = test_df.copy()\n",
    "# tv_linear_df = tv_df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1a9a2-72e5-4edd-a532-42990e5d6a15",
   "metadata": {},
   "source": [
    "### Forecasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c1987-3246-45df-884c-864bde832811",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "I'll hard-code them for now, but in the future may Optuna them. May want to create a dict of all the kwargs to be used for all the models, with the model names as keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42a07e24-4627-47b2-bcc9-906b45194d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "#     'holidays':holidays_train, # will add this in-function\n",
    "    'n_changepoints':10,\n",
    "    'changepoint_range':0.4,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_prior_scale':25,\n",
    "    'holidays_prior_scale':100,\n",
    "    'changepoint_prior_scale':0.01,\n",
    "    'interval_width':0.5,\n",
    "    'uncertainty_samples':False\n",
    "}\n",
    "\n",
    "neuralprophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "    'n_changepoints':10,\n",
    "    'changepoints_range':0.4,\n",
    "    'trend_reg':1,\n",
    "    'trend_reg_threshold':False,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_reg':1,\n",
    "    'n_forecasts':365,\n",
    "    'normalize':'off'\n",
    "}\n",
    "\n",
    "# for pytorch / skorch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tcn_kwargs = {\n",
    "#     'module': estimator, # will be handled at-call\n",
    "#     'criterion': nn.MSELoss, # consider enhancement here\n",
    "#     \"lr\": 0.01, # default is 0.01\n",
    "#     'optimizer':Adam,\n",
    "#     'max_epochs':10, # default is 10\n",
    "#     'device': 'cpu'#device,\n",
    "    \n",
    "}\n",
    "\n",
    "tcn_skorch_kwargs = {\n",
    "    'module__num_inputs':1,\n",
    "    'module__num_channels':[20] * 6,\n",
    "    'module__output_sz':1, #2 * samples_per_hour,\n",
    "    'module__kernel_size':5,\n",
    "    'module__dropout':0.1,\n",
    "    'max_epochs':30, # 60,\n",
    "    'batch_size':256,\n",
    "    'lr':2e-3,\n",
    "    'optimizer':torch.optim.Adam,\n",
    "    'train_split':None,\n",
    "}\n",
    "\n",
    "mlp_skorch_kwargs = {\n",
    "    'module__n_inputs': tv_df.shape[1],\n",
    "    'module__hidden_units': 200, \n",
    "    'module__dropout': 0.2,\n",
    "    'max_epochs':25, # 60,\n",
    "    'batch_size':256,\n",
    "    'lr':2e-3,\n",
    "    'optimizer':torch.optim.Adam,\n",
    "    'train_split':None,\n",
    "}\n",
    "\n",
    "\n",
    "# model_params['hyperparams'] = str(neuralprophet_kwargs)\n",
    "# model_params['holiday_source'] = 'Prophet builtin for each country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f95a6f1-57cb-4daf-8c8c-7b525d51794c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26298, 208)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1adf90b-96bf-4003-8a2b-e881fb0123c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 4207,\n",
       " 'learning_rate': 0.05378597302351865,\n",
       " 'reg_alpha': 0.0067949392113948815,\n",
       " 'reg_lambda': 0.04865823628931899,\n",
       " 'subsample': 0.212875760245356,\n",
       " 'min_child_weight': 6.997692447967251,\n",
       " 'colsample_bytree': 0.9824893256584818,\n",
       " 'gamma': 0.10395228539921328,\n",
       " 'max_depth': 5}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgboost_params = load(studypath/'optuna_xgboost_study-20220126213551.joblib').best_trial.params\n",
    "best_xgboost_params['max_depth'] = best_xgboost_params['depth']\n",
    "del best_xgboost_params['depth']\n",
    "best_xgboost_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3fc9b9b-0111-4010-8ade-e0c86339d14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iterations': 10529,\n",
       " 'learning_rate': 0.07026263205443048,\n",
       " 'random_strength': 44,\n",
       " 'od_wait': 261,\n",
       " 'reg_lambda': 35.672029887566374,\n",
       " 'border_count': 57,\n",
       " 'min_child_samples': 19,\n",
       " 'leaf_estimation_iterations': 2,\n",
       " 'max_depth': 3}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_catboost_params = load(studypath/'optuna_catboost_study-20220127082356.joblib').best_trial.params\n",
    "best_catboost_params['max_depth'] = best_catboost_params['depth']\n",
    "del best_catboost_params['depth']\n",
    "best_catboost_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34a9f43c-d0fc-4318-8c2a-1efa405f07f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 6078,\n",
       " 'learning_rate': 0.03612108919426432,\n",
       " 'reg_alpha': 0.008631524966022684,\n",
       " 'reg_lambda': 0.19537138720003774,\n",
       " 'subsample': 0.9601129223632775,\n",
       " 'min_child_samples': 24,\n",
       " 'num_leaves': 235,\n",
       " 'colsample_bytree': 0.920126987868937,\n",
       " 'max_depth': 3}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lightgbm_params = load(studypath/'optuna_lightgbm_study-20220127171126.joblib').best_trial.params\n",
    "best_lightgbm_params['max_depth'] = best_lightgbm_params['depth']\n",
    "del best_lightgbm_params['depth']\n",
    "best_lightgbm_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7452345e-fb0c-45dd-8f73-052280897e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {\n",
    "    # universal\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "#     'eval_metric': ['mae', 'mape', 'rmse'],\n",
    "#     'sampling_method': 'gradient_based',\n",
    "#     'grow_policy': 'lossguide',\n",
    "    \n",
    "    # best of 500 trials on Optuna\n",
    "    **best_xgboost_params\n",
    "}\n",
    "\n",
    "\n",
    "lightgbm_params = {\n",
    "    # universal\n",
    "    'objective': 'mse',\n",
    "#     'random_state': 42,\n",
    "    'device_type': 'cpu',\n",
    "    'n_jobs': -1,\n",
    "#                 eval_metric='auc',\n",
    "#     'device_type': 'gpu',\n",
    "#     'max_bin': 63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     'gpu_use_dp': False,\n",
    "#     'max_depth': 0,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'subsample': .15,\n",
    "#     'n_estimators': 1500,\n",
    "    **best_lightgbm_params\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    # universal\n",
    "#     'task_type':'GPU',\n",
    "#     'silent':True,\n",
    "#     'random_state':42,\n",
    "    \n",
    "    # from trial 4 (of 5) via Optuna\n",
    "    **best_catboost_params\n",
    "}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f38f08-c75d-470f-9504-049c4d6146d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd9c74e-1f65-4dd1-8cc1-845a1ff8d309",
   "metadata": {},
   "source": [
    "#### Temporal Convolutional Network\n",
    "\n",
    "Implementation from https://www.kaggle.com/ceshine/pytorch-temporal-convolutional-networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9cad2d00-74f3-4d9d-8b3b-f60927ec405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TemporalBlock(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "#         super(TemporalBlock, self).__init__()\n",
    "        \n",
    "#         # this is the first convolutional layer; note that it foregoes padding irrespective of argument\n",
    "#         self.conv1 = weight_norm(nn.Conv2d(n_inputs, n_outputs, (1, kernel_size),\n",
    "#                                            stride=stride, padding=0, dilation=dilation))\n",
    "#         # the padding is then added after the first conv layer\n",
    "#         self.pad = torch.nn.ZeroPad2d((padding, 0, 0, 0))\n",
    "#         # this is a very standard choice\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         # the second convolutional layer in the block is identical to the first, but now padding has been added to the input\n",
    "#         self.conv2 = weight_norm(nn.Conv2d(n_outputs, n_outputs, (1, kernel_size),\n",
    "#                                            stride=stride, padding=0, dilation=dilation))\n",
    "        \n",
    "#         # this simply strings together the above architectural elements, for convenience I guess\n",
    "#         self.net = nn.Sequential(self.pad, self.conv1, self.relu, self.dropout,\n",
    "#                                  self.pad, self.conv2, self.relu, self.dropout)\n",
    "        \n",
    "#         # if the n_outputs is nonzero, this adds on a final convlutional layer to ensure that we get the desired number of outputs\n",
    "#         self.downsample = nn.Conv1d(\n",
    "#             n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         # this initializes the weights as specified in the separate weight initialization method, below\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         # this method initializes the weights for the Conv1D and Conv2D layers, plus the Downsample layer (if it's used)\n",
    "#         self.conv1.weight.data.normal_(0, 0.01)\n",
    "#         self.conv2.weight.data.normal_(0, 0.01)\n",
    "#         if self.downsample is not None:\n",
    "#             self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # note the nice one-liner here, to add in the requisite number of dimensions both inbound to the NN and outbound\n",
    "#         out = self.net(x.unsqueeze(2)).squeeze(2) # original\n",
    "# #         out = self.net(x.unsqueeze(3)).squeeze(3) # my revision to address RuntimeError: Expected 4-dimensional input for 4-dimensional weight [32, 128, 1, 2], but got 3-dimensional input of size [128, 244, 2] instead\n",
    "# #         out = self.net(x.unsqueeze(3)).squeeze(2) # further revision to address IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\n",
    "#         # is this a residual, then?\n",
    "#         res = x if self.downsample is None else self.downsample(x)\n",
    "#         return self.relu(out + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb8cf494-61be-4ac6-bfd4-75e3a3786607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TemporalConvNet(nn.Module):\n",
    "#     def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "#         '''\n",
    "#         What does num_channels mean? See Obsidian 202201270954... It seems that it should be a \n",
    "#         list, with the number of hidden channels (i.e. activation units in each hidden layer), \n",
    "#         repeated the number of hidden layers there are. E.g. [25,25,25,25]. An alternate idea:\n",
    "#         it's [hidden_size]*(level_size-1) + [embedding_size]\n",
    "        \n",
    "#         I think that \n",
    "#         '''\n",
    "        \n",
    "#         super(TemporalConvNet, self).__init__()\n",
    "#         layers = []\n",
    "#         num_levels = len(num_channels)\n",
    "#         for i in range(num_levels):\n",
    "#             dilation_size = 2 ** i\n",
    "#             in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "#             out_channels = num_channels[i]\n",
    "#             layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "#                                      padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "#         self.network = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5cbc7b59-1dee-4aa3-9a20-e6236992605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TCNModel(nn.Module):\n",
    "#     def __init__(self, num_channels, kernel_size=2, dropout=0.2):\n",
    "#         super(TCNModel, self).__init__()\n",
    "#         self.tcn = TemporalConvNet(\n",
    "#             128, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.decoder = nn.Linear(num_channels[-1], 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.decoder(self.dropout(self.tcn(x)[:, :, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa494d-d127-48d6-87ee-9e66ce4cdf77",
   "metadata": {},
   "source": [
    "Going to use the [original implementation](https://github.com/locuslab/TCN/blob/master/TCN/tcn.py) (via the discussion [here](https://www.ethanrosenthal.com/2019/02/18/time-series-for-scikit-learn-people-part3/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ccb0937b-1c28-4555-9f8f-15c17d9b8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, hidden_units, dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense0 = nn.Linear(n_inputs, hidden_units)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        self.dropout0 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units // 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.dense2 = nn.Linear(hidden_units // 2, (hidden_units // 2) // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.dense3 = nn.Linear((hidden_units // 2) // 2, ((hidden_units // 2) // 2) // 2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.head = nn.Linear(((hidden_units // 2) // 2) // 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout0(self.relu0(self.dense0(x)))\n",
    "        x = self.dropout1(self.relu1(self.dense1(x)))\n",
    "        x = self.dropout2(self.relu2(self.dense2(x)))\n",
    "        x = self.dropout3(self.relu3(self.dense3(x)))\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2be25f9b-9e9f-4736-a9ac-46029fbd7703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, output_sz,\n",
    "                 kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
    "                                     dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size,\n",
    "                                     dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_sz)\n",
    "        self.last_activation = nn.ReLU()\n",
    "        self.output_sz = output_sz\n",
    "        # self.float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_sz = x.shape[0]\n",
    "        out = self.network(x.unsqueeze(1))\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.linear(out).mean(dim=1)\n",
    "        out = out.to(dtype=torch.float32) # my addition\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e350f-b1b3-4383-8242-19506a389c10",
   "metadata": {},
   "source": [
    "#### Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32330b-fdd4-41de-95e9-ce2ec0cabf8d",
   "metadata": {},
   "source": [
    "##### NeuralProphet\n",
    "I'm leaving the folds as they are. ~~Label encoding shouldn't matter -- the values are just being iterated over anyway.~~ It does matter because the Prophets use the strings to identify countries' holidays to add. Not sure about doing the target transform -- if you try it, just have the trainer call pass `target='target'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55823ee3-106b-4bb2-80ea-261525cafee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_folds = [\n",
    "    ('2015-01-01', '2018-01-01'),\n",
    "    ('2018-01-01', '2019-01-01'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c0dd9b5-3818-40c3-ac85-4fb833cfd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_tv_df = tv_df_encoded.copy() # encoded_tv_df.copy()\n",
    "# prophet_test_df = test_df_encoded.copy() # encoded_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "863448a8-8069-4d46-98a7-37c13e6f385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in ['country', 'product', 'store']:\n",
    "#     prophet_tv_df[feature] = orig_train_df[feature]\n",
    "#     prophet_test_df[feature] = orig_test_df[feature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24e2403b-869b-4ca7-b3d2-119e1d1720ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_tv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f347d9a0-166c-4ebf-bd0a-3aea11e4d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_enc = le_dict['country'].transform(countries)\n",
    "# stores_enc = le_dict['store'].transform(stores)\n",
    "# products_enc = le_dict['product'].transform(products)\n",
    "\n",
    "# countries, countries_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33d3cbbc-2518-47e1-ac2d-75a3f6e824b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def neuralprophet_trainer(model_kwargs=neuralprophet_kwargs, countries=countries, stores=stores, products=products, folds=prophet_folds, \n",
    "                          tv_df=tv_df, test_df=test_df,\n",
    "#                           df_train=tv_df, df_test=test_df, \n",
    "                          target='num_sold', wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    # no label encoding here -- but test it with too\n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', target: 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', target: 'y'})\n",
    "\n",
    "#                     model = Prophet(**prophet_kwargs)\n",
    "                    model = NeuralProphet(**model_kwargs)\n",
    "\n",
    "                    model = model.add_country_holidays(country_name=country) # uses FacebookProphet or NeuralProphet API to add holidays\n",
    "                    print(train.columns)\n",
    "                    model.fit(train, freq='D') # neuralprophet\n",
    "                    # prophet\n",
    "#                     train_predictions = model.predict(train[['ds']])['yhat']\n",
    "#                     val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    # neuralprophet\n",
    "                    train_predictions = model.predict(train)['yhat1']\n",
    "                    val_predictions = model.predict(val)['yhat1']\n",
    "                    df_train.loc[train_idx, 'neuralprophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'neuralprophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test['y'] = np.nan\n",
    "                    test_predictions = model.predict(test)['yhat1']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'neuralprophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "#     train_\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train['neuralprophet_forecast'], df_test['neuralprophet_forecast']#, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1f716-7669-495e-b8c3-44b053d95537",
   "metadata": {},
   "source": [
    "##### Prophet Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e743f32b-d1bf-4eeb-85b9-e43f33c25de5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prophet_trainer(prophet_kwargs=prophet_kwargs, countries=countries, stores=stores, products=products, folds=prophet_folds, \n",
    "                    tv_df=tv_df, test_df=test_df,\n",
    "#                           df_train=tv_df, df_test=test_df, \n",
    "                    target='num_sold', wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "                    \n",
    "#                     print(train_idx)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', target]].reset_index(drop=True)\n",
    "#                     print(train.shape)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', target: 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', target: 'y'})\n",
    "\n",
    "                    model = Prophet(**prophet_kwargs)\n",
    "\n",
    "                    model.add_country_holidays(country_name=country) # uses FacebookProphet API to add holidays\n",
    "                    model.fit(train)\n",
    "        \n",
    "                    train_predictions = model.predict(train[['ds']])['yhat']\n",
    "                    val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    df_train.loc[train_idx, 'prophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test_predictions = model.predict(test[['ds']])['yhat']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'prophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train['prophet_forecast'], df_test['prophet_forecast']#, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82559992-18c5-4b13-b1a4-e3060f6d15da",
   "metadata": {},
   "source": [
    "##### Scikit-Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a8981e5-ffb7-42ef-949b-2793b0294d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sklearn_trainer(estimator, model_kwargs={}, tv_df=tv_df, test_df=test_df, #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                    folds=prophet_folds, countries=countries, stores=stores, products=products, target='target',\n",
    "#                     by_combo=True, \n",
    "#                     model_type=None, # None -> fully scikit-learn compatible; alternatives are 'skorch' or 'gbm'\n",
    "                    wandb_tracked=False):\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, tv_df = label_encoder(df_train) # should leave broader scope's tv_df alone\n",
    "    _, test_df = label_encoder(df_test) # should leave broader scope's test_df alone\n",
    "    del df_train, df_test\n",
    "    \n",
    "#     scaler = RobustScaler()\n",
    "#     tv_df = scaler.fit_transform(tv_df)\n",
    "#     test_df = scaler.transform(test_df)\n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    # drop whichever version of the dependent variable is not being used\n",
    "#     for df in [tv_df, test_df]:\n",
    "    if target == 'num_sold': \n",
    "        tv_df = tv_df.drop(columns=['target'])\n",
    "        test_df = test_df.drop(columns=['target'])\n",
    "    else:\n",
    "        tv_df = tv_df.drop(columns=['num_sold'])\n",
    "        test_df = test_df.drop(columns=['num_sold'])\n",
    "            \n",
    "#     print(\"'num_sold' in test_df.columns == \", 'num_sold' in test_df.columns)\n",
    "    \n",
    "#     y = tv_df['target']\n",
    "#     scaler = RobustScaler()\n",
    "#     X = scaler.fit_transform(tv_df)\n",
    "#     X_test = scaler.transform(test_df)\n",
    "    \n",
    "    # handling each combination of country, store, and product separately\n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (tv_df['date'] >= start) &\\\n",
    "                                (tv_df['date'] < end) &\\\n",
    "                                (tv_df['country'] == country) &\\\n",
    "                                (tv_df['store'] == store) &\\\n",
    "                                (tv_df['product'] == product)\n",
    "\n",
    "#                     print(train_idx)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = tv_df.loc[train_idx, :].reset_index(drop=True)\n",
    "#                         print(train.shape)\n",
    "\n",
    "                    val_idx = (tv_df['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (tv_df['date'] < folds[fold + 1][1]) &\\\n",
    "                              (tv_df['country'] == country) &\\\n",
    "                              (tv_df['store'] == store) &\\\n",
    "                              (tv_df['product'] == product)\n",
    "\n",
    "                    val = tv_df.loc[val_idx, :].reset_index(drop=True)\n",
    "\n",
    "                    test_idx = (test_df['country'] == country) &\\\n",
    "                               (test_df['store'] == store) &\\\n",
    "                               (test_df['product'] == product)\n",
    "                    test = test_df.loc[test_idx, :].reset_index(drop=True)\n",
    "\n",
    "                    # with the training and validation sets sorted out, make them integers for model fitting\n",
    "                    for df in [train, val, test]:\n",
    "                        df['date'] = df['date'].map(dt.datetime.toordinal)\n",
    "                    if 'model_forecast' in train.columns:\n",
    "                        X = train.drop(columns=[target, 'model_forecast'])\n",
    "                        X_valid = val.drop(columns=[target, 'model_forecast'])\n",
    "                        X_test = test.drop(columns=[target, 'model_forecast'])\n",
    "                    else:\n",
    "                        X = train.drop(columns=[target])\n",
    "                        X_valid = val.drop(columns=[target])\n",
    "                        X_test = test.drop(columns=[target])\n",
    "\n",
    "                    y = train[target]\n",
    "                    y_valid = val[target]\n",
    "\n",
    "\n",
    "#                         print(type(X), type(y))\n",
    "#                         print(f\"X has {X.isna().any().sum()} NaNs\")\n",
    "#                         print(f\"y has {y.isna().sum()} NaNs\")\n",
    "#                     print(X_test.info())\n",
    "#                     print(y_valid.dtype)\n",
    "    \n",
    "#                     if model_type == 'skorch':\n",
    "# #                         for df in [X, X_valid, X_test]:\n",
    "# # #                             df['date'] = df['date'].apply(dt.datetime.toordinal)\n",
    "# #                             df = torch.tensor(df.to_numpy(dtype=np.float32))\n",
    "# #                         for target in [y, y_valid]:\n",
    "# #                             target = torch.tensor(np.array(target))\n",
    "# # #                             target = target.reshape(-1,1)\n",
    "# #                             target = target.unsqueeze(0)\n",
    "#                         X = torch.tensor(X.to_numpy(dtype=np.float32))\n",
    "#                         X_valid = torch.tensor(X_valid.to_numpy(dtype=np.float32))\n",
    "#                         X_test = torch.tensor(X_test.to_numpy(dtype=np.float32))\n",
    "            \n",
    "#                         y = torch.tensor(np.array(y)).reshape(-1,1)\n",
    "#                         y_valid = torch.tensor(np.array(y)).reshape(-1,1)\n",
    "    \n",
    "#                         tcn_kwargs = {\n",
    "#                             'num_channels': [32,32,32,32],\n",
    "#                         }\n",
    "#                         print(type(y), type(y_valid))\n",
    "# #                         y = y.reshape(-1,1)\n",
    "# #                         y_valid = y_valid.reshape(-1,1)\n",
    "#                         # create the Datasets\n",
    "                \n",
    "#                         # create the DataLoaders\n",
    "\n",
    "#                         # instantiate the wrapper\n",
    "#                         model = NeuralNetRegressor(\n",
    "#                             module=estimator(**tcn_kwargs),\n",
    "#                             **model_kwargs\n",
    "#                         )\n",
    "#                     elif model_type=='gbm':\n",
    "                        \n",
    "#                     else:\n",
    "                    model = estimator(**model_kwargs)\n",
    "\n",
    "                    model.fit(X,y)\n",
    "\n",
    "                    model_train_preds = model.predict(X)\n",
    "                    model_valid_preds = model.predict(X_valid)\n",
    "                    model_test_preds = model.predict(X_test)\n",
    "\n",
    "                    tv_df.loc[train_idx, 'model_forecast'] = model_train_preds#.values\n",
    "                    tv_df.loc[val_idx, 'model_forecast'] =  model_valid_preds#.values\n",
    "                    test_df.loc[test_idx, 'model_forecast'] = model_test_preds#.values\n",
    "\n",
    "\n",
    "    # reverse the dependent variable transform if appropriate\n",
    "    if target == 'target':\n",
    "#             model_tv_preds = np.multiply(np.exp(model_tv_preds), tv_df['gdp']**gdp_exponent)\n",
    "        tv_df['model_forecast'] = np.exp(tv_df['model_forecast']) * tv_df['gdp']**gdp_exponent\n",
    "#             output_tv_df['model_forecast'] = np.exp(output_tv_df['model_forecast']) * output_tv_df['gdp']**gdp_exponent\n",
    "\n",
    "#             model_test_preds = np.multiply(np.exp(model_test_preds), test_df['gdp']**gdp_exponent)\n",
    "        test_df['model_forecast'] = np.exp(test_df['model_forecast']) * test_df['gdp']**gdp_exponent\n",
    "#             output_test_df['model_forecast'] = np.exp(output_test_df['model_forecast']) * output_test_df['gdp']**gdp_exponent\n",
    "#             model_test_preds = np.exp(model_test_preds) * test_df['gdp']**gdp_exponent\n",
    "        \n",
    "#         tv_df['model_forecast'] = model_tv_preds\n",
    "#         test_df['model_forecast'] = model_test_preds\n",
    "#     return output_tv_df, output_test_df\n",
    "    return tv_df['model_forecast'], test_df['model_forecast']\n",
    "#     return tv_df['model_forecast'], test_df['model_forecast']\n",
    "#     return model_tv_preds, model_test_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cedf6-bf1d-4f6f-bfe6-30fcd8f2bba1",
   "metadata": {},
   "source": [
    "##### Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f7211a28-45e7-4cf0-a3f3-9b205bee5781",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def skorch_trainer(model=TemporalConvNet, model_kwargs={}, tv_df=tv_df, test_df=test_df, #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "#                 countries=countries, stores=stores, products=products, random_seed=SEED,\n",
    "#                 target='target', wandb_tracked=False, forecasting=True):\n",
    "    \n",
    "#     # preprocessing\n",
    "    \n",
    "#     if USE_GPU and torch.cuda.is_available():\n",
    "#         device = 'cuda' \n",
    "#     else:\n",
    "#         device = 'cpu'\n",
    "    \n",
    "#     # start by creating working copies of dataframes to avoid mutation\n",
    "#     working_tv_df = tv_df.copy()\n",
    "#     working_test_df = test_df.copy()\n",
    "    \n",
    "#     # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "#     le_dict, working_tv_df = label_encoder(working_tv_df) # should leave broader scope's tv_df alone\n",
    "#     _, working_test_df = label_encoder(working_test_df) # should leave broader scope's test_df alone\n",
    "# #     del df_train, df_test\n",
    "    \n",
    "#     # encode the lists of countries, stores, and products\n",
    "#     countries = le_dict['country'].transform(countries)\n",
    "#     stores = le_dict['store'].transform(stores)\n",
    "#     products = le_dict['product'].transform(products)\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         exmodel_config['arch'] = arch\n",
    "# #         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "#         wandb.init(\n",
    "#             project=\"202201_Kaggle_tabular_playground\",\n",
    "#             save_code=True,\n",
    "#             tags=wandb_config['tags'],\n",
    "#             name=wandb_config['name'],\n",
    "#             notes=wandb_config['notes'],\n",
    "#             config=exmodel_config\n",
    "#     )\n",
    "    \n",
    "#     if forecasting: # if not, implement GroupKFold\n",
    "#         train_df = working_tv_df[working_tv_df['date'] < '2018-01-01']\n",
    "#         valid_df = working_tv_df[working_tv_df['date'] >= '2018-01-01']\n",
    "    \n",
    "#     # convert the dates to ordinals\n",
    "#     train_df['date'] = train_df['date'].map(dt.datetime.toordinal)\n",
    "#     valid_df['date'] = valid_df['date'].map(dt.datetime.toordinal)\n",
    "#     working_test_df['date'] = working_test_df['date'].map(dt.datetime.toordinal)\n",
    "    \n",
    "#     # typecast to np.float32\n",
    "#     train_df = train_df.astype(np.float32)\n",
    "#     valid_df = valid_df.astype(np.float32)\n",
    "#     working_test_df = working_test_df.astype(np.float32)\n",
    "    \n",
    "#     # clean up features\n",
    "#     X = train_df.drop(columns=['num_sold', 'target'])\n",
    "#     y = train_df[target]\n",
    "    \n",
    "#     X_valid = valid_df.drop(columns=['num_sold', 'target'])\n",
    "#     y_valid = valid_df[target]\n",
    "    \n",
    "#     X_test = working_test_df.drop(columns=['num_sold', 'target'])\n",
    "    \n",
    "#     # tensorify\n",
    "#     X = torch.tensor(X.values, dtype=torch.float32)\n",
    "#     X_valid = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "#     X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "#     y = torch.tensor(np.array(y).reshape(-1,1), dtype=torch.float32)\n",
    "#     y_valid = torch.tensor(np.array(y_valid).reshape(-1,1), dtype=torch.float32)\n",
    "    \n",
    "#     print(X.shape, y.shape)\n",
    "#     print(X.dtype, y.dtype)\n",
    "    \n",
    "#     model = NeuralNetRegressor(\n",
    "#         module=model,\n",
    "#         module__num_inputs=1,\n",
    "#         module__num_channels=[10] * 11,\n",
    "#         module__output_sz=1, #2 * samples_per_hour,\n",
    "#         module__kernel_size=5,\n",
    "#         module__dropout=0.0,\n",
    "#         max_epochs=3, # 60,\n",
    "#         batch_size=256,\n",
    "#         lr=2e-3,\n",
    "#         optimizer=torch.optim.Adam,\n",
    "#         device=device,\n",
    "#     #     iterator_train__shuffle=True,\n",
    "#     #     callbacks=[GradientNormClipping(gradient_clip_value=1,\n",
    "#     #                                     gradient_clip_norm_type=2)],\n",
    "#         train_split=None,\n",
    "#     )\n",
    "    \n",
    "#     model.fit(X,y)\n",
    "    \n",
    "#     y_valid_preds = model.predict(X_valid)\n",
    "# #     tv_preds = model.predict()\n",
    "#     test_preds = model.predict(X_test)\n",
    "    \n",
    "# #     print(f\"SMAPE on validation set (2018) is: {SMAPE(y_pred=y_valid_preds, y_true=y_valid)}\")\n",
    "    \n",
    "#     return model, y_valid_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43cd0304-fcca-47bb-b7ee-b20706c6ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c9865ab-6dcc-48a9-9f93-a3ba95bc57a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def skorch_trainer(arch, model_kwargs={}, tv_df=tv_df, test_df=test_df, folds=prophet_folds,#X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                countries=countries, stores=stores, products=products, random_seed=SEED,\n",
    "                target='target', wandb_tracked=False, forecasting=True):\n",
    "    \n",
    "    models = {\n",
    "        'TCN': TemporalConvNet,\n",
    "        'MLP': MLP,\n",
    "    }\n",
    "    \n",
    "    # preprocessing\n",
    "    \n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        device = 'cuda' \n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    \n",
    "    # start by creating working copies of dataframes to avoid mutation\n",
    "#     working_tv_df = tv_df.copy()\n",
    "#     working_test_df = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, tv_df = label_encoder(tv_df) # should leave broader scope's tv_df alone\n",
    "    _, test_df = label_encoder(test_df) # should leave broader scope's test_df alone\n",
    "#     del df_train, df_test\n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "#     y_tv = tv_df['num_sold']\n",
    "    tv_preds = pd.Series(0, index=tv_df.index)\n",
    "    test_preds = pd.Series(0, index=test_df.index)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    # handling each combination of country, store, and product separately\n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                print(f\"Training {le_dict['country'].inverse_transform([country])}, {le_dict['store'].inverse_transform([store])}, {le_dict['product'].inverse_transform([product])}\")\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    \n",
    "                    model = models[arch]\n",
    "                    \n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (tv_df['date'] >= start) &\\\n",
    "                                (tv_df['date'] < end) &\\\n",
    "                                (tv_df['country'] == country) &\\\n",
    "                                (tv_df['store'] == store) &\\\n",
    "                                (tv_df['product'] == product)\n",
    "\n",
    "#                     print(train_idx)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = tv_df.loc[train_idx, :].reset_index(drop=True)\n",
    "#                         print(train.shape)\n",
    "\n",
    "                    val_idx = (tv_df['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (tv_df['date'] < folds[fold + 1][1]) &\\\n",
    "                              (tv_df['country'] == country) &\\\n",
    "                              (tv_df['store'] == store) &\\\n",
    "                              (tv_df['product'] == product)\n",
    "\n",
    "                    val = tv_df.loc[val_idx, :].reset_index(drop=True)\n",
    "\n",
    "                    test_idx = (test_df['country'] == country) &\\\n",
    "                               (test_df['store'] == store) &\\\n",
    "                               (test_df['product'] == product)\n",
    "                    test = test_df.loc[test_idx, :].reset_index(drop=True)\n",
    "                    \n",
    "                    y = train[target]\n",
    "                    y_valid = val[target]\n",
    "                    \n",
    "                    # with the training and validation sets sorted out, make them integers for model fitting\n",
    "                    for df in [train, val, test]:\n",
    "                        df['date'] = df['date'].map(dt.datetime.toordinal)\n",
    "                        df = df.drop(columns=['num_sold', 'target'], inplace=True)\n",
    "#                         df = df.astype(np.float32)\n",
    "                    \n",
    "#                     print(train.columns)\n",
    "#                     print(train.dtypes)\n",
    "#                     train_df = train_df.astype(np.float32)\n",
    "                    X, X_valid, X_test = train.astype(np.float32), val.astype(np.float32), test.astype(np.float32)\n",
    "#                         for feature in ['num_sold', 'target', 'model_forecast']:\n",
    "#                             if feature in df.columns:\n",
    "#                                 df = df.drop(columns=feature)\n",
    "#                     if 'model_forecast' in train.columns:\n",
    "#                         X = train.drop(columns=['num_sold', 'target', 'model_forecast'])\n",
    "#                         X_valid = val.drop(columns=['num_sold', 'target', 'model_forecast'])\n",
    "#                         X_test = test.drop(columns=['num_sold', 'target', 'model_forecast'])\n",
    "#                     else:\n",
    "#                         X = train.drop(columns=['num_sold', 'target'])\n",
    "#                         X_valid = val.drop(columns=['num_sold', 'target'])\n",
    "#                         X_test = test.drop(columns=['num_sold', 'target'])\n",
    "\n",
    "                    \n",
    "                    \n",
    "#                     X = train_df.drop(columns=['num_sold', 'target'])\n",
    "#                     y = train_df[target]\n",
    "\n",
    "#                     X_valid = valid_df.drop(columns=['num_sold', 'target'])\n",
    "#                     y_valid = valid_df[target]\n",
    "\n",
    "#                     X_test = working_test_df.drop(columns=['num_sold', 'target'])\n",
    "\n",
    "                    # tensorify\n",
    "#                     print(X.dtypes)\n",
    "#                     print(type(X.values))\n",
    "                    X = torch.tensor(X.values, dtype=torch.float32)\n",
    "                    X_valid = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "                    X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "                    y = torch.tensor(np.array(y).reshape(-1,1), dtype=torch.float32)\n",
    "                    y_valid = torch.tensor(np.array(y_valid).reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "#                     print(X.shape, y.shape)\n",
    "#                     print(X.dtype, y.dtype)\n",
    "\n",
    "                    net = NeuralNetRegressor(\n",
    "                        module=model,\n",
    "                        device=device,\n",
    "                        **model_kwargs\n",
    "                    #     iterator_train__shuffle=True,\n",
    "#                         callbacks=[Checkpoint(dirname=modelpath/'20220128-TCN-country{country}-store{store}-product{product}/')],\n",
    "                    #     callbacks=[GradientNormClipping(gradient_clip_value=1,\n",
    "                    #                                     gradient_clip_norm_type=2)],\n",
    "                        \n",
    "                    )\n",
    "\n",
    "                    net.fit(X,y)\n",
    "                    \n",
    "                    net.save_params(f_params=modelpath/f'20220128-TCN-country{country}-store{store}-product{product}-model_params.pkl')\n",
    "            \n",
    "                    y_train_preds = np.squeeze(net.predict(X))\n",
    "                    y_valid_preds = np.squeeze(net.predict(X_valid))\n",
    "                    fold_test_preds = np.squeeze(net.predict(X_test))\n",
    "#                     print(f\"Shape of fold test preds is {fold_test_preds.shape}\")\n",
    "\n",
    "                    tv_preds[train_idx] = y_train_preds\n",
    "                    tv_preds[val_idx] = y_valid_preds\n",
    "                    test_preds[test_idx] = fold_test_preds\n",
    "            \n",
    "                    print(f\"Valid SMAPE for {le_dict['country'].inverse_transform([country])}, {le_dict['store'].inverse_transform([store])}, {le_dict['product'].inverse_transform([product])} is {SMAPE(y_true=tv_df.loc[val_idx, 'num_sold'], y_pred=y_valid_preds)}\")\n",
    "                    \n",
    "    # reverse the dependent variable transform if appropriate\n",
    "    if target == 'target':\n",
    "#             model_tv_preds = np.multiply(np.exp(model_tv_preds), tv_df['gdp']**gdp_exponent)\n",
    "#         tv_df['model_forecast'] = np.exp(tv_df['model_forecast']) * tv_df['gdp']**gdp_exponent\n",
    "        tv_preds = np.exp(tv_preds) * tv_df['gdp']**gdp_exponent\n",
    "        test_preds = np.exp(test_preds) * test_df['gdp']**gdp_exponent\n",
    "        \n",
    "    return tv_preds, test_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0506dd3-d1cc-456a-87eb-2e52e751d85c",
   "metadata": {},
   "source": [
    "##### GBMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "691d09f7-18da-4121-8d0d-e3c6399c06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5625e7a5-724c-45ec-9173-e56c8812f745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gbm_trainer(arch:str, model_kwargs={}, tv_df=tv_df, test_df=test_df,  #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                countries=countries, stores=stores, products=products, random_seed=SEED,\n",
    "                target='target', wandb_tracked=False):\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    X = tv_df.copy()\n",
    "    X_test = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, X = label_encoder(X) # should leave broader scope's tv_df alone\n",
    "    _, X_test = label_encoder(X_test) # should leave broader scope's test_df alone\n",
    "#     del df_train, df_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     train_smape = 0\n",
    "#     val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "        )\n",
    "    \n",
    "    # drop whichever version of the dependent variable is not being used\n",
    "#     for df in [tv_df, test_df]:\n",
    "    y = X[target]\n",
    "#     for df in [X, X_test]:\n",
    "#         df = df.drop(columns=['num_sold', 'target'])\n",
    "    X = X.drop(columns=['num_sold', 'target'])\n",
    "    X_test = X_test.drop(columns=['num_sold', 'target'])\n",
    "#     X = X.drop(columns)\n",
    "#     if target == 'num_sold': \n",
    "#         y = X['num_sold']\n",
    "#         X = X.drop(columns=['target'])\n",
    "#         X_test = X_test.drop(columns=['target'])\n",
    "#     else:\n",
    "#         X = X.drop(columns=['num_sold'])\n",
    "#         X_test = X_test.drop(columns=['num_sold'])\n",
    "    \n",
    "#     scaler = RobustScaler()\n",
    "#     X = scaler.fit_transform(X)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "    \n",
    "    kfold = GroupKFold(n_splits=4)\n",
    "    oof_preds = pd.Series(0, index=tv_df.index)\n",
    "#     oof_preds, oof_y = [], []\n",
    "#     test_preds = np.zeros((X_test.shape[0]))\n",
    "    test_preds = pd.Series(0, index=test_df.index)\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(tv_df, groups=tv_df.date.dt.year)):\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        # remove dates \n",
    "#         for df in [X, X_test]:\n",
    "#             df = df.drop(columns=['date'])\n",
    "        if 'date' in X.columns:\n",
    "            X = X.drop(columns=['date'])\n",
    "            X_test = X_test.drop(columns=['date'])#, 'num_sold'])\n",
    "        \n",
    "        y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "        X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:]\n",
    "        \n",
    "        if arch == 'xgboost':\n",
    "            model = XGBRegressor(\n",
    "                tree_method= 'gpu_hist',\n",
    "                predictor= 'gpu_predictor',\n",
    "                eval_metric= ['mae', 'mape'],\n",
    "                sampling_method= 'gradient_based',\n",
    "                grow_policy= 'lossguide',\n",
    "                seed=random_seed,\n",
    "                objective='reg:squarederror',\n",
    "                **model_kwargs)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        elif arch == 'lightgbm':\n",
    "            model = LGBMRegressor(\n",
    "                random_state=random_seed,\n",
    "                **model_kwargs)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        \n",
    "        elif arch == 'catboost':\n",
    "            model = CatBoostRegressor(\n",
    "                task_type='GPU',\n",
    "                silent=True,\n",
    "                random_state=random_seed,\n",
    "                **model_kwargs)\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_preds = model.predict(X_valid)\n",
    "        \n",
    "        oof_preds[valid_ids] = y_valid_preds\n",
    "#         oof_preds.extend(y_valid_preds)\n",
    "#         oof_y.extend(y_valid)\n",
    "        \n",
    "        if arch == 'catboost':\n",
    "            test_preds += model.predict(X_test).flatten()\n",
    "        else:\n",
    "            test_preds += model.predict(X_test)\n",
    "        \n",
    "#         fold_smape = SMAPE(y_true=y_valid, y_pred=y_valid_preds)\n",
    "#         print(f\"FOLD {fold} OOF SMAPE: {fold_smape}\")\n",
    "    test_preds /= 4 # taking the average of the test preds\n",
    "    \n",
    "    if target == 'target':\n",
    "        oof_preds = np.exp(oof_preds) * tv_df['gdp']**gdp_exponent\n",
    "        test_preds = np.exp(test_preds) * test_df['gdp']**gdp_exponent\n",
    "        \n",
    "    smape = SMAPE(y_pred=oof_preds, y_true=tv_df['num_sold'])\n",
    "#     print(\"Lengths of oof_preds and tv_df[target] are same? \", len(oof_preds) == len(tv_df[target]))\n",
    "#     print(oof_preds[:10])\n",
    "#     print(tv_df[target][:10])\n",
    "    print(f\"SMAPE: {smape}\")\n",
    "    if wandb_tracked:\n",
    "        wandb.log({\n",
    "            'arch': arch,\n",
    "            'SMAPE': smape,\n",
    "            'model_params': str(model_kwargs),\n",
    "            'model_seed': random_state\n",
    "        })\n",
    "        wandb.finish()\n",
    "    return oof_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4390d93d-f686-4e9b-830c-f0dc1078f272",
   "metadata": {},
   "source": [
    "#### Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b788268c-8610-423d-a5c4-9c3bdb3b992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# mlp_tv_preds, mlp_test_preds = skorch_trainer(model=MLP, model_kwargs=mlp_skorch_kwargs, wandb_tracked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ecbdde9-1aa7-4f2a-a770-c96d0205b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# mlp_tv_preds, mlp_test_preds = skorch_trainer(model_kwargs=mlp_skorch_kwargs, wandb_tracked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7fe91fd0-34c9-4826-8494-ff79e276ea41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "# tcn_tv_preds, tcn_test_preds = skorch_trainer(arch='TCN', model_kwargs=tcn_skorch_kwargs, wandb_tracked=False)# sklearn_trainer(estimator=TemporalConvNet, model_type='skorch')#use_skorch=True)\n",
    "# dump(tcn_tv_preds, predpath/'20220130-TCN-baseline-tv-30epochs-preds.joblib')\n",
    "# dump(tcn_test_preds, predpath/'20220130-TCN-baseline-test-30epochs-preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "90ebd4e4-268f-4b45-9227-1a37439a9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE(y_true=tv_df['num_sold'], y_pred=tcn_tv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "edddf172-d764-4662-a105-cc297a15d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_skorch_kwargs = {\n",
    "    'module__num_inputs':1,\n",
    "    'module__num_channels':[20] * 6,\n",
    "    'module__output_sz':1, #2 * samples_per_hour,\n",
    "    'module__kernel_size':5,\n",
    "    'module__dropout':0.1,\n",
    "    'max_epochs':80, # 60,\n",
    "    'batch_size':256,\n",
    "    'lr':2e-3,\n",
    "    'optimizer':torch.optim.Adam,\n",
    "    'train_split':None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d3a01ce-780a-4d13-8e94-d6408a12245c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "# tcn_tv_preds, tcn_test_preds = skorch_trainer(arch='TCN', model_kwargs=tcn_skorch_kwargs, wandb_tracked=False)# sklearn_trainer(estimator=TemporalConvNet, model_type='skorch')#use_skorch=True)\n",
    "# dump(tcn_tv_preds, predpath/'20220130-TCN-baseline-tv-80epochs-preds.joblib')\n",
    "# dump(tcn_test_preds, predpath/'20220130-TCN-baseline-test-80epochs-preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "15392bf0-53e7-4162-9cd3-b9d1106781e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE(y_true=tv_df['num_sold'], y_pred=tcn_tv_preds) # 163.04267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c087bde-c5c2-41b0-b4d4-4969adeb72f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "# tcn_tv_preds, tcn_test_preds = skorch_trainer(arch='TCN', model_kwargs=tcn_skorch_kwargs, wandb_tracked=False)# sklearn_trainer(estimator=TemporalConvNet, model_type='skorch')#use_skorch=True)\n",
    "# dump(tcn_tv_preds, predpath/'20220130-TCN-baseline-tv-80epochs-preds.joblib')\n",
    "# dump(tcn_test_preds, predpath/'20220130-TCN-baseline-test-80epochs-preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f85cf304-b943-4922-b7c9-c7e9fa659389",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_skorch_kwargs = {\n",
    "    'module__num_inputs':1,\n",
    "    'module__num_channels':[20] * 6,\n",
    "    'module__output_sz':1, #2 * samples_per_hour,\n",
    "    'module__kernel_size':5,\n",
    "    'module__dropout':0.15,\n",
    "    'max_epochs':225, # 60,\n",
    "    'batch_size':256,\n",
    "    'lr':2e-3,\n",
    "    'optimizer':torch.optim.Adam,\n",
    "    'train_split':None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3db19a-d6a8-4555-a38d-f91bb9138356",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1153373.1461\u001b[0m  3.9563\n",
      "      2   \u001b[36m217525.7468\u001b[0m  2.9652\n",
      "      3    \u001b[36m52449.8343\u001b[0m  2.9992\n",
      "      4    \u001b[36m39891.7574\u001b[0m  2.9833\n",
      "      5    \u001b[36m39629.3753\u001b[0m  2.9966\n",
      "      6     \u001b[36m7372.5095\u001b[0m  2.9809\n",
      "      7     8369.4916  2.9733\n",
      "      8    10446.4935  3.0074\n",
      "      9     \u001b[36m6138.7386\u001b[0m  3.0757\n",
      "     10     \u001b[36m3923.8833\u001b[0m  4.8289\n",
      "     11     4010.7920  3.9657\n",
      "     12     4084.1456  3.0736\n",
      "     13     \u001b[36m3825.4094\u001b[0m  3.8119\n",
      "     14     \u001b[36m3140.2787\u001b[0m  3.7087\n",
      "     15     \u001b[36m2978.0102\u001b[0m  5.0061\n",
      "     16     3186.5650  3.8288\n",
      "     17     \u001b[36m2778.3079\u001b[0m  4.0379\n",
      "     18     2787.5868  3.1802\n",
      "     19     2846.4488  3.1071\n",
      "     20     \u001b[36m2593.9680\u001b[0m  3.1491\n",
      "     21     \u001b[36m2484.3932\u001b[0m  3.0583\n",
      "     22     \u001b[36m2425.5868\u001b[0m  3.0280\n",
      "     23     \u001b[36m2110.6003\u001b[0m  3.2714\n",
      "     24     2123.2398  3.0761\n",
      "     25     2158.9633  3.0593\n",
      "     26     \u001b[36m2089.9231\u001b[0m  3.0577\n",
      "     27     \u001b[36m2036.9138\u001b[0m  3.1253\n",
      "     28     \u001b[36m1908.0899\u001b[0m  3.1169\n",
      "     29     1994.9966  3.7302\n",
      "     30     \u001b[36m1722.4682\u001b[0m  3.0612\n",
      "     31     1792.7555  2.9938\n",
      "     32     1800.6030  2.9621\n",
      "     33     \u001b[36m1637.1560\u001b[0m  2.9680\n",
      "     34     \u001b[36m1420.3023\u001b[0m  3.0246\n",
      "     35     1508.9034  4.8422\n",
      "     36     1505.9867  5.0321\n",
      "     37     1586.7816  3.2582\n",
      "     38     \u001b[36m1412.5686\u001b[0m  3.0177\n",
      "     39     1444.8392  2.9801\n",
      "     40     \u001b[36m1382.2660\u001b[0m  2.9785\n",
      "     41     \u001b[36m1235.9254\u001b[0m  3.0195\n",
      "     42     \u001b[36m1207.4920\u001b[0m  2.9694\n",
      "     43     \u001b[36m1130.4910\u001b[0m  3.0202\n",
      "     44     \u001b[36m1108.6326\u001b[0m  2.9742\n",
      "     45     \u001b[36m1040.3797\u001b[0m  2.9922\n",
      "     46     \u001b[36m1001.4690\u001b[0m  2.9744\n",
      "     47      \u001b[36m915.6817\u001b[0m  2.9871\n",
      "     48     1003.1277  3.0177\n",
      "     49      955.1931  2.9825\n",
      "     50      \u001b[36m892.2230\u001b[0m  2.9638\n",
      "     51      911.4772  2.9897\n",
      "     52      \u001b[36m880.3609\u001b[0m  2.9682\n",
      "     53      \u001b[36m848.7128\u001b[0m  3.0318\n",
      "     54      \u001b[36m763.9189\u001b[0m  3.0115\n",
      "     55      811.4154  2.9683\n",
      "     56      \u001b[36m752.0587\u001b[0m  3.0158\n",
      "     57      \u001b[36m730.9878\u001b[0m  2.9866\n",
      "     58      746.2206  3.0370\n",
      "     59      \u001b[36m728.7683\u001b[0m  2.9672\n",
      "     60      \u001b[36m673.9341\u001b[0m  2.9785\n",
      "     61      683.7705  2.9905\n",
      "     62      707.6493  2.9866\n",
      "     63      \u001b[36m652.6760\u001b[0m  3.0195\n",
      "     64      669.0846  2.9731\n",
      "     65      \u001b[36m639.2061\u001b[0m  2.9845\n",
      "     66      \u001b[36m591.8881\u001b[0m  2.9839\n",
      "     67      624.1288  2.9680\n",
      "     68      608.0681  3.0576\n",
      "     69      \u001b[36m533.3109\u001b[0m  2.9678\n",
      "     70      \u001b[36m504.8613\u001b[0m  3.0064\n",
      "     71      534.1978  2.9625\n",
      "     72      530.7691  2.9816\n",
      "     73      549.3336  3.0216\n",
      "     74      531.6752  3.0043\n",
      "     75      \u001b[36m480.0927\u001b[0m  2.9860\n",
      "     76      \u001b[36m470.4677\u001b[0m  2.9781\n",
      "     77      \u001b[36m443.1998\u001b[0m  2.9656\n",
      "     78      483.4447  3.0574\n",
      "     79      484.3025  2.9655\n",
      "     80      472.0521  2.9935\n",
      "     81      449.2045  3.0049\n",
      "     82      452.6320  2.9620\n",
      "     83      \u001b[36m409.3594\u001b[0m  3.0358\n",
      "     84      439.9092  3.0248\n",
      "     85      421.7855  2.9859\n",
      "     86      423.6661  2.9702\n",
      "     87      432.3488  2.9809\n",
      "     88      \u001b[36m371.0813\u001b[0m  3.0441\n",
      "     89      \u001b[36m366.7447\u001b[0m  2.9664\n",
      "     90      \u001b[36m354.9597\u001b[0m  2.9670\n",
      "     91      377.8636  2.9825\n",
      "     92      370.6140  2.9507\n",
      "     93      398.9822  3.0243\n",
      "     94      368.7076  3.0032\n",
      "     95      403.5274  2.9853\n",
      "     96      372.9523  2.9782\n",
      "     97      \u001b[36m346.4946\u001b[0m  2.9670\n",
      "     98      366.4381  3.0516\n",
      "     99      \u001b[36m345.1177\u001b[0m  2.9655\n",
      "    100      \u001b[36m331.9556\u001b[0m  2.9665\n",
      "    101      359.9708  3.0040\n",
      "    102      \u001b[36m306.3882\u001b[0m  2.9668\n",
      "    103      316.6229  3.0159\n",
      "    104      325.1523  2.9615\n",
      "    105      \u001b[36m297.9286\u001b[0m  2.9698\n",
      "    106      303.9654  2.9759\n",
      "    107      305.2516  2.9533\n",
      "    108      305.5622  3.0133\n",
      "    109      \u001b[36m280.0611\u001b[0m  2.9731\n",
      "    110      287.3408  2.9923\n",
      "    111      302.9631  2.9592\n",
      "    112      \u001b[36m260.2556\u001b[0m  2.9452\n",
      "    113      303.0436  3.0005\n",
      "    114      284.7366  2.9503\n",
      "    115      280.7231  2.9938\n",
      "    116      \u001b[36m232.1308\u001b[0m  2.9492\n",
      "    117      267.1162  2.9387\n",
      "    118      247.0163  3.0220\n",
      "    119      260.0251  2.9561\n",
      "    120      255.7695  2.9465\n",
      "    121      240.6507  2.9903\n",
      "    122      236.1413  2.9410\n",
      "    123      \u001b[36m227.2240\u001b[0m  2.9780\n",
      "    124      247.2585  2.9518\n",
      "    125      231.9620  2.9439\n",
      "    126      228.9770  2.9525\n",
      "    127      242.6714  2.9440\n",
      "    128      235.1519  3.1715\n",
      "    129      241.4607  4.1918\n",
      "    130      241.4640  4.4837\n",
      "    131      \u001b[36m206.1202\u001b[0m  3.0006\n",
      "    132      217.3415  2.9623\n",
      "    133      210.5869  2.9952\n",
      "    134      221.9193  3.0120\n",
      "    135      \u001b[36m189.5075\u001b[0m  2.9625\n",
      "    136      \u001b[36m183.1491\u001b[0m  2.9904\n",
      "    137      200.5626  2.9895\n",
      "    138      193.8639  2.9934\n",
      "    139      201.5953  2.9827\n",
      "    140      215.0350  2.9688\n",
      "    141      194.8394  3.0143\n",
      "    142      184.8972  3.2200\n",
      "    143      186.3475  3.0371\n",
      "    144      186.1893  2.9808\n",
      "    145      \u001b[36m181.4730\u001b[0m  2.9721\n",
      "    146      \u001b[36m163.6363\u001b[0m  3.0016\n",
      "    147      185.7859  2.9919\n",
      "    148      175.4745  3.0301\n",
      "    149      167.5283  2.9806\n",
      "    150      189.3530  3.0429\n",
      "    151      \u001b[36m160.1189\u001b[0m  2.9776\n",
      "    152      \u001b[36m156.8311\u001b[0m  2.9606\n",
      "    153      \u001b[36m150.0910\u001b[0m  3.0234\n",
      "    154      161.5389  3.0292\n",
      "    155      163.3538  2.9830\n",
      "    156      180.8963  2.9726\n",
      "    157      173.6789  2.9922\n",
      "    158      155.8426  3.0061\n",
      "    159      158.8012  2.9652\n",
      "    160      152.5518  2.9822\n",
      "    161      \u001b[36m144.2402\u001b[0m  2.9797\n",
      "    162      158.4874  2.9626\n",
      "    163      165.3043  3.0396\n",
      "    164      149.9705  2.9691\n",
      "    165      149.2615  2.9672\n",
      "    166      \u001b[36m140.4775\u001b[0m  2.9627\n",
      "    167      156.2236  2.9630\n",
      "    168      \u001b[36m128.2423\u001b[0m  3.0291\n",
      "    169      158.1109  2.9517\n",
      "    170      138.3107  2.9669\n",
      "    171      148.9133  3.0050\n",
      "    172      132.1813  2.9583\n",
      "    173      136.5971  3.0019\n",
      "    174      128.4228  2.9677\n",
      "    175      131.5492  2.9403\n",
      "    176      \u001b[36m112.0535\u001b[0m  2.9482\n",
      "    177      126.5132  2.9736\n",
      "    178      138.2652  2.9632\n",
      "    179      130.3270  2.9851\n",
      "    180      132.8241  2.9771\n",
      "    181      125.0792  2.9806\n",
      "    182      125.6427  2.9270\n",
      "    183      122.0502  2.9588\n",
      "    184      118.1470  2.9307\n",
      "    185      \u001b[36m107.9597\u001b[0m  2.9283\n",
      "    186      117.8649  3.8037\n",
      "    187      121.0646  3.8081\n",
      "    188      114.6360  3.5389\n",
      "    189      121.0272  3.0642\n",
      "    190      121.3297  3.0298\n",
      "    191      117.1924  2.9989\n",
      "    192      \u001b[36m105.9616\u001b[0m  3.0019\n",
      "    193      113.5710  3.0371\n",
      "    194      107.7836  2.9670\n",
      "    195      106.9447  3.0526\n",
      "    196      125.4267  2.9618\n",
      "    197      125.3334  3.5704\n",
      "    198      115.4722  3.0311\n",
      "    199      115.5814  3.0049\n",
      "    200       \u001b[36m97.8483\u001b[0m  3.0122\n",
      "    201      100.0919  2.9763\n",
      "    202      107.0839  3.0201\n",
      "    203      104.5674  3.3013\n",
      "    204      111.9768  3.5983\n",
      "    205       \u001b[36m91.4317\u001b[0m  3.0144\n",
      "    206       93.1738  2.9979\n",
      "    207      103.2095  2.9907\n",
      "    208      101.0807  2.9945\n",
      "    209       92.6205  2.9904\n",
      "    210       93.1397  3.0303\n",
      "    211       94.0397  2.9680\n",
      "    212       \u001b[36m90.1998\u001b[0m  3.0158\n",
      "    213      102.8514  3.0040\n",
      "    214       \u001b[36m84.4342\u001b[0m  2.9518\n",
      "    215       89.9359  3.0020\n",
      "    216       99.0263  3.0044\n",
      "    217       91.9640  2.9895\n",
      "    218       91.4284  2.9679\n",
      "    219       86.8475  2.9839\n",
      "    220       88.5304  3.0267\n",
      "    221       85.5617  2.9870\n",
      "    222       \u001b[36m81.2203\u001b[0m  3.0010\n",
      "    223       \u001b[36m76.8639\u001b[0m  2.9791\n",
      "    224       84.0343  2.9702\n",
      "    225       \u001b[36m76.6931\u001b[0m  3.2773\n",
      "Valid SMAPE for ['Sweden'], ['KaggleMart'], ['Kaggle Mug'] is 200.0\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m846604.1405\u001b[0m  3.0163\n",
      "      2   \u001b[36m306481.5688\u001b[0m  3.9599\n",
      "      3    \u001b[36m64837.6235\u001b[0m  3.8770\n",
      "      4    75601.3661  4.7302\n",
      "      5    \u001b[36m13077.0381\u001b[0m  4.6027\n",
      "      6    20473.5117  3.0414\n",
      "      7    19307.4473  3.1492\n",
      "      8     \u001b[36m7534.5702\u001b[0m  2.9872\n",
      "      9     \u001b[36m6644.1747\u001b[0m  2.9371\n",
      "     10     7308.2246  3.1732\n",
      "     11     \u001b[36m5436.0381\u001b[0m  2.9881\n",
      "     12     \u001b[36m4162.3499\u001b[0m  2.9890\n",
      "     13     4454.7113  2.9824\n",
      "     14     4329.7374  2.9724\n",
      "     15     \u001b[36m3948.7427\u001b[0m  2.9582\n",
      "     16     \u001b[36m3672.9697\u001b[0m  3.0184\n",
      "     17     \u001b[36m3597.0976\u001b[0m  4.5667\n",
      "     18     \u001b[36m3415.6309\u001b[0m  3.1545\n",
      "     19     \u001b[36m3074.5316\u001b[0m  3.1034\n",
      "     20     \u001b[36m3005.4529\u001b[0m  3.2324\n",
      "     21     \u001b[36m2907.8270\u001b[0m  3.0655\n",
      "     22     \u001b[36m2721.1387\u001b[0m  3.0723\n",
      "     23     \u001b[36m2615.3803\u001b[0m  3.1184\n",
      "     24     \u001b[36m2565.9691\u001b[0m  3.0870\n",
      "     25     \u001b[36m2356.4385\u001b[0m  3.0286\n",
      "     26     2492.5102  3.0405\n",
      "     27     \u001b[36m2280.8610\u001b[0m  3.0226\n",
      "     28     \u001b[36m2115.6609\u001b[0m  3.3806\n",
      "     29     \u001b[36m2016.0923\u001b[0m  3.5813\n",
      "     30     2021.5345  3.5791\n",
      "     31     \u001b[36m2010.5262\u001b[0m  4.0920\n",
      "     32     \u001b[36m1784.5596\u001b[0m  4.1967\n",
      "     33     1812.4818  4.3024\n",
      "     34     \u001b[36m1745.5658\u001b[0m  4.4651\n",
      "     35     \u001b[36m1653.2938\u001b[0m  3.0144\n",
      "     36     \u001b[36m1554.4444\u001b[0m  3.2371\n",
      "     37     \u001b[36m1553.4601\u001b[0m  3.6066\n",
      "     38     \u001b[36m1535.8050\u001b[0m  4.3348\n",
      "     39     \u001b[36m1420.6402\u001b[0m  3.8908\n",
      "     40     \u001b[36m1309.9681\u001b[0m  3.7533\n",
      "     41     1334.3836  2.9838\n",
      "     42     \u001b[36m1241.5657\u001b[0m  3.4989\n",
      "     43     \u001b[36m1143.8848\u001b[0m  3.6248\n",
      "     44     1244.5687  4.6537\n",
      "     45     1200.8638  3.0015\n",
      "     46     \u001b[36m1109.1682\u001b[0m  4.6345\n",
      "     47     \u001b[36m1047.2203\u001b[0m  3.0180\n",
      "     48      \u001b[36m937.4338\u001b[0m  3.1808\n",
      "     49     1036.8628  3.1191\n",
      "     50      \u001b[36m920.9018\u001b[0m  3.1612\n",
      "     51      927.5962  3.0445\n",
      "     52      \u001b[36m908.1558\u001b[0m  3.6473\n",
      "     53      \u001b[36m853.2559\u001b[0m  4.6016\n",
      "     54      \u001b[36m827.7767\u001b[0m  3.8358\n",
      "     55      \u001b[36m815.8772\u001b[0m  5.1672\n",
      "     56      831.7271  4.4140\n",
      "     57      \u001b[36m781.5478\u001b[0m  4.3428\n",
      "     58      825.6384  4.0843\n",
      "     59      \u001b[36m767.8718\u001b[0m  3.4683\n",
      "     60      \u001b[36m704.1067\u001b[0m  4.3099\n",
      "     61      \u001b[36m688.9924\u001b[0m  5.1030\n",
      "     62      689.1026  4.0497\n",
      "     63      696.9730  3.3066\n",
      "     64      \u001b[36m625.3415\u001b[0m  3.4757\n",
      "     65      688.1951  3.5736\n",
      "     66      \u001b[36m604.9416\u001b[0m  3.0843\n",
      "     67      \u001b[36m570.5692\u001b[0m  2.9876\n",
      "     68      604.3713  3.0166\n"
     ]
    }
   ],
   "source": [
    "# %%time \n",
    "tcn_tv_preds, tcn_test_preds = skorch_trainer(arch='TCN', model_kwargs=tcn_skorch_kwargs, wandb_tracked=False)# sklearn_trainer(estimator=TemporalConvNet, model_type='skorch')#use_skorch=True)\n",
    "dump(tcn_tv_preds, predpath/'20220130-TCN-baseline-tv-225epochs-preds.joblib')\n",
    "dump(tcn_test_preds, predpath/'20220130-TCN-baseline-test-225epochs-preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c0dc54f-8a0e-4970-8067-a67d26d4ee65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.04267849672794"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMAPE(y_true=tv_df['num_sold'], y_pred=tcn_tv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6dcebcb-2c6c-4174-8e6b-ba9c72b957f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"optuna_forecasting_{datetime.now().strftime('%Y%m%d')}.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77542044-e971-4d36-bc3f-1dd3b5693517",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmodel_config = {\n",
    "    'cross-validation': 'GroupKFold(n_split=4)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10ecd73c-a9e8-4de2-8459-06e29513b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['study', 'TCN'],\n",
    "    'notes': \"Optuna study of forecasting methods -- TCN\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12a3f107-7bed-4c41-8767-a5caab390b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'catboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "acb57d3e-b179-4b01-8bdc-0b8f38db0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally from https://www.kaggle.com/satorushibata/optimize-catboost-hyperparameter-with-optuna-gpu\n",
    "def objective(trial, arch=arch):#, tune_fold=tune_fold):\n",
    "    \"\"\"\n",
    "    Wrapper around cross_validation_trainer to test different model hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if arch == 'catboost':\n",
    "        model_params = {\n",
    "            'iterations' : trial.suggest_int('iterations', 2000, 30000),                         \n",
    "            'depth' : trial.suggest_int('depth', 3, 10),                                       \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.5),               \n",
    "            'random_strength': trial.suggest_int('random_strength', 0, 100), \n",
    "    #         'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n",
    "    #         'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['MVC', 'Bernoulli']),#, 'Poisson']),\n",
    "            'od_wait': trial.suggest_int('od_wait', 20, 2000),\n",
    "            'reg_lambda': trial.suggest_uniform('reg_lambda', 2, 70), # aka l2_leaf_reg\n",
    "            'border_count': trial.suggest_int('border_count', 50, 275),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 20), # aka min_data_in_leaf\n",
    "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 5),\n",
    "#             'task_type':'GPU',\n",
    "#             'verbose': False,\n",
    "# #             'silent':True,\n",
    "#             'random_state':42,\n",
    "            # 'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
    "    #         'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "    #         'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "            # 'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "    #         'max_leaves': trial.suggest_int('max_leaves', 32, 128)\n",
    "        }\n",
    "        \n",
    "    elif arch == 'lightgbm':\n",
    "        pass # todo -- fill in tomorrow\n",
    "        \n",
    "    elif arch == 'xgboost':\n",
    "        model_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 10000), # was 900-4500 for CPU\n",
    "            'max_depth' : trial.suggest_int('depth', 3, 10),                                       \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.3),               \n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 50),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 30),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
    "    #         'booster': trial.suggest_categorical('boosting_type', ['gbtree', 'dart']),\n",
    "            'min_child_weight': trial.suggest_uniform('min_child_weight', 0.001, 12),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "            'gamma': trial.suggest_uniform('gamma', 0.1, 10),\n",
    "#             'tree_method': 'gpu_hist',\n",
    "#             'predictor': 'gpu_predictor',\n",
    "#             'eval_metric': ['mae', 'mape'],\n",
    "#             'sampling_method': 'gradient_based',\n",
    "#             'seed': 42,\n",
    "#             'grow_policy': 'lossguide',\n",
    "#             'max_leaves': 255,\n",
    "#             'lambda': 100,\n",
    "#     'n_estimators': 3000,\n",
    "#             'objective': 'reg:squarederror',\n",
    "#             'n_estimators': 500,\n",
    "#     'verbose': True,\n",
    "            \n",
    "        } \n",
    "        \n",
    "    elif arch == 'TCN':\n",
    "        levels = trial.suggest_int('levels', 5, 15)\n",
    "        hidden_units = trial.suggest_int('hidden_units', 5,30)\n",
    "#             optimizer = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
    "\n",
    "        model_params = {\n",
    "            'module__num_inputs': 1,\n",
    "            'module__output_sz': 1,\n",
    "            'module__num_channels': [hidden_units] * (levels-1),\n",
    "            'module__kernel_size': trial.suggest_int('kernel_size', 2, 10),\n",
    "            'module__dropout': trial.suggest_uniform('dropout', 0, 0.2),\n",
    "            'batch_size': 256,\n",
    "#                 'batch_size': trial.suggest_categorical('batch_size', [64,128,256]),\n",
    "            'lr': trial.suggest_loguniform('learning_rate', 0.001, 0.3),\n",
    "            'max_epochs': 25,\n",
    "            'optimizer': torch.optim.Adam,\n",
    "            'device': 'cuda',\n",
    "            'train_split': None\n",
    "        }\n",
    "    \n",
    "    return gbm_trainer(arch=arch, model_kwargs=model_params, wandb_tracked=False)#, telegram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66ca82d5-a467-4afc-94e4-6f5f47ece425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/uncategorized/runs/3bbrnt0c\" target=\"_blank\">optuna_forecasting_20220128_210412</a></strong> to <a href=\"https://wandb.ai/hushifang/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d3334-32e1-473f-9907-9d8f490a5b6e",
   "metadata": {},
   "source": [
    "#### TCN Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2401663-5ff3-4f90-aec5-188e1cd168fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'catboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca13b4c9-e386-4a23-afc5-6da359f2bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e2d9eaa-b5f5-471b-b0bd-c1a6590dcfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-28 21:04:18,609]\u001b[0m A new study created in memory with name: TCN_study-20220128210418\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "study = optuna.create_study(direction = \"minimize\", \n",
    "                            sampler = TPESampler(seed=int(SEED)), \n",
    "                            study_name=f\"{arch}_study-{start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d44df046-2455-4d6f-8d5a-2e3067b7c729",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m9989444.0402\u001b[0m  7.8163\n",
      "      2   \u001b[36m571126.2379\u001b[0m  6.4996\n",
      "      3   \u001b[36m154326.1146\u001b[0m  6.4912\n",
      "      4    \u001b[36m24990.7754\u001b[0m  6.5515\n",
      "      5     \u001b[36m8297.2409\u001b[0m  6.4820\n",
      "      6     \u001b[36m4456.4714\u001b[0m  6.5146\n",
      "      7     \u001b[36m2815.6780\u001b[0m  6.4533\n",
      "      8     \u001b[36m2088.5209\u001b[0m  6.4815\n",
      "      9     \u001b[36m1645.7043\u001b[0m  6.4836\n",
      "     10     \u001b[36m1348.6061\u001b[0m  6.4641\n",
      "     11     \u001b[36m1182.2014\u001b[0m  6.5029\n",
      "     12     \u001b[36m1030.6186\u001b[0m  6.4959\n",
      "     13      \u001b[36m987.7621\u001b[0m  6.6142\n",
      "     14      \u001b[36m897.5140\u001b[0m  6.5040\n",
      "     15      \u001b[36m860.9743\u001b[0m  6.5176\n",
      "     16      \u001b[36m832.0357\u001b[0m  6.5637\n",
      "     17      \u001b[36m807.5422\u001b[0m  6.4913\n",
      "     18      \u001b[36m780.0090\u001b[0m  6.5448\n",
      "     19      \u001b[36m730.6163\u001b[0m  6.5068\n",
      "     20      \u001b[36m702.5889\u001b[0m  6.5428\n",
      "     21      \u001b[36m668.7426\u001b[0m  6.5148\n",
      "     22      \u001b[36m651.0798\u001b[0m  6.5572\n",
      "     23      \u001b[36m623.5456\u001b[0m  6.5414\n",
      "     24      \u001b[36m590.2398\u001b[0m  6.5180\n",
      "     25      \u001b[36m571.2284\u001b[0m  6.5517\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m5222007.3312\u001b[0m  6.5841\n",
      "      2    \u001b[36m40077.2543\u001b[0m  6.4964\n",
      "      3      \u001b[36m424.6338\u001b[0m  6.5903\n",
      "      4      \u001b[36m258.6721\u001b[0m  6.4913\n",
      "      5      404.2591  6.5847\n",
      "      6      486.4055  6.4942\n",
      "      7      502.2693  6.5186\n",
      "      8      500.8662  6.4934\n",
      "      9      491.9397  6.4713\n",
      "     10      447.2392  6.5127\n",
      "     11      406.0802  6.4514\n",
      "     12      357.6130  6.4872\n",
      "     13      332.4350  6.5138\n",
      "     14      287.1657  6.5129\n",
      "     15      271.8719  6.5344\n",
      "     16      \u001b[36m242.8580\u001b[0m  6.6157\n",
      "     17      \u001b[36m226.7971\u001b[0m  6.5845\n",
      "     18      \u001b[36m192.8631\u001b[0m  6.4893\n",
      "     19      \u001b[36m168.5082\u001b[0m  6.5359\n",
      "     20      \u001b[36m162.4698\u001b[0m  6.4680\n",
      "     21      \u001b[36m146.4225\u001b[0m  6.4930\n",
      "     22      \u001b[36m133.9036\u001b[0m  6.4608\n",
      "     23      \u001b[36m117.2700\u001b[0m  6.4618\n",
      "     24       \u001b[36m95.5021\u001b[0m  6.4529\n",
      "     25      100.1011  6.4545\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m16604694.2911\u001b[0m  6.6758\n",
      "      2  \u001b[36m1114767.2639\u001b[0m  6.5207\n",
      "      3   \u001b[36m111004.7742\u001b[0m  6.5058\n",
      "      4    \u001b[36m25809.3405\u001b[0m  6.5512\n",
      "      5    \u001b[36m10645.2450\u001b[0m  6.4908\n",
      "      6     \u001b[36m6020.6281\u001b[0m  6.5255\n",
      "      7     \u001b[36m4076.0709\u001b[0m  6.5778\n",
      "      8     \u001b[36m3003.9254\u001b[0m  6.4761\n",
      "      9     \u001b[36m2502.3096\u001b[0m  6.5354\n",
      "     10     \u001b[36m2148.7766\u001b[0m  6.5256\n",
      "     11     \u001b[36m1984.7610\u001b[0m  6.5355\n",
      "     12     \u001b[36m1752.6980\u001b[0m  6.5154\n",
      "     13     \u001b[36m1603.2196\u001b[0m  6.5154\n",
      "     14     \u001b[36m1555.1503\u001b[0m  6.4447\n",
      "     15     \u001b[36m1452.4394\u001b[0m  6.4504\n",
      "     16     \u001b[36m1451.5392\u001b[0m  6.4938\n",
      "     17     \u001b[36m1347.3337\u001b[0m  6.4496\n",
      "     18     \u001b[36m1320.8178\u001b[0m  6.5023\n",
      "     19     \u001b[36m1277.6885\u001b[0m  6.4455\n",
      "     20     1279.9662  6.4879\n",
      "     21     \u001b[36m1158.9659\u001b[0m  6.4394\n",
      "     22     \u001b[36m1093.3809\u001b[0m  6.4585\n",
      "     23     \u001b[36m1051.9862\u001b[0m  6.4734\n",
      "     24     \u001b[36m1022.1733\u001b[0m  6.4331\n",
      "     25     1036.2554  6.5235\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m32634064.0073\u001b[0m  6.5901\n",
      "      2  \u001b[36m3567454.8615\u001b[0m  6.4489\n",
      "      3    \u001b[36m65596.7450\u001b[0m  6.4868\n",
      "      4     \u001b[36m1609.1671\u001b[0m  6.4406\n",
      "      5      \u001b[36m600.1608\u001b[0m  6.4523\n",
      "      6     1096.8825  6.5357\n",
      "      7     1214.9353  6.5018\n",
      "      8     1299.4382  6.5137\n",
      "      9     1245.1708  6.4782\n",
      "     10     1197.5063  6.5554\n",
      "     11     1135.1452  6.4629\n",
      "     12     1022.0390  6.4824\n",
      "     13      954.5330  6.5122\n",
      "     14      882.4714  6.4876\n",
      "     15      816.3861  6.5403\n",
      "     16      771.4712  6.4963\n",
      "     17      722.5132  6.5371\n",
      "     18      685.7534  6.5541\n",
      "     19      \u001b[36m599.7509\u001b[0m  6.5137\n",
      "     20      \u001b[36m540.7652\u001b[0m  6.4746\n",
      "     21      \u001b[36m527.6843\u001b[0m  6.4743\n",
      "     22      \u001b[36m471.6376\u001b[0m  6.5332\n",
      "     23      \u001b[36m434.8067\u001b[0m  6.5034\n",
      "     24      \u001b[36m426.0160\u001b[0m  6.5187\n",
      "     25      \u001b[36m368.4824\u001b[0m  6.4762\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m8936942.0628\u001b[0m  6.5508\n",
      "      2   \u001b[36m893714.4714\u001b[0m  6.5472\n",
      "      3    \u001b[36m67878.5538\u001b[0m  6.4740\n",
      "      4     \u001b[36m9234.6133\u001b[0m  6.4851\n",
      "      5     \u001b[36m2863.4443\u001b[0m  6.5088\n",
      "      6     \u001b[36m1301.3566\u001b[0m  6.4689\n",
      "      7      \u001b[36m749.0168\u001b[0m  6.4614\n",
      "      8      \u001b[36m507.8070\u001b[0m  6.4417\n",
      "      9      \u001b[36m407.0376\u001b[0m  6.4888\n",
      "     10      \u001b[36m349.3396\u001b[0m  6.4751\n",
      "     11      \u001b[36m294.2834\u001b[0m  6.4928\n",
      "     12      \u001b[36m264.0791\u001b[0m  6.4918\n",
      "     13      \u001b[36m254.7502\u001b[0m  6.4376\n",
      "     14      \u001b[36m233.4418\u001b[0m  6.5023\n",
      "     15      \u001b[36m228.2116\u001b[0m  6.4791\n",
      "     16      \u001b[36m202.6662\u001b[0m  6.5382\n",
      "     17      204.2855  6.4953\n",
      "     18      \u001b[36m195.3569\u001b[0m  6.5044\n",
      "     19      \u001b[36m178.1910\u001b[0m  6.5018\n",
      "     20      \u001b[36m163.9617\u001b[0m  6.5265\n",
      "     21      \u001b[36m157.5714\u001b[0m  6.5464\n",
      "     22      \u001b[36m152.0216\u001b[0m  6.4765\n",
      "     23      \u001b[36m132.1021\u001b[0m  6.5236\n",
      "     24      140.7231  6.5332\n",
      "     25      135.7992  6.4941\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2229763.6966\u001b[0m  6.5585\n",
      "      2    \u001b[36m65468.5136\u001b[0m  6.5011\n",
      "      3      \u001b[36m458.3543\u001b[0m  6.5173\n",
      "      4      765.9578  6.5360\n",
      "      5      961.5975  6.4861\n",
      "      6      887.3139  6.5526\n",
      "      7      767.3377  6.4605\n",
      "      8      671.6531  6.5101\n",
      "      9      544.8239  6.4423\n",
      "     10      \u001b[36m450.5570\u001b[0m  6.4711\n",
      "     11      \u001b[36m350.0590\u001b[0m  6.4993\n",
      "     12      \u001b[36m296.3372\u001b[0m  6.4702\n",
      "     13      \u001b[36m242.5993\u001b[0m  6.5284\n",
      "     14      \u001b[36m212.1641\u001b[0m  6.6551\n",
      "     15      \u001b[36m175.8342\u001b[0m  6.5039\n",
      "     16      \u001b[36m153.8749\u001b[0m  6.4563\n",
      "     17      \u001b[36m117.7911\u001b[0m  6.4641\n",
      "     18      \u001b[36m102.8727\u001b[0m  6.4954\n",
      "     19       \u001b[36m99.5547\u001b[0m  6.4960\n",
      "     20       \u001b[36m79.6213\u001b[0m  6.5155\n",
      "     21       \u001b[36m63.6671\u001b[0m  6.5030\n",
      "     22       \u001b[36m60.7262\u001b[0m  6.5454\n",
      "     23       \u001b[36m54.6432\u001b[0m  6.5017\n",
      "     24       \u001b[36m48.4992\u001b[0m  6.5374\n",
      "     25       \u001b[36m46.1284\u001b[0m  6.4787\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m13497180.7121\u001b[0m  6.6184\n",
      "      2   \u001b[36m910668.1690\u001b[0m  6.5258\n",
      "      3   \u001b[36m119320.9882\u001b[0m  6.5062\n",
      "      4    \u001b[36m14184.1649\u001b[0m  6.4871\n",
      "      5     \u001b[36m4839.8070\u001b[0m  6.5144\n",
      "      6     \u001b[36m2539.0748\u001b[0m  6.4600\n",
      "      7     \u001b[36m1728.2354\u001b[0m  6.5201\n",
      "      8     \u001b[36m1338.6388\u001b[0m  6.4571\n",
      "      9     \u001b[36m1101.5563\u001b[0m  6.4845\n",
      "     10      \u001b[36m974.1804\u001b[0m  6.4904\n",
      "     11      \u001b[36m867.5436\u001b[0m  6.4474\n",
      "     12      \u001b[36m766.6970\u001b[0m  6.5398\n",
      "     13      \u001b[36m717.9039\u001b[0m  6.5238\n",
      "     14      \u001b[36m660.7917\u001b[0m  6.5200\n",
      "     15      \u001b[36m625.3272\u001b[0m  6.5291\n",
      "     16      \u001b[36m581.7335\u001b[0m  6.4921\n",
      "     17      \u001b[36m574.2003\u001b[0m  6.5034\n",
      "     18      \u001b[36m537.0664\u001b[0m  6.4945\n",
      "     19      \u001b[36m536.4676\u001b[0m  6.5568\n",
      "     20      \u001b[36m483.3737\u001b[0m  6.4990\n",
      "     21      484.7201  6.5138\n",
      "     22      \u001b[36m450.1607\u001b[0m  6.4908\n",
      "     23      \u001b[36m443.0296\u001b[0m  6.5163\n",
      "     24      \u001b[36m380.9810\u001b[0m  6.4875\n",
      "     25      397.6743  6.4855\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m18894324.9168\u001b[0m  6.5370\n",
      "      2   \u001b[36m391857.7484\u001b[0m  6.4769\n",
      "      3   \u001b[36m294446.0365\u001b[0m  6.4434\n",
      "      4    \u001b[36m64305.2836\u001b[0m  6.5207\n",
      "      5    \u001b[36m17331.4136\u001b[0m  6.5430\n",
      "      6     \u001b[36m6650.4746\u001b[0m  6.5251\n",
      "      7     \u001b[36m3465.8947\u001b[0m  6.4591\n",
      "      8     \u001b[36m2107.7251\u001b[0m  6.4647\n",
      "      9     \u001b[36m1602.4934\u001b[0m  6.4915\n",
      "     10     \u001b[36m1202.9025\u001b[0m  6.4825\n",
      "     11      \u001b[36m900.2997\u001b[0m  6.4963\n",
      "     12      \u001b[36m819.4181\u001b[0m  6.4720\n",
      "     13      \u001b[36m715.3478\u001b[0m  6.5226\n",
      "     14      \u001b[36m686.7406\u001b[0m  6.4717\n",
      "     15      \u001b[36m605.2284\u001b[0m  6.5177\n",
      "     16      \u001b[36m566.6501\u001b[0m  6.4975\n",
      "     17      \u001b[36m526.6999\u001b[0m  6.5065\n",
      "     18      \u001b[36m490.7228\u001b[0m  6.5378\n",
      "     19      \u001b[36m454.5094\u001b[0m  6.4664\n",
      "     20      481.7739  6.4837\n",
      "     21      \u001b[36m380.1651\u001b[0m  6.5499\n",
      "     22      429.7805  6.5521\n",
      "     23      419.9814  6.5380\n",
      "     24      463.1165  6.5025\n",
      "     25      402.3179  6.4994\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m16114719.0194\u001b[0m  6.5549\n",
      "      2  \u001b[36m1065402.3205\u001b[0m  6.4521\n",
      "      3   \u001b[36m160385.3840\u001b[0m  6.4929\n",
      "      4    \u001b[36m21590.2273\u001b[0m  6.4736\n",
      "      5     \u001b[36m7332.9313\u001b[0m  6.5282\n",
      "      6     \u001b[36m3739.7202\u001b[0m  6.4858\n",
      "      7     \u001b[36m2592.1019\u001b[0m  6.5086\n",
      "      8     \u001b[36m1912.1544\u001b[0m  6.5133\n",
      "      9     \u001b[36m1587.8018\u001b[0m  6.5234\n",
      "     10     \u001b[36m1377.1717\u001b[0m  6.5485\n",
      "     11     \u001b[36m1252.6662\u001b[0m  6.5500\n",
      "     12     \u001b[36m1093.1322\u001b[0m  6.5147\n",
      "     13     \u001b[36m1039.1162\u001b[0m  6.5188\n",
      "     14     \u001b[36m1031.9783\u001b[0m  6.5453\n",
      "     15      \u001b[36m959.7062\u001b[0m  6.5556\n",
      "     16      \u001b[36m921.2935\u001b[0m  6.5265\n",
      "     17      \u001b[36m896.8742\u001b[0m  6.5414\n",
      "     18      \u001b[36m826.4069\u001b[0m  6.4954\n",
      "     19      829.5898  6.6001\n",
      "     20      \u001b[36m789.0306\u001b[0m  6.5002\n",
      "     21      \u001b[36m757.0212\u001b[0m  6.5119\n",
      "     22      \u001b[36m695.6219\u001b[0m  6.4678\n",
      "     23      \u001b[36m656.9519\u001b[0m  6.4693\n",
      "     24      \u001b[36m640.6641\u001b[0m  6.6121\n",
      "     25      \u001b[36m607.7045\u001b[0m  6.4959\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m9374732.4766\u001b[0m  6.5598\n",
      "      2   \u001b[36m556665.0893\u001b[0m  6.5048\n",
      "      3    \u001b[36m94814.0218\u001b[0m  6.4493\n",
      "      4    \u001b[36m17406.8282\u001b[0m  6.5013\n",
      "      5     \u001b[36m4577.0131\u001b[0m  6.4514\n",
      "      6     \u001b[36m1944.0536\u001b[0m  6.4562\n",
      "      7     \u001b[36m1143.8093\u001b[0m  6.4716\n",
      "      8      \u001b[36m830.3499\u001b[0m  6.5091\n",
      "      9      \u001b[36m673.6269\u001b[0m  6.5184\n",
      "     10      \u001b[36m599.5678\u001b[0m  6.4837\n",
      "     11      \u001b[36m549.8249\u001b[0m  6.5996\n",
      "     12      \u001b[36m506.8539\u001b[0m  6.5254\n",
      "     13      \u001b[36m490.7345\u001b[0m  6.4945\n",
      "     14      \u001b[36m463.8587\u001b[0m  6.4942\n",
      "     15      \u001b[36m459.2094\u001b[0m  6.4520\n",
      "     16      464.8628  6.5062\n",
      "     17      \u001b[36m421.2314\u001b[0m  6.4777\n",
      "     18      \u001b[36m419.7435\u001b[0m  6.4923\n",
      "     19      \u001b[36m400.4858\u001b[0m  6.4824\n",
      "     20      405.9437  6.5381\n",
      "     21      \u001b[36m378.4097\u001b[0m  6.4530\n",
      "     22      398.7212  6.4660\n",
      "     23      \u001b[36m372.1488\u001b[0m  6.4962\n",
      "     24      \u001b[36m363.7373\u001b[0m  6.5151\n",
      "     25      \u001b[36m331.5385\u001b[0m  6.5698\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m8626668.6401\u001b[0m  6.7103\n",
      "      2    \u001b[36m59805.6276\u001b[0m  6.5123\n",
      "      3    74539.5183  6.5107\n",
      "      4    \u001b[36m32001.6741\u001b[0m  6.4392\n",
      "      5    \u001b[36m10450.2339\u001b[0m  6.4463\n",
      "      6     \u001b[36m3595.1164\u001b[0m  6.4670\n",
      "      7     \u001b[36m1408.6046\u001b[0m  6.4475\n",
      "      8      \u001b[36m780.0210\u001b[0m  6.4908\n",
      "      9      \u001b[36m527.7649\u001b[0m  6.4456\n",
      "     10      \u001b[36m355.3622\u001b[0m  6.5427\n",
      "     11      \u001b[36m299.5872\u001b[0m  6.4587\n",
      "     12      \u001b[36m275.5417\u001b[0m  6.4645\n",
      "     13      \u001b[36m249.9599\u001b[0m  6.5003\n",
      "     14      \u001b[36m218.1284\u001b[0m  6.4742\n",
      "     15      235.7825  6.5430\n",
      "     16      \u001b[36m180.6219\u001b[0m  6.4940\n",
      "     17      204.6754  6.6359\n",
      "     18      \u001b[36m173.2936\u001b[0m  6.5226\n",
      "     19      178.6644  6.4949\n",
      "     20      \u001b[36m168.5185\u001b[0m  6.4387\n",
      "     21      \u001b[36m143.2107\u001b[0m  6.4394\n",
      "     22      158.6752  6.5495\n",
      "     23      153.7531  6.4772\n",
      "     24      153.8749  6.5740\n",
      "     25      \u001b[36m139.6053\u001b[0m  6.5006\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m13523418.3932\u001b[0m  6.5235\n",
      "      2  \u001b[36m1323152.6410\u001b[0m  6.4887\n",
      "      3    \u001b[36m91786.5048\u001b[0m  6.4539\n",
      "      4     \u001b[36m6064.6388\u001b[0m  6.4789\n",
      "      5     \u001b[36m1324.8477\u001b[0m  6.4711\n",
      "      6      \u001b[36m584.3954\u001b[0m  6.4912\n",
      "      7      \u001b[36m372.0802\u001b[0m  6.5374\n",
      "      8      \u001b[36m265.2733\u001b[0m  6.4768\n",
      "      9      \u001b[36m234.4801\u001b[0m  6.5147\n",
      "     10      \u001b[36m211.4267\u001b[0m  6.4761\n",
      "     11      \u001b[36m194.0551\u001b[0m  6.5538\n",
      "     12      \u001b[36m169.3487\u001b[0m  6.5101\n",
      "     13      \u001b[36m164.0005\u001b[0m  6.4724\n",
      "     14      \u001b[36m153.0303\u001b[0m  6.5188\n",
      "     15      165.9974  6.5367\n",
      "     16      \u001b[36m138.8157\u001b[0m  6.6275\n",
      "     17      146.1939  6.4593\n",
      "     18      \u001b[36m132.7428\u001b[0m  6.4912\n",
      "     19      140.5305  6.4753\n",
      "     20      140.6022  6.4501\n",
      "     21      \u001b[36m119.9645\u001b[0m  6.5409\n",
      "     22      127.7680  6.4833\n",
      "     23      \u001b[36m116.0218\u001b[0m  6.5040\n",
      "     24      118.2356  6.4538\n",
      "     25      \u001b[36m115.0405\u001b[0m  6.5227\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m9492068.4078\u001b[0m  6.5514\n",
      "      2   \u001b[36m144840.6555\u001b[0m  6.4825\n",
      "      3     \u001b[36m2192.9728\u001b[0m  6.5843\n",
      "      4      \u001b[36m131.7414\u001b[0m  6.6080\n",
      "      5      231.1167  6.5945\n",
      "      6      333.2205  6.5168\n",
      "      7      388.5233  6.4142\n",
      "      8      418.0168  6.4827\n",
      "      9      387.7935  6.4333\n",
      "     10      390.4189  6.4905\n",
      "     11      368.2413  6.4903\n",
      "     12      357.9467  6.4617\n",
      "     13      337.3622  6.4843\n",
      "     14      299.6107  6.4697\n",
      "     15      284.0436  6.5145\n",
      "     16      251.8419  6.4632\n",
      "     17      224.4077  6.4829\n",
      "     18      212.8278  6.4842\n",
      "     19      192.6474  6.4624\n",
      "     20      178.5133  6.5180\n",
      "     21      161.7355  6.5092\n",
      "     22      145.5868  6.5139\n",
      "     23      139.7203  6.4754\n",
      "     24      144.9889  6.4998\n",
      "     25      \u001b[36m112.5595\u001b[0m  6.4676\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m12075846.9657\u001b[0m  6.5332\n",
      "      2    \u001b[36m54961.8792\u001b[0m  6.4671\n",
      "      3   121046.8053  6.5097\n",
      "      4    78680.5600  6.4653\n",
      "      5    \u001b[36m40888.6045\u001b[0m  6.5153\n",
      "      6    \u001b[36m20706.5045\u001b[0m  6.4680\n",
      "      7    \u001b[36m11575.6710\u001b[0m  6.5048\n",
      "      8     \u001b[36m6900.6234\u001b[0m  6.4607\n",
      "      9     \u001b[36m4895.5415\u001b[0m  6.4747\n",
      "     10     \u001b[36m3438.9397\u001b[0m  6.5432\n",
      "     11     \u001b[36m2720.5758\u001b[0m  6.4682\n",
      "     12     \u001b[36m2156.6311\u001b[0m  6.5385\n",
      "     13     \u001b[36m1835.6631\u001b[0m  6.4991\n",
      "     14     \u001b[36m1581.7229\u001b[0m  6.5367\n",
      "     15     \u001b[36m1425.2377\u001b[0m  6.4889\n",
      "     16     \u001b[36m1221.1941\u001b[0m  6.4858\n",
      "     17     \u001b[36m1097.3567\u001b[0m  6.4999\n",
      "     18     \u001b[36m1026.1406\u001b[0m  6.4773\n",
      "     19      \u001b[36m917.5562\u001b[0m  6.5063\n",
      "     20      \u001b[36m857.8734\u001b[0m  6.4860\n",
      "     21      \u001b[36m784.1691\u001b[0m  6.5354\n",
      "     22      \u001b[36m698.6202\u001b[0m  6.4850\n",
      "     23      \u001b[36m656.3144\u001b[0m  6.4715\n",
      "     24      \u001b[36m629.5266\u001b[0m  6.6555\n",
      "     25      \u001b[36m583.4990\u001b[0m  6.4627\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m9095177.3011\u001b[0m  6.5255\n",
      "      2    \u001b[36m93894.2357\u001b[0m  6.4978\n",
      "      3     \u001b[36m1614.8159\u001b[0m  6.5048\n",
      "      4      \u001b[36m227.3302\u001b[0m  6.5366\n",
      "      5       \u001b[36m83.9389\u001b[0m  6.4735\n",
      "      6       \u001b[36m61.8557\u001b[0m  6.5710\n",
      "      7       \u001b[36m45.0688\u001b[0m  6.4975\n",
      "      8       \u001b[36m40.0412\u001b[0m  6.4768\n",
      "      9       40.6030  6.4852\n",
      "     10       \u001b[36m33.8756\u001b[0m  6.4649\n",
      "     11       37.9134  6.5027\n",
      "     12       34.0428  6.4393\n",
      "     13       38.1224  6.4953\n",
      "     14       \u001b[36m33.2423\u001b[0m  6.4367\n",
      "     15       33.7773  6.5413\n",
      "     16       34.0822  6.5022\n",
      "     17       35.5515  6.4691\n",
      "     18       33.5279  6.5031\n",
      "     19       34.3913  6.4619\n",
      "     20       \u001b[36m29.8117\u001b[0m  6.5104\n",
      "     21       32.5028  6.4786\n",
      "     22       \u001b[36m28.8945\u001b[0m  6.4874\n",
      "     23       35.3898  6.7548\n",
      "     24       35.4891  6.5008\n",
      "     25       30.8457  6.5096\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m20525915.3942\u001b[0m  6.5838\n",
      "      2  \u001b[36m1907590.5265\u001b[0m  6.4938\n",
      "      3    \u001b[36m88147.4241\u001b[0m  6.5139\n",
      "      4    \u001b[36m12210.9425\u001b[0m  6.4747\n",
      "      5     \u001b[36m4048.5379\u001b[0m  6.5118\n",
      "      6     \u001b[36m2105.6348\u001b[0m  6.4629\n",
      "      7     \u001b[36m1458.0856\u001b[0m  6.4877\n",
      "      8     \u001b[36m1094.3112\u001b[0m  6.5191\n",
      "      9      \u001b[36m907.0715\u001b[0m  6.4627\n",
      "     10      \u001b[36m809.3575\u001b[0m  6.5127\n",
      "     11      \u001b[36m715.3577\u001b[0m  6.4610\n",
      "     12      \u001b[36m662.4391\u001b[0m  6.5082\n",
      "     13      \u001b[36m648.7049\u001b[0m  6.4619\n",
      "     14      \u001b[36m615.8014\u001b[0m  6.4812\n",
      "     15      \u001b[36m559.9742\u001b[0m  6.4733\n",
      "     16      \u001b[36m544.1047\u001b[0m  6.5044\n",
      "     17      \u001b[36m522.0500\u001b[0m  6.5278\n",
      "     18      \u001b[36m506.1682\u001b[0m  6.4632\n",
      "     19      \u001b[36m453.2968\u001b[0m  6.5083\n",
      "     20      472.6005  6.4593\n",
      "     21      \u001b[36m452.1913\u001b[0m  6.4764\n",
      "     22      \u001b[36m405.3718\u001b[0m  6.4982\n",
      "     23      \u001b[36m404.0699\u001b[0m  6.4741\n",
      "     24      \u001b[36m391.9332\u001b[0m  6.5883\n",
      "     25      \u001b[36m356.0895\u001b[0m  6.4498\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m39568964.0274\u001b[0m  6.5433\n",
      "      2  \u001b[36m3000635.0228\u001b[0m  6.5107\n",
      "      3   \u001b[36m142915.0101\u001b[0m  6.4699\n",
      "      4    \u001b[36m15769.8234\u001b[0m  6.5055\n",
      "      5     \u001b[36m3849.2813\u001b[0m  6.4916\n",
      "      6     \u001b[36m1629.6584\u001b[0m  6.4720\n",
      "      7     \u001b[36m1013.7443\u001b[0m  6.5309\n",
      "      8      \u001b[36m668.4953\u001b[0m  6.5151\n",
      "      9      \u001b[36m520.8981\u001b[0m  6.5567\n",
      "     10      \u001b[36m451.1702\u001b[0m  6.4348\n",
      "     11      \u001b[36m388.1938\u001b[0m  6.4833\n",
      "     12      397.8921  6.4363\n",
      "     13      \u001b[36m344.8426\u001b[0m  6.4316\n",
      "     14      \u001b[36m341.8971\u001b[0m  6.4839\n",
      "     15      \u001b[36m314.4241\u001b[0m  6.4436\n",
      "     16      \u001b[36m311.8710\u001b[0m  6.5120\n",
      "     17      \u001b[36m288.3239\u001b[0m  6.4364\n",
      "     18      334.6322  6.4521\n",
      "     19      304.0553  6.4538\n",
      "     20      290.4104  6.4669\n",
      "     21      \u001b[36m274.4874\u001b[0m  6.4595\n",
      "     22      \u001b[36m266.3387\u001b[0m  6.4732\n",
      "     23      \u001b[36m240.9060\u001b[0m  6.4980\n",
      "     24      251.2772  6.4700\n",
      "     25      270.2769  6.5165\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m11835556.2993\u001b[0m  6.6509\n",
      "      2  \u001b[36m1587047.5328\u001b[0m  6.4816\n",
      "      3    \u001b[36m59301.9595\u001b[0m  6.4876\n",
      "      4     \u001b[36m3349.8394\u001b[0m  6.4721\n",
      "      5      \u001b[36m438.5301\u001b[0m  6.4305\n",
      "      6      \u001b[36m116.8716\u001b[0m  6.4623\n",
      "      7       \u001b[36m71.3284\u001b[0m  6.4739\n",
      "      8       \u001b[36m65.4106\u001b[0m  6.5258\n",
      "      9       \u001b[36m57.3584\u001b[0m  6.4937\n",
      "     10       \u001b[36m54.5874\u001b[0m  6.4980\n",
      "     11       63.3985  6.4895\n",
      "     12       65.6886  6.4296\n",
      "     13       67.2274  6.5090\n",
      "     14       63.6050  6.4683\n",
      "     15       58.4067  6.4652\n",
      "     16       58.4269  6.4261\n",
      "     17       59.4412  6.4621\n",
      "     18       57.9247  6.4537\n",
      "     19       59.2584  6.4759\n",
      "     20       56.7770  6.4884\n",
      "     21       59.0951  6.4372\n",
      "     22       60.2236  6.5231\n",
      "     23       54.9491  6.4733\n",
      "     24       \u001b[36m50.7435\u001b[0m  6.5451\n",
      "     25       53.8921  6.4795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-28 21:55:44,816]\u001b[0m Trial 0 finished with value: 186.9313362534814 and parameters: {'levels': 9, 'hidden_units': 29, 'kernel_size': 8, 'dropout': 0.11973169683940732, 'learning_rate': 0.0024348773534554596}. Best is trial 0 with value: 186.9313362534814.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch       train_loss     dur\n",
      "-------  ---------------  ------\n",
      "      1  \u001b[36m9533590051.5635\u001b[0m  1.1263\n",
      "      2       \u001b[36m88.5613\u001b[0m  1.0947\n",
      "      3        \u001b[36m2.7288\u001b[0m  1.0885\n",
      "      4        \u001b[36m1.9986\u001b[0m  1.0866\n",
      "      5        2.4183  1.0927\n",
      "      6        3.0343  1.0932\n",
      "      7        3.2452  1.0929\n",
      "      8        3.4945  1.1046\n",
      "      9        3.7282  1.1042\n",
      "     10        3.6954  1.0967\n",
      "     11        3.8321  1.0913\n",
      "     12        3.8428  1.1103\n",
      "     13        3.7185  1.1116\n",
      "     14        3.7934  1.0999\n",
      "     15        3.8267  1.0865\n",
      "     16        3.7251  1.0889\n",
      "     17        3.8333  1.0944\n",
      "     18        3.8042  1.0911\n",
      "     19        3.7117  1.0938\n",
      "     20        3.8037  1.0889\n",
      "     21        3.7806  1.0894\n",
      "     22        3.7344  1.1105\n",
      "     23        3.7866  1.1127\n",
      "     24        3.7896  1.1156\n",
      "     25        3.6955  1.1007\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m475427425.2777\u001b[0m  1.0892\n",
      "      2      \u001b[36m232.2152\u001b[0m  1.0874\n",
      "      3        \u001b[36m8.3325\u001b[0m  1.0879\n",
      "      4       14.2032  1.0889\n",
      "      5       15.7214  1.0876\n",
      "      6       16.1013  1.0892\n",
      "      7       16.2915  1.0910\n",
      "      8       16.4150  1.1013\n",
      "      9       16.4697  1.0830\n",
      "     10       16.4890  1.1035\n",
      "     11       16.5064  1.1070\n",
      "     12       16.4999  1.0789\n",
      "     13       16.5464  1.0795\n",
      "     14       16.5071  1.0827\n",
      "     15       16.5211  1.0783\n",
      "     16       16.5250  1.0861\n",
      "     17       16.5452  1.0826\n",
      "     18       16.5198  1.0795\n",
      "     19       16.5159  1.0811\n",
      "     20       16.5278  1.0842\n",
      "     21       16.5146  1.0930\n",
      "     22       16.5242  1.0937\n",
      "     23       16.5228  1.0892\n",
      "     24       16.5004  1.0966\n",
      "     25       16.4989  1.1355\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m699041036.8570\u001b[0m  1.0945\n",
      "      2       \u001b[36m84.2949\u001b[0m  1.0856\n",
      "      3       \u001b[36m12.4001\u001b[0m  1.0913\n",
      "      4        \u001b[36m7.3480\u001b[0m  1.1215\n",
      "      5        \u001b[36m6.5295\u001b[0m  1.0990\n",
      "      6        \u001b[36m6.4286\u001b[0m  1.0929\n",
      "      7        \u001b[36m6.3654\u001b[0m  1.1065\n",
      "      8        \u001b[36m6.3250\u001b[0m  1.0834\n",
      "      9        \u001b[36m6.2986\u001b[0m  1.1057\n",
      "     10        \u001b[36m6.2814\u001b[0m  1.0970\n",
      "     11        \u001b[36m6.2687\u001b[0m  1.1116\n",
      "     12        \u001b[36m6.2597\u001b[0m  1.0837\n",
      "     13        \u001b[36m6.2528\u001b[0m  1.1317\n",
      "     14        \u001b[36m6.2467\u001b[0m  1.0980\n",
      "     15        \u001b[36m6.2415\u001b[0m  1.1062\n",
      "     16        \u001b[36m6.2366\u001b[0m  1.1051\n",
      "     17        \u001b[36m6.2318\u001b[0m  1.1140\n",
      "     18        \u001b[36m6.2267\u001b[0m  1.1142\n",
      "     19        \u001b[36m6.2220\u001b[0m  1.0922\n",
      "     20        \u001b[36m6.2173\u001b[0m  1.0872\n",
      "     21        \u001b[36m6.2123\u001b[0m  1.0878\n",
      "     22        \u001b[36m6.2075\u001b[0m  1.0889\n",
      "     23        \u001b[36m6.2024\u001b[0m  1.0897\n",
      "     24        \u001b[36m6.1972\u001b[0m  1.0867\n",
      "     25        \u001b[36m6.1918\u001b[0m  1.0966\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch       train_loss     dur\n",
      "-------  ---------------  ------\n",
      "      1  \u001b[36m6597168400.7992\u001b[0m  1.0901\n",
      "      2      \u001b[36m953.1581\u001b[0m  1.1096\n",
      "      3      \u001b[36m113.6560\u001b[0m  1.0929\n",
      "      4      204.0676  1.0847\n",
      "      5      210.7830  1.0893\n",
      "      6      194.8665  1.0805\n",
      "      7      187.1496  1.0811\n",
      "      8      183.7500  1.0836\n",
      "      9      171.5546  1.0857\n",
      "     10      160.6895  1.0867\n",
      "     11      166.2823  1.0898\n",
      "     12      152.2595  1.0883\n",
      "     13      152.6028  1.1289\n",
      "     14      145.4740  1.1184\n",
      "     15      141.9025  1.1103\n",
      "     16      135.0362  1.0887\n",
      "     17      134.5790  1.0988\n",
      "     18      125.3791  1.0843\n",
      "     19      121.6605  1.0882\n",
      "     20      117.7605  1.0928\n",
      "     21      117.1207  1.1773\n",
      "     22      \u001b[36m108.3905\u001b[0m  1.0949\n",
      "     23      109.8002  1.0871\n",
      "     24      \u001b[36m101.5852\u001b[0m  1.0966\n",
      "     25       \u001b[36m96.4972\u001b[0m  1.0845\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch        train_loss     dur\n",
      "-------  ----------------  ------\n",
      "      1  \u001b[36m22245538782.5244\u001b[0m  1.1417\n",
      "      2      \u001b[36m671.9418\u001b[0m  1.0907\n",
      "      3     1809.1757  1.1041\n",
      "      4     3140.3877  1.0915\n",
      "      5     3558.0594  1.0996\n",
      "      6     3411.5948  1.0904\n",
      "      7     3367.9415  1.0983\n",
      "      8     3154.6706  1.0954\n",
      "      9     2986.5608  1.1064\n",
      "     10     2824.3599  1.0865\n",
      "     11     2741.9988  1.0943\n",
      "     12     2595.4627  1.0878\n",
      "     13     2478.3300  1.1072\n",
      "     14     2475.3954  1.0951\n",
      "     15     2456.3201  1.1090\n",
      "     16     2411.9119  1.0846\n",
      "     17     2355.9657  1.0842\n",
      "     18     2306.6213  1.0848\n",
      "     19     2358.5277  1.0999\n",
      "     20     2241.3893  1.0868\n",
      "     21     2256.2946  1.0818\n",
      "     22     2146.3959  1.0960\n",
      "     23     2203.2378  1.0925\n",
      "     24     2170.9079  1.0887\n",
      "     25     2078.8273  1.0887\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m102314725.8219\u001b[0m  1.1072\n",
      "      2      \u001b[36m748.0558\u001b[0m  1.0865\n",
      "      3        \u001b[36m9.6850\u001b[0m  1.1106\n",
      "      4        \u001b[36m8.2510\u001b[0m  1.0859\n",
      "      5        \u001b[36m8.0220\u001b[0m  1.0849\n",
      "      6        \u001b[36m7.8992\u001b[0m  1.0833\n",
      "      7        \u001b[36m7.7932\u001b[0m  1.1029\n",
      "      8        \u001b[36m7.7044\u001b[0m  1.0841\n",
      "      9        \u001b[36m7.6453\u001b[0m  1.0854\n",
      "     10        \u001b[36m7.5938\u001b[0m  1.0865\n",
      "     11        \u001b[36m7.5288\u001b[0m  1.0845\n",
      "     12        \u001b[36m7.4557\u001b[0m  1.0839\n",
      "     13        \u001b[36m7.4161\u001b[0m  1.0834\n",
      "     14        \u001b[36m7.3542\u001b[0m  1.0838\n",
      "     15        \u001b[36m7.2911\u001b[0m  1.1127\n",
      "     16        \u001b[36m7.2399\u001b[0m  1.1022\n",
      "     17        \u001b[36m7.1776\u001b[0m  1.0841\n",
      "     18        \u001b[36m7.1143\u001b[0m  1.0852\n",
      "     19        \u001b[36m7.0414\u001b[0m  1.0831\n",
      "     20        \u001b[36m6.9915\u001b[0m  1.0842\n",
      "     21        \u001b[36m6.9216\u001b[0m  1.0934\n",
      "     22        \u001b[36m6.8659\u001b[0m  1.0857\n",
      "     23        \u001b[36m6.7938\u001b[0m  1.1053\n",
      "     24        \u001b[36m6.7408\u001b[0m  1.0853\n",
      "     25        \u001b[36m6.6650\u001b[0m  1.0831\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1084497.8009\u001b[0m  1.0926\n",
      "      2       \u001b[36m24.4599\u001b[0m  1.0816\n",
      "      3       32.4461  1.1082\n",
      "      4       29.0615  1.1112\n",
      "      5       \u001b[36m21.1138\u001b[0m  1.0828\n",
      "      6       \u001b[36m18.6263\u001b[0m  1.0825\n",
      "      7       \u001b[36m16.5574\u001b[0m  1.0837\n",
      "      8       \u001b[36m16.1059\u001b[0m  1.0860\n",
      "      9       \u001b[36m15.9920\u001b[0m  1.0824\n",
      "     10       \u001b[36m15.6967\u001b[0m  1.0818\n",
      "     11       \u001b[36m15.4320\u001b[0m  1.0988\n",
      "     12       \u001b[36m15.3628\u001b[0m  1.0797\n",
      "     13       15.6051  1.0810\n",
      "     14       \u001b[36m15.1349\u001b[0m  1.0791\n",
      "     15       15.4987  1.1248\n",
      "     16       15.2615  1.0947\n",
      "     17       15.5634  1.1067\n",
      "     18       15.1504  1.1089\n",
      "     19       \u001b[36m14.9948\u001b[0m  1.0798\n",
      "     20       \u001b[36m14.6554\u001b[0m  1.0947\n",
      "     21       \u001b[36m14.4790\u001b[0m  1.0948\n",
      "     22       14.8455  1.0894\n",
      "     23       14.9998  1.0954\n",
      "     24       14.5220  1.0957\n",
      "     25       14.5396  1.0932\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch       train_loss     dur\n",
      "-------  ---------------  ------\n",
      "      1  \u001b[36m6328426673.9834\u001b[0m  1.1154\n",
      "      2      \u001b[36m104.9595\u001b[0m  1.0891\n",
      "      3       \u001b[36m28.6261\u001b[0m  1.0905\n",
      "      4       75.1726  1.1126\n",
      "      5       98.1221  1.1207\n",
      "      6       99.9083  1.0856\n",
      "      7       99.7024  1.0805\n",
      "      8       99.3306  1.0803\n",
      "      9       89.6400  1.0784\n",
      "     10       89.8882  1.0844\n",
      "     11       86.5461  1.0812\n",
      "     12       80.9889  1.0821\n",
      "     13       80.2737  1.0865\n",
      "     14       74.4079  1.0808\n",
      "     15       73.8590  1.1021\n",
      "     16       72.8393  1.0868\n",
      "     17       69.3453  1.0855\n",
      "     18       68.3377  1.1121\n",
      "     19       62.8594  1.0914\n",
      "     20       63.9065  1.0856\n",
      "     21       59.2797  1.0845\n",
      "     22       57.1607  1.0867\n",
      "     23       53.2466  1.0881\n",
      "     24       54.5336  1.0957\n",
      "     25       50.1884  1.0887\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch       train_loss     dur\n",
      "-------  ---------------  ------\n",
      "      1  \u001b[36m2059036824.0841\u001b[0m  1.0846\n",
      "      2      \u001b[36m427.9147\u001b[0m  1.0836\n",
      "      3       \u001b[36m18.7251\u001b[0m  1.0916\n",
      "      4       49.8018  1.1018\n",
      "      5       85.6219  1.0844\n",
      "      6      100.0035  1.1139\n",
      "      7      103.5209  1.1125\n",
      "      8      100.3652  1.0822\n",
      "      9      101.0778  1.0833\n",
      "     10       98.2798  1.0804\n",
      "     11       98.8041  1.0805\n",
      "     12       96.7360  1.0803\n",
      "     13       94.4226  1.0835\n",
      "     14       95.6723  1.0811\n",
      "     15       97.4431  1.0828\n",
      "     16       95.2209  1.0808\n",
      "     17       91.7126  1.0875\n",
      "     18       93.3170  1.0822\n",
      "     19       88.8544  1.1026\n",
      "     20       90.7352  1.0946\n",
      "     21       88.5132  1.1041\n",
      "     22       90.7966  1.0777\n",
      "     23       88.7488  1.0790\n",
      "     24       88.0793  1.0789\n",
      "     25       87.1653  1.0809\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1097817.8798\u001b[0m  1.0861\n",
      "      2       \u001b[36m15.2951\u001b[0m  1.0814\n",
      "      3       15.3886  1.0807\n",
      "      4       16.0702  1.0820\n",
      "      5       16.3247  1.0817\n",
      "      6       16.3802  1.0816\n",
      "      7       16.4015  1.1084\n",
      "      8       16.4019  1.1033\n",
      "      9       16.3891  1.1035\n",
      "     10       16.3675  1.0984\n",
      "     11       16.3398  1.0923\n",
      "     12       16.3079  1.0938\n",
      "     13       16.2728  1.0950\n",
      "     14       16.2352  1.0929\n",
      "     15       16.1955  1.0907\n",
      "     16       16.1541  1.0879\n",
      "     17       16.1111  1.0888\n",
      "     18       16.0666  1.0868\n",
      "     19       16.0209  1.1025\n",
      "     20       15.9739  1.1057\n",
      "     21       15.9258  1.1377\n",
      "     22       15.8765  1.1287\n",
      "     23       15.8261  1.0876\n",
      "     24       15.7747  1.0837\n",
      "     25       15.7223  1.0838\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch        train_loss     dur\n",
      "-------  ----------------  ------\n",
      "      1  \u001b[36m15725935099.6257\u001b[0m  1.1066\n",
      "      2      \u001b[36m714.0159\u001b[0m  1.0905\n",
      "      3       \u001b[36m12.5375\u001b[0m  1.0875\n",
      "      4       \u001b[36m10.0422\u001b[0m  1.0833\n",
      "      5       28.5863  1.0881\n",
      "      6       31.7610  1.0904\n",
      "      7       32.4409  1.0916\n",
      "      8       31.4696  1.1023\n",
      "      9       30.3696  1.0960\n",
      "     10       30.0128  1.1159\n",
      "     11       28.2182  1.0885\n",
      "     12       27.1117  1.0879\n",
      "     13       27.1308  1.0866\n",
      "     14       26.1194  1.0911\n",
      "     15       25.7122  1.1030\n",
      "     16       25.0088  1.0914\n",
      "     17       25.9041  1.0885\n",
      "     18       25.2771  1.0865\n",
      "     19       24.7087  1.0864\n",
      "     20       25.2820  1.0885\n",
      "     21       24.4755  1.1124\n",
      "     22       25.0547  1.1257\n",
      "     23       24.2347  1.1136\n",
      "     24       23.3031  1.0900\n",
      "     25       25.0876  1.0874\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch       train_loss     dur\n",
      "-------  ---------------  ------\n",
      "      1  \u001b[36m6775525044.2181\u001b[0m  1.1076\n",
      "      2      \u001b[36m609.2202\u001b[0m  1.0889\n",
      "      3        \u001b[36m7.6559\u001b[0m  1.0856\n",
      "      4        \u001b[36m4.6754\u001b[0m  1.1071\n",
      "      5        \u001b[36m1.8644\u001b[0m  1.0879\n",
      "      6        \u001b[36m0.8612\u001b[0m  1.0863\n",
      "      7        \u001b[36m0.7018\u001b[0m  1.0864\n",
      "      8        \u001b[36m0.5835\u001b[0m  1.0917\n",
      "      9        \u001b[36m0.5512\u001b[0m  1.0906\n",
      "     10        \u001b[36m0.5226\u001b[0m  1.1184\n",
      "     11        \u001b[36m0.5224\u001b[0m  1.1002\n",
      "     12        \u001b[36m0.5185\u001b[0m  1.0893\n",
      "     13        0.5400  1.1807\n",
      "     14        \u001b[36m0.4997\u001b[0m  1.0896\n",
      "     15        0.5334  1.0929\n",
      "     16        \u001b[36m0.4822\u001b[0m  1.0921\n",
      "     17        0.5200  1.0899\n",
      "     18        0.4829  1.0881\n",
      "     19        \u001b[36m0.4713\u001b[0m  1.0865\n",
      "     20        0.4939  1.0893\n",
      "     21        0.5238  1.0994\n",
      "     22        \u001b[36m0.4577\u001b[0m  1.0923\n",
      "     23        0.5121  1.1181\n",
      "     24        0.4732  1.0838\n",
      "     25        0.5147  1.1128\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m120339110.5741\u001b[0m  1.1049\n",
      "      2       \u001b[36m30.0631\u001b[0m  1.0840\n",
      "      3        \u001b[36m9.9444\u001b[0m  1.0834\n",
      "      4        \u001b[36m8.1931\u001b[0m  1.0973\n",
      "      5        8.5737  1.0856\n",
      "      6        8.3845  1.1113\n",
      "      7        8.2622  1.1122\n",
      "      8        \u001b[36m8.1818\u001b[0m  1.1038\n",
      "      9        \u001b[36m8.1259\u001b[0m  1.0864\n",
      "     10        \u001b[36m8.0852\u001b[0m  1.0924\n",
      "     11        \u001b[36m8.0537\u001b[0m  1.1382\n",
      "     12        \u001b[36m8.0271\u001b[0m  1.1367\n",
      "     13        \u001b[36m8.0036\u001b[0m  1.1197\n",
      "     14        \u001b[36m7.9818\u001b[0m  1.1086\n",
      "     15        \u001b[36m7.9608\u001b[0m  1.1157\n",
      "     16        \u001b[36m7.9395\u001b[0m  1.1103\n",
      "     17        \u001b[36m7.9190\u001b[0m  1.0990\n",
      "     18        \u001b[36m7.8973\u001b[0m  1.0860\n",
      "     19        \u001b[36m7.8760\u001b[0m  1.0945\n",
      "     20        \u001b[36m7.8537\u001b[0m  1.0850\n",
      "     21        \u001b[36m7.8314\u001b[0m  1.0854\n",
      "     22        \u001b[36m7.8084\u001b[0m  1.0917\n",
      "     23        \u001b[36m7.7852\u001b[0m  1.1013\n",
      "     24        \u001b[36m7.7608\u001b[0m  1.1053\n",
      "     25        \u001b[36m7.7370\u001b[0m  1.0908\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m21063897.2409\u001b[0m  1.0891\n",
      "      2      \u001b[36m129.6719\u001b[0m  1.1031\n",
      "      3       \u001b[36m13.3354\u001b[0m  1.0894\n",
      "      4       \u001b[36m12.5550\u001b[0m  1.0896\n",
      "      5       \u001b[36m12.2279\u001b[0m  1.0891\n",
      "      6       \u001b[36m12.0184\u001b[0m  1.0908\n",
      "      7       \u001b[36m11.8970\u001b[0m  1.0843\n",
      "      8       \u001b[36m11.7799\u001b[0m  1.0844\n",
      "      9       \u001b[36m11.7149\u001b[0m  1.0824\n",
      "     10       \u001b[36m11.6753\u001b[0m  1.0840\n",
      "     11       \u001b[36m11.6438\u001b[0m  1.0856\n",
      "     12       \u001b[36m11.6163\u001b[0m  1.1124\n",
      "     13       \u001b[36m11.6044\u001b[0m  1.1163\n",
      "     14       \u001b[36m11.5853\u001b[0m  1.0890\n",
      "     15       \u001b[36m11.5717\u001b[0m  1.0888\n",
      "     16       \u001b[36m11.5667\u001b[0m  1.0864\n",
      "     17       \u001b[36m11.5524\u001b[0m  1.0859\n",
      "     18       \u001b[36m11.5423\u001b[0m  1.1032\n",
      "     19       \u001b[36m11.5392\u001b[0m  1.0844\n",
      "     20       \u001b[36m11.5301\u001b[0m  1.0851\n",
      "     21       \u001b[36m11.5122\u001b[0m  1.0867\n",
      "     22       \u001b[36m11.5055\u001b[0m  1.0865\n",
      "     23       \u001b[36m11.4957\u001b[0m  1.0882\n",
      "     24       \u001b[36m11.4848\u001b[0m  1.0856\n",
      "     25       \u001b[36m11.4763\u001b[0m  1.0906\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m471906337.2474\u001b[0m  1.1207\n",
      "      2        \u001b[36m7.2563\u001b[0m  1.0912\n",
      "      3       10.8876  1.0894\n",
      "      4       11.1616  1.0886\n",
      "      5       11.3055  1.0914\n",
      "      6       11.3799  1.0875\n",
      "      7       11.4167  1.0856\n",
      "      8       11.4463  1.1014\n",
      "      9       11.4658  1.0845\n",
      "     10       11.4775  1.0826\n",
      "     11       11.4842  1.0876\n",
      "     12       11.4879  1.0844\n",
      "     13       11.4897  1.1006\n",
      "     14       11.4902  1.0858\n",
      "     15       11.4900  1.0919\n",
      "     16       11.4893  1.0834\n",
      "     17       11.4884  1.0829\n",
      "     18       11.4872  1.0869\n",
      "     19       11.4858  1.0812\n",
      "     20       11.4844  1.0825\n",
      "     21       11.4828  1.0882\n",
      "     22       11.4812  1.0815\n",
      "     23       11.4796  1.0829\n",
      "     24       11.4779  1.0843\n",
      "     25       11.4762  1.0847\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m36356904.2212\u001b[0m  1.1240\n",
      "      2        \u001b[36m6.8751\u001b[0m  1.1040\n",
      "      3       15.2275  1.0894\n",
      "      4       16.2938  1.0862\n",
      "      5       16.5171  1.0840\n",
      "      6       16.5871  1.0854\n",
      "      7       16.5902  1.0855\n",
      "      8       16.5956  1.0871\n",
      "      9       16.5826  1.0868\n",
      "     10       16.5785  1.0845\n",
      "     11       16.5682  1.0851\n",
      "     12       16.5534  1.0842\n",
      "     13       16.5467  1.0861\n",
      "     14       16.5427  1.0877\n",
      "     15       16.5305  1.1095\n",
      "     16       16.5338  1.1361\n",
      "     17       16.5228  1.0840\n",
      "     18       16.5176  1.0858\n",
      "     19       16.5091  1.0894\n",
      "     20       16.4992  1.0832\n",
      "     21       16.4966  1.0833\n",
      "     22       16.4831  1.0881\n",
      "     23       16.4783  1.0815\n",
      "     24       16.4607  1.0850\n",
      "     25       16.4589  1.0835\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m231884459.3482\u001b[0m  1.1038\n",
      "      2      \u001b[36m131.1464\u001b[0m  1.0831\n",
      "      3       \u001b[36m22.7069\u001b[0m  1.0848\n",
      "      4       \u001b[36m18.2925\u001b[0m  1.1122\n",
      "      5       \u001b[36m17.4954\u001b[0m  1.0820\n",
      "      6       \u001b[36m17.4252\u001b[0m  1.0868\n",
      "      7       17.5363  1.0992\n",
      "      8       17.6459  1.0819\n",
      "      9       17.7508  1.0851\n",
      "     10       17.8320  1.0812\n",
      "     11       17.8945  1.0819\n",
      "     12       17.9277  1.0907\n",
      "     13       17.8920  1.0871\n",
      "     14       17.8559  1.0846\n",
      "     15       17.8772  1.0803\n",
      "     16       17.8266  1.1082\n",
      "     17       17.7636  1.0805\n",
      "     18       17.7397  1.1088\n",
      "     19       17.7022  1.0791\n",
      "     20       17.6104  1.0851\n",
      "     21       17.5839  1.0855\n",
      "     22       17.5544  1.0961\n",
      "     23       17.4401  1.0936\n",
      "     24       \u001b[36m17.3993\u001b[0m  1.0926\n",
      "     25       \u001b[36m17.3568\u001b[0m  1.1071\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m4265640.2936\u001b[0m  1.0962\n",
      "      2     \u001b[36m1247.6753\u001b[0m  1.0931\n",
      "      3      \u001b[36m671.2975\u001b[0m  1.0972\n",
      "      4      \u001b[36m343.8025\u001b[0m  1.1254\n",
      "      5      \u001b[36m230.6095\u001b[0m  1.1191\n",
      "      6      \u001b[36m174.5343\u001b[0m  1.1161\n",
      "      7      \u001b[36m116.9177\u001b[0m  1.1179\n",
      "      8      \u001b[36m110.3286\u001b[0m  1.1141\n",
      "      9      113.6202  1.1103\n",
      "     10       \u001b[36m89.4987\u001b[0m  1.0936\n",
      "     11       \u001b[36m81.4956\u001b[0m  1.0886\n",
      "     12       \u001b[36m57.9807\u001b[0m  1.0945\n",
      "     13       70.1694  1.0864\n",
      "     14       58.9021  1.0883\n",
      "     15       \u001b[36m53.3799\u001b[0m  1.0884\n",
      "     16       54.2783  1.1042\n",
      "     17       59.1218  1.1143\n",
      "     18       \u001b[36m46.0303\u001b[0m  1.1182\n",
      "     19       53.6675  1.1184\n",
      "     20       47.1068  1.0904\n",
      "     21       51.6535  1.0917\n",
      "     22       53.8896  1.0880\n",
      "     23       \u001b[36m40.1364\u001b[0m  1.0951\n",
      "     24       \u001b[36m35.2860\u001b[0m  1.0982\n",
      "     25       39.1002  1.0954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-28 22:04:22,783]\u001b[0m Trial 1 finished with value: 183.7933072985064 and parameters: {'levels': 6, 'hidden_units': 6, 'kernel_size': 9, 'dropout': 0.12022300234864176, 'learning_rate': 0.05675206026988748}. Best is trial 1 with value: 183.7933072985064.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m7559518.6137\u001b[0m  2.6383\n",
      "      2   \u001b[36m330051.0477\u001b[0m  2.6158\n",
      "      3   \u001b[36m102382.7048\u001b[0m  2.6231\n",
      "      4     \u001b[36m8247.7897\u001b[0m  2.6164\n",
      "      5      \u001b[36m642.4258\u001b[0m  2.5987\n",
      "      6       \u001b[36m55.1664\u001b[0m  2.5964\n",
      "      7       \u001b[36m34.3402\u001b[0m  2.6011\n",
      "      8       45.3532  2.6353\n",
      "      9       60.1788  2.6369\n",
      "     10       60.8910  2.6137\n",
      "     11       68.9910  2.6132\n",
      "     12       67.2066  2.6154\n",
      "     13       65.0104  2.6057\n",
      "     14       62.9307  2.6998\n",
      "     15       59.4475  2.6162\n",
      "     16       57.6542  2.6995\n",
      "     17       56.5804  2.6130\n",
      "     18       52.5259  2.6094\n",
      "     19       48.6731  2.6206\n",
      "     20       45.3568  2.6602\n",
      "     21       45.5117  2.6192\n",
      "     22       43.8541  2.6038\n",
      "     23       38.9923  2.6260\n",
      "     24       38.3746  2.6135\n",
      "     25       34.6467  2.6432\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m3327935.2802\u001b[0m  2.6612\n",
      "      2    \u001b[36m91982.2445\u001b[0m  2.6769\n",
      "      3    \u001b[36m62040.3117\u001b[0m  2.6084\n",
      "      4    \u001b[36m11792.8134\u001b[0m  2.6397\n",
      "      5     \u001b[36m2000.0050\u001b[0m  2.6409\n",
      "      6      \u001b[36m470.4461\u001b[0m  2.6122\n",
      "      7      \u001b[36m169.1494\u001b[0m  2.5995\n",
      "      8       \u001b[36m72.9576\u001b[0m  2.5996\n",
      "      9       \u001b[36m39.2253\u001b[0m  2.6002\n",
      "     10       \u001b[36m28.3175\u001b[0m  2.6151\n",
      "     11       \u001b[36m18.4490\u001b[0m  2.6076\n",
      "     12       \u001b[36m17.0282\u001b[0m  2.5984\n",
      "     13       \u001b[36m14.1051\u001b[0m  2.5994\n",
      "     14       \u001b[36m13.1494\u001b[0m  2.6108\n",
      "     15       13.6500  2.5974\n",
      "     16       14.6596  2.6374\n",
      "     17       \u001b[36m12.6187\u001b[0m  2.5976\n",
      "     18       13.4202  2.6703\n",
      "     19       13.5016  2.6919\n",
      "     20       14.8055  2.6005\n",
      "     21       13.9469  2.5989\n",
      "     22       13.0952  2.6504\n",
      "     23       \u001b[36m11.8652\u001b[0m  2.6190\n",
      "     24       12.6678  2.6708\n",
      "     25       \u001b[36m11.8080\u001b[0m  2.6061\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m8818745.4608\u001b[0m  2.7100\n",
      "      2   \u001b[36m941479.9161\u001b[0m  2.6122\n",
      "      3    \u001b[36m54018.1305\u001b[0m  2.6086\n",
      "      4     \u001b[36m2015.2088\u001b[0m  2.6108\n",
      "      5      \u001b[36m140.8037\u001b[0m  2.6301\n",
      "      6       \u001b[36m20.4571\u001b[0m  2.6494\n",
      "      7       20.7399  2.6829\n",
      "      8       28.4878  2.6128\n",
      "      9       33.8568  2.6011\n",
      "     10       35.6767  2.6337\n",
      "     11       34.7381  2.6161\n",
      "     12       34.0244  2.6501\n",
      "     13       33.0189  2.6350\n",
      "     14       31.8256  2.6151\n",
      "     15       29.6761  2.6182\n",
      "     16       28.3551  2.6160\n",
      "     17       26.2197  2.6259\n",
      "     18       25.1511  2.6295\n",
      "     19       23.8126  2.6377\n",
      "     20       20.9784  2.6086\n",
      "     21       \u001b[36m19.2948\u001b[0m  2.6094\n",
      "     22       \u001b[36m16.9730\u001b[0m  2.6247\n",
      "     23       17.7575  2.6214\n",
      "     24       17.7379  2.6657\n",
      "     25       \u001b[36m15.2921\u001b[0m  2.6208\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m16823729.9818\u001b[0m  2.6480\n",
      "      2  \u001b[36m1756975.0468\u001b[0m  2.6074\n",
      "      3    \u001b[36m55459.4291\u001b[0m  2.6746\n",
      "      4     \u001b[36m1226.5247\u001b[0m  2.6104\n",
      "      5       \u001b[36m47.2889\u001b[0m  2.6013\n",
      "      6       \u001b[36m23.6514\u001b[0m  2.5984\n",
      "      7       35.8791  2.6029\n",
      "      8       49.3083  2.6127\n",
      "      9       53.6854  2.6402\n",
      "     10       53.8233  2.6025\n",
      "     11       53.3734  2.6148\n",
      "     12       50.5755  2.6085\n",
      "     13       50.4159  2.6040\n",
      "     14       45.7344  2.6364\n",
      "     15       41.0430  2.6285\n",
      "     16       40.4782  2.6020\n",
      "     17       37.3836  2.6190\n",
      "     18       37.3479  2.5973\n",
      "     19       35.7274  2.6016\n",
      "     20       32.5207  2.6309\n",
      "     21       31.3849  2.6242\n",
      "     22       27.8546  2.6009\n",
      "     23       26.7768  2.6041\n",
      "     24       24.3472  2.6322\n",
      "     25       23.7708  2.5997\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m9852728.7318\u001b[0m  2.6726\n",
      "      2   \u001b[36m942736.9581\u001b[0m  2.6544\n",
      "      3     \u001b[36m7990.7239\u001b[0m  2.6092\n",
      "      4     9001.0229  2.6062\n",
      "      5     \u001b[36m1938.2246\u001b[0m  2.6432\n",
      "      6      \u001b[36m121.1537\u001b[0m  2.6412\n",
      "      7      477.0666  2.6143\n",
      "      8      529.4947  2.6186\n",
      "      9      272.7506  2.6129\n",
      "     10      \u001b[36m100.4421\u001b[0m  2.6065\n",
      "     11       \u001b[36m33.9718\u001b[0m  2.6661\n",
      "     12       \u001b[36m27.2145\u001b[0m  2.6231\n",
      "     13       31.6562  2.6024\n",
      "     14       38.0774  2.5920\n",
      "     15       32.4381  2.6018\n",
      "     16       29.1714  2.5977\n",
      "     17       \u001b[36m25.6338\u001b[0m  2.6504\n",
      "     18       \u001b[36m24.2664\u001b[0m  2.5973\n",
      "     19       24.7507  2.5986\n",
      "     20       \u001b[36m22.7800\u001b[0m  2.6012\n",
      "     21       22.9909  2.5972\n",
      "     22       \u001b[36m21.6941\u001b[0m  2.6306\n",
      "     23       24.5452  2.6222\n",
      "     24       22.5796  2.5997\n",
      "     25       23.1600  2.5990\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m18790628.8394\u001b[0m  2.6534\n",
      "      2  \u001b[36m2135385.8490\u001b[0m  2.6898\n",
      "      3    \u001b[36m80062.8772\u001b[0m  2.6122\n",
      "      4     \u001b[36m3515.8404\u001b[0m  2.6265\n",
      "      5      \u001b[36m268.2257\u001b[0m  2.6122\n",
      "      6       \u001b[36m47.0530\u001b[0m  2.6072\n",
      "      7       \u001b[36m16.1055\u001b[0m  2.6414\n",
      "      8       \u001b[36m13.4583\u001b[0m  2.6362\n",
      "      9       13.6391  2.5959\n",
      "     10       15.2542  2.6057\n",
      "     11       15.2590  2.6069\n",
      "     12       15.0850  2.6062\n",
      "     13       15.3629  2.6176\n",
      "     14       15.2514  2.6135\n",
      "     15       14.8133  2.6134\n",
      "     16       13.4930  2.6089\n",
      "     17       \u001b[36m12.8896\u001b[0m  2.6066\n",
      "     18       \u001b[36m12.7427\u001b[0m  2.6056\n",
      "     19       \u001b[36m12.6977\u001b[0m  2.7100\n",
      "     20       \u001b[36m12.5766\u001b[0m  2.6209\n",
      "     21       \u001b[36m11.3649\u001b[0m  2.6298\n",
      "     22       12.4182  2.6034\n",
      "     23       11.3764  2.6142\n",
      "     24       \u001b[36m11.2812\u001b[0m  2.6374\n",
      "     25       13.3678  2.6419\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m4658950.7075\u001b[0m  2.6578\n",
      "      2   \u001b[36m689768.6257\u001b[0m  2.6133\n",
      "      3    \u001b[36m93584.3385\u001b[0m  2.6127\n",
      "      4     \u001b[36m4093.9833\u001b[0m  2.6640\n",
      "      5      \u001b[36m115.4245\u001b[0m  2.6136\n",
      "      6      159.7458  2.6146\n",
      "      7      248.5405  2.6100\n",
      "      8      266.1935  2.6118\n",
      "      9      261.9696  2.6425\n",
      "     10      232.5673  2.6340\n",
      "     11      210.0315  2.6203\n",
      "     12      193.3701  2.6181\n",
      "     13      170.7778  2.6354\n",
      "     14      136.3046  2.6145\n",
      "     15      119.2215  2.6388\n",
      "     16      \u001b[36m104.6082\u001b[0m  2.6412\n",
      "     17       \u001b[36m85.6286\u001b[0m  2.6082\n",
      "     18       \u001b[36m73.5194\u001b[0m  2.6080\n",
      "     19       \u001b[36m64.8161\u001b[0m  2.6052\n",
      "     20       \u001b[36m53.8730\u001b[0m  2.6048\n",
      "     21       \u001b[36m41.5583\u001b[0m  2.6610\n",
      "     22       \u001b[36m37.2787\u001b[0m  2.6076\n",
      "     23       \u001b[36m32.2472\u001b[0m  2.6038\n",
      "     24       \u001b[36m27.2497\u001b[0m  2.6059\n",
      "     25       \u001b[36m24.8478\u001b[0m  2.6152\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m13040414.2728\u001b[0m  2.6803\n",
      "      2   \u001b[36m964028.5147\u001b[0m  2.6104\n",
      "      3     \u001b[36m9901.5205\u001b[0m  2.6104\n",
      "      4      \u001b[36m505.6831\u001b[0m  2.6144\n",
      "      5       \u001b[36m76.1890\u001b[0m  2.6300\n",
      "      6       \u001b[36m20.6581\u001b[0m  2.6368\n",
      "      7       \u001b[36m13.5002\u001b[0m  2.5976\n",
      "      8       13.6304  2.5965\n",
      "      9       \u001b[36m13.2206\u001b[0m  2.5941\n",
      "     10       \u001b[36m13.0441\u001b[0m  2.5955\n",
      "     11       14.4765  2.6310\n",
      "     12       13.5763  2.6740\n",
      "     13       15.0577  2.6149\n",
      "     14       14.1280  2.6046\n",
      "     15       13.2421  2.6011\n",
      "     16       15.4639  2.5999\n",
      "     17       \u001b[36m12.4411\u001b[0m  2.6098\n",
      "     18       13.1540  2.6071\n",
      "     19       14.6358  2.5966\n",
      "     20       13.0514  2.5961\n",
      "     21       13.8904  2.6279\n",
      "     22       13.3842  2.6096\n",
      "     23       12.9394  2.6462\n",
      "     24       13.2426  2.6993\n",
      "     25       13.9039  2.6098\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m5132007.1862\u001b[0m  2.6491\n",
      "      2   \u001b[36m125132.8731\u001b[0m  2.6431\n",
      "      3    \u001b[36m89277.5500\u001b[0m  2.6434\n",
      "      4    \u001b[36m13115.2317\u001b[0m  2.6144\n",
      "      5     \u001b[36m2343.9912\u001b[0m  2.6022\n",
      "      6      \u001b[36m696.9646\u001b[0m  2.5933\n",
      "      7      \u001b[36m292.2382\u001b[0m  2.5941\n",
      "      8      \u001b[36m166.9877\u001b[0m  2.6411\n",
      "      9      \u001b[36m112.1705\u001b[0m  2.5931\n",
      "     10       \u001b[36m83.4583\u001b[0m  2.5947\n",
      "     11       \u001b[36m69.8350\u001b[0m  2.6063\n",
      "     12       \u001b[36m60.2743\u001b[0m  2.6830\n",
      "     13       \u001b[36m49.5344\u001b[0m  2.6443\n",
      "     14       \u001b[36m42.7144\u001b[0m  2.6791\n",
      "     15       \u001b[36m40.2202\u001b[0m  2.6000\n",
      "     16       \u001b[36m35.7943\u001b[0m  2.6117\n",
      "     17       \u001b[36m32.9173\u001b[0m  2.6038\n",
      "     18       \u001b[36m28.5296\u001b[0m  2.6110\n",
      "     19       \u001b[36m26.6433\u001b[0m  2.6537\n",
      "     20       \u001b[36m24.4546\u001b[0m  2.6785\n",
      "     21       \u001b[36m22.0388\u001b[0m  2.6077\n",
      "     22       \u001b[36m21.4182\u001b[0m  2.6941\n",
      "     23       \u001b[36m17.8892\u001b[0m  2.6121\n",
      "     24       \u001b[36m16.7741\u001b[0m  2.6004\n",
      "     25       \u001b[36m15.6259\u001b[0m  2.6301\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m10411136.3168\u001b[0m  2.6405\n",
      "      2  \u001b[36m1115958.1435\u001b[0m  2.6265\n",
      "      3    \u001b[36m72591.6825\u001b[0m  2.6103\n",
      "      4     \u001b[36m5473.7476\u001b[0m  2.6394\n",
      "      5     \u001b[36m1127.2994\u001b[0m  2.6569\n",
      "      6      \u001b[36m330.5216\u001b[0m  2.6296\n",
      "      7      \u001b[36m119.8942\u001b[0m  2.6089\n",
      "      8       \u001b[36m60.1064\u001b[0m  2.6151\n",
      "      9       \u001b[36m37.4135\u001b[0m  2.6113\n",
      "     10       \u001b[36m24.1797\u001b[0m  2.6612\n",
      "     11       \u001b[36m22.9441\u001b[0m  2.6096\n",
      "     12       \u001b[36m18.0633\u001b[0m  2.6317\n",
      "     13       \u001b[36m15.6102\u001b[0m  2.6178\n",
      "     14       15.6403  2.6175\n",
      "     15       \u001b[36m15.1286\u001b[0m  2.6266\n",
      "     16       \u001b[36m14.6019\u001b[0m  2.6628\n",
      "     17       \u001b[36m14.1694\u001b[0m  2.6024\n",
      "     18       \u001b[36m12.9872\u001b[0m  2.6034\n",
      "     19       \u001b[36m12.6642\u001b[0m  2.6039\n",
      "     20       13.6637  2.6179\n",
      "     21       12.6994  2.6340\n",
      "     22       \u001b[36m11.8998\u001b[0m  2.6338\n",
      "     23       \u001b[36m11.6425\u001b[0m  2.6069\n",
      "     24       \u001b[36m11.1611\u001b[0m  2.6121\n",
      "     25       11.9316  2.6067\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m7125569.2300\u001b[0m  2.6895\n",
      "      2   \u001b[36m284425.8198\u001b[0m  2.5998\n",
      "      3   \u001b[36m120945.1121\u001b[0m  2.6003\n",
      "      4    \u001b[36m11912.3374\u001b[0m  2.5910\n",
      "      5     \u001b[36m1115.2571\u001b[0m  2.6056\n",
      "      6      \u001b[36m232.3013\u001b[0m  2.6126\n",
      "      7       \u001b[36m63.0479\u001b[0m  2.6187\n",
      "      8       \u001b[36m28.2639\u001b[0m  2.5915\n",
      "      9       \u001b[36m17.4050\u001b[0m  2.5976\n",
      "     10       \u001b[36m13.8333\u001b[0m  2.6073\n",
      "     11       \u001b[36m12.2504\u001b[0m  2.6270\n",
      "     12       12.9957  2.6266\n",
      "     13       \u001b[36m11.0182\u001b[0m  2.6257\n",
      "     14       12.0803  2.6394\n",
      "     15       12.1844  2.6093\n",
      "     16       11.3052  2.6129\n",
      "     17       13.6274  2.6174\n",
      "     18       12.4197  2.6961\n",
      "     19       \u001b[36m10.8262\u001b[0m  2.6148\n",
      "     20       11.5769  2.6121\n",
      "     21       11.9429  2.6166\n",
      "     22       12.5900  2.6072\n",
      "     23       12.6194  2.6084\n",
      "     24       11.9850  2.7022\n",
      "     25       11.6717  2.6190\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m8193721.8910\u001b[0m  2.6540\n",
      "      2   \u001b[36m807172.2949\u001b[0m  2.5906\n",
      "      3    \u001b[36m38628.7087\u001b[0m  2.6441\n",
      "      4      \u001b[36m294.1853\u001b[0m  2.5930\n",
      "      5       \u001b[36m70.2670\u001b[0m  2.6205\n",
      "      6      135.1295  2.7021\n",
      "      7      148.5004  2.6906\n",
      "      8      152.4019  2.6233\n",
      "      9      153.2419  2.6148\n",
      "     10      148.8869  2.6069\n",
      "     11      141.5394  2.6057\n",
      "     12      138.7180  2.5980\n",
      "     13      131.6735  2.6071\n",
      "     14      129.8416  2.6272\n",
      "     15      120.8454  2.6544\n",
      "     16      115.1102  2.6163\n",
      "     17      109.0622  2.6237\n",
      "     18      103.5141  2.6605\n",
      "     19       98.1139  2.6115\n",
      "     20       91.2849  2.6808\n",
      "     21       87.1540  2.6123\n",
      "     22       81.2762  2.6237\n",
      "     23       75.4353  2.6112\n",
      "     24       72.9008  2.6106\n",
      "     25       \u001b[36m69.8353\u001b[0m  2.6084\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m28316310.0693\u001b[0m  2.6400\n",
      "      2  \u001b[36m2490640.5801\u001b[0m  2.6158\n",
      "      3    \u001b[36m70469.2024\u001b[0m  2.6051\n",
      "      4     \u001b[36m2523.3349\u001b[0m  2.6077\n",
      "      5      \u001b[36m287.9955\u001b[0m  2.6643\n",
      "      6       \u001b[36m70.4233\u001b[0m  2.6099\n",
      "      7      109.9770  2.6124\n",
      "      8      132.3353  2.6086\n",
      "      9      145.0220  2.6120\n",
      "     10      140.9139  2.6154\n",
      "     11      136.7339  2.6597\n",
      "     12      130.8048  2.6114\n",
      "     13      119.1370  2.6111\n",
      "     14      106.7492  2.6317\n",
      "     15       96.6201  2.7006\n",
      "     16       84.8526  2.6396\n",
      "     17       76.3687  2.6891\n",
      "     18       73.6141  2.6239\n",
      "     19       \u001b[36m61.9359\u001b[0m  2.6160\n",
      "     20       \u001b[36m61.1629\u001b[0m  2.6172\n",
      "     21       \u001b[36m58.3959\u001b[0m  2.6131\n",
      "     22       \u001b[36m57.6531\u001b[0m  2.6571\n",
      "     23       \u001b[36m55.4316\u001b[0m  2.6013\n",
      "     24       55.7541  2.5999\n",
      "     25       \u001b[36m47.4686\u001b[0m  2.6130\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m13315812.0066\u001b[0m  2.6558\n",
      "      2  \u001b[36m1566914.8011\u001b[0m  2.6239\n",
      "      3   \u001b[36m162960.6448\u001b[0m  2.6056\n",
      "      4     \u001b[36m4920.0273\u001b[0m  2.6092\n",
      "      5      \u001b[36m423.1985\u001b[0m  2.6024\n",
      "      6     1222.9522  2.6089\n",
      "      7     1492.1859  2.6517\n",
      "      8     1348.8341  2.6120\n",
      "      9     1148.1194  2.5986\n",
      "     10      884.2968  2.6284\n",
      "     11      687.4160  2.5970\n",
      "     12      527.9313  2.5972\n",
      "     13      \u001b[36m398.9372\u001b[0m  2.6471\n",
      "     14      \u001b[36m305.8220\u001b[0m  2.5988\n",
      "     15      \u001b[36m223.8856\u001b[0m  2.5954\n",
      "     16      \u001b[36m176.4191\u001b[0m  2.5957\n",
      "     17      \u001b[36m140.2830\u001b[0m  2.6024\n",
      "     18       \u001b[36m93.6155\u001b[0m  2.6348\n",
      "     19       \u001b[36m43.5506\u001b[0m  2.6469\n",
      "     20       58.7123  2.6071\n",
      "     21       46.2958  2.6072\n",
      "     22       \u001b[36m42.1060\u001b[0m  2.6124\n",
      "     23       43.9306  2.6105\n",
      "     24       47.8635  2.6260\n",
      "     25       46.9434  2.6285\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m11865772.0995\u001b[0m  2.6306\n",
      "      2   \u001b[36m985092.2578\u001b[0m  2.6197\n",
      "      3    \u001b[36m26236.3738\u001b[0m  2.6230\n",
      "      4      \u001b[36m644.7242\u001b[0m  2.6613\n",
      "      5       \u001b[36m22.4006\u001b[0m  2.6916\n",
      "      6       42.6130  2.6707\n",
      "      7       74.4955  2.6692\n",
      "      8       89.6605  2.6032\n",
      "      9       95.4881  2.6308\n",
      "     10       97.1861  2.6713\n",
      "     11       94.5143  2.6116\n",
      "     12       90.6043  2.5928\n",
      "     13       82.3695  2.6000\n",
      "     14       76.4409  2.5978\n",
      "     15       67.3746  2.6504\n",
      "     16       61.8358  2.5954\n",
      "     17       55.8423  2.6207\n",
      "     18       48.4115  2.6218\n",
      "     19       43.3346  2.6048\n",
      "     20       39.0052  2.6241\n",
      "     21       32.4196  2.6316\n",
      "     22       28.2159  2.6048\n",
      "     23       24.1275  2.6070\n",
      "     24       \u001b[36m20.8109\u001b[0m  2.6117\n",
      "     25       \u001b[36m17.4256\u001b[0m  2.6333\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m9345160.1647\u001b[0m  2.6379\n",
      "      2   \u001b[36m226861.0185\u001b[0m  2.6054\n",
      "      3   \u001b[36m197903.8492\u001b[0m  2.6086\n",
      "      4    \u001b[36m29157.0662\u001b[0m  2.6042\n",
      "      5     \u001b[36m4189.5728\u001b[0m  2.7079\n",
      "      6     \u001b[36m1041.5224\u001b[0m  2.6228\n",
      "      7      \u001b[36m282.9418\u001b[0m  2.5952\n",
      "      8      \u001b[36m110.5216\u001b[0m  2.5957\n",
      "      9      \u001b[36m101.9164\u001b[0m  2.5960\n",
      "     10      110.0576  2.5960\n",
      "     11      129.7946  2.6220\n",
      "     12      131.6931  2.6212\n",
      "     13      128.2674  2.5973\n",
      "     14      121.5994  2.5968\n",
      "     15      126.0847  2.5990\n",
      "     16      113.1475  2.6023\n",
      "     17      103.1581  2.6516\n",
      "     18      \u001b[36m100.0741\u001b[0m  2.6117\n",
      "     19       \u001b[36m89.2983\u001b[0m  2.6284\n",
      "     20       \u001b[36m81.9531\u001b[0m  2.6114\n",
      "     21       \u001b[36m76.6607\u001b[0m  2.6133\n",
      "     22       84.0560  2.6252\n",
      "     23       82.6205  2.6643\n",
      "     24       \u001b[36m73.7006\u001b[0m  2.6041\n",
      "     25       \u001b[36m71.7127\u001b[0m  2.6084\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m7426863.2194\u001b[0m  2.6446\n",
      "      2   \u001b[36m673466.5135\u001b[0m  2.7118\n",
      "      3   \u001b[36m119981.3480\u001b[0m  2.6652\n",
      "      4    \u001b[36m14979.2422\u001b[0m  2.6351\n",
      "      5     \u001b[36m2325.7118\u001b[0m  2.6133\n",
      "      6      \u001b[36m371.0415\u001b[0m  2.6081\n",
      "      7       \u001b[36m58.8666\u001b[0m  2.6134\n",
      "      8       \u001b[36m31.8117\u001b[0m  2.6700\n",
      "      9       50.1675  2.6094\n",
      "     10       64.7079  2.6164\n",
      "     11       82.3243  2.6269\n",
      "     12       83.7247  2.6066\n",
      "     13       89.5044  2.6310\n",
      "     14       81.4542  2.6443\n",
      "     15       79.1698  2.6067\n",
      "     16       69.0900  2.6044\n",
      "     17       63.9450  2.6005\n",
      "     18       61.1373  2.6083\n",
      "     19       59.6297  2.6307\n",
      "     20       54.3153  2.6250\n",
      "     21       51.0086  2.6021\n",
      "     22       42.1077  2.6067\n",
      "     23       41.6825  2.6122\n",
      "     24       37.3938  2.6047\n",
      "     25       35.5415  2.6421\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m5016530.3282\u001b[0m  2.6532\n",
      "      2   \u001b[36m645729.1747\u001b[0m  2.6103\n",
      "      3   \u001b[36m109932.2231\u001b[0m  2.6096\n",
      "      4     \u001b[36m2304.4243\u001b[0m  2.6376\n",
      "      5      \u001b[36m737.8427\u001b[0m  2.6159\n",
      "      6     1365.9518  2.6102\n",
      "      7     1418.1157  2.6095\n",
      "      8     1279.6216  2.6125\n",
      "      9     1096.4722  2.6296\n",
      "     10      832.5098  2.6606\n",
      "     11      \u001b[36m614.4914\u001b[0m  2.6173\n",
      "     12      \u001b[36m467.8906\u001b[0m  2.6149\n",
      "     13      \u001b[36m351.2381\u001b[0m  2.6083\n",
      "     14      \u001b[36m255.1131\u001b[0m  2.6089\n",
      "     15      \u001b[36m193.4006\u001b[0m  2.6224\n",
      "     16      \u001b[36m154.2734\u001b[0m  2.6168\n",
      "     17      \u001b[36m117.7442\u001b[0m  2.6072\n",
      "     18       \u001b[36m95.8115\u001b[0m  2.6045\n",
      "     19       \u001b[36m76.2602\u001b[0m  2.6046\n",
      "     20       \u001b[36m64.6096\u001b[0m  2.6084\n",
      "     21       64.6689  2.6570\n",
      "     22       \u001b[36m53.3897\u001b[0m  2.6973\n",
      "     23       57.6334  2.6257\n",
      "     24       \u001b[36m45.6123\u001b[0m  2.6100\n",
      "     25       45.6471  2.6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-28 22:25:13,202]\u001b[0m Trial 2 finished with value: 125.41270930622379 and parameters: {'levels': 5, 'hidden_units': 30, 'kernel_size': 9, 'dropout': 0.04246782213565523, 'learning_rate': 0.002820996133514492}. Best is trial 2 with value: 125.41270930622379.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2397509.7377\u001b[0m  2.0714\n",
      "      2   \u001b[36m270052.4017\u001b[0m  2.0241\n",
      "      3     \u001b[36m3451.1750\u001b[0m  2.0045\n",
      "      4      \u001b[36m449.7370\u001b[0m  2.0101\n",
      "      5      725.3389  2.0028\n",
      "      6      714.5832  2.0014\n",
      "      7      664.6335  2.0233\n",
      "      8      593.6320  2.0272\n",
      "      9      510.4050  1.9879\n",
      "     10      461.0395  1.9910\n",
      "     11      \u001b[36m401.1120\u001b[0m  1.9918\n",
      "     12      \u001b[36m344.9302\u001b[0m  1.9951\n",
      "     13      \u001b[36m300.2435\u001b[0m  2.0009\n",
      "     14      \u001b[36m262.1213\u001b[0m  2.0024\n",
      "     15      \u001b[36m226.6617\u001b[0m  2.0246\n",
      "     16      \u001b[36m194.2275\u001b[0m  2.0224\n",
      "     17      \u001b[36m173.2695\u001b[0m  1.9996\n",
      "     18      \u001b[36m150.3804\u001b[0m  2.0025\n",
      "     19      \u001b[36m132.1832\u001b[0m  2.0023\n",
      "     20      \u001b[36m121.6507\u001b[0m  2.0025\n",
      "     21      \u001b[36m109.2557\u001b[0m  1.9939\n",
      "     22       \u001b[36m93.1460\u001b[0m  1.9968\n",
      "     23       \u001b[36m92.2134\u001b[0m  2.0067\n",
      "     24       \u001b[36m77.6435\u001b[0m  2.0024\n",
      "     25       \u001b[36m74.0456\u001b[0m  2.0004\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2094449.8513\u001b[0m  2.0304\n",
      "      2    \u001b[36m59778.5424\u001b[0m  1.9993\n",
      "      3    96111.5637  2.0033\n",
      "      4    \u001b[36m22984.4588\u001b[0m  2.0189\n",
      "      5     \u001b[36m2963.6042\u001b[0m  2.0227\n",
      "      6     \u001b[36m2383.5136\u001b[0m  1.9990\n",
      "      7     3599.1538  1.9968\n",
      "      8     3646.0683  1.9988\n",
      "      9     2755.5131  2.0028\n",
      "     10     \u001b[36m1932.7341\u001b[0m  1.9952\n",
      "     11     \u001b[36m1314.8774\u001b[0m  2.0224\n",
      "     12     \u001b[36m1058.4902\u001b[0m  2.0278\n",
      "     13      \u001b[36m934.0815\u001b[0m  2.0047\n",
      "     14      \u001b[36m879.2676\u001b[0m  1.9936\n",
      "     15      \u001b[36m871.1969\u001b[0m  1.9928\n",
      "     16      902.7265  1.9860\n",
      "     17      873.9395  1.9915\n",
      "     18      \u001b[36m825.4705\u001b[0m  1.9954\n",
      "     19      \u001b[36m748.5532\u001b[0m  2.0195\n",
      "     20      781.4420  2.0179\n",
      "     21      \u001b[36m732.1161\u001b[0m  1.9893\n",
      "     22      \u001b[36m704.6087\u001b[0m  2.0142\n",
      "     23      \u001b[36m702.5663\u001b[0m  2.0005\n",
      "     24      \u001b[36m633.0677\u001b[0m  2.0337\n",
      "     25      652.5539  1.9830\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2936940.0646\u001b[0m  2.0576\n",
      "      2   \u001b[36m488481.6516\u001b[0m  2.0715\n",
      "      3    \u001b[36m14896.1466\u001b[0m  2.0142\n",
      "      4     \u001b[36m1330.9062\u001b[0m  2.0050\n",
      "      5     3121.8562  2.0028\n",
      "      6     3651.1367  2.0029\n",
      "      7     3543.6100  2.0188\n",
      "      8     3025.3462  2.0276\n",
      "      9     2612.4873  2.0055\n",
      "     10     2224.7374  2.0000\n",
      "     11     1759.4220  1.9983\n",
      "     12     1529.4779  1.9974\n",
      "     13     \u001b[36m1202.4763\u001b[0m  1.9993\n",
      "     14     \u001b[36m1015.6569\u001b[0m  2.0029\n",
      "     15      \u001b[36m819.7302\u001b[0m  2.0178\n",
      "     16      \u001b[36m696.7613\u001b[0m  2.0187\n",
      "     17      \u001b[36m597.5708\u001b[0m  2.0229\n",
      "     18      \u001b[36m495.7059\u001b[0m  2.0001\n",
      "     19      \u001b[36m416.4512\u001b[0m  1.9940\n",
      "     20      \u001b[36m360.5296\u001b[0m  1.9995\n",
      "     21      \u001b[36m348.5155\u001b[0m  2.0007\n",
      "     22      \u001b[36m271.1470\u001b[0m  2.0310\n",
      "     23      283.0515  2.0234\n",
      "     24      \u001b[36m218.4093\u001b[0m  2.0059\n",
      "     25      \u001b[36m212.8535\u001b[0m  1.9966\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1987260.5508\u001b[0m  2.0317\n",
      "      2    \u001b[36m71526.7162\u001b[0m  1.9960\n",
      "      3    \u001b[36m43532.1643\u001b[0m  2.0273\n",
      "      4     \u001b[36m9670.1233\u001b[0m  2.0259\n",
      "      5     \u001b[36m2581.9142\u001b[0m  1.9966\n",
      "      6     \u001b[36m1410.1016\u001b[0m  1.9959\n",
      "      7     \u001b[36m1229.2026\u001b[0m  2.0020\n",
      "      8     1293.8722  2.0025\n",
      "      9     \u001b[36m1226.1084\u001b[0m  1.9954\n",
      "     10     1257.8026  1.9905\n",
      "     11     \u001b[36m1098.6046\u001b[0m  1.9908\n",
      "     12      \u001b[36m903.4549\u001b[0m  2.0217\n",
      "     13      \u001b[36m889.6748\u001b[0m  2.0109\n",
      "     14      \u001b[36m768.0743\u001b[0m  2.0104\n",
      "     15      \u001b[36m742.6669\u001b[0m  1.9899\n",
      "     16      772.0210  1.9784\n",
      "     17      779.1299  1.9761\n",
      "     18      \u001b[36m689.9300\u001b[0m  2.0017\n",
      "     19      762.5818  2.0146\n",
      "     20      788.1069  2.0110\n",
      "     21      754.4422  2.0220\n",
      "     22      709.9673  2.0102\n",
      "     23      \u001b[36m680.7740\u001b[0m  2.0042\n",
      "     24      \u001b[36m613.9978\u001b[0m  2.0100\n",
      "     25      622.2968  1.9998\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2356395.7283\u001b[0m  2.0234\n",
      "      2   \u001b[36m271950.6316\u001b[0m  2.0880\n",
      "      3     \u001b[36m7967.0047\u001b[0m  2.0062\n",
      "      4     \u001b[36m1022.0642\u001b[0m  2.0143\n",
      "      5      \u001b[36m230.1552\u001b[0m  2.0053\n",
      "      6       \u001b[36m62.9011\u001b[0m  2.0067\n",
      "      7       \u001b[36m20.8829\u001b[0m  1.9950\n",
      "      8       \u001b[36m16.6224\u001b[0m  2.0175\n",
      "      9       17.1469  2.0362\n",
      "     10       19.3763  2.0195\n",
      "     11       18.6659  1.9858\n",
      "     12       18.3579  1.9968\n",
      "     13       19.1108  1.9913\n",
      "     14       18.6558  2.0191\n",
      "     15       20.8177  2.0733\n",
      "     16       18.4134  2.0250\n",
      "     17       17.3749  1.9939\n",
      "     18       18.2711  1.9961\n",
      "     19       17.1928  1.9901\n",
      "     20       \u001b[36m15.5343\u001b[0m  1.9913\n",
      "     21       \u001b[36m15.3537\u001b[0m  1.9880\n",
      "     22       \u001b[36m15.1452\u001b[0m  2.0176\n",
      "     23       15.3031  2.0107\n",
      "     24       16.5342  1.9857\n",
      "     25       \u001b[36m14.7781\u001b[0m  1.9949\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1142594.9989\u001b[0m  2.0217\n",
      "      2    \u001b[36m14139.8012\u001b[0m  2.0000\n",
      "      3     \u001b[36m4100.7041\u001b[0m  2.0328\n",
      "      4     \u001b[36m3323.9606\u001b[0m  1.9979\n",
      "      5     \u001b[36m2148.1454\u001b[0m  1.9977\n",
      "      6     \u001b[36m1415.9046\u001b[0m  2.0038\n",
      "      7     \u001b[36m1070.6642\u001b[0m  2.0158\n",
      "      8      \u001b[36m794.0588\u001b[0m  1.9955\n",
      "      9      \u001b[36m663.0456\u001b[0m  1.9977\n",
      "     10      \u001b[36m572.8548\u001b[0m  1.9917\n",
      "     11      \u001b[36m480.7835\u001b[0m  2.0443\n",
      "     12      \u001b[36m414.7225\u001b[0m  1.9958\n",
      "     13      \u001b[36m360.2676\u001b[0m  2.0023\n",
      "     14      \u001b[36m318.0962\u001b[0m  2.0050\n",
      "     15      \u001b[36m274.4040\u001b[0m  2.0042\n",
      "     16      \u001b[36m252.1425\u001b[0m  1.9995\n",
      "     17      \u001b[36m215.8748\u001b[0m  1.9931\n",
      "     18      \u001b[36m173.1635\u001b[0m  2.0182\n",
      "     19      \u001b[36m165.6393\u001b[0m  2.0011\n",
      "     20      168.1160  2.0036\n",
      "     21      \u001b[36m150.8326\u001b[0m  1.9984\n",
      "     22      \u001b[36m129.7997\u001b[0m  1.9955\n",
      "     23      \u001b[36m126.8454\u001b[0m  1.9917\n",
      "     24      \u001b[36m113.0156\u001b[0m  1.9926\n",
      "     25       \u001b[36m99.7648\u001b[0m  1.9937\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2747896.6802\u001b[0m  2.0322\n",
      "      2    \u001b[36m69499.0767\u001b[0m  1.9991\n",
      "      3     \u001b[36m5927.0350\u001b[0m  2.0023\n",
      "      4     9358.0470  2.0105\n",
      "      5     8344.1145  1.9964\n",
      "      6     6130.3792  1.9965\n",
      "      7     \u001b[36m4347.2814\u001b[0m  2.0466\n",
      "      8     \u001b[36m2893.8265\u001b[0m  1.9958\n",
      "      9     \u001b[36m2010.3380\u001b[0m  1.9933\n",
      "     10     \u001b[36m1512.5974\u001b[0m  1.9982\n",
      "     11     \u001b[36m1024.7675\u001b[0m  2.0064\n",
      "     12      \u001b[36m792.4056\u001b[0m  2.0037\n",
      "     13      \u001b[36m567.9830\u001b[0m  1.9910\n",
      "     14      \u001b[36m453.5479\u001b[0m  2.0226\n",
      "     15      \u001b[36m418.3281\u001b[0m  2.0227\n",
      "     16      \u001b[36m349.5644\u001b[0m  2.0421\n",
      "     17      \u001b[36m255.3114\u001b[0m  2.0085\n",
      "     18      \u001b[36m215.3703\u001b[0m  1.9886\n",
      "     19      246.5280  1.9866\n",
      "     20      247.7943  1.9944\n",
      "     21      \u001b[36m185.1317\u001b[0m  2.0057\n",
      "     22      \u001b[36m165.3844\u001b[0m  2.0580\n",
      "     23      \u001b[36m165.0156\u001b[0m  2.0061\n",
      "     24      \u001b[36m154.8255\u001b[0m  2.0024\n",
      "     25      155.6315  2.0023\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1660893.5981\u001b[0m  2.0495\n",
      "      2    \u001b[36m53491.3420\u001b[0m  2.0037\n",
      "      3    73890.5018  2.0421\n",
      "      4    \u001b[36m18486.9495\u001b[0m  1.9906\n",
      "      5     \u001b[36m3161.9217\u001b[0m  1.9959\n",
      "      6     \u001b[36m1160.4399\u001b[0m  2.0157\n",
      "      7     1446.2852  1.9852\n",
      "      8     1783.5479  1.9949\n",
      "      9     1684.8892  1.9925\n",
      "     10     1407.7030  2.0058\n",
      "     11     \u001b[36m1127.6064\u001b[0m  2.0192\n",
      "     12      \u001b[36m899.8084\u001b[0m  1.9961\n",
      "     13      \u001b[36m791.6693\u001b[0m  2.0024\n",
      "     14      \u001b[36m701.1326\u001b[0m  1.9923\n",
      "     15      \u001b[36m659.4987\u001b[0m  1.9965\n",
      "     16      664.0027  1.9977\n",
      "     17      \u001b[36m532.7424\u001b[0m  1.9921\n",
      "     18      584.4713  2.0517\n",
      "     19      568.4963  1.9891\n",
      "     20      602.0289  1.9844\n",
      "     21      \u001b[36m485.4674\u001b[0m  1.9872\n",
      "     22      516.3832  1.9882\n",
      "     23      \u001b[36m460.2703\u001b[0m  1.9872\n",
      "     24      \u001b[36m453.5053\u001b[0m  1.9916\n",
      "     25      465.6017  1.9954\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2556027.3410\u001b[0m  2.0290\n",
      "      2   \u001b[36m373865.2745\u001b[0m  1.9898\n",
      "      3    \u001b[36m19060.2506\u001b[0m  1.9866\n",
      "      4      \u001b[36m771.9406\u001b[0m  1.9911\n",
      "      5      \u001b[36m165.3565\u001b[0m  1.9949\n",
      "      6       \u001b[36m82.1130\u001b[0m  2.0282\n",
      "      7       \u001b[36m57.0797\u001b[0m  2.0244\n",
      "      8       \u001b[36m48.6902\u001b[0m  1.9993\n",
      "      9       \u001b[36m46.1232\u001b[0m  2.0115\n",
      "     10       \u001b[36m39.7514\u001b[0m  2.0079\n",
      "     11       \u001b[36m36.6022\u001b[0m  1.9960\n",
      "     12       37.9390  1.9951\n",
      "     13       \u001b[36m32.3435\u001b[0m  1.9942\n",
      "     14       \u001b[36m32.0710\u001b[0m  2.0457\n",
      "     15       34.4777  1.9973\n",
      "     16       \u001b[36m31.2070\u001b[0m  1.9973\n",
      "     17       33.6784  1.9964\n",
      "     18       \u001b[36m27.6451\u001b[0m  1.9971\n",
      "     19       32.6321  1.9950\n",
      "     20       33.2085  1.9959\n",
      "     21       34.4677  2.0240\n",
      "     22       30.8582  1.9965\n",
      "     23       33.7875  2.0467\n",
      "     24       31.4414  2.0084\n",
      "     25       28.2987  1.9885\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2463632.5949\u001b[0m  2.0165\n",
      "      2    \u001b[36m30700.1297\u001b[0m  2.0012\n",
      "      3    46338.0417  2.0183\n",
      "      4    \u001b[36m13598.8473\u001b[0m  1.9796\n",
      "      5     \u001b[36m3977.3933\u001b[0m  2.0575\n",
      "      6     \u001b[36m1459.2939\u001b[0m  2.0246\n",
      "      7      \u001b[36m629.3064\u001b[0m  1.9891\n",
      "      8      \u001b[36m385.0107\u001b[0m  1.9934\n",
      "      9      \u001b[36m247.2337\u001b[0m  1.9852\n",
      "     10      \u001b[36m197.1270\u001b[0m  2.0182\n",
      "     11      215.5546  2.0178\n",
      "     12      \u001b[36m164.8806\u001b[0m  1.9907\n",
      "     13      \u001b[36m123.5513\u001b[0m  1.9898\n",
      "     14      169.7469  1.9897\n",
      "     15      138.1784  1.9899\n",
      "     16      \u001b[36m119.0507\u001b[0m  1.9894\n",
      "     17       \u001b[36m98.0793\u001b[0m  2.0005\n",
      "     18       \u001b[36m96.2388\u001b[0m  2.0107\n",
      "     19       99.8119  1.9906\n",
      "     20       98.1738  1.9881\n",
      "     21      105.6984  1.9880\n",
      "     22       \u001b[36m90.5774\u001b[0m  2.0107\n",
      "     23       \u001b[36m89.2659\u001b[0m  1.9898\n",
      "     24       \u001b[36m74.4930\u001b[0m  1.9844\n",
      "     25       80.3311  2.0177\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m825734.0332\u001b[0m  2.1085\n",
      "      2    \u001b[36m59318.3081\u001b[0m  2.0156\n",
      "      3    \u001b[36m15719.5885\u001b[0m  1.9918\n",
      "      4     \u001b[36m1575.6025\u001b[0m  1.9949\n",
      "      5      \u001b[36m346.8588\u001b[0m  1.9940\n",
      "      6      \u001b[36m173.1111\u001b[0m  2.0445\n",
      "      7      \u001b[36m143.2766\u001b[0m  2.0386\n",
      "      8      \u001b[36m131.0295\u001b[0m  2.0612\n",
      "      9      \u001b[36m130.3461\u001b[0m  2.0061\n",
      "     10      \u001b[36m114.0702\u001b[0m  2.0363\n",
      "     11      \u001b[36m107.9564\u001b[0m  2.0280\n",
      "     12      \u001b[36m104.2843\u001b[0m  2.0152\n",
      "     13      \u001b[36m100.0849\u001b[0m  1.9988\n",
      "     14      102.4569  2.0096\n",
      "     15       \u001b[36m88.1636\u001b[0m  1.9782\n",
      "     16       93.7846  1.9781\n",
      "     17       \u001b[36m83.6358\u001b[0m  1.9794\n",
      "     18       \u001b[36m77.2651\u001b[0m  1.9719\n",
      "     19       84.6338  1.9735\n",
      "     20       80.6367  1.9941\n",
      "     21       78.3761  1.9868\n",
      "     22       \u001b[36m71.4019\u001b[0m  1.9991\n",
      "     23       74.6780  1.9764\n",
      "     24       \u001b[36m67.5410\u001b[0m  1.9810\n",
      "     25       \u001b[36m61.8144\u001b[0m  1.9950\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m888298.5813\u001b[0m  2.0380\n",
      "      2    \u001b[36m45088.1372\u001b[0m  2.0565\n",
      "      3    \u001b[36m27715.4706\u001b[0m  2.1948\n",
      "      4     \u001b[36m4684.0750\u001b[0m  2.1015\n",
      "      5     \u001b[36m1069.8020\u001b[0m  2.0057\n",
      "      6      \u001b[36m844.8987\u001b[0m  2.0071\n",
      "      7     1098.9549  2.0189\n",
      "      8     1025.7348  2.0074\n",
      "      9      868.8141  2.0200\n",
      "     10      \u001b[36m686.9102\u001b[0m  2.0198\n",
      "     11      \u001b[36m448.8483\u001b[0m  2.0098\n",
      "     12      \u001b[36m372.9242\u001b[0m  1.9926\n",
      "     13      \u001b[36m321.0640\u001b[0m  1.9949\n",
      "     14      329.9573  1.9953\n",
      "     15      \u001b[36m295.4675\u001b[0m  1.9907\n",
      "     16      298.1264  1.9835\n",
      "     17      297.0558  2.0039\n",
      "     18      \u001b[36m291.9076\u001b[0m  2.0452\n",
      "     19      \u001b[36m271.3185\u001b[0m  2.0049\n",
      "     20      280.5574  1.9910\n",
      "     21      \u001b[36m248.0983\u001b[0m  2.0204\n",
      "     22      270.3742  1.9905\n",
      "     23      250.3317  1.9951\n",
      "     24      \u001b[36m231.7440\u001b[0m  2.0003\n",
      "     25      \u001b[36m219.1556\u001b[0m  2.0234\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1870711.6036\u001b[0m  2.0615\n",
      "      2   \u001b[36m192996.9688\u001b[0m  1.9965\n",
      "      3     \u001b[36m5686.1504\u001b[0m  1.9901\n",
      "      4      \u001b[36m342.7548\u001b[0m  1.9924\n",
      "      5      \u001b[36m102.5626\u001b[0m  1.9896\n",
      "      6       \u001b[36m62.6738\u001b[0m  2.0385\n",
      "      7       \u001b[36m34.5615\u001b[0m  1.9915\n",
      "      8       \u001b[36m31.2670\u001b[0m  1.9965\n",
      "      9       \u001b[36m25.7380\u001b[0m  2.0025\n",
      "     10       \u001b[36m22.1505\u001b[0m  1.9965\n",
      "     11       \u001b[36m21.3111\u001b[0m  1.9961\n",
      "     12       \u001b[36m18.4028\u001b[0m  1.9907\n",
      "     13       20.9617  2.0365\n",
      "     14       \u001b[36m17.6676\u001b[0m  2.0105\n",
      "     15       19.3597  1.9965\n",
      "     16       21.7322  2.0005\n",
      "     17       20.5903  2.0100\n",
      "     18       21.1464  1.9896\n",
      "     19       21.3878  1.9938\n",
      "     20       19.8959  1.9847\n",
      "     21       21.9699  2.0299\n",
      "     22       21.8387  1.9832\n",
      "     23       19.6491  1.9850\n",
      "     24       24.4358  1.9980\n",
      "     25       20.4126  2.0000\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2942839.5333\u001b[0m  2.0260\n",
      "      2   \u001b[36m147465.6950\u001b[0m  2.0286\n",
      "      3     \u001b[36m7867.0857\u001b[0m  2.0028\n",
      "      4     \u001b[36m1386.6048\u001b[0m  1.9930\n",
      "      5      \u001b[36m353.0392\u001b[0m  1.9905\n",
      "      6      \u001b[36m118.3104\u001b[0m  1.9941\n",
      "      7       \u001b[36m75.4746\u001b[0m  2.0159\n",
      "      8       \u001b[36m52.8256\u001b[0m  1.9911\n",
      "      9       \u001b[36m42.4706\u001b[0m  2.0053\n",
      "     10       \u001b[36m38.1576\u001b[0m  2.0320\n",
      "     11       \u001b[36m33.1089\u001b[0m  1.9958\n",
      "     12       \u001b[36m29.8624\u001b[0m  2.0293\n",
      "     13       30.3187  2.0182\n",
      "     14       \u001b[36m27.7261\u001b[0m  2.0084\n",
      "     15       28.3541  2.0087\n",
      "     16       29.6983  2.0175\n",
      "     17       27.9831  2.0585\n",
      "     18       \u001b[36m25.9065\u001b[0m  2.0107\n",
      "     19       \u001b[36m24.2434\u001b[0m  2.0059\n",
      "     20       24.9844  2.0005\n",
      "     21       24.3630  1.9976\n",
      "     22       \u001b[36m21.7322\u001b[0m  2.0034\n",
      "     23       23.5755  2.0095\n",
      "     24       24.4173  2.0279\n",
      "     25       25.6010  2.0231\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m5982580.9206\u001b[0m  2.0345\n",
      "      2   \u001b[36m309323.5415\u001b[0m  1.9960\n",
      "      3     \u001b[36m6105.3302\u001b[0m  2.0004\n",
      "      4      \u001b[36m290.3605\u001b[0m  1.9973\n",
      "      5      \u001b[36m167.0735\u001b[0m  2.0333\n",
      "      6      186.2845  2.0417\n",
      "      7      194.4929  2.0387\n",
      "      8      184.0349  2.0737\n",
      "      9      222.7204  2.0805\n",
      "     10      181.0786  2.0710\n",
      "     11      178.9183  2.0223\n",
      "     12      \u001b[36m161.3701\u001b[0m  1.9881\n",
      "     13      162.4614  2.0236\n",
      "     14      \u001b[36m145.7761\u001b[0m  1.9973\n",
      "     15      148.9645  2.0167\n",
      "     16      \u001b[36m144.7919\u001b[0m  1.9952\n",
      "     17      159.2641  1.9938\n",
      "     18      \u001b[36m119.6256\u001b[0m  1.9936\n",
      "     19      129.8900  1.9976\n",
      "     20      124.3653  2.0302\n",
      "     21      \u001b[36m105.1370\u001b[0m  2.0172\n",
      "     22      107.5218  1.9909\n",
      "     23      121.5517  1.9837\n",
      "     24       \u001b[36m99.3578\u001b[0m  1.9911\n",
      "     25      103.6346  1.9914\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m7837771.3741\u001b[0m  2.0356\n",
      "      2   \u001b[36m821197.8137\u001b[0m  2.0201\n",
      "      3    \u001b[36m29940.5643\u001b[0m  1.9866\n",
      "      4     \u001b[36m5414.6792\u001b[0m  1.9877\n",
      "      5     \u001b[36m1823.2462\u001b[0m  1.9884\n",
      "      6      \u001b[36m886.0985\u001b[0m  1.9881\n",
      "      7      \u001b[36m582.7409\u001b[0m  1.9897\n",
      "      8      \u001b[36m438.7452\u001b[0m  1.9929\n",
      "      9      \u001b[36m344.2671\u001b[0m  2.0282\n",
      "     10      \u001b[36m293.0817\u001b[0m  1.9917\n",
      "     11      \u001b[36m282.8119\u001b[0m  1.9933\n",
      "     12      \u001b[36m257.5065\u001b[0m  1.9938\n",
      "     13      \u001b[36m242.9313\u001b[0m  1.9999\n",
      "     14      \u001b[36m235.3809\u001b[0m  1.9931\n",
      "     15      \u001b[36m218.5380\u001b[0m  2.0063\n",
      "     16      \u001b[36m202.1823\u001b[0m  1.9914\n",
      "     17      204.8543  2.0111\n",
      "     18      \u001b[36m199.2542\u001b[0m  1.9856\n",
      "     19      \u001b[36m184.0482\u001b[0m  2.0267\n",
      "     20      \u001b[36m174.2237\u001b[0m  1.9968\n",
      "     21      183.2222  1.9827\n",
      "     22      \u001b[36m163.3774\u001b[0m  1.9838\n",
      "     23      164.0192  1.9862\n",
      "     24      \u001b[36m152.1725\u001b[0m  2.0280\n",
      "     25      \u001b[36m142.7216\u001b[0m  1.9893\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1769016.4918\u001b[0m  2.0330\n",
      "      2    \u001b[36m22960.7489\u001b[0m  2.0099\n",
      "      3    26746.7413  1.9927\n",
      "      4     \u001b[36m8877.3868\u001b[0m  1.9874\n",
      "      5     \u001b[36m2858.2696\u001b[0m  2.0315\n",
      "      6     \u001b[36m1083.9312\u001b[0m  2.0287\n",
      "      7      \u001b[36m557.0953\u001b[0m  2.0068\n",
      "      8      \u001b[36m345.6395\u001b[0m  2.0148\n",
      "      9      \u001b[36m239.9815\u001b[0m  2.0033\n",
      "     10      \u001b[36m188.9968\u001b[0m  2.0125\n",
      "     11      \u001b[36m176.6861\u001b[0m  2.0024\n",
      "     12      \u001b[36m127.2886\u001b[0m  2.0473\n",
      "     13      128.2076  2.0433\n",
      "     14      148.3888  2.0157\n",
      "     15      \u001b[36m119.0104\u001b[0m  2.0200\n",
      "     16      131.5592  2.0348\n",
      "     17      \u001b[36m115.4389\u001b[0m  1.9933\n",
      "     18      126.5715  1.9961\n",
      "     19      121.6618  1.9968\n",
      "     20      \u001b[36m115.2068\u001b[0m  1.9997\n",
      "     21      \u001b[36m114.2167\u001b[0m  1.9841\n",
      "     22      123.2313  1.9918\n",
      "     23      118.1647  1.9916\n",
      "     24      \u001b[36m113.2771\u001b[0m  1.9860\n",
      "     25      \u001b[36m111.4826\u001b[0m  1.9873\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2605243.1312\u001b[0m  2.0451\n",
      "      2    \u001b[36m76343.0429\u001b[0m  2.0115\n",
      "      3     \u001b[36m1649.0839\u001b[0m  1.9903\n",
      "      4      \u001b[36m300.3762\u001b[0m  1.9842\n",
      "      5      \u001b[36m162.4969\u001b[0m  1.9769\n",
      "      6      \u001b[36m109.9651\u001b[0m  1.9880\n",
      "      7       \u001b[36m87.0733\u001b[0m  1.9786\n",
      "      8       \u001b[36m73.0075\u001b[0m  2.0081\n",
      "      9       75.2911  2.0118\n",
      "     10       73.0338  1.9868\n",
      "     11       \u001b[36m65.1244\u001b[0m  1.9932\n",
      "     12       \u001b[36m59.0262\u001b[0m  1.9926\n",
      "     13       63.1505  1.9844\n",
      "     14       66.4325  1.9910\n",
      "     15       63.4131  2.0184\n",
      "     16       \u001b[36m55.8101\u001b[0m  1.9983\n",
      "     17       66.7611  2.0170\n",
      "     18       67.1467  1.9943\n",
      "     19       59.6552  1.9931\n",
      "     20       61.1487  1.9955\n",
      "     21       68.4542  2.0029\n",
      "     22       58.2674  1.9829\n",
      "     23       \u001b[36m55.6579\u001b[0m  2.0054\n",
      "     24       60.3652  2.0082\n",
      "     25       58.3485  1.9835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-28 22:41:04,677]\u001b[0m Trial 3 finished with value: 162.46530357407934 and parameters: {'levels': 7, 'hidden_units': 12, 'kernel_size': 6, 'dropout': 0.08638900372842316, 'learning_rate': 0.005265139631677754}. Best is trial 2 with value: 125.41270930622379.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m902246.6396\u001b[0m  3.8012\n",
      "      2     \u001b[36m6116.5337\u001b[0m  3.8838\n",
      "      3     7038.1536  3.8480\n",
      "      4     \u001b[36m3237.7246\u001b[0m  3.7951\n",
      "      5     \u001b[36m1486.4079\u001b[0m  3.7912\n",
      "      6      \u001b[36m626.8518\u001b[0m  3.7955\n",
      "      7      \u001b[36m391.9582\u001b[0m  3.8323\n",
      "      8      \u001b[36m226.9329\u001b[0m  3.8003\n",
      "      9      \u001b[36m194.4591\u001b[0m  3.8522\n",
      "     10      \u001b[36m146.2306\u001b[0m  3.8266\n",
      "     11      \u001b[36m121.5439\u001b[0m  3.8282\n",
      "     12      \u001b[36m111.9750\u001b[0m  3.7964\n",
      "     13      \u001b[36m106.2607\u001b[0m  3.8094\n",
      "     14      \u001b[36m103.6061\u001b[0m  3.8210\n",
      "     15       \u001b[36m94.1801\u001b[0m  3.8401\n",
      "     16       \u001b[36m86.3876\u001b[0m  3.8144\n",
      "     17       94.4578  3.8103\n",
      "     18       \u001b[36m81.6846\u001b[0m  3.8383\n",
      "     19       82.7481  3.8248\n",
      "     20       93.8133  3.8120\n",
      "     21       \u001b[36m62.9439\u001b[0m  3.8167\n",
      "     22       80.4074  3.8403\n",
      "     23       76.6021  3.8433\n",
      "     24       65.6257  3.7998\n",
      "     25       \u001b[36m54.4559\u001b[0m  3.7878\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1600444.0841\u001b[0m  3.8275\n",
      "      2     \u001b[36m8847.2951\u001b[0m  3.7980\n",
      "      3      \u001b[36m549.5963\u001b[0m  3.7999\n",
      "      4      \u001b[36m104.4682\u001b[0m  3.8214\n",
      "      5       \u001b[36m38.8196\u001b[0m  3.8529\n",
      "      6       \u001b[36m25.8091\u001b[0m  3.9399\n",
      "      7       \u001b[36m22.0452\u001b[0m  3.9226\n",
      "      8       \u001b[36m20.5374\u001b[0m  3.8245\n",
      "      9       21.0450  3.8088\n",
      "     10       \u001b[36m18.9461\u001b[0m  3.8855\n",
      "     11       \u001b[36m18.7863\u001b[0m  3.8157\n",
      "     12       19.1282  3.8399\n",
      "     13       18.9672  3.8182\n",
      "     14       \u001b[36m18.0897\u001b[0m  3.8356\n",
      "     15       \u001b[36m17.5742\u001b[0m  3.8070\n",
      "     16       \u001b[36m17.0012\u001b[0m  3.8596\n",
      "     17       \u001b[36m16.2615\u001b[0m  3.8005\n",
      "     18       \u001b[36m15.4749\u001b[0m  3.8031\n",
      "     19       \u001b[36m15.3406\u001b[0m  3.7954\n",
      "     20       \u001b[36m14.6421\u001b[0m  3.8468\n",
      "     21       \u001b[36m14.0495\u001b[0m  3.8229\n",
      "     22       \u001b[36m13.8705\u001b[0m  3.8280\n",
      "     23       \u001b[36m13.2521\u001b[0m  3.8026\n",
      "     24       \u001b[36m12.7110\u001b[0m  3.8392\n",
      "     25       \u001b[36m12.2762\u001b[0m  3.8140\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2619668.6697\u001b[0m  3.8816\n",
      "      2       \u001b[36m65.8403\u001b[0m  3.9044\n",
      "      3       \u001b[36m12.2724\u001b[0m  3.9087\n",
      "      4        \u001b[36m7.6675\u001b[0m  3.8960\n",
      "      5        8.8589  3.8737\n",
      "      6        \u001b[36m7.1664\u001b[0m  3.8473\n",
      "      7        7.1985  3.8070\n",
      "      8        7.7960  3.8149\n",
      "      9        7.2811  3.8091\n",
      "     10        \u001b[36m7.0216\u001b[0m  3.8575\n",
      "     11        8.1066  3.8598\n",
      "     12        \u001b[36m6.3133\u001b[0m  3.8361\n",
      "     13        8.2811  3.8058\n",
      "     14        8.6903  3.8711\n",
      "     15        9.2951  3.8042\n",
      "     16        7.4078  3.8059\n",
      "     17        9.9229  3.8369\n",
      "     18        8.3276  3.8293\n",
      "     19        7.7566  3.8000\n",
      "     20        8.7137  3.8007\n",
      "     21        7.9246  3.8081\n",
      "     22        7.9096  3.8251\n",
      "     23        6.9536  3.7977\n",
      "     24        6.9003  3.8054\n",
      "     25        7.9884  3.8271\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m5664719.6329\u001b[0m  3.8506\n",
      "      2   \u001b[36m135103.2099\u001b[0m  3.8026\n",
      "      3     \u001b[36m9716.4346\u001b[0m  3.8117\n",
      "      4      \u001b[36m505.1773\u001b[0m  3.8278\n",
      "      5      \u001b[36m170.9146\u001b[0m  3.7965\n",
      "      6      \u001b[36m129.0305\u001b[0m  3.8014\n",
      "      7      137.4831  3.8195\n",
      "      8       \u001b[36m97.1352\u001b[0m  3.8100\n",
      "      9       \u001b[36m95.3329\u001b[0m  3.7985\n",
      "     10       \u001b[36m77.5397\u001b[0m  3.8003\n",
      "     11       81.2043  3.8148\n",
      "     12       83.1972  3.8240\n",
      "     13       \u001b[36m70.6795\u001b[0m  3.8046\n",
      "     14       \u001b[36m62.9911\u001b[0m  3.8112\n",
      "     15       \u001b[36m58.1768\u001b[0m  3.8311\n",
      "     16       62.9163  3.8465\n",
      "     17       \u001b[36m48.2619\u001b[0m  3.8413\n",
      "     18       50.1035  3.8108\n",
      "     19       \u001b[36m46.5182\u001b[0m  3.8286\n",
      "     20       \u001b[36m39.7541\u001b[0m  3.7988\n",
      "     21       \u001b[36m37.7024\u001b[0m  3.8041\n",
      "     22       \u001b[36m28.9148\u001b[0m  3.8135\n",
      "     23       39.6499  3.8348\n",
      "     24       36.9290  3.7994\n",
      "     25       34.3836  3.8029\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m259679.9381\u001b[0m  3.8786\n",
      "      2      \u001b[36m558.5797\u001b[0m  3.8027\n",
      "      3       \u001b[36m28.5392\u001b[0m  3.8005\n",
      "      4       \u001b[36m18.6847\u001b[0m  3.8152\n",
      "      5       \u001b[36m17.0526\u001b[0m  3.8439\n",
      "      6       \u001b[36m16.6316\u001b[0m  3.8055\n",
      "      7       \u001b[36m15.7591\u001b[0m  3.8146\n",
      "      8       16.5388  3.8097\n",
      "      9       \u001b[36m15.2878\u001b[0m  3.8581\n",
      "     10       \u001b[36m15.2493\u001b[0m  3.8102\n",
      "     11       \u001b[36m15.1576\u001b[0m  3.8130\n",
      "     12       15.2989  3.8081\n",
      "     13       15.2467  3.8571\n",
      "     14       \u001b[36m14.3497\u001b[0m  3.8105\n",
      "     15       \u001b[36m14.2834\u001b[0m  3.8155\n",
      "     16       14.2915  3.8047\n",
      "     17       14.3719  3.8523\n",
      "     18       14.6602  3.8026\n",
      "     19       14.5198  3.8200\n",
      "     20       14.6263  3.8123\n",
      "     21       \u001b[36m14.2201\u001b[0m  3.8660\n",
      "     22       \u001b[36m14.0004\u001b[0m  3.8091\n",
      "     23       \u001b[36m13.8671\u001b[0m  3.8667\n",
      "     24       \u001b[36m13.3449\u001b[0m  3.8251\n",
      "     25       14.0427  3.8695\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2061296.8547\u001b[0m  3.8592\n",
      "      2   \u001b[36m115504.9402\u001b[0m  3.8028\n",
      "      3     \u001b[36m3536.0181\u001b[0m  3.8301\n",
      "      4      \u001b[36m195.0144\u001b[0m  3.7906\n",
      "      5       \u001b[36m47.2714\u001b[0m  3.8017\n",
      "      6       \u001b[36m35.8387\u001b[0m  3.7901\n",
      "      7       \u001b[36m23.7996\u001b[0m  3.8472\n",
      "      8       27.5150  3.7978\n",
      "      9       \u001b[36m18.7683\u001b[0m  3.8063\n",
      "     10       \u001b[36m18.3980\u001b[0m  3.7989\n",
      "     11       18.4524  3.8354\n",
      "     12       \u001b[36m17.1031\u001b[0m  3.7982\n",
      "     13       21.6402  3.8001\n",
      "     14       \u001b[36m15.6179\u001b[0m  3.8095\n",
      "     15       18.4096  3.9324\n",
      "     16       28.2190  3.8209\n",
      "     17       18.8805  3.8266\n",
      "     18       \u001b[36m15.5380\u001b[0m  3.8215\n",
      "     19       \u001b[36m14.9442\u001b[0m  3.8625\n",
      "     20       18.9365  3.8134\n",
      "     21       16.5750  3.8036\n",
      "     22       17.3409  3.8286\n",
      "     23       20.4215  3.8218\n",
      "     24       17.1037  3.8125\n",
      "     25       16.5853  3.8202\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2378878.3771\u001b[0m  3.8508\n",
      "      2    \u001b[36m35705.6214\u001b[0m  3.8319\n",
      "      3     \u001b[36m1656.4730\u001b[0m  3.8390\n",
      "      4      \u001b[36m448.0757\u001b[0m  3.8327\n",
      "      5      \u001b[36m200.6603\u001b[0m  3.8385\n",
      "      6      \u001b[36m105.6411\u001b[0m  3.8142\n",
      "      7       \u001b[36m86.6908\u001b[0m  3.8143\n",
      "      8       \u001b[36m82.4848\u001b[0m  3.8354\n",
      "      9       \u001b[36m76.2629\u001b[0m  3.8364\n",
      "     10       \u001b[36m67.7240\u001b[0m  3.8101\n",
      "     11       \u001b[36m63.0349\u001b[0m  3.8104\n",
      "     12       \u001b[36m55.9218\u001b[0m  3.8187\n",
      "     13       \u001b[36m55.0618\u001b[0m  3.8237\n",
      "     14       57.9661  3.8393\n",
      "     15       57.1185  3.8468\n",
      "     16       61.7453  3.8509\n",
      "     17       \u001b[36m53.3038\u001b[0m  3.8133\n",
      "     18       65.5349  3.8280\n",
      "     19       55.4005  3.8821\n",
      "     20       \u001b[36m41.0504\u001b[0m  3.8260\n",
      "     21       \u001b[36m34.6328\u001b[0m  3.8125\n",
      "     22       50.4079  3.7987\n",
      "     23       40.0655  3.8041\n",
      "     24       46.8600  3.8360\n",
      "     25       42.6951  3.8074\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1986808.6458\u001b[0m  3.8569\n",
      "      2    \u001b[36m15955.7930\u001b[0m  3.8379\n",
      "      3     \u001b[36m5438.7386\u001b[0m  3.8014\n",
      "      4      \u001b[36m607.9209\u001b[0m  3.7959\n",
      "      5      \u001b[36m306.2240\u001b[0m  3.8210\n",
      "      6      \u001b[36m222.4387\u001b[0m  3.8577\n",
      "      7      \u001b[36m182.3779\u001b[0m  3.8125\n",
      "      8      \u001b[36m171.3667\u001b[0m  3.8076\n",
      "      9      \u001b[36m160.6413\u001b[0m  3.8117\n",
      "     10      \u001b[36m159.5592\u001b[0m  3.8401\n",
      "     11      167.8601  3.8202\n",
      "     12      170.9178  3.8174\n",
      "     13      \u001b[36m144.3383\u001b[0m  3.8122\n",
      "     14      \u001b[36m123.5574\u001b[0m  3.9099\n",
      "     15      130.7535  3.8986\n",
      "     16      \u001b[36m122.5260\u001b[0m  3.9101\n",
      "     17      \u001b[36m118.3917\u001b[0m  3.8907\n",
      "     18      \u001b[36m108.9263\u001b[0m  3.8596\n",
      "     19      115.5156  3.8900\n",
      "     20      119.3937  3.8358\n",
      "     21      113.1257  3.8384\n",
      "     22       \u001b[36m96.0632\u001b[0m  3.8496\n",
      "     23       \u001b[36m89.5937\u001b[0m  3.8384\n",
      "     24       93.1406  3.8467\n",
      "     25       \u001b[36m84.6627\u001b[0m  3.8172\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2131583.4902\u001b[0m  3.8581\n",
      "      2     \u001b[36m6780.1173\u001b[0m  3.8107\n",
      "      3     9229.2831  3.8244\n",
      "      4     6855.9160  3.8365\n",
      "      5     \u001b[36m3185.3943\u001b[0m  3.8256\n",
      "      6     \u001b[36m1385.8946\u001b[0m  3.8101\n",
      "      7      \u001b[36m887.0429\u001b[0m  3.8436\n",
      "      8      \u001b[36m633.3917\u001b[0m  3.8264\n",
      "      9      \u001b[36m430.2483\u001b[0m  3.8104\n",
      "     10      \u001b[36m339.7217\u001b[0m  3.8239\n",
      "     11      \u001b[36m280.5016\u001b[0m  3.8784\n",
      "     12      \u001b[36m247.5122\u001b[0m  3.8396\n",
      "     13      \u001b[36m217.3266\u001b[0m  3.8776\n",
      "     14      \u001b[36m177.9600\u001b[0m  3.8421\n",
      "     15      \u001b[36m155.7743\u001b[0m  3.8291\n",
      "     16      \u001b[36m140.7056\u001b[0m  3.8225\n",
      "     17      \u001b[36m120.2164\u001b[0m  3.8137\n",
      "     18      \u001b[36m104.9109\u001b[0m  3.8123\n",
      "     19      107.0569  3.8564\n",
      "     20      \u001b[36m102.8747\u001b[0m  3.8126\n",
      "     21       \u001b[36m81.9780\u001b[0m  3.8123\n",
      "     22       84.5813  3.8066\n",
      "     23       84.0014  3.8535\n",
      "     24       \u001b[36m79.9987\u001b[0m  3.7992\n",
      "     25       \u001b[36m65.4846\u001b[0m  3.8161\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1760404.4376\u001b[0m  3.8924\n",
      "      2     \u001b[36m5965.7067\u001b[0m  3.8045\n",
      "      3      \u001b[36m211.3419\u001b[0m  3.8013\n",
      "      4       \u001b[36m67.1357\u001b[0m  3.8008\n",
      "      5       \u001b[36m44.2947\u001b[0m  3.8639\n",
      "      6       \u001b[36m26.9513\u001b[0m  3.8655\n",
      "      7       \u001b[36m18.1073\u001b[0m  3.9083\n",
      "      8       \u001b[36m16.2206\u001b[0m  3.8711\n",
      "      9       16.6153  3.8520\n",
      "     10       \u001b[36m15.3241\u001b[0m  3.7929\n",
      "     11       16.5152  3.8144\n",
      "     12       \u001b[36m14.9404\u001b[0m  3.8313\n",
      "     13       15.9174  3.8552\n",
      "     14       15.8301  3.8204\n",
      "     15       \u001b[36m14.1674\u001b[0m  3.8150\n",
      "     16       16.3918  3.8264\n",
      "     17       15.4171  3.8535\n",
      "     18       14.9083  3.8384\n",
      "     19       \u001b[36m13.9794\u001b[0m  3.8263\n",
      "     20       15.0431  3.8115\n",
      "     21       14.5538  3.8276\n",
      "     22       \u001b[36m13.2254\u001b[0m  3.8126\n",
      "     23       15.7097  3.8054\n",
      "     24       13.6445  3.8120\n",
      "     25       \u001b[36m11.5850\u001b[0m  3.8502\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2971858.6330\u001b[0m  3.8338\n",
      "      2   \u001b[36m171512.7044\u001b[0m  3.8206\n",
      "      3    \u001b[36m24701.3575\u001b[0m  3.8612\n",
      "      4     \u001b[36m4293.4383\u001b[0m  3.8228\n",
      "      5      \u001b[36m925.6132\u001b[0m  3.8183\n",
      "      6      \u001b[36m454.9375\u001b[0m  3.8134\n",
      "      7      527.2562  3.8681\n",
      "      8      478.1845  3.8123\n",
      "      9      \u001b[36m410.2837\u001b[0m  3.8273\n",
      "     10      \u001b[36m347.2448\u001b[0m  3.8174\n",
      "     11      \u001b[36m323.6392\u001b[0m  3.8369\n",
      "     12      \u001b[36m314.5667\u001b[0m  3.8227\n",
      "     13      327.3241  3.8124\n",
      "     14      \u001b[36m284.3406\u001b[0m  3.8394\n",
      "     15      303.2032  3.8263\n",
      "     16      291.7701  3.8189\n",
      "     17      293.0613  3.8209\n",
      "     18      \u001b[36m279.8349\u001b[0m  3.8344\n",
      "     19      288.0013  3.8460\n",
      "     20      \u001b[36m274.6394\u001b[0m  3.8121\n",
      "     21      \u001b[36m258.3696\u001b[0m  3.8221\n",
      "     22      \u001b[36m232.0101\u001b[0m  3.8119\n",
      "     23      251.0352  3.8337\n",
      "     24      250.4216  3.8047\n",
      "     25      \u001b[36m229.2361\u001b[0m  3.8530\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1206602.6591\u001b[0m  3.8667\n",
      "      2     \u001b[36m1930.7461\u001b[0m  3.8159\n",
      "      3      \u001b[36m881.5876\u001b[0m  3.8079\n",
      "      4      \u001b[36m766.5924\u001b[0m  3.8149\n",
      "      5      \u001b[36m694.1399\u001b[0m  3.8311\n",
      "      6      \u001b[36m635.6679\u001b[0m  3.8245\n",
      "      7      \u001b[36m568.9822\u001b[0m  3.8115\n",
      "      8      \u001b[36m517.6223\u001b[0m  3.8637\n",
      "      9      \u001b[36m465.9440\u001b[0m  3.8055\n",
      "     10      \u001b[36m411.9123\u001b[0m  3.8106\n",
      "     11      \u001b[36m364.7244\u001b[0m  3.8100\n",
      "     12      \u001b[36m326.6159\u001b[0m  3.8604\n",
      "     13      \u001b[36m272.8385\u001b[0m  3.8104\n",
      "     14      \u001b[36m263.9814\u001b[0m  3.8099\n",
      "     15      \u001b[36m238.5524\u001b[0m  3.8538\n",
      "     16      \u001b[36m208.8487\u001b[0m  3.8344\n",
      "     17      \u001b[36m184.1734\u001b[0m  3.8117\n",
      "     18      \u001b[36m167.1630\u001b[0m  3.8100\n",
      "     19      \u001b[36m157.2178\u001b[0m  3.8119\n",
      "     20      \u001b[36m136.1571\u001b[0m  3.8721\n",
      "     21      \u001b[36m129.2323\u001b[0m  3.8143\n",
      "     22      \u001b[36m113.3218\u001b[0m  3.8092\n",
      "     23      \u001b[36m112.1346\u001b[0m  3.8096\n",
      "     24       \u001b[36m91.7970\u001b[0m  3.8438\n",
      "     25       93.6484  3.7800\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1778926.9691\u001b[0m  3.8626\n",
      "      2     \u001b[36m6880.4190\u001b[0m  3.8406\n",
      "      3     8441.0992  3.8149\n",
      "      4     \u001b[36m3292.5555\u001b[0m  3.8578\n",
      "      5     \u001b[36m1659.0032\u001b[0m  3.8320\n",
      "      6     \u001b[36m1109.0547\u001b[0m  3.8679\n",
      "      7      \u001b[36m712.1662\u001b[0m  3.8169\n",
      "      8      \u001b[36m514.3384\u001b[0m  3.8131\n",
      "      9      \u001b[36m408.9297\u001b[0m  3.8265\n",
      "     10      \u001b[36m315.0222\u001b[0m  3.8562\n",
      "     11      \u001b[36m265.8761\u001b[0m  3.8071\n",
      "     12      \u001b[36m219.7177\u001b[0m  3.8127\n",
      "     13      \u001b[36m192.6678\u001b[0m  3.8079\n",
      "     14      \u001b[36m168.2725\u001b[0m  3.8446\n",
      "     15      \u001b[36m151.3187\u001b[0m  3.8178\n",
      "     16      \u001b[36m129.5365\u001b[0m  3.8137\n",
      "     17      \u001b[36m110.4323\u001b[0m  3.8110\n",
      "     18      \u001b[36m106.0779\u001b[0m  3.8697\n",
      "     19       \u001b[36m87.8130\u001b[0m  3.8119\n",
      "     20       \u001b[36m81.9087\u001b[0m  3.7996\n",
      "     21       \u001b[36m79.1681\u001b[0m  3.8219\n",
      "     22       79.7217  3.8263\n",
      "     23       \u001b[36m58.8395\u001b[0m  3.8098\n",
      "     24       \u001b[36m57.3955\u001b[0m  3.7931\n",
      "     25       \u001b[36m50.9266\u001b[0m  3.8143\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2751295.3825\u001b[0m  3.8555\n",
      "      2     \u001b[36m7583.7740\u001b[0m  3.8064\n",
      "      3     \u001b[36m1635.4264\u001b[0m  3.8263\n",
      "      4      \u001b[36m867.5459\u001b[0m  3.8373\n",
      "      5      \u001b[36m570.8232\u001b[0m  3.8176\n",
      "      6      \u001b[36m460.9642\u001b[0m  3.8148\n",
      "      7      \u001b[36m378.9444\u001b[0m  3.8401\n",
      "      8      \u001b[36m314.2837\u001b[0m  3.8208\n",
      "      9      \u001b[36m285.7031\u001b[0m  3.8053\n",
      "     10      \u001b[36m222.8442\u001b[0m  3.8349\n",
      "     11      243.3427  3.8613\n",
      "     12      231.0373  3.9120\n",
      "     13      \u001b[36m218.4084\u001b[0m  3.8013\n",
      "     14      246.1222  3.8183\n",
      "     15      \u001b[36m214.3897\u001b[0m  3.8118\n",
      "     16      \u001b[36m190.3828\u001b[0m  3.8304\n",
      "     17      223.3780  3.8112\n",
      "     18      194.9175  3.8110\n",
      "     19      \u001b[36m167.7550\u001b[0m  3.8704\n",
      "     20      182.9535  3.7945\n",
      "     21      175.6132  3.8022\n",
      "     22      194.3481  3.8004\n",
      "     23      177.6409  3.8648\n",
      "     24      \u001b[36m135.0837\u001b[0m  3.7967\n",
      "     25      \u001b[36m131.4252\u001b[0m  3.8027\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2665172.8911\u001b[0m  3.8992\n",
      "      2    \u001b[36m83936.4922\u001b[0m  3.8173\n",
      "      3     \u001b[36m8689.8454\u001b[0m  3.8213\n",
      "      4      \u001b[36m982.4684\u001b[0m  3.9015\n",
      "      5      \u001b[36m506.0965\u001b[0m  3.8670\n",
      "      6      \u001b[36m354.5189\u001b[0m  3.8131\n",
      "      7      \u001b[36m285.2255\u001b[0m  3.8149\n",
      "      8      \u001b[36m246.0536\u001b[0m  3.8195\n",
      "      9      \u001b[36m225.0702\u001b[0m  3.8549\n",
      "     10      \u001b[36m183.5454\u001b[0m  3.8227\n",
      "     11      \u001b[36m156.8677\u001b[0m  3.8173\n",
      "     12      \u001b[36m144.4712\u001b[0m  3.8164\n",
      "     13      \u001b[36m129.5029\u001b[0m  3.9055\n",
      "     14      145.8525  3.8685\n",
      "     15      \u001b[36m124.0214\u001b[0m  3.8420\n",
      "     16      \u001b[36m122.3607\u001b[0m  3.8163\n",
      "     17       \u001b[36m97.4117\u001b[0m  3.8393\n",
      "     18      104.1292  3.8231\n",
      "     19      103.8634  3.8036\n",
      "     20      102.1583  3.8049\n",
      "     21      103.7022  3.8578\n",
      "     22       \u001b[36m88.5810\u001b[0m  3.8125\n",
      "     23       \u001b[36m77.2857\u001b[0m  3.8382\n",
      "     24       80.3941  3.8132\n",
      "     25       94.8102  3.8641\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m4087734.7486\u001b[0m  3.8334\n",
      "      2    \u001b[36m24004.0925\u001b[0m  3.8122\n",
      "      3     \u001b[36m1054.3527\u001b[0m  3.8387\n",
      "      4     \u001b[36m1032.8019\u001b[0m  3.8182\n",
      "      5      \u001b[36m701.7758\u001b[0m  3.8140\n",
      "      6      \u001b[36m538.5989\u001b[0m  3.8114\n",
      "      7      \u001b[36m452.6996\u001b[0m  3.8599\n",
      "      8      \u001b[36m388.2268\u001b[0m  3.8111\n",
      "      9      \u001b[36m316.0986\u001b[0m  3.8079\n",
      "     10      \u001b[36m299.6678\u001b[0m  3.8321\n",
      "     11      \u001b[36m259.9099\u001b[0m  3.8398\n",
      "     12      \u001b[36m235.4467\u001b[0m  3.8151\n",
      "     13      \u001b[36m212.0056\u001b[0m  3.8122\n",
      "     14      \u001b[36m197.5016\u001b[0m  3.8234\n",
      "     15      \u001b[36m170.3255\u001b[0m  3.8100\n",
      "     16      \u001b[36m149.0516\u001b[0m  3.8224\n",
      "     17      \u001b[36m144.0571\u001b[0m  3.8320\n",
      "     18      \u001b[36m139.3670\u001b[0m  3.8424\n",
      "     19      \u001b[36m108.9139\u001b[0m  3.8492\n",
      "     20      114.8301  3.8216\n",
      "     21      115.5775  3.8726\n",
      "     22      \u001b[36m103.0083\u001b[0m  3.8378\n",
      "     23      106.6545  3.8731\n",
      "     24       \u001b[36m81.8611\u001b[0m  3.8222\n",
      "     25       90.7775  3.8049\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1196443.5515\u001b[0m  3.8619\n",
      "      2     \u001b[36m5755.9049\u001b[0m  3.8188\n",
      "      3     5885.7431  3.8190\n",
      "      4     \u001b[36m3729.9300\u001b[0m  3.8227\n",
      "      5     \u001b[36m2290.1253\u001b[0m  3.8375\n",
      "      6     \u001b[36m1202.0150\u001b[0m  3.8085\n",
      "      7      \u001b[36m595.2641\u001b[0m  3.8196\n",
      "      8      \u001b[36m314.3638\u001b[0m  3.8402\n",
      "      9      \u001b[36m153.7279\u001b[0m  3.8096\n",
      "     10       \u001b[36m89.6442\u001b[0m  3.8108\n",
      "     11       \u001b[36m43.8461\u001b[0m  3.8131\n",
      "     12       \u001b[36m28.5934\u001b[0m  3.8654\n",
      "     13       \u001b[36m18.0065\u001b[0m  3.8146\n",
      "     14       18.1552  3.8186\n",
      "     15       18.8087  3.8136\n",
      "     16       18.5122  3.8440\n",
      "     17       \u001b[36m17.0388\u001b[0m  3.8114\n",
      "     18       \u001b[36m15.3628\u001b[0m  3.8201\n",
      "     19       \u001b[36m15.2763\u001b[0m  3.8152\n",
      "     20       17.2086  3.8346\n",
      "     21       18.3351  3.8086\n",
      "     22       18.3408  3.8153\n",
      "     23       16.2718  3.8128\n",
      "     24       16.1126  3.8504\n",
      "     25       15.6635  3.8079\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1828286.9704\u001b[0m  3.8746\n",
      "      2   \u001b[36m115969.7962\u001b[0m  3.8567\n",
      "      3    \u001b[36m12148.0055\u001b[0m  3.8202\n",
      "      4     \u001b[36m1423.8074\u001b[0m  3.8184\n",
      "      5      \u001b[36m422.9638\u001b[0m  3.8126\n",
      "      6      \u001b[36m212.5711\u001b[0m  3.8684\n",
      "      7      \u001b[36m133.6767\u001b[0m  3.8147\n",
      "      8       \u001b[36m99.1284\u001b[0m  3.8054\n",
      "      9       \u001b[36m78.9776\u001b[0m  3.8069\n",
      "     10       \u001b[36m67.0685\u001b[0m  3.8489\n",
      "     11       \u001b[36m60.6819\u001b[0m  3.8120\n",
      "     12       \u001b[36m53.9075\u001b[0m  3.8064\n",
      "     13       \u001b[36m48.4571\u001b[0m  3.8039\n",
      "     14       \u001b[36m43.5607\u001b[0m  3.8350\n",
      "     15       \u001b[36m40.4239\u001b[0m  3.8078\n",
      "     16       \u001b[36m37.4503\u001b[0m  3.8057\n",
      "     17       \u001b[36m33.9402\u001b[0m  3.7983\n",
      "     18       \u001b[36m30.0604\u001b[0m  3.8592\n",
      "     19       \u001b[36m29.6066\u001b[0m  3.8051\n",
      "     20       \u001b[36m27.0431\u001b[0m  3.8088\n",
      "     21       \u001b[36m26.5905\u001b[0m  3.8274\n",
      "     22       \u001b[36m23.5833\u001b[0m  3.8294\n",
      "     23       \u001b[36m21.1842\u001b[0m  3.8631\n",
      "     24       \u001b[36m20.4657\u001b[0m  3.8653\n",
      "     25       \u001b[36m20.2134\u001b[0m  3.8298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-28 23:10:46,840]\u001b[0m Trial 4 finished with value: 177.01128490058045 and parameters: {'levels': 11, 'hidden_units': 8, 'kernel_size': 4, 'dropout': 0.07327236865873835, 'learning_rate': 0.013481575603601416}. Best is trial 2 with value: 125.41270930622379.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1  \u001b[36m1452042.1934\u001b[0m  13.0292\n",
      "      2   \u001b[36m448697.3595\u001b[0m  12.9382\n",
      "      3   \u001b[36m247843.4624\u001b[0m  12.9817\n",
      "      4   \u001b[36m140825.8465\u001b[0m  13.0797\n",
      "      5    \u001b[36m58121.8732\u001b[0m  12.9727\n",
      "      6    85480.2217  12.9584\n",
      "      7    \u001b[36m32238.9918\u001b[0m  12.9692\n",
      "      8    35836.9040  12.9729\n",
      "      9    34446.3219  12.9125\n",
      "     10    \u001b[36m24227.1691\u001b[0m  12.9252\n",
      "     11    26507.2415  12.9582\n",
      "     12    \u001b[36m22224.7831\u001b[0m  12.9687\n",
      "     13    \u001b[36m20971.1763\u001b[0m  12.9681\n",
      "     14    \u001b[36m19260.3152\u001b[0m  13.0050\n",
      "     15    \u001b[36m18361.2483\u001b[0m  12.9487\n",
      "     16    \u001b[36m17110.5092\u001b[0m  12.9252\n",
      "     17    \u001b[36m15693.2886\u001b[0m  12.8957\n",
      "     18    15976.1002  12.9705\n",
      "     19    \u001b[36m15101.1048\u001b[0m  12.9501\n",
      "     20    \u001b[36m13125.6841\u001b[0m  12.9649\n",
      "     21    13820.4205  13.0105\n",
      "     22    \u001b[36m12575.5105\u001b[0m  12.9612\n",
      "     23    \u001b[36m12041.6777\u001b[0m  12.9558\n",
      "     24    \u001b[36m11860.9324\u001b[0m  12.8981\n",
      "     25    \u001b[36m10861.3677\u001b[0m  12.9271\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1  \u001b[36m5612443.5779\u001b[0m  12.9754\n",
      "      2   \u001b[36m578464.4224\u001b[0m  12.9945\n",
      "      3   768082.8976  13.0677\n",
      "      4   \u001b[36m130109.8494\u001b[0m  12.9475\n",
      "      5    \u001b[36m20575.1326\u001b[0m  13.0063\n",
      "      6    53520.1645  12.9727\n",
      "      7    66011.2218  12.9433\n",
      "      8    58467.4461  12.9649\n",
      "      9    31292.4633  12.9046\n",
      "     10    \u001b[36m15098.6273\u001b[0m  12.9700\n",
      "     11     \u001b[36m9617.2910\u001b[0m  12.9555\n",
      "     12    10222.9789  12.9563\n",
      "     13    11923.0474  12.9604\n",
      "     14    10653.6551  12.9786\n",
      "     15    10166.1081  12.9746\n",
      "     16     \u001b[36m8680.2181\u001b[0m  12.9267\n",
      "     17     \u001b[36m8477.4678\u001b[0m  12.9183\n",
      "     18     \u001b[36m8248.3772\u001b[0m  12.9463\n",
      "     19     \u001b[36m7942.7449\u001b[0m  12.9610\n",
      "     20     \u001b[36m7934.6094\u001b[0m  12.9803\n",
      "     21     \u001b[36m6759.1113\u001b[0m  12.9509\n",
      "     22     6934.5967  12.9389\n",
      "     23     7513.1917  12.9660\n",
      "     24     7254.4131  12.9239\n",
      "     25     7335.8756  12.9872\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1   \u001b[36m762603.2137\u001b[0m  13.0644\n",
      "      2   \u001b[36m302669.5666\u001b[0m  12.9195\n",
      "      3    \u001b[36m61097.8130\u001b[0m  12.9250\n",
      "      4    \u001b[36m11609.0149\u001b[0m  12.9523\n",
      "      5    30002.9389  12.9538\n",
      "      6    25153.7371  12.9481\n",
      "      7    11843.7229  13.0118\n",
      "      8     \u001b[36m4409.8613\u001b[0m  12.9078\n",
      "      9     \u001b[36m3835.5453\u001b[0m  12.9371\n",
      "     10     5199.1400  12.9781\n",
      "     11     4666.0093  12.9443\n",
      "     12     \u001b[36m3687.9270\u001b[0m  12.9485\n",
      "     13     \u001b[36m2944.3211\u001b[0m  12.9424\n",
      "     14     2996.1886  12.9442\n",
      "     15     3028.8153  12.9427\n",
      "     16     \u001b[36m2834.5227\u001b[0m  12.9224\n",
      "     17     \u001b[36m2542.9833\u001b[0m  12.9590\n",
      "     18     2705.1282  12.9615\n",
      "     19     \u001b[36m2430.8316\u001b[0m  12.9327\n",
      "     20     2565.9041  12.9471\n",
      "     21     \u001b[36m2186.2827\u001b[0m  12.9620\n",
      "     22     2274.9359  12.9323\n",
      "     23     \u001b[36m2100.0230\u001b[0m  12.8894\n",
      "     24     \u001b[36m2039.0215\u001b[0m  12.9016\n",
      "     25     2189.1653  12.9391\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch     train_loss      dur\n",
      "-------  -------------  -------\n",
      "      1  \u001b[36m28400221.3285\u001b[0m  12.9771\n",
      "      2  \u001b[36m5315774.3485\u001b[0m  12.9703\n",
      "      3  \u001b[36m1368546.6423\u001b[0m  12.9425\n",
      "      4   \u001b[36m399296.8174\u001b[0m  12.9635\n",
      "      5   \u001b[36m103670.1038\u001b[0m  12.9657\n",
      "      6    \u001b[36m17818.3953\u001b[0m  12.9492\n",
      "      7     \u001b[36m6378.3885\u001b[0m  12.9871\n",
      "      8    17843.7208  12.8904\n",
      "      9    21241.1046  12.9509\n",
      "     10    15623.9691  13.0147\n",
      "     11     8633.5560  12.9642\n",
      "     12     \u001b[36m4130.9653\u001b[0m  12.9886\n",
      "     13     \u001b[36m2858.8327\u001b[0m  12.9840\n",
      "     14     3038.5767  12.9481\n",
      "     15     3219.4971  12.9194\n",
      "     16     3052.9353  12.9249\n",
      "     17     3321.1176  12.9281\n",
      "     18     3191.5119  12.9517\n",
      "     19     3060.0759  12.9711\n",
      "     20     2962.2205  12.9456\n",
      "     21     \u001b[36m2708.8973\u001b[0m  12.9716\n",
      "     22     \u001b[36m2522.9139\u001b[0m  12.9116\n",
      "     23     2715.8784  12.9157\n",
      "     24     2741.2538  13.0575\n",
      "     25     2649.6611  13.0192\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1   \u001b[36m206102.8946\u001b[0m  12.9688\n",
      "      2    \u001b[36m92015.4477\u001b[0m  13.0062\n",
      "      3    \u001b[36m59648.7864\u001b[0m  13.0027\n",
      "      4    \u001b[36m41306.6266\u001b[0m  12.9860\n",
      "      5    \u001b[36m18774.7896\u001b[0m  12.9713\n",
      "      6    21614.1348  12.9655\n",
      "      7    \u001b[36m12783.3610\u001b[0m  12.9258\n",
      "      8    \u001b[36m12392.7988\u001b[0m  13.0309\n",
      "      9    \u001b[36m11032.6310\u001b[0m  13.0255\n",
      "     10     \u001b[36m8803.3240\u001b[0m  12.9458\n",
      "     11     9305.5494  12.9462\n",
      "     12     \u001b[36m7162.1844\u001b[0m  13.0324\n",
      "     13     \u001b[36m6608.3058\u001b[0m  12.9573\n",
      "     14     \u001b[36m6255.6335\u001b[0m  12.9640\n",
      "     15     6331.0191  12.8978\n",
      "     16     6297.0964  12.9711\n",
      "     17     \u001b[36m5697.4746\u001b[0m  12.9714\n",
      "     18     \u001b[36m5446.2913\u001b[0m  12.9537\n",
      "     19     \u001b[36m4497.9384\u001b[0m  12.9959\n",
      "     20     \u001b[36m4332.1038\u001b[0m  13.0302\n",
      "     21     \u001b[36m4117.1109\u001b[0m  13.0053\n",
      "     22     4454.4993  12.9352\n",
      "     23     \u001b[36m3859.0675\u001b[0m  12.9158\n",
      "     24     \u001b[36m3551.9964\u001b[0m  12.9516\n",
      "     25     \u001b[36m3494.2998\u001b[0m  12.9670\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1    \u001b[36m28878.8790\u001b[0m  12.9933\n",
      "      2    \u001b[36m12132.1278\u001b[0m  12.9619\n",
      "      3     \u001b[36m8312.1989\u001b[0m  12.9746\n",
      "      4     \u001b[36m3337.0601\u001b[0m  12.9822\n",
      "      5     \u001b[36m2886.6667\u001b[0m  13.0018\n",
      "      6     \u001b[36m1728.0633\u001b[0m  12.9806\n",
      "      7     \u001b[36m1502.3979\u001b[0m  12.9995\n",
      "      8      \u001b[36m986.2245\u001b[0m  12.9727\n",
      "      9      \u001b[36m880.0469\u001b[0m  12.9658\n",
      "     10      \u001b[36m737.0318\u001b[0m  12.9677\n",
      "     11      \u001b[36m706.1094\u001b[0m  12.9638\n",
      "     12      \u001b[36m554.5009\u001b[0m  12.9517\n",
      "     13      \u001b[36m551.0797\u001b[0m  12.9571\n",
      "     14      \u001b[36m537.4139\u001b[0m  12.9337\n",
      "     15      \u001b[36m493.6069\u001b[0m  12.9449\n",
      "     16      \u001b[36m465.8957\u001b[0m  12.9681\n",
      "     17      \u001b[36m452.9678\u001b[0m  12.9597\n",
      "     18      \u001b[36m426.6861\u001b[0m  12.9658\n",
      "     19      \u001b[36m374.9991\u001b[0m  12.9736\n",
      "     20      388.6173  12.9447\n",
      "     21      \u001b[36m363.0489\u001b[0m  12.9595\n",
      "     22      \u001b[36m344.7875\u001b[0m  12.9004\n",
      "     23      \u001b[36m316.6216\u001b[0m  12.9696\n",
      "     24      \u001b[36m295.4801\u001b[0m  12.9917\n",
      "     25      \u001b[36m275.2320\u001b[0m  12.9944\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1  \u001b[36m4509448.1880\u001b[0m  13.0386\n",
      "      2   \u001b[36m239630.4375\u001b[0m  13.0410\n",
      "      3   477696.1490  12.9681\n",
      "      4    \u001b[36m87877.0048\u001b[0m  12.9648\n",
      "      5    \u001b[36m10560.6493\u001b[0m  12.9332\n",
      "      6    22316.1555  12.9628\n",
      "      7    32250.8556  12.8956\n",
      "      8    29259.3869  13.0050\n",
      "      9    20507.0619  12.9602\n",
      "     10    12658.0690  12.9544\n",
      "     11     \u001b[36m8442.8682\u001b[0m  12.9731\n",
      "     12     \u001b[36m5629.0427\u001b[0m  12.9477\n",
      "     13     5883.1313  12.9531\n",
      "     14     6168.1897  12.9231\n",
      "     15     6177.6106  12.9259\n",
      "     16     5877.9019  12.9631\n",
      "     17     \u001b[36m5326.3357\u001b[0m  13.0046\n",
      "     18     \u001b[36m5047.7066\u001b[0m  13.0745\n",
      "     19     5092.2289  12.9476\n",
      "     20     \u001b[36m4981.4997\u001b[0m  12.9462\n",
      "     21     \u001b[36m4895.1701\u001b[0m  12.9160\n",
      "     22     \u001b[36m4598.2526\u001b[0m  12.8769\n",
      "     23     \u001b[36m4535.5902\u001b[0m  12.9522\n",
      "     24     4809.3071  12.9404\n",
      "     25     4743.3061  12.9415\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1  \u001b[36m1267448.6683\u001b[0m  13.0743\n",
      "      2   \u001b[36m561748.7523\u001b[0m  12.9723\n",
      "      3    \u001b[36m38842.4015\u001b[0m  12.9307\n",
      "      4   121688.9455  12.9618\n",
      "      5    76017.8914  12.9466\n",
      "      6    \u001b[36m14470.1566\u001b[0m  12.8887\n",
      "      7    17761.0853  12.9252\n",
      "      8    24402.9840  12.9671\n",
      "      9    \u001b[36m13261.9541\u001b[0m  12.9549\n",
      "     10     \u001b[36m8380.2204\u001b[0m  12.9918\n",
      "     11    10050.8271  12.9723\n",
      "     12     9703.8093  12.9528\n",
      "     13     \u001b[36m7959.2807\u001b[0m  12.9458\n",
      "     14     \u001b[36m7549.6261\u001b[0m  12.8880\n",
      "     15     8013.5270  12.9532\n",
      "     16     \u001b[36m7110.4978\u001b[0m  12.9661\n",
      "     17     \u001b[36m6983.8305\u001b[0m  12.9375\n",
      "     18     7101.7550  12.9543\n",
      "     19     7086.9820  13.0264\n",
      "     20     \u001b[36m6156.9990\u001b[0m  12.9493\n",
      "     21     \u001b[36m5941.2921\u001b[0m  12.9131\n",
      "     22     6043.4960  12.9227\n",
      "     23     \u001b[36m5869.3488\u001b[0m  12.9402\n",
      "     24     6010.9317  12.9590\n",
      "     25     \u001b[36m5207.3333\u001b[0m  12.9280\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1   \u001b[36m764575.7157\u001b[0m  12.9772\n",
      "      2   \u001b[36m230300.8602\u001b[0m  12.9408\n",
      "      3    \u001b[36m53460.9958\u001b[0m  13.0078\n",
      "      4     \u001b[36m7161.0528\u001b[0m  12.9593\n",
      "      5    18661.4025  12.9555\n",
      "      6    19593.4724  12.8631\n",
      "      7    10642.7748  12.9186\n",
      "      8     \u001b[36m5216.3163\u001b[0m  12.9255\n",
      "      9     \u001b[36m3267.3874\u001b[0m  12.9472\n",
      "     10     \u001b[36m3154.1026\u001b[0m  12.9592\n",
      "     11     3615.0353  12.9736\n",
      "     12     3241.4884  12.9318\n",
      "     13     \u001b[36m2690.4681\u001b[0m  12.9608\n",
      "     14     \u001b[36m2547.7927\u001b[0m  13.0205\n",
      "     15     \u001b[36m2289.2042\u001b[0m  12.9299\n",
      "     16     2418.9228  12.9519\n",
      "     17     2333.5083  12.9194\n",
      "     18     \u001b[36m2238.5526\u001b[0m  12.9297\n",
      "     19     \u001b[36m2089.0378\u001b[0m  12.9620\n",
      "     20     2089.7789  12.8962\n",
      "     21     \u001b[36m1986.3592\u001b[0m  12.9259\n",
      "     22     \u001b[36m1765.8015\u001b[0m  12.9848\n",
      "     23     1873.6728  12.9666\n",
      "     24     1767.8013  12.9852\n",
      "     25     \u001b[36m1728.1868\u001b[0m  13.0106\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1  \u001b[36m1735813.3978\u001b[0m  13.0226\n",
      "      2   \u001b[36m789557.5161\u001b[0m  12.9661\n",
      "      3    \u001b[36m59425.4418\u001b[0m  12.9571\n",
      "      4   179135.3829  12.9660\n",
      "      5    97410.1009  12.9141\n",
      "      6    \u001b[36m20945.6759\u001b[0m  12.9792\n",
      "      7    36435.1742  12.9684\n",
      "      8    35764.5289  12.9488\n",
      "      9    \u001b[36m16877.6922\u001b[0m  12.9469\n",
      "     10    \u001b[36m16044.6225\u001b[0m  12.9477\n",
      "     11    18446.6082  12.9457\n",
      "     12    \u001b[36m14504.2252\u001b[0m  12.9630\n",
      "     13    \u001b[36m12031.2088\u001b[0m  12.8941\n",
      "     14    13040.9634  12.9488\n",
      "     15    12824.6717  13.1131\n",
      "     16    \u001b[36m11237.8404\u001b[0m  12.9660\n",
      "     17    11508.0794  12.9537\n",
      "     18    \u001b[36m10763.2259\u001b[0m  12.9478\n",
      "     19    10770.3740  12.9256\n",
      "     20    \u001b[36m10359.3934\u001b[0m  12.8944\n",
      "     21    \u001b[36m10227.4398\u001b[0m  12.9242\n",
      "     22     \u001b[36m9890.6804\u001b[0m  12.9528\n",
      "     23     \u001b[36m9480.4481\u001b[0m  12.9648\n",
      "     24     \u001b[36m9339.1403\u001b[0m  12.9863\n",
      "     25     \u001b[36m8877.2355\u001b[0m  12.9665\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1   \u001b[36m563916.0673\u001b[0m  12.9983\n",
      "      2    \u001b[36m96116.2451\u001b[0m  12.9710\n",
      "      3   101119.4533  12.9519\n",
      "      4    \u001b[36m46919.3243\u001b[0m  12.9734\n",
      "      5    \u001b[36m32843.3816\u001b[0m  12.9033\n",
      "      6    \u001b[36m18906.9530\u001b[0m  12.9663\n",
      "      7    19782.9728  12.9492\n",
      "      8    \u001b[36m11190.4421\u001b[0m  13.0048\n",
      "      9    12716.7416  13.0948\n",
      "     10    \u001b[36m10159.4707\u001b[0m  13.0521\n",
      "     11     \u001b[36m9889.8637\u001b[0m  12.9718\n",
      "     12     \u001b[36m8730.8866\u001b[0m  12.9269\n",
      "     13     \u001b[36m8374.9913\u001b[0m  12.9191\n",
      "     14     \u001b[36m8040.7969\u001b[0m  12.9626\n",
      "     15     \u001b[36m6806.9902\u001b[0m  12.9588\n",
      "     16     7245.5710  12.9651\n",
      "     17     6881.2823  13.0694\n",
      "     18     \u001b[36m6430.8404\u001b[0m  13.0127\n",
      "     19     6446.6485  12.9756\n",
      "     20     \u001b[36m6027.8541\u001b[0m  12.8940\n",
      "     21     \u001b[36m5598.8889\u001b[0m  12.9702\n",
      "     22     5690.6426  12.9759\n",
      "     23     5675.7530  12.9679\n",
      "     24     \u001b[36m5296.6547\u001b[0m  12.9579\n",
      "     25     5339.3229  12.9592\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1   \u001b[36m611683.1848\u001b[0m  13.0708\n",
      "      2   \u001b[36m108280.6819\u001b[0m  12.9699\n",
      "      3   150495.9623  12.9676\n",
      "      4    \u001b[36m43373.9385\u001b[0m  12.9300\n",
      "      5    56619.6488  12.9072\n",
      "      6    \u001b[36m20021.4138\u001b[0m  12.9726\n",
      "      7    28533.1179  12.9545\n",
      "      8    \u001b[36m16305.0573\u001b[0m  12.9378\n",
      "      9    17652.7392  13.0082\n",
      "     10    \u001b[36m13944.4451\u001b[0m  12.9845\n",
      "     11    \u001b[36m13067.0539\u001b[0m  12.9605\n",
      "     12    \u001b[36m11976.7779\u001b[0m  12.9140\n",
      "     13    \u001b[36m10355.9659\u001b[0m  12.9336\n",
      "     14    10701.0593  12.9581\n",
      "     15    \u001b[36m10217.3539\u001b[0m  12.9709\n",
      "     16     \u001b[36m8937.7131\u001b[0m  12.9522\n",
      "     17     \u001b[36m8733.9631\u001b[0m  12.9712\n",
      "     18     \u001b[36m8596.2321\u001b[0m  12.9956\n",
      "     19     \u001b[36m7679.7991\u001b[0m  12.9214\n",
      "     20     \u001b[36m7602.7802\u001b[0m  12.9885\n",
      "     21     \u001b[36m6755.0503\u001b[0m  12.9693\n",
      "     22     7254.0402  12.9524\n",
      "     23     \u001b[36m6665.9563\u001b[0m  12.9505\n",
      "     24     \u001b[36m6451.4899\u001b[0m  12.9592\n",
      "     25     6650.7743  12.9577\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1   \u001b[36m926083.0963\u001b[0m  12.9870\n",
      "      2   \u001b[36m187115.3622\u001b[0m  12.9526\n",
      "      3   212936.0019  12.9674\n",
      "      4    \u001b[36m44730.3226\u001b[0m  12.9589\n",
      "      5    64576.1443  12.9399\n",
      "      6    \u001b[36m41261.4650\u001b[0m  12.9405\n",
      "      7    \u001b[36m20633.6150\u001b[0m  12.9533\n",
      "      8    25584.6252  12.9546\n",
      "      9    \u001b[36m14903.0091\u001b[0m  12.9256\n",
      "     10    \u001b[36m14710.4440\u001b[0m  12.9623\n",
      "     11    \u001b[36m12336.4468\u001b[0m  12.9112\n",
      "     12    \u001b[36m10696.0333\u001b[0m  12.9254\n",
      "     13    12005.5895  13.0334\n",
      "     14    10962.0910  12.9603\n",
      "     15     \u001b[36m9707.2578\u001b[0m  12.9621\n",
      "     16     \u001b[36m9377.3031\u001b[0m  12.9615\n",
      "     17     \u001b[36m8741.2280\u001b[0m  12.9390\n",
      "     18     9237.6164  12.9499\n",
      "     19     8813.4264  12.8771\n",
      "     20     \u001b[36m7637.4943\u001b[0m  12.9372\n",
      "     21     8283.0438  12.9393\n",
      "     22     \u001b[36m7137.2177\u001b[0m  12.9577\n",
      "     23     \u001b[36m6827.8427\u001b[0m  12.9860\n",
      "     24     7187.5863  12.9554\n",
      "     25     7086.5604  12.9364\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1  \u001b[36m2055379.4005\u001b[0m  13.0078\n",
      "      2   \u001b[36m806683.2876\u001b[0m  12.9770\n",
      "      3    \u001b[36m84935.6519\u001b[0m  12.9314\n",
      "      4    \u001b[36m61414.4443\u001b[0m  12.8808\n",
      "      5   101277.6297  12.9505\n",
      "      6    62159.2856  12.9824\n",
      "      7    \u001b[36m21408.8243\u001b[0m  12.9587\n",
      "      8     \u001b[36m7600.3719\u001b[0m  12.9698\n",
      "      9    10435.4326  12.9547\n",
      "     10    12375.5810  13.0015\n",
      "     11     9944.9411  12.8968\n",
      "     12     \u001b[36m7507.7245\u001b[0m  12.9560\n",
      "     13     \u001b[36m5938.9511\u001b[0m  12.9440\n",
      "     14     \u001b[36m5736.1276\u001b[0m  12.9516\n",
      "     15     6680.0286  13.0164\n",
      "     16     5738.7493  13.0183\n",
      "     17     \u001b[36m5334.3776\u001b[0m  12.9707\n",
      "     18     5635.5527  12.9196\n",
      "     19     \u001b[36m5257.4726\u001b[0m  12.9476\n",
      "     20     5418.3805  12.9627\n",
      "     21     \u001b[36m4951.3974\u001b[0m  12.9559\n",
      "     22     4964.3160  12.9607\n",
      "     23     4995.5535  12.9608\n",
      "     24     \u001b[36m4703.6318\u001b[0m  12.9589\n",
      "     25     \u001b[36m4515.2317\u001b[0m  13.0084\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch     train_loss      dur\n",
      "-------  -------------  -------\n",
      "      1  \u001b[36m11317882.5274\u001b[0m  13.0068\n",
      "      2   \u001b[36m591627.0744\u001b[0m  12.9426\n",
      "      3   \u001b[36m413769.7609\u001b[0m  12.8999\n",
      "      4   463383.2516  12.9422\n",
      "      5   \u001b[36m130309.6595\u001b[0m  12.9879\n",
      "      6    \u001b[36m21371.9177\u001b[0m  13.0754\n",
      "      7    \u001b[36m12584.9594\u001b[0m  12.9726\n",
      "      8    16853.4918  12.9569\n",
      "      9    21580.5570  13.0123\n",
      "     10    21484.9626  12.9279\n",
      "     11    16094.7605  12.8969\n",
      "     12    13117.9341  12.9536\n",
      "     13    \u001b[36m10008.3855\u001b[0m  12.9422\n",
      "     14     \u001b[36m7993.1186\u001b[0m  12.9415\n",
      "     15     \u001b[36m6726.7361\u001b[0m  12.9546\n",
      "     16     \u001b[36m6515.7002\u001b[0m  12.9447\n",
      "     17     \u001b[36m6078.7249\u001b[0m  12.9435\n",
      "     18     6495.5100  12.8975\n",
      "     19     \u001b[36m5998.1669\u001b[0m  12.9354\n",
      "     20     \u001b[36m5726.5270\u001b[0m  12.9227\n",
      "     21     6554.9972  12.9455\n",
      "     22     6197.4113  12.9543\n",
      "     23     \u001b[36m5428.4065\u001b[0m  12.9514\n",
      "     24     5719.7870  12.9484\n",
      "     25     5493.7380  13.0071\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1   \u001b[36m513372.0568\u001b[0m  13.0250\n",
      "      2    \u001b[36m94026.1327\u001b[0m  13.0104\n",
      "      3    \u001b[36m74812.4457\u001b[0m  12.9267\n",
      "      4    \u001b[36m13955.0472\u001b[0m  12.9578\n",
      "      5     \u001b[36m2119.2473\u001b[0m  12.9488\n",
      "      6     3218.5288  12.9996\n",
      "      7     4976.1210  12.9639\n",
      "      8     4906.5914  12.9568\n",
      "      9     3519.8445  12.9525\n",
      "     10     2461.0141  12.9269\n",
      "     11     \u001b[36m1471.7868\u001b[0m  12.9886\n",
      "     12     \u001b[36m1126.5184\u001b[0m  12.9446\n",
      "     13      \u001b[36m975.0010\u001b[0m  12.9700\n",
      "     14     1063.8567  12.9444\n",
      "     15     1106.8063  12.9739\n",
      "     16     1116.7226  13.0881\n",
      "     17      978.9850  12.9110\n",
      "     18      \u001b[36m911.8278\u001b[0m  12.9116\n",
      "     19      \u001b[36m856.0649\u001b[0m  12.9546\n",
      "     20      880.0353  12.9412\n",
      "     21      928.7671  13.0166\n",
      "     22      866.5865  12.9538\n",
      "     23      898.5749  12.9643\n",
      "     24      \u001b[36m793.2243\u001b[0m  12.9582\n",
      "     25      837.7505  12.8914\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1   \u001b[36m269808.2453\u001b[0m  13.0120\n",
      "      2   \u001b[36m111467.0526\u001b[0m  12.9299\n",
      "      3    \u001b[36m74343.5228\u001b[0m  12.9190\n",
      "      4    \u001b[36m47197.6837\u001b[0m  12.9579\n",
      "      5    \u001b[36m26616.2792\u001b[0m  12.9993\n",
      "      6    26759.2590  12.9647\n",
      "      7    \u001b[36m23681.4464\u001b[0m  12.9344\n",
      "      8    \u001b[36m17320.8120\u001b[0m  12.9445\n",
      "      9    18307.6214  12.9263\n",
      "     10    \u001b[36m16441.8447\u001b[0m  12.8804\n",
      "     11    \u001b[36m13886.6765\u001b[0m  12.9597\n",
      "     12    \u001b[36m12780.1381\u001b[0m  12.9357\n",
      "     13    \u001b[36m11212.5805\u001b[0m  12.9333\n",
      "     14    \u001b[36m10618.5410\u001b[0m  13.0666\n",
      "     15    \u001b[36m10350.3564\u001b[0m  12.9546\n",
      "     16     \u001b[36m9135.6114\u001b[0m  12.9515\n",
      "     17     \u001b[36m8524.2179\u001b[0m  12.8897\n",
      "     18     \u001b[36m8196.8188\u001b[0m  12.9557\n",
      "     19     \u001b[36m7591.7994\u001b[0m  12.9504\n",
      "     20     \u001b[36m7264.7214\u001b[0m  12.9812\n",
      "     21     \u001b[36m7162.5536\u001b[0m  13.0030\n",
      "     22     \u001b[36m6573.5537\u001b[0m  13.0079\n",
      "     23     \u001b[36m6126.8230\u001b[0m  13.0146\n",
      "     24     \u001b[36m5570.3179\u001b[0m  12.9504\n",
      "     25     5789.7336  13.0422\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1  \u001b[36m3890174.2920\u001b[0m  13.0062\n",
      "      2   \u001b[36m689192.6839\u001b[0m  12.8832\n",
      "      3   \u001b[36m415863.6195\u001b[0m  12.9446\n",
      "      4    \u001b[36m21896.0962\u001b[0m  13.0390\n",
      "      5    38961.2139  13.0214\n",
      "      6    67080.2219  12.9531\n",
      "      7    55324.3381  13.0134\n",
      "      8    25959.8670  12.9773\n",
      "      9     \u001b[36m9193.9650\u001b[0m  12.9325\n",
      "     10     \u001b[36m5415.7508\u001b[0m  12.9188\n",
      "     11     7439.0815  12.9558\n",
      "     12     7505.7776  12.9543\n",
      "     13     6264.1094  12.9684\n",
      "     14     \u001b[36m5184.3102\u001b[0m  12.9627\n",
      "     15     \u001b[36m4377.2459\u001b[0m  12.9566\n",
      "     16     4667.4628  13.0147\n",
      "     17     4806.5504  12.8823\n",
      "     18     4632.8844  12.9988\n",
      "     19     4527.6146  13.0418\n",
      "     20     4459.0006  12.9500\n",
      "     21     \u001b[36m4254.3598\u001b[0m  12.9599\n",
      "     22     \u001b[36m4182.2896\u001b[0m  12.9503\n",
      "     23     4333.3132  12.9384\n",
      "     24     \u001b[36m4013.8469\u001b[0m  12.8920\n",
      "     25     4175.9953  12.9261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-29 00:49:31,464]\u001b[0m Trial 5 finished with value: 183.47279702580352 and parameters: {'levels': 13, 'hidden_units': 10, 'kernel_size': 6, 'dropout': 0.1184829137724085, 'learning_rate': 0.0013033567475147442}. Best is trial 2 with value: 125.41270930622379.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m87171469.9944\u001b[0m  3.1136\n",
      "      2  \u001b[36m1297311.2682\u001b[0m  3.0851\n",
      "      3  3609046.5333  3.1276\n",
      "      4  7669477.8563  3.0823\n",
      "      5  2316681.1542  3.0741\n",
      "      6  \u001b[36m1167469.8436\u001b[0m  3.0774\n",
      "      7   \u001b[36m458391.7308\u001b[0m  3.0856\n",
      "      8   \u001b[36m180608.4852\u001b[0m  3.1308\n",
      "      9   \u001b[36m101553.2354\u001b[0m  3.0955\n",
      "     10   112363.6756  3.1031\n",
      "     11    \u001b[36m45930.0603\u001b[0m  3.0979\n",
      "     12    \u001b[36m34401.1001\u001b[0m  3.0982\n",
      "     13    36058.9921  3.2289\n",
      "     14    \u001b[36m28790.7239\u001b[0m  3.1212\n",
      "     15    \u001b[36m22767.0669\u001b[0m  3.1008\n",
      "     16    \u001b[36m18861.6166\u001b[0m  3.1032\n",
      "     17    \u001b[36m17691.0687\u001b[0m  3.0926\n",
      "     18    \u001b[36m16615.3639\u001b[0m  3.1456\n",
      "     19    \u001b[36m16101.5454\u001b[0m  3.1120\n",
      "     20    17254.3784  3.1036\n",
      "     21    \u001b[36m14496.7880\u001b[0m  3.2017\n",
      "     22    17008.4496  3.1803\n",
      "     23    \u001b[36m12482.6767\u001b[0m  3.1074\n",
      "     24    13545.0794  3.0968\n",
      "     25    12561.9723  3.0979\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch         train_loss     dur\n",
      "-------  -----------------  ------\n",
      "      1  \u001b[36m225415865229.1948\u001b[0m  3.1610\n",
      "      2  \u001b[36m38023917.7226\u001b[0m  3.0882\n",
      "      3  135868046.1314  3.0889\n",
      "      4  109838467.0949  3.0871\n",
      "      5  41650524.7883  3.0793\n",
      "      6  \u001b[36m29306989.5766\u001b[0m  3.1147\n",
      "      7  \u001b[36m27511920.4088\u001b[0m  3.0944\n",
      "      8  \u001b[36m25427688.5985\u001b[0m  3.0835\n",
      "      9  \u001b[36m20728274.6861\u001b[0m  3.0852\n",
      "     10  \u001b[36m16767675.5766\u001b[0m  3.1056\n",
      "     11  \u001b[36m12938177.4015\u001b[0m  3.1553\n",
      "     12  \u001b[36m9842893.0000\u001b[0m  3.0750\n",
      "     13  \u001b[36m8182547.6204\u001b[0m  3.1169\n",
      "     14  \u001b[36m7105773.7664\u001b[0m  3.0962\n",
      "     15  \u001b[36m5291754.2847\u001b[0m  3.1021\n",
      "     16  \u001b[36m4863355.9307\u001b[0m  3.1234\n",
      "     17  \u001b[36m4165127.7026\u001b[0m  3.0933\n",
      "     18  \u001b[36m3469539.8759\u001b[0m  3.0946\n",
      "     19  \u001b[36m2918888.5602\u001b[0m  3.0953\n",
      "     20  \u001b[36m2599153.1916\u001b[0m  3.1180\n",
      "     21  \u001b[36m2344856.8914\u001b[0m  3.0996\n",
      "     22  \u001b[36m1837586.0484\u001b[0m  3.0938\n",
      "     23  \u001b[36m1623681.8276\u001b[0m  3.0963\n",
      "     24  \u001b[36m1383661.6396\u001b[0m  3.0801\n",
      "     25  \u001b[36m1100851.9261\u001b[0m  3.0865\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m269430697.9478\u001b[0m  3.1563\n",
      "      2   \u001b[36m567106.4649\u001b[0m  3.0930\n",
      "      3   \u001b[36m487677.2281\u001b[0m  3.0865\n",
      "      4   607689.7856  3.1281\n",
      "      5   \u001b[36m429352.5671\u001b[0m  3.0867\n",
      "      6   \u001b[36m218071.0961\u001b[0m  3.0813\n",
      "      7   \u001b[36m159706.3853\u001b[0m  3.0830\n",
      "      8   \u001b[36m119088.8331\u001b[0m  3.0779\n",
      "      9   142930.9827  3.1347\n",
      "     10   \u001b[36m100944.0898\u001b[0m  3.1174\n",
      "     11    \u001b[36m65959.8414\u001b[0m  3.0948\n",
      "     12    \u001b[36m62454.5187\u001b[0m  3.0822\n",
      "     13    73676.6421  3.0891\n",
      "     14    \u001b[36m45442.4003\u001b[0m  3.1076\n",
      "     15    \u001b[36m39444.9827\u001b[0m  3.0884\n",
      "     16    50830.2985  3.0838\n",
      "     17    41486.2553  3.0868\n",
      "     18    \u001b[36m27031.4223\u001b[0m  3.1840\n",
      "     19    46799.4388  3.1491\n",
      "     20    \u001b[36m24555.2519\u001b[0m  3.0815\n",
      "     21    38484.4656  3.1474\n",
      "     22    45324.2013  3.1879\n",
      "     23    \u001b[36m18019.2771\u001b[0m  3.2036\n",
      "     24    24112.8002  3.0934\n",
      "     25    \u001b[36m17519.6566\u001b[0m  3.0724\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m90402730.2224\u001b[0m  3.1330\n",
      "      2  \u001b[36m15734097.3841\u001b[0m  3.1055\n",
      "      3  \u001b[36m1378926.9580\u001b[0m  3.0770\n",
      "      4  1639359.2534  3.0882\n",
      "      5    \u001b[36m86112.2860\u001b[0m  3.0919\n",
      "      6    \u001b[36m14177.4996\u001b[0m  3.0880\n",
      "      7    23061.3644  3.1295\n",
      "      8     \u001b[36m5680.3417\u001b[0m  3.0880\n",
      "      9      \u001b[36m749.8630\u001b[0m  3.0862\n",
      "     10      \u001b[36m570.2195\u001b[0m  3.0858\n",
      "     11      598.6545  3.1983\n",
      "     12      \u001b[36m505.0066\u001b[0m  3.1112\n",
      "     13      590.9150  3.0964\n",
      "     14      \u001b[36m408.1433\u001b[0m  3.0930\n",
      "     15      412.0590  3.0796\n",
      "     16      \u001b[36m296.3381\u001b[0m  3.1221\n",
      "     17      \u001b[36m263.3964\u001b[0m  3.1177\n",
      "     18      \u001b[36m262.1242\u001b[0m  3.1084\n",
      "     19      \u001b[36m196.6957\u001b[0m  3.0781\n",
      "     20      \u001b[36m171.1632\u001b[0m  3.0894\n",
      "     21      173.5178  3.1127\n",
      "     22      \u001b[36m161.9497\u001b[0m  3.0856\n",
      "     23      \u001b[36m136.3933\u001b[0m  3.0870\n",
      "     24      \u001b[36m135.2789\u001b[0m  3.0839\n",
      "     25      159.2865  3.0826\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch         train_loss     dur\n",
      "-------  -----------------  ------\n",
      "      1  \u001b[36m603081288011.9662\u001b[0m  3.1042\n",
      "      2  \u001b[36m23070666.5401\u001b[0m  3.1605\n",
      "      3  173617659.0949  3.1155\n",
      "      4  354288992.0000  3.0856\n",
      "      5  407023610.1606  3.1426\n",
      "      6  283427743.1825  3.1379\n",
      "      7  201938467.6204  3.1858\n",
      "      8  76866159.2847  3.1968\n",
      "      9  \u001b[36m14840933.3139\u001b[0m  3.1570\n",
      "     10  \u001b[36m9597041.7299\u001b[0m  3.1179\n",
      "     11  10403290.4380  3.0880\n",
      "     12  \u001b[36m6337340.6752\u001b[0m  3.0882\n",
      "     13  \u001b[36m4342684.5018\u001b[0m  3.0875\n",
      "     14  \u001b[36m3224720.6058\u001b[0m  3.1419\n",
      "     15  \u001b[36m1752060.9188\u001b[0m  3.0877\n",
      "     16  1753351.7609  3.0832\n",
      "     17  \u001b[36m1455910.9635\u001b[0m  3.0786\n",
      "     18  1859442.5858  3.0937\n",
      "     19  1541895.7655  3.1614\n",
      "     20  \u001b[36m1176890.1401\u001b[0m  3.1169\n",
      "     21  1698114.4489  3.0964\n",
      "     22  1570069.5693  3.0879\n",
      "     23  1475930.5730  3.0874\n",
      "     24  \u001b[36m1066152.4836\u001b[0m  3.1536\n",
      "     25  1182023.7824  3.1147\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch         train_loss     dur\n",
      "-------  -----------------  ------\n",
      "      1  \u001b[36m666814902923.2623\u001b[0m  3.1248\n",
      "      2  \u001b[36m144366285.5474\u001b[0m  3.1055\n",
      "      3  963799894.4234  3.1149\n",
      "      4  818031564.3796  3.0900\n",
      "      5  166145180.7445  3.0851\n",
      "      6  157250703.8832  3.0978\n",
      "      7  \u001b[36m100844066.8029\u001b[0m  3.1283\n",
      "      8  \u001b[36m71803294.0146\u001b[0m  3.1130\n",
      "      9  \u001b[36m63483949.0803\u001b[0m  3.0988\n",
      "     10  64478484.5547  3.0884\n",
      "     11  \u001b[36m57086008.5547\u001b[0m  3.1249\n",
      "     12  \u001b[36m48935607.7372\u001b[0m  3.1945\n",
      "     13  \u001b[36m38250196.2628\u001b[0m  3.0797\n",
      "     14  \u001b[36m34424369.9708\u001b[0m  3.1412\n",
      "     15  \u001b[36m31363342.9489\u001b[0m  3.1818\n",
      "     16  \u001b[36m24601754.2336\u001b[0m  3.0837\n",
      "     17  \u001b[36m21717528.8978\u001b[0m  3.1082\n",
      "     18  \u001b[36m21226752.0438\u001b[0m  3.0673\n",
      "     19  \u001b[36m19040658.0730\u001b[0m  3.0673\n",
      "     20  \u001b[36m17312486.1971\u001b[0m  3.0673\n",
      "     21  \u001b[36m16422742.6642\u001b[0m  3.0649\n",
      "     22  \u001b[36m15208177.0949\u001b[0m  3.1631\n",
      "     23  \u001b[36m13573648.6058\u001b[0m  3.1100\n",
      "     24  13824176.4964  3.0957\n",
      "     25  \u001b[36m12006464.0146\u001b[0m  3.0975\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m921452.4545\u001b[0m  3.1028\n",
      "      2    \u001b[36m24939.7169\u001b[0m  3.0872\n",
      "      3     \u001b[36m8692.9898\u001b[0m  3.0837\n",
      "      4    14736.5202  3.0835\n",
      "      5    14486.6375  3.1144\n",
      "      6     \u001b[36m6206.0998\u001b[0m  3.1161\n",
      "      7     \u001b[36m4359.0782\u001b[0m  3.0812\n",
      "      8     \u001b[36m2887.0648\u001b[0m  3.0875\n",
      "      9     \u001b[36m2314.0882\u001b[0m  3.0963\n",
      "     10     \u001b[36m1402.6794\u001b[0m  3.1146\n",
      "     11     \u001b[36m1338.5026\u001b[0m  3.1031\n",
      "     12     1346.1287  3.0882\n",
      "     13     \u001b[36m1260.0917\u001b[0m  3.0904\n",
      "     14      \u001b[36m838.6202\u001b[0m  3.0874\n",
      "     15      \u001b[36m836.3552\u001b[0m  3.1411\n",
      "     16      \u001b[36m648.9789\u001b[0m  3.0968\n",
      "     17      \u001b[36m395.0334\u001b[0m  3.1122\n",
      "     18      440.3105  3.1335\n",
      "     19      \u001b[36m366.0757\u001b[0m  3.1054\n",
      "     20      454.4754  3.1429\n",
      "     21      \u001b[36m312.8879\u001b[0m  3.0912\n",
      "     22      355.4013  3.1045\n",
      "     23      \u001b[36m208.2785\u001b[0m  3.0997\n",
      "     24      347.9083  3.0946\n",
      "     25      303.2519  3.1467\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1482407.7064\u001b[0m  3.1244\n",
      "      2   \u001b[36m150076.0821\u001b[0m  3.0797\n",
      "      3    \u001b[36m99597.5909\u001b[0m  3.1162\n",
      "      4    \u001b[36m28579.7010\u001b[0m  3.1109\n",
      "      5     \u001b[36m8374.9261\u001b[0m  3.0777\n",
      "      6     8625.4755  3.0727\n",
      "      7     \u001b[36m3788.5279\u001b[0m  3.0729\n",
      "      8     \u001b[36m3003.6333\u001b[0m  3.1488\n",
      "      9     \u001b[36m1302.1207\u001b[0m  3.0936\n",
      "     10     1345.7425  3.0800\n",
      "     11     \u001b[36m1181.5447\u001b[0m  3.0739\n",
      "     12      \u001b[36m926.3243\u001b[0m  3.0842\n",
      "     13      \u001b[36m832.4577\u001b[0m  3.1445\n",
      "     14      \u001b[36m595.0270\u001b[0m  3.0905\n",
      "     15      637.5163  3.1060\n",
      "     16      \u001b[36m429.5859\u001b[0m  3.1066\n",
      "     17      \u001b[36m370.8492\u001b[0m  3.0980\n",
      "     18      461.6674  3.1272\n",
      "     19      \u001b[36m315.8247\u001b[0m  3.1104\n",
      "     20      416.8732  3.1166\n",
      "     21      \u001b[36m299.3187\u001b[0m  3.0954\n",
      "     22      \u001b[36m283.3571\u001b[0m  3.0944\n",
      "     23      \u001b[36m268.8376\u001b[0m  3.1486\n",
      "     24      289.7974  3.0840\n",
      "     25      297.5788  3.1027\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch       train_loss     dur\n",
      "-------  ---------------  ------\n",
      "      1  \u001b[36m4430840895.8601\u001b[0m  3.1163\n",
      "      2  \u001b[36m2164066.9120\u001b[0m  3.1057\n",
      "      3  28756962.5985  3.0881\n",
      "      4  11327561.9270  3.0815\n",
      "      5  9042253.7938  3.0764\n",
      "      6  5198830.1496  3.1248\n",
      "      7  2666452.7719  3.0831\n",
      "      8  2359531.9891  3.0894\n",
      "      9  \u001b[36m1786798.0976\u001b[0m  3.0793\n",
      "     10  \u001b[36m1368241.9361\u001b[0m  3.0783\n",
      "     11  \u001b[36m1302829.1396\u001b[0m  3.1374\n",
      "     12  \u001b[36m1013739.3577\u001b[0m  3.0801\n",
      "     13   \u001b[36m903551.9220\u001b[0m  3.0924\n",
      "     14   \u001b[36m808617.5780\u001b[0m  3.0921\n",
      "     15   \u001b[36m713603.7395\u001b[0m  3.0889\n",
      "     16   \u001b[36m623781.0949\u001b[0m  3.1101\n",
      "     17   \u001b[36m534849.6715\u001b[0m  3.0756\n",
      "     18   \u001b[36m522761.3203\u001b[0m  3.0812\n",
      "     19   \u001b[36m486146.6099\u001b[0m  3.0826\n",
      "     20   488649.6499  3.1201\n",
      "     21   \u001b[36m455455.2409\u001b[0m  3.1931\n",
      "     22   \u001b[36m411825.6346\u001b[0m  3.0917\n",
      "     23   \u001b[36m330350.9172\u001b[0m  3.0895\n",
      "     24   \u001b[36m320068.8095\u001b[0m  3.0869\n",
      "     25   383156.6049  3.1193\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch         train_loss     dur\n",
      "-------  -----------------  ------\n",
      "      1  \u001b[36m185361088697.1236\u001b[0m  3.1708\n",
      "      2  \u001b[36m61447748.9051\u001b[0m  3.0913\n",
      "      3  924648063.5328  3.0894\n",
      "      4  196344555.6533  3.1551\n",
      "      5  \u001b[36m4573948.6405\u001b[0m  3.1020\n",
      "      6  41180340.7883  3.0909\n",
      "      7  71540593.4599  3.0886\n",
      "      8  26755059.4343  3.0912\n",
      "      9  \u001b[36m1605541.4325\u001b[0m  3.1179\n",
      "     10  5734670.7883  3.1031\n",
      "     11  2775460.3613  3.1005\n",
      "     12  1672061.2404  3.1264\n",
      "     13   \u001b[36m575278.5141\u001b[0m  3.0863\n",
      "     14   \u001b[36m272155.9781\u001b[0m  3.1538\n",
      "     15   379282.8942  3.0805\n",
      "     16   344067.3755  3.0800\n",
      "     17   370662.7815  3.0869\n",
      "     18   340525.1829  3.1149\n",
      "     19   294093.3797  3.1118\n",
      "     20   302910.5536  3.0908\n",
      "     21   276413.6569  3.0874\n",
      "     22   276699.5328  3.0840\n",
      "     23   \u001b[36m244645.1022\u001b[0m  3.1091\n",
      "     24   \u001b[36m208253.7346\u001b[0m  3.0982\n",
      "     25   259603.0516  3.0834\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m24372364.3219\u001b[0m  3.2150\n",
      "      2   \u001b[36m602366.2858\u001b[0m  3.1193\n",
      "      3   \u001b[36m161109.2104\u001b[0m  3.0816\n",
      "      4   \u001b[36m130668.3919\u001b[0m  3.0890\n",
      "      5    \u001b[36m38230.9271\u001b[0m  3.0870\n",
      "      6    52233.9430  3.0845\n",
      "      7    \u001b[36m25433.0578\u001b[0m  3.1716\n",
      "      8    \u001b[36m20744.5377\u001b[0m  3.1110\n",
      "      9    \u001b[36m10662.2524\u001b[0m  3.0926\n",
      "     10     \u001b[36m9422.0480\u001b[0m  3.0877\n",
      "     11     \u001b[36m5861.3840\u001b[0m  3.0792\n",
      "     12     \u001b[36m4085.6383\u001b[0m  3.1105\n",
      "     13     \u001b[36m3881.4614\u001b[0m  3.0784\n",
      "     14     \u001b[36m3385.3148\u001b[0m  3.0851\n",
      "     15     3669.1837  3.0931\n",
      "     16     3459.6118  3.1202\n",
      "     17     \u001b[36m2378.4478\u001b[0m  3.1256\n",
      "     18     2514.5633  3.1317\n",
      "     19     \u001b[36m2366.4067\u001b[0m  3.0975\n",
      "     20     \u001b[36m1811.6392\u001b[0m  3.0940\n",
      "     21     1964.6612  3.1361\n",
      "     22     1907.6785  3.1282\n",
      "     23     \u001b[36m1597.8571\u001b[0m  3.0909\n",
      "     24     \u001b[36m1508.3055\u001b[0m  3.0911\n",
      "     25     \u001b[36m1386.2836\u001b[0m  3.0883\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch          train_loss     dur\n",
      "-------  ------------------  ------\n",
      "      1  \u001b[36m3471225317412.8506\u001b[0m  3.1364\n",
      "      2  \u001b[36m4089418.8102\u001b[0m  3.0796\n",
      "      3  69643899.0949  3.0797\n",
      "      4  530571509.2555  3.0839\n",
      "      5  483229699.2701  3.1250\n",
      "      6  183724112.2920  3.1344\n",
      "      7  27157981.9270  3.1305\n",
      "      8  66062378.4526  3.1636\n",
      "      9  47606543.8248  3.0826\n",
      "     10  36891026.0438  3.1375\n",
      "     11  37687871.6496  3.0817\n",
      "     12  34686424.3650  3.1848\n",
      "     13  34398820.4818  3.0901\n",
      "     14  28797535.9708  3.1133\n",
      "     15  27490894.4088  3.0909\n",
      "     16  22262203.1533  3.0911\n",
      "     17  19497883.2263  3.0805\n",
      "     18  18655197.1533  3.0884\n",
      "     19  18749738.4964  3.0925\n",
      "     20  18171873.8978  3.1494\n",
      "     21  16990768.1095  3.1152\n",
      "     22  16037617.7883  3.1088\n",
      "     23  16543546.8905  3.0934\n",
      "     24  14601625.2263  3.1284\n",
      "     25  14494658.1752  3.0860\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1258715.6653\u001b[0m  3.1477\n",
      "      2      \u001b[36m408.0496\u001b[0m  3.0905\n",
      "      3    23594.6913  3.1327\n",
      "      4    44807.4400  3.0967\n",
      "      5     6278.5930  3.0946\n",
      "      6     1283.8003  3.1050\n",
      "      7      \u001b[36m358.6349\u001b[0m  3.0933\n",
      "      8      \u001b[36m143.7346\u001b[0m  3.1146\n",
      "      9      203.0415  3.0961\n",
      "     10      176.4738  3.1017\n",
      "     11      219.3075  3.0922\n",
      "     12      191.7950  3.1139\n",
      "     13      190.1200  3.1022\n",
      "     14      190.1906  3.0903\n",
      "     15       \u001b[36m88.4539\u001b[0m  3.1022\n",
      "     16       \u001b[36m84.6682\u001b[0m  3.1346\n",
      "     17       \u001b[36m76.8833\u001b[0m  3.0934\n",
      "     18       \u001b[36m61.0739\u001b[0m  3.1125\n",
      "     19       \u001b[36m47.3941\u001b[0m  3.0859\n",
      "     20       \u001b[36m43.3054\u001b[0m  3.0704\n",
      "     21       \u001b[36m32.8105\u001b[0m  3.0583\n",
      "     22       32.9130  3.1085\n",
      "     23      268.4403  3.0722\n",
      "     24       \u001b[36m18.5520\u001b[0m  3.0713\n",
      "     25       18.6120  3.0991\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch         train_loss     dur\n",
      "-------  -----------------  ------\n",
      "      1  \u001b[36m279796849440.3020\u001b[0m  3.1693\n",
      "      2  \u001b[36m61225918.2482\u001b[0m  3.0918\n",
      "      3  360101165.3139  3.1464\n",
      "      4  94941482.2774  3.1135\n",
      "      5  77674522.8613  3.1059\n",
      "      6  \u001b[36m48974937.2555\u001b[0m  3.1635\n",
      "      7  \u001b[36m42555638.4818\u001b[0m  3.1074\n",
      "      8  \u001b[36m27272852.3504\u001b[0m  3.1041\n",
      "      9  \u001b[36m18486940.4818\u001b[0m  3.0992\n",
      "     10  \u001b[36m13267493.0219\u001b[0m  3.1241\n",
      "     11  \u001b[36m9980059.2299\u001b[0m  3.1279\n",
      "     12  \u001b[36m8165391.5401\u001b[0m  3.0983\n",
      "     13  \u001b[36m7525997.8120\u001b[0m  3.1006\n",
      "     14  \u001b[36m6075611.1350\u001b[0m  3.1008\n",
      "     15  \u001b[36m4556471.5073\u001b[0m  3.1433\n",
      "     16  \u001b[36m4406858.1095\u001b[0m  3.0921\n",
      "     17  \u001b[36m3706611.3869\u001b[0m  3.0944\n",
      "     18  \u001b[36m3695316.1588\u001b[0m  3.0887\n",
      "     19  \u001b[36m3355265.9781\u001b[0m  3.0843\n",
      "     20  \u001b[36m2888101.8431\u001b[0m  3.1109\n",
      "     21  \u001b[36m2843912.9964\u001b[0m  3.1066\n",
      "     22  \u001b[36m2669112.6113\u001b[0m  3.0946\n",
      "     23  \u001b[36m2480862.3212\u001b[0m  3.1301\n",
      "     24  \u001b[36m2429526.7062\u001b[0m  3.0937\n",
      "     25  2723879.4037  3.1367\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch        train_loss     dur\n",
      "-------  ----------------  ------\n",
      "      1  \u001b[36m41231650669.4837\u001b[0m  3.1137\n",
      "      2   \u001b[36m307469.5285\u001b[0m  3.0817\n",
      "      3  1640568.8613  3.1067\n",
      "      4  15323147.0219  3.1272\n",
      "      5  12928171.4343  3.0770\n",
      "      6  13039872.2336  3.1125\n",
      "      7  12565296.4234  3.1219\n",
      "      8  7276741.9161  3.1113\n",
      "      9  6539930.7117  3.1627\n",
      "     10  5794964.4818  3.1459\n",
      "     11  3917447.1898  3.1865\n",
      "     12  3707965.6533  3.1757\n",
      "     13  3865663.3066  3.1443\n",
      "     14  3130845.4690  3.1013\n",
      "     15  2329116.2226  3.0995\n",
      "     16  2099767.6661  3.0922\n",
      "     17  1645954.4945  3.0855\n",
      "     18  1519554.4599  3.1236\n",
      "     19  1895756.2765  3.1393\n",
      "     20  1991707.4635  3.1180\n",
      "     21  1769299.7810  3.0946\n",
      "     22  1381384.2993  3.0883\n",
      "     23  1158440.5347  3.1405\n",
      "     24  1084472.9790  3.0907\n",
      "     25   947425.3412  3.0902\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch        train_loss     dur\n",
      "-------  ----------------  ------\n",
      "      1  \u001b[36m51706811866.8962\u001b[0m  3.1437\n",
      "      2  \u001b[36m5807481.5036\u001b[0m  3.1259\n",
      "      3  87230391.9416  3.1003\n",
      "      4  227847572.6715  3.0942\n",
      "      5  55807784.2628  3.0972\n",
      "      6  16728344.1022  3.1392\n",
      "      7  6571180.2135  3.0831\n",
      "      8  \u001b[36m2233253.5611\u001b[0m  3.0815\n",
      "      9   \u001b[36m826681.7614\u001b[0m  3.0813\n",
      "     10   955279.2860  3.0755\n",
      "     11   883662.2924  3.1212\n",
      "     12   \u001b[36m784588.1615\u001b[0m  3.0826\n",
      "     13   \u001b[36m745021.0716\u001b[0m  3.0844\n",
      "     14   856432.7445  3.0898\n",
      "     15   771723.5046  3.0901\n",
      "     16   998372.7947  3.1153\n",
      "     17   795195.8139  3.0861\n",
      "     18   \u001b[36m732625.5255\u001b[0m  3.0906\n",
      "     19   735850.2185  3.0886\n",
      "     20   806118.2984  3.1030\n",
      "     21   \u001b[36m654190.2295\u001b[0m  3.1545\n",
      "     22   685535.4594  3.0810\n",
      "     23   \u001b[36m586668.3161\u001b[0m  3.0886\n",
      "     24   692583.6948  3.0902\n",
      "     25   617196.2911  3.0885\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m823905443.4714\u001b[0m  3.1303\n",
      "      2  \u001b[36m3295282.0794\u001b[0m  3.0956\n",
      "      3   \u001b[36m601419.5039\u001b[0m  3.0908\n",
      "      4   \u001b[36m453970.8391\u001b[0m  3.1134\n",
      "      5   \u001b[36m261171.4742\u001b[0m  3.1300\n",
      "      6    \u001b[36m38990.1239\u001b[0m  3.0964\n",
      "      7   105134.9179  3.0887\n",
      "      8    87642.8459  3.1015\n",
      "      9    \u001b[36m14604.6208\u001b[0m  3.1410\n",
      "     10    32189.7545  3.0737\n",
      "     11    \u001b[36m12163.1858\u001b[0m  3.0784\n",
      "     12    13151.5731  3.0745\n",
      "     13    19638.3092  3.0777\n",
      "     14     \u001b[36m9705.7646\u001b[0m  3.1116\n",
      "     15    11759.8846  3.1152\n",
      "     16     \u001b[36m7440.9347\u001b[0m  3.1000\n",
      "     17    15402.4683  3.1057\n",
      "     18     \u001b[36m7233.0262\u001b[0m  3.0925\n",
      "     19     7569.9053  3.1412\n",
      "     20    12954.4329  3.0986\n",
      "     21     \u001b[36m6767.6765\u001b[0m  3.1016\n",
      "     22     8115.4011  3.0967\n",
      "     23     8785.4484  3.0964\n",
      "     24    10953.0629  3.2151\n",
      "     25    11158.8589  3.1007\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch        train_loss     dur\n",
      "-------  ----------------  ------\n",
      "      1  \u001b[36m30278141300.5255\u001b[0m  3.0990\n",
      "      2  \u001b[36m10901708.1752\u001b[0m  3.1223\n",
      "      3  222597937.7518  3.0721\n",
      "      4  298532709.2555  3.0981\n",
      "      5  145901552.1168  3.0992\n",
      "      6  67846102.8613  3.0871\n",
      "      7  11205387.7956  3.1173\n",
      "      8  12199042.8504  3.0832\n",
      "      9  \u001b[36m5598240.2336\u001b[0m  3.1013\n",
      "     10  5904915.6204  3.1210\n",
      "     11  \u001b[36m5133967.5949\u001b[0m  3.1063\n",
      "     12  \u001b[36m5115964.2445\u001b[0m  3.1453\n",
      "     13  \u001b[36m5086901.7993\u001b[0m  3.0957\n",
      "     14  \u001b[36m4949893.2737\u001b[0m  3.1020\n",
      "     15  \u001b[36m4751497.2299\u001b[0m  3.0941\n",
      "     16  \u001b[36m4107228.7865\u001b[0m  3.0944\n",
      "     17  \u001b[36m3659176.1515\u001b[0m  3.1347\n",
      "     18  3813681.1369  3.1608\n",
      "     19  \u001b[36m3578773.6131\u001b[0m  3.1476\n",
      "     20  \u001b[36m3324429.2682\u001b[0m  3.1218\n",
      "     21  3582927.8230  3.1158\n",
      "     22  \u001b[36m2835636.7609\u001b[0m  3.0992\n",
      "     23  3157310.5949  3.1078\n",
      "     24  \u001b[36m2694890.6369\u001b[0m  3.1216\n",
      "     25  \u001b[36m2548827.9745\u001b[0m  3.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-29 01:14:00,204]\u001b[0m Trial 6 finished with value: 185.8315131304004 and parameters: {'levels': 11, 'hidden_units': 9, 'kernel_size': 2, 'dropout': 0.18977710745066667, 'learning_rate': 0.24659691172104828}. Best is trial 2 with value: 125.41270930622379.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1760978.8847\u001b[0m  5.8869\n",
      "      2    \u001b[36m17173.7633\u001b[0m  5.8163\n",
      "      3     \u001b[36m1490.2790\u001b[0m  5.8436\n",
      "      4     \u001b[36m1127.4064\u001b[0m  5.8321\n",
      "      5      \u001b[36m842.1285\u001b[0m  5.8545\n",
      "      6      \u001b[36m584.1625\u001b[0m  5.8577\n",
      "      7      \u001b[36m394.4980\u001b[0m  5.8094\n",
      "      8      \u001b[36m340.8007\u001b[0m  5.8734\n",
      "      9      340.9987  5.8191\n",
      "     10      \u001b[36m305.9722\u001b[0m  5.8247\n",
      "     11      \u001b[36m261.7824\u001b[0m  5.8556\n",
      "     12      \u001b[36m236.0139\u001b[0m  5.8312\n",
      "     13      264.7327  5.8745\n",
      "     14      282.2684  5.8230\n",
      "     15      \u001b[36m219.5848\u001b[0m  5.8236\n",
      "     16      \u001b[36m208.7805\u001b[0m  5.9054\n",
      "     17      227.3462  5.8381\n",
      "     18      \u001b[36m155.9629\u001b[0m  5.8604\n",
      "     19      198.8633  5.7994\n",
      "     20      185.0721  5.8095\n",
      "     21      171.9505  5.8434\n",
      "     22      167.6291  5.8014\n",
      "     23      158.0085  5.8355\n",
      "     24      174.2989  5.8359\n",
      "     25      164.5642  5.8052\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2126885.7452\u001b[0m  5.9456\n",
      "      2    \u001b[36m40464.5300\u001b[0m  5.9007\n",
      "      3     \u001b[36m2153.8628\u001b[0m  5.8567\n",
      "      4      \u001b[36m403.4260\u001b[0m  5.8313\n",
      "      5      \u001b[36m160.3836\u001b[0m  5.8523\n",
      "      6      \u001b[36m127.0176\u001b[0m  5.8519\n",
      "      7      \u001b[36m102.0906\u001b[0m  5.8973\n",
      "      8       \u001b[36m92.2160\u001b[0m  5.8689\n",
      "      9       \u001b[36m78.1325\u001b[0m  5.8246\n",
      "     10       91.2625  5.8728\n",
      "     11       79.9397  5.8780\n",
      "     12       \u001b[36m74.8322\u001b[0m  5.8511\n",
      "     13       \u001b[36m74.4323\u001b[0m  5.8799\n",
      "     14       76.5235  5.8359\n",
      "     15       \u001b[36m70.7052\u001b[0m  5.8607\n",
      "     16       \u001b[36m65.8591\u001b[0m  5.8344\n",
      "     17       74.3371  5.8297\n",
      "     18       67.6857  5.8788\n",
      "     19       66.5005  5.8278\n",
      "     20       66.6700  5.8309\n",
      "     21       \u001b[36m65.6925\u001b[0m  5.8374\n",
      "     22       66.2075  5.8160\n",
      "     23       \u001b[36m64.7483\u001b[0m  5.8435\n",
      "     24       65.3277  5.8189\n",
      "     25       \u001b[36m62.4939\u001b[0m  5.8082\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m440004.8040\u001b[0m  5.8637\n",
      "      2     \u001b[36m2429.4411\u001b[0m  5.8719\n",
      "      3     2542.2566  5.8218\n",
      "      4     \u001b[36m1442.2810\u001b[0m  5.8274\n",
      "      5      \u001b[36m750.3640\u001b[0m  5.9001\n",
      "      6      \u001b[36m426.0971\u001b[0m  5.8412\n",
      "      7      \u001b[36m283.9378\u001b[0m  5.8587\n",
      "      8      \u001b[36m257.1981\u001b[0m  5.8227\n",
      "      9      \u001b[36m193.3283\u001b[0m  5.8383\n",
      "     10      \u001b[36m160.7819\u001b[0m  5.8835\n",
      "     11      \u001b[36m159.2730\u001b[0m  5.8174\n",
      "     12      \u001b[36m114.9880\u001b[0m  5.8540\n",
      "     13      \u001b[36m102.3910\u001b[0m  5.8523\n",
      "     14       \u001b[36m92.5859\u001b[0m  5.8338\n",
      "     15       \u001b[36m73.9861\u001b[0m  5.8658\n",
      "     16       75.2951  5.8171\n",
      "     17       \u001b[36m64.3317\u001b[0m  5.8157\n",
      "     18       \u001b[36m59.1698\u001b[0m  5.8713\n",
      "     19       66.8741  5.8165\n",
      "     20       \u001b[36m45.2631\u001b[0m  5.8733\n",
      "     21       91.7271  5.8147\n",
      "     22       49.2420  5.8048\n",
      "     23       46.0073  5.8646\n",
      "     24       51.0213  5.8169\n",
      "     25       47.3450  5.8457\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m654945.7626\u001b[0m  5.8745\n",
      "      2    \u001b[36m31123.7675\u001b[0m  5.8520\n",
      "      3    \u001b[36m13800.4578\u001b[0m  5.8234\n",
      "      4     \u001b[36m1766.6158\u001b[0m  5.8511\n",
      "      5      \u001b[36m277.9372\u001b[0m  5.8547\n",
      "      6       \u001b[36m75.1166\u001b[0m  5.8192\n",
      "      7       \u001b[36m47.7446\u001b[0m  5.8636\n",
      "      8       \u001b[36m35.8265\u001b[0m  5.8317\n",
      "      9       \u001b[36m32.2890\u001b[0m  5.8308\n",
      "     10       \u001b[36m30.8572\u001b[0m  5.8409\n",
      "     11       \u001b[36m30.5464\u001b[0m  5.8068\n",
      "     12       33.5240  5.8773\n",
      "     13       \u001b[36m26.4233\u001b[0m  5.8279\n",
      "     14       33.3816  5.8244\n",
      "     15       27.5862  5.8857\n",
      "     16       \u001b[36m26.3087\u001b[0m  5.8030\n",
      "     17       26.3715  5.8317\n",
      "     18       28.4430  5.8040\n",
      "     19       \u001b[36m26.0206\u001b[0m  5.8250\n",
      "     20       \u001b[36m23.2312\u001b[0m  5.8452\n",
      "     21       \u001b[36m22.2734\u001b[0m  5.9081\n",
      "     22       26.0337  5.8794\n",
      "     23       24.3830  5.8306\n",
      "     24       23.1002  5.8298\n",
      "     25       26.9350  5.8797\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m749257.7741\u001b[0m  5.9289\n",
      "      2     \u001b[36m3394.6620\u001b[0m  5.9219\n",
      "      3     \u001b[36m2894.5698\u001b[0m  5.8309\n",
      "      4     \u001b[36m1670.1710\u001b[0m  5.8689\n",
      "      5      \u001b[36m959.7250\u001b[0m  5.8354\n",
      "      6      \u001b[36m541.8288\u001b[0m  5.8194\n",
      "      7      \u001b[36m234.4354\u001b[0m  5.8678\n",
      "      8      \u001b[36m104.8000\u001b[0m  5.8243\n",
      "      9       \u001b[36m66.2062\u001b[0m  5.8708\n",
      "     10       \u001b[36m58.1267\u001b[0m  5.8380\n",
      "     11       60.8320  5.8061\n",
      "     12       \u001b[36m47.7218\u001b[0m  5.8665\n",
      "     13       55.1877  5.8764\n",
      "     14       58.2529  5.8554\n",
      "     15       54.8053  5.8612\n",
      "     16       50.7263  5.8289\n",
      "     17       55.4353  5.8630\n",
      "     18       \u001b[36m44.4300\u001b[0m  5.8103\n",
      "     19       \u001b[36m43.1766\u001b[0m  5.8229\n",
      "     20       43.5711  5.8467\n",
      "     21       44.3327  5.8069\n",
      "     22       \u001b[36m41.0234\u001b[0m  5.8333\n",
      "     23       \u001b[36m35.1669\u001b[0m  5.8158\n",
      "     24       36.6774  5.8103\n",
      "     25       39.4327  5.8408\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2081855.3589\u001b[0m  5.9029\n",
      "      2    \u001b[36m33712.7591\u001b[0m  5.7785\n",
      "      3     \u001b[36m1812.9088\u001b[0m  5.7731\n",
      "      4      \u001b[36m672.1677\u001b[0m  5.9295\n",
      "      5      \u001b[36m439.3680\u001b[0m  5.8000\n",
      "      6      \u001b[36m334.3013\u001b[0m  5.8997\n",
      "      7      \u001b[36m277.8698\u001b[0m  5.8322\n",
      "      8      \u001b[36m272.0310\u001b[0m  5.8554\n",
      "      9      \u001b[36m239.0684\u001b[0m  5.8956\n",
      "     10      \u001b[36m205.7377\u001b[0m  5.8693\n",
      "     11      \u001b[36m202.2356\u001b[0m  5.8373\n",
      "     12      \u001b[36m185.7825\u001b[0m  5.8440\n",
      "     13      \u001b[36m177.3416\u001b[0m  5.8163\n",
      "     14      \u001b[36m163.3600\u001b[0m  5.8577\n",
      "     15      \u001b[36m143.9128\u001b[0m  5.8304\n",
      "     16      \u001b[36m141.5638\u001b[0m  5.8988\n",
      "     17      \u001b[36m123.8053\u001b[0m  5.8431\n",
      "     18      132.8927  5.8327\n",
      "     19      \u001b[36m121.6130\u001b[0m  5.8939\n",
      "     20      \u001b[36m115.8596\u001b[0m  5.8753\n",
      "     21      \u001b[36m110.7671\u001b[0m  5.8609\n",
      "     22      \u001b[36m100.1011\u001b[0m  5.8775\n",
      "     23       \u001b[36m98.6144\u001b[0m  5.8201\n",
      "     24       99.8020  5.8631\n",
      "     25       \u001b[36m98.1060\u001b[0m  5.8192\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1110429.1345\u001b[0m  5.9045\n",
      "      2    \u001b[36m20539.6716\u001b[0m  5.8157\n",
      "      3    \u001b[36m14542.5807\u001b[0m  5.8045\n",
      "      4     \u001b[36m5554.2955\u001b[0m  5.8542\n",
      "      5     \u001b[36m1531.2227\u001b[0m  5.8018\n",
      "      6      \u001b[36m550.9273\u001b[0m  5.8380\n",
      "      7      \u001b[36m291.5395\u001b[0m  5.8128\n",
      "      8      \u001b[36m253.3977\u001b[0m  5.8307\n",
      "      9      \u001b[36m220.1783\u001b[0m  5.8715\n",
      "     10      \u001b[36m200.4923\u001b[0m  5.8270\n",
      "     11      \u001b[36m185.3039\u001b[0m  5.8812\n",
      "     12      193.7228  5.8311\n",
      "     13      \u001b[36m173.1957\u001b[0m  5.8455\n",
      "     14      \u001b[36m170.5117\u001b[0m  5.8947\n",
      "     15      176.5327  5.8360\n",
      "     16      \u001b[36m164.8386\u001b[0m  5.8925\n",
      "     17      \u001b[36m156.9878\u001b[0m  5.9058\n",
      "     18      157.2173  5.8244\n",
      "     19      \u001b[36m156.9576\u001b[0m  5.8654\n",
      "     20      \u001b[36m139.3811\u001b[0m  5.8124\n",
      "     21      \u001b[36m123.4104\u001b[0m  5.8616\n",
      "     22      132.7654  5.8170\n",
      "     23      135.6989  5.8258\n",
      "     24      \u001b[36m110.5472\u001b[0m  5.8791\n",
      "     25      130.0813  5.8596\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1273735.2729\u001b[0m  5.9021\n",
      "      2    \u001b[36m21290.7771\u001b[0m  5.8263\n",
      "      3     \u001b[36m2895.1197\u001b[0m  5.8734\n",
      "      4     \u001b[36m1497.2876\u001b[0m  5.8690\n",
      "      5      \u001b[36m741.3968\u001b[0m  6.0037\n",
      "      6      \u001b[36m320.8747\u001b[0m  5.9295\n",
      "      7      \u001b[36m178.7950\u001b[0m  5.8448\n",
      "      8      \u001b[36m109.2762\u001b[0m  5.8749\n",
      "      9       \u001b[36m71.1391\u001b[0m  5.8270\n",
      "     10       89.5967  5.8312\n",
      "     11      101.5931  5.8900\n",
      "     12       \u001b[36m54.0993\u001b[0m  5.8273\n",
      "     13       57.1302  5.8653\n",
      "     14       \u001b[36m45.7726\u001b[0m  5.8331\n",
      "     15       71.5516  5.8328\n",
      "     16       59.2957  5.8677\n",
      "     17       \u001b[36m30.1709\u001b[0m  5.8832\n",
      "     18       \u001b[36m24.0680\u001b[0m  5.9532\n",
      "     19       \u001b[36m21.8897\u001b[0m  5.9062\n",
      "     20       \u001b[36m21.4674\u001b[0m  5.8418\n",
      "     21       26.0303  5.8847\n",
      "     22       \u001b[36m16.8668\u001b[0m  5.8288\n",
      "     23       \u001b[36m15.4223\u001b[0m  5.8578\n",
      "     24       22.1547  5.8540\n",
      "     25       18.5750  5.8257\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m717592.1574\u001b[0m  5.8969\n",
      "      2     \u001b[36m6012.8103\u001b[0m  5.8188\n",
      "      3      \u001b[36m636.7547\u001b[0m  5.8597\n",
      "      4      \u001b[36m256.2869\u001b[0m  5.8455\n",
      "      5      \u001b[36m158.7047\u001b[0m  5.8913\n",
      "      6      \u001b[36m103.0939\u001b[0m  5.8348\n",
      "      7       \u001b[36m92.1204\u001b[0m  5.8280\n",
      "      8       \u001b[36m68.9199\u001b[0m  5.8836\n",
      "      9       \u001b[36m44.2078\u001b[0m  5.8314\n",
      "     10       46.5595  5.8607\n",
      "     11       52.3116  5.8324\n",
      "     12       47.1782  5.8445\n",
      "     13       73.2597  5.8769\n",
      "     14       53.3775  5.8953\n",
      "     15       58.5729  5.8582\n",
      "     16       50.5071  5.8396\n",
      "     17       \u001b[36m37.0968\u001b[0m  5.8094\n",
      "     18       53.6063  5.8535\n",
      "     19       \u001b[36m33.8350\u001b[0m  5.8051\n",
      "     20       34.5475  5.8061\n",
      "     21       37.5991  5.8279\n",
      "     22       35.3050  5.7977\n",
      "     23       46.4752  5.8720\n",
      "     24       40.9061  5.8384\n",
      "     25       39.8280  5.8341\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2064962.1542\u001b[0m  5.8804\n",
      "      2    \u001b[36m19030.8179\u001b[0m  5.8440\n",
      "      3     \u001b[36m1906.8778\u001b[0m  5.8230\n",
      "      4     \u001b[36m1227.5162\u001b[0m  5.8807\n",
      "      5     \u001b[36m1095.1751\u001b[0m  5.8686\n",
      "      6      \u001b[36m836.9402\u001b[0m  5.8324\n",
      "      7      \u001b[36m712.4067\u001b[0m  5.8521\n",
      "      8      \u001b[36m628.0153\u001b[0m  5.8670\n",
      "      9      \u001b[36m595.1491\u001b[0m  5.8727\n",
      "     10      \u001b[36m521.7478\u001b[0m  5.9167\n",
      "     11      \u001b[36m484.7539\u001b[0m  5.8346\n",
      "     12      501.1969  5.8397\n",
      "     13      \u001b[36m447.3125\u001b[0m  5.8479\n",
      "     14      \u001b[36m399.6841\u001b[0m  5.8155\n",
      "     15      \u001b[36m381.1269\u001b[0m  5.8650\n",
      "     16      385.0180  5.9042\n",
      "     17      \u001b[36m346.7904\u001b[0m  5.8380\n",
      "     18      354.0171  5.8710\n",
      "     19      \u001b[36m338.7103\u001b[0m  5.8258\n",
      "     20      \u001b[36m323.2639\u001b[0m  5.8799\n",
      "     21      \u001b[36m284.0994\u001b[0m  5.8353\n",
      "     22      \u001b[36m280.8052\u001b[0m  5.9196\n",
      "     23      299.8212  5.8873\n",
      "     24      \u001b[36m238.7841\u001b[0m  5.8423\n",
      "     25      \u001b[36m237.2642\u001b[0m  5.8608\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1676513.9653\u001b[0m  5.8812\n",
      "      2    \u001b[36m29785.5102\u001b[0m  5.8801\n",
      "      3    \u001b[36m19454.0546\u001b[0m  5.8712\n",
      "      4     \u001b[36m6731.7025\u001b[0m  5.8764\n",
      "      5     \u001b[36m2422.5294\u001b[0m  5.9301\n",
      "      6     \u001b[36m1619.5049\u001b[0m  5.8957\n",
      "      7     \u001b[36m1167.6414\u001b[0m  5.8833\n",
      "      8      \u001b[36m937.5612\u001b[0m  5.8295\n",
      "      9      \u001b[36m779.9896\u001b[0m  5.8698\n",
      "     10      \u001b[36m737.5799\u001b[0m  5.8781\n",
      "     11      \u001b[36m672.6116\u001b[0m  5.8362\n",
      "     12      \u001b[36m586.5912\u001b[0m  5.8610\n",
      "     13      \u001b[36m554.2506\u001b[0m  5.8358\n",
      "     14      \u001b[36m441.8876\u001b[0m  5.8431\n",
      "     15      446.2361  5.8772\n",
      "     16      \u001b[36m371.2059\u001b[0m  5.8238\n",
      "     17      \u001b[36m342.6870\u001b[0m  5.8768\n",
      "     18      \u001b[36m289.3773\u001b[0m  5.8322\n",
      "     19      303.2672  5.8330\n",
      "     20      \u001b[36m278.0548\u001b[0m  5.8719\n",
      "     21      \u001b[36m268.8646\u001b[0m  5.8296\n",
      "     22      \u001b[36m223.9236\u001b[0m  5.8820\n",
      "     23      \u001b[36m161.6245\u001b[0m  5.8321\n",
      "     24      186.2405  5.8786\n",
      "     25      \u001b[36m148.3970\u001b[0m  5.8717\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1422650.3077\u001b[0m  5.8972\n",
      "      2     \u001b[36m5451.0844\u001b[0m  5.8363\n",
      "      3     \u001b[36m4942.3211\u001b[0m  5.8061\n",
      "      4     \u001b[36m2609.2251\u001b[0m  5.8576\n",
      "      5      \u001b[36m862.5409\u001b[0m  5.8123\n",
      "      6      \u001b[36m364.7591\u001b[0m  5.8139\n",
      "      7      413.9754  5.8305\n",
      "      8      414.1326  5.7897\n",
      "      9      \u001b[36m354.3167\u001b[0m  5.8504\n",
      "     10      \u001b[36m331.9451\u001b[0m  5.7939\n",
      "     11      \u001b[36m311.5352\u001b[0m  5.7960\n",
      "     12      \u001b[36m259.0166\u001b[0m  5.8669\n",
      "     13      \u001b[36m250.5940\u001b[0m  5.8101\n",
      "     14      \u001b[36m247.5438\u001b[0m  5.8511\n",
      "     15      \u001b[36m216.0109\u001b[0m  5.7968\n",
      "     16      224.4721  5.7975\n",
      "     17      219.8105  5.8440\n",
      "     18      224.7260  5.8980\n",
      "     19      \u001b[36m196.8455\u001b[0m  5.8613\n",
      "     20      216.3709  5.8449\n",
      "     21      197.1047  5.8061\n",
      "     22      \u001b[36m194.2134\u001b[0m  5.8627\n",
      "     23      195.3584  5.8173\n",
      "     24      \u001b[36m178.9096\u001b[0m  5.9023\n",
      "     25      \u001b[36m166.2687\u001b[0m  5.8809\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m901515.1913\u001b[0m  5.9132\n",
      "      2    \u001b[36m23541.1075\u001b[0m  5.8263\n",
      "      3     \u001b[36m9284.3605\u001b[0m  5.8199\n",
      "      4      \u001b[36m771.8846\u001b[0m  5.8663\n",
      "      5      \u001b[36m353.5382\u001b[0m  5.8216\n",
      "      6      \u001b[36m180.9126\u001b[0m  5.8700\n",
      "      7      \u001b[36m112.4020\u001b[0m  5.8272\n",
      "      8       \u001b[36m82.3356\u001b[0m  5.8320\n",
      "      9       \u001b[36m64.3046\u001b[0m  5.8836\n",
      "     10       \u001b[36m41.3028\u001b[0m  5.8341\n",
      "     11       \u001b[36m31.7715\u001b[0m  5.8566\n",
      "     12       34.1976  5.8495\n",
      "     13       \u001b[36m28.5341\u001b[0m  5.8833\n",
      "     14       \u001b[36m27.3927\u001b[0m  5.9001\n",
      "     15       \u001b[36m23.4139\u001b[0m  5.8255\n",
      "     16       23.7339  5.8394\n",
      "     17       26.0301  5.8544\n",
      "     18       24.7353  5.8340\n",
      "     19       37.9913  5.8890\n",
      "     20       24.2147  5.8371\n",
      "     21       \u001b[36m22.5242\u001b[0m  5.8430\n",
      "     22       \u001b[36m21.6460\u001b[0m  5.8809\n",
      "     23       \u001b[36m21.1891\u001b[0m  5.8255\n",
      "     24       23.5425  5.8726\n",
      "     25       27.7790  5.8306\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m369119.0526\u001b[0m  5.8996\n",
      "      2    \u001b[36m19705.2150\u001b[0m  5.8281\n",
      "      3     \u001b[36m7750.4548\u001b[0m  5.8767\n",
      "      4     \u001b[36m1640.2528\u001b[0m  5.8423\n",
      "      5      \u001b[36m407.7063\u001b[0m  5.8284\n",
      "      6      \u001b[36m181.2975\u001b[0m  5.8774\n",
      "      7      \u001b[36m130.1990\u001b[0m  5.8660\n",
      "      8       \u001b[36m77.4206\u001b[0m  5.8372\n",
      "      9       \u001b[36m68.2771\u001b[0m  5.8422\n",
      "     10       \u001b[36m56.8117\u001b[0m  5.8136\n",
      "     11       \u001b[36m49.0080\u001b[0m  5.9434\n",
      "     12       50.2518  5.8170\n",
      "     13       49.3787  5.8347\n",
      "     14       \u001b[36m41.5711\u001b[0m  5.8576\n",
      "     15       43.6219  5.8540\n",
      "     16       43.6838  5.8933\n",
      "     17       43.8131  5.8234\n",
      "     18       \u001b[36m39.6990\u001b[0m  5.8176\n",
      "     19       41.9403  5.8712\n",
      "     20       \u001b[36m39.2966\u001b[0m  5.8282\n",
      "     21       \u001b[36m39.2636\u001b[0m  5.8606\n",
      "     22       \u001b[36m31.2878\u001b[0m  5.8439\n",
      "     23       33.3777  5.8463\n",
      "     24       33.0024  5.9063\n",
      "     25       33.6497  5.8400\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m333019.8085\u001b[0m  5.8895\n",
      "      2     \u001b[36m9630.5289\u001b[0m  5.8874\n",
      "      3     \u001b[36m5055.9507\u001b[0m  5.8959\n",
      "      4     \u001b[36m1145.3944\u001b[0m  5.8451\n",
      "      5      \u001b[36m581.5632\u001b[0m  5.8215\n",
      "      6      \u001b[36m338.3572\u001b[0m  5.8681\n",
      "      7      \u001b[36m231.6763\u001b[0m  5.8284\n",
      "      8      \u001b[36m169.1212\u001b[0m  5.8784\n",
      "      9      \u001b[36m113.7829\u001b[0m  5.8129\n",
      "     10       \u001b[36m96.6487\u001b[0m  5.7941\n",
      "     11       \u001b[36m68.7827\u001b[0m  5.9281\n",
      "     12       69.7163  5.8320\n",
      "     13       \u001b[36m46.2779\u001b[0m  5.9097\n",
      "     14       62.0809  5.8122\n",
      "     15       \u001b[36m28.0345\u001b[0m  5.8012\n",
      "     16       34.1745  5.8611\n",
      "     17       28.8650  5.8093\n",
      "     18       36.8788  5.8344\n",
      "     19       31.3480  5.8105\n",
      "     20       42.7098  5.8174\n",
      "     21       \u001b[36m17.3769\u001b[0m  5.9060\n",
      "     22       17.6324  5.8436\n",
      "     23       18.3590  5.8985\n",
      "     24       18.9376  5.8707\n",
      "     25       22.6159  5.8320\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m3682000.0882\u001b[0m  5.8447\n",
      "      2   \u001b[36m205799.4472\u001b[0m  5.8327\n",
      "      3    \u001b[36m55537.5499\u001b[0m  5.8616\n",
      "      4     \u001b[36m9952.0051\u001b[0m  5.8571\n",
      "      5     \u001b[36m3082.9980\u001b[0m  5.8735\n",
      "      6     \u001b[36m1469.8114\u001b[0m  5.8154\n",
      "      7      \u001b[36m864.1574\u001b[0m  5.8146\n",
      "      8      \u001b[36m678.3337\u001b[0m  5.8572\n",
      "      9      \u001b[36m522.3641\u001b[0m  5.8093\n",
      "     10      \u001b[36m435.3192\u001b[0m  5.8649\n",
      "     11      439.2181  5.8111\n",
      "     12      \u001b[36m337.5768\u001b[0m  5.8185\n",
      "     13      \u001b[36m326.3608\u001b[0m  5.8636\n",
      "     14      \u001b[36m293.7583\u001b[0m  5.8096\n",
      "     15      354.4489  5.8177\n",
      "     16      \u001b[36m269.6581\u001b[0m  5.8396\n",
      "     17      272.8310  5.8396\n",
      "     18      \u001b[36m241.6081\u001b[0m  5.8645\n",
      "     19      273.6022  5.8058\n",
      "     20      \u001b[36m228.4518\u001b[0m  5.8169\n",
      "     21      270.2862  5.8709\n",
      "     22      \u001b[36m218.6536\u001b[0m  5.8142\n",
      "     23      263.8122  5.8380\n",
      "     24      \u001b[36m213.9273\u001b[0m  5.8113\n",
      "     25      \u001b[36m187.3111\u001b[0m  5.8196\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch     train_loss     dur\n",
      "-------  -------------  ------\n",
      "      1  \u001b[36m22348583.6106\u001b[0m  5.8769\n",
      "      2   \u001b[36m211741.8358\u001b[0m  5.8346\n",
      "      3   \u001b[36m135993.3849\u001b[0m  5.7855\n",
      "      4    \u001b[36m40646.4317\u001b[0m  5.7884\n",
      "      5    \u001b[36m14640.4099\u001b[0m  5.9714\n",
      "      6     \u001b[36m6541.4598\u001b[0m  5.8245\n",
      "      7     \u001b[36m3166.3070\u001b[0m  5.9188\n",
      "      8     \u001b[36m2107.1936\u001b[0m  5.9008\n",
      "      9     \u001b[36m1542.4861\u001b[0m  5.8337\n",
      "     10     \u001b[36m1209.5478\u001b[0m  5.8764\n",
      "     11     1359.9247  5.8368\n",
      "     12     \u001b[36m1190.1098\u001b[0m  5.8766\n",
      "     13     \u001b[36m1060.5776\u001b[0m  5.8852\n",
      "     14     1080.4529  5.9421\n",
      "     15     \u001b[36m1009.4208\u001b[0m  5.8780\n",
      "     16      \u001b[36m957.0206\u001b[0m  5.8294\n",
      "     17      \u001b[36m941.5936\u001b[0m  5.8366\n",
      "     18      \u001b[36m865.3832\u001b[0m  5.8986\n",
      "     19      \u001b[36m825.5720\u001b[0m  5.8511\n",
      "     20      844.7512  5.8759\n",
      "     21      \u001b[36m728.7686\u001b[0m  5.8344\n",
      "     22      818.7976  5.8270\n",
      "     23      825.9790  5.8796\n",
      "     24      763.7093  5.8356\n",
      "     25      \u001b[36m714.9168\u001b[0m  5.8688\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1274216.9324\u001b[0m  5.9305\n",
      "      2    \u001b[36m12825.3612\u001b[0m  5.8805\n",
      "      3      \u001b[36m868.4721\u001b[0m  5.8242\n",
      "      4      886.5463  5.8269\n",
      "      5      \u001b[36m671.9495\u001b[0m  5.8515\n",
      "      6      \u001b[36m456.6560\u001b[0m  5.8760\n",
      "      7      \u001b[36m394.8441\u001b[0m  5.8716\n",
      "      8      \u001b[36m343.6150\u001b[0m  5.8254\n",
      "      9      \u001b[36m285.5455\u001b[0m  5.8158\n",
      "     10      \u001b[36m244.5176\u001b[0m  5.8463\n",
      "     11      245.6492  5.8407\n",
      "     12      \u001b[36m195.4057\u001b[0m  5.8755\n",
      "     13      \u001b[36m153.9276\u001b[0m  5.8044\n",
      "     14      \u001b[36m150.7972\u001b[0m  5.8189\n",
      "     15      \u001b[36m147.4666\u001b[0m  5.8744\n",
      "     16      \u001b[36m138.7827\u001b[0m  5.8683\n",
      "     17      \u001b[36m119.6181\u001b[0m  5.8497\n",
      "     18      \u001b[36m101.2768\u001b[0m  5.8306\n",
      "     19       \u001b[36m97.7397\u001b[0m  5.8308\n",
      "     20      104.0456  5.8762\n",
      "     21      100.9327  5.8177\n",
      "     22       \u001b[36m81.2319\u001b[0m  5.8354\n",
      "     23       \u001b[36m80.9599\u001b[0m  5.8376\n",
      "     24       82.1124  5.8127\n",
      "     25       \u001b[36m61.7731\u001b[0m  5.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-29 01:59:37,596]\u001b[0m Trial 7 finished with value: 173.85287906902292 and parameters: {'levels': 13, 'hidden_units': 12, 'kernel_size': 2, 'dropout': 0.1368466053024314, 'learning_rate': 0.012311503632415646}. Best is trial 2 with value: 125.41270930622379.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1054581.8619\u001b[0m  2.0187\n",
      "      2    \u001b[36m64363.0174\u001b[0m  1.9786\n",
      "      3    \u001b[36m63501.3772\u001b[0m  1.9750\n",
      "      4    \u001b[36m12702.7770\u001b[0m  2.1050\n",
      "      5     \u001b[36m2038.1203\u001b[0m  1.9607\n",
      "      6     \u001b[36m1094.6722\u001b[0m  1.9659\n",
      "      7     1359.8649  2.0141\n",
      "      8     1401.9224  1.9656\n",
      "      9     1360.2399  1.9541\n",
      "     10     1216.1992  1.9444\n",
      "     11      \u001b[36m984.2731\u001b[0m  1.9866\n",
      "     12      \u001b[36m825.2546\u001b[0m  1.9755\n",
      "     13      \u001b[36m700.9634\u001b[0m  1.9656\n",
      "     14      \u001b[36m576.4528\u001b[0m  1.9663\n",
      "     15      576.5364  1.9656\n",
      "     16      \u001b[36m506.3203\u001b[0m  1.9582\n",
      "     17      \u001b[36m491.3902\u001b[0m  1.9672\n",
      "     18      533.6684  1.9684\n",
      "     19      519.0538  1.9931\n",
      "     20      \u001b[36m466.1548\u001b[0m  1.9738\n",
      "     21      \u001b[36m437.7286\u001b[0m  1.9541\n",
      "     22      462.8083  1.9973\n",
      "     23      \u001b[36m434.5779\u001b[0m  1.9345\n",
      "     24      439.3940  2.0098\n",
      "     25      \u001b[36m416.4694\u001b[0m  2.0358\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m153549.5258\u001b[0m  2.0109\n",
      "      2    \u001b[36m29476.2736\u001b[0m  1.9976\n",
      "      3    \u001b[36m19332.3153\u001b[0m  1.9697\n",
      "      4     \u001b[36m7022.2051\u001b[0m  1.9613\n",
      "      5     7360.3662  1.9619\n",
      "      6     \u001b[36m3931.0024\u001b[0m  1.9587\n",
      "      7     \u001b[36m1924.6986\u001b[0m  1.9835\n",
      "      8     2168.4685  1.9915\n",
      "      9     \u001b[36m1561.8421\u001b[0m  1.9691\n",
      "     10     \u001b[36m1094.1703\u001b[0m  1.9584\n",
      "     11      \u001b[36m919.5789\u001b[0m  1.9653\n",
      "     12      \u001b[36m917.0167\u001b[0m  1.9712\n",
      "     13      \u001b[36m839.6620\u001b[0m  1.9688\n",
      "     14      \u001b[36m605.4561\u001b[0m  1.9816\n",
      "     15      \u001b[36m557.6737\u001b[0m  1.9941\n",
      "     16      \u001b[36m530.4602\u001b[0m  1.9755\n",
      "     17      530.9620  2.0236\n",
      "     18      \u001b[36m456.4844\u001b[0m  1.9673\n",
      "     19      \u001b[36m455.0085\u001b[0m  1.9674\n",
      "     20      \u001b[36m416.8353\u001b[0m  1.9758\n",
      "     21      \u001b[36m395.2045\u001b[0m  1.9678\n",
      "     22      \u001b[36m385.7552\u001b[0m  1.9643\n",
      "     23      \u001b[36m326.4215\u001b[0m  2.0202\n",
      "     24      346.6820  1.9741\n",
      "     25      \u001b[36m288.6151\u001b[0m  1.9653\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m104783.2460\u001b[0m  2.0770\n",
      "      2    \u001b[36m35153.1145\u001b[0m  2.0160\n",
      "      3    \u001b[36m13300.3068\u001b[0m  1.9862\n",
      "      4     \u001b[36m9987.7564\u001b[0m  2.1015\n",
      "      5     \u001b[36m5580.5422\u001b[0m  2.0500\n",
      "      6     \u001b[36m4348.2309\u001b[0m  2.0073\n",
      "      7     \u001b[36m2743.5358\u001b[0m  1.9645\n",
      "      8     \u001b[36m2123.3200\u001b[0m  1.9773\n",
      "      9     \u001b[36m1538.5808\u001b[0m  2.0960\n",
      "     10     \u001b[36m1406.4919\u001b[0m  1.9653\n",
      "     11     \u001b[36m1035.4786\u001b[0m  1.9691\n",
      "     12      \u001b[36m952.3453\u001b[0m  1.9682\n",
      "     13      \u001b[36m864.2403\u001b[0m  1.9429\n",
      "     14      \u001b[36m783.5992\u001b[0m  1.9439\n",
      "     15      \u001b[36m675.2908\u001b[0m  1.9444\n",
      "     16      \u001b[36m610.1120\u001b[0m  1.9422\n",
      "     17      \u001b[36m513.4179\u001b[0m  1.9508\n",
      "     18      521.8340  1.9467\n",
      "     19      \u001b[36m494.7337\u001b[0m  2.0102\n",
      "     20      547.6790  1.9993\n",
      "     21      \u001b[36m459.0764\u001b[0m  1.9776\n",
      "     22      \u001b[36m445.2599\u001b[0m  1.9673\n",
      "     23      455.8154  1.9615\n",
      "     24      \u001b[36m417.2100\u001b[0m  1.9692\n",
      "     25      \u001b[36m383.2996\u001b[0m  1.9803\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m233937.8897\u001b[0m  2.0064\n",
      "      2    \u001b[36m42263.7103\u001b[0m  1.9443\n",
      "      3     \u001b[36m6724.2846\u001b[0m  2.0366\n",
      "      4     8876.9639  1.9619\n",
      "      5     \u001b[36m5184.2606\u001b[0m  1.9719\n",
      "      6     \u001b[36m2085.5196\u001b[0m  1.9600\n",
      "      7     \u001b[36m1341.6974\u001b[0m  1.9854\n",
      "      8     1495.6041  1.9980\n",
      "      9     \u001b[36m1299.8438\u001b[0m  1.9631\n",
      "     10     \u001b[36m1073.3926\u001b[0m  1.9784\n",
      "     11      \u001b[36m953.0472\u001b[0m  1.9731\n",
      "     12      \u001b[36m826.6555\u001b[0m  1.9748\n",
      "     13      \u001b[36m790.0218\u001b[0m  1.9880\n",
      "     14      817.8367  2.0377\n",
      "     15      \u001b[36m782.7414\u001b[0m  2.0259\n",
      "     16      792.3495  1.9562\n",
      "     17      \u001b[36m683.6035\u001b[0m  1.9816\n",
      "     18      \u001b[36m654.8222\u001b[0m  1.9690\n",
      "     19      \u001b[36m593.6135\u001b[0m  1.9542\n",
      "     20      \u001b[36m583.5845\u001b[0m  1.9528\n",
      "     21      \u001b[36m580.4262\u001b[0m  1.9549\n",
      "     22      586.9176  1.9512\n",
      "     23      \u001b[36m492.9789\u001b[0m  1.9535\n",
      "     24      551.7613  1.9512\n",
      "     25      506.7676  1.9621\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m121960.3659\u001b[0m  2.0365\n",
      "      2    \u001b[36m51688.7118\u001b[0m  1.9714\n",
      "      3    \u001b[36m19347.2379\u001b[0m  1.9673\n",
      "      4    \u001b[36m10644.6033\u001b[0m  2.0100\n",
      "      5     \u001b[36m4449.3543\u001b[0m  1.9645\n",
      "      6     5503.1938  1.9652\n",
      "      7     \u001b[36m2503.9304\u001b[0m  1.9629\n",
      "      8     \u001b[36m1925.4308\u001b[0m  1.9681\n",
      "      9     1999.9795  1.9734\n",
      "     10     \u001b[36m1410.7983\u001b[0m  1.9622\n",
      "     11     \u001b[36m1154.5185\u001b[0m  1.9843\n",
      "     12     \u001b[36m1121.9963\u001b[0m  1.9619\n",
      "     13      \u001b[36m959.0715\u001b[0m  1.9542\n",
      "     14      \u001b[36m833.6678\u001b[0m  1.9638\n",
      "     15      \u001b[36m747.2076\u001b[0m  1.9585\n",
      "     16      \u001b[36m663.9472\u001b[0m  1.9587\n",
      "     17      \u001b[36m623.9795\u001b[0m  2.0059\n",
      "     18      632.1167  1.9836\n",
      "     19      \u001b[36m558.7858\u001b[0m  2.0424\n",
      "     20      \u001b[36m541.1891\u001b[0m  1.9636\n",
      "     21      \u001b[36m434.1127\u001b[0m  1.9549\n",
      "     22      453.7637  1.9622\n",
      "     23      \u001b[36m411.3640\u001b[0m  1.9604\n",
      "     24      \u001b[36m402.3843\u001b[0m  1.9616\n",
      "     25      \u001b[36m391.3130\u001b[0m  1.9821\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m212674.7976\u001b[0m  2.0165\n",
      "      2    \u001b[36m80410.4020\u001b[0m  1.9584\n",
      "      3    \u001b[36m28207.2863\u001b[0m  1.9754\n",
      "      4    \u001b[36m18546.1464\u001b[0m  1.9659\n",
      "      5     \u001b[36m7375.0694\u001b[0m  1.9604\n",
      "      6     8440.6566  1.9621\n",
      "      7     \u001b[36m4008.4108\u001b[0m  2.0017\n",
      "      8     \u001b[36m3241.3470\u001b[0m  1.9930\n",
      "      9     \u001b[36m3179.7339\u001b[0m  1.9676\n",
      "     10     \u001b[36m2090.1926\u001b[0m  1.9659\n",
      "     11     \u001b[36m1836.9231\u001b[0m  1.9826\n",
      "     12     \u001b[36m1814.8198\u001b[0m  1.9803\n",
      "     13     \u001b[36m1396.8479\u001b[0m  1.9653\n",
      "     14     \u001b[36m1219.2792\u001b[0m  1.9705\n",
      "     15     1246.1540  1.9855\n",
      "     16     \u001b[36m1082.4936\u001b[0m  2.0002\n",
      "     17     \u001b[36m1070.6845\u001b[0m  2.0084\n",
      "     18     \u001b[36m1059.8600\u001b[0m  1.9754\n",
      "     19      \u001b[36m906.7798\u001b[0m  1.9665\n",
      "     20      \u001b[36m887.0479\u001b[0m  1.9681\n",
      "     21      \u001b[36m837.2363\u001b[0m  1.9683\n",
      "     22      \u001b[36m797.8562\u001b[0m  1.9561\n",
      "     23      \u001b[36m712.6581\u001b[0m  2.0235\n",
      "     24      724.2202  1.9592\n",
      "     25      748.8156  1.9619\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m113212.3066\u001b[0m  2.0114\n",
      "      2    \u001b[36m44480.4505\u001b[0m  1.9607\n",
      "      3    \u001b[36m19028.9782\u001b[0m  1.9626\n",
      "      4    \u001b[36m10136.7461\u001b[0m  2.0110\n",
      "      5     \u001b[36m2677.6926\u001b[0m  1.9599\n",
      "      6     3363.6826  1.9635\n",
      "      7     \u001b[36m2360.1360\u001b[0m  1.9512\n",
      "      8     \u001b[36m1054.3162\u001b[0m  1.9531\n",
      "      9      \u001b[36m706.6461\u001b[0m  1.9309\n",
      "     10      721.0863  1.9377\n",
      "     11      750.1016  1.9491\n",
      "     12      \u001b[36m545.9915\u001b[0m  1.9397\n",
      "     13      \u001b[36m467.1513\u001b[0m  1.9363\n",
      "     14      \u001b[36m421.1486\u001b[0m  1.9332\n",
      "     15      \u001b[36m367.6064\u001b[0m  1.9596\n",
      "     16      373.0405  1.9536\n",
      "     17      368.5052  1.9524\n",
      "     18      \u001b[36m337.5578\u001b[0m  1.9484\n",
      "     19      \u001b[36m317.7790\u001b[0m  2.0062\n",
      "     20      321.3376  1.9814\n",
      "     21      321.0507  1.9614\n",
      "     22      \u001b[36m267.2935\u001b[0m  1.9591\n",
      "     23      301.1822  1.9609\n",
      "     24      284.7552  1.9959\n",
      "     25      \u001b[36m237.3254\u001b[0m  2.0097\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m169783.5862\u001b[0m  2.0019\n",
      "      2    \u001b[36m34126.3308\u001b[0m  1.9668\n",
      "      3    \u001b[36m17462.5333\u001b[0m  1.9582\n",
      "      4     \u001b[36m6846.9555\u001b[0m  2.0499\n",
      "      5     7966.1307  1.9726\n",
      "      6     \u001b[36m2917.4222\u001b[0m  1.9688\n",
      "      7     \u001b[36m2373.6004\u001b[0m  1.9659\n",
      "      8     \u001b[36m2283.3667\u001b[0m  1.9855\n",
      "      9     \u001b[36m1483.0497\u001b[0m  1.9347\n",
      "     10     \u001b[36m1269.7677\u001b[0m  1.9367\n",
      "     11     \u001b[36m1266.0150\u001b[0m  1.9361\n",
      "     12     \u001b[36m1110.8107\u001b[0m  1.9648\n",
      "     13      \u001b[36m896.2779\u001b[0m  1.9590\n",
      "     14      984.0422  1.9706\n",
      "     15      \u001b[36m868.3176\u001b[0m  1.9824\n",
      "     16      \u001b[36m787.2497\u001b[0m  1.9653\n",
      "     17      \u001b[36m728.4175\u001b[0m  1.9733\n",
      "     18      \u001b[36m681.0178\u001b[0m  1.9633\n",
      "     19      \u001b[36m663.2476\u001b[0m  1.9674\n",
      "     20      664.8469  1.9833\n",
      "     21      \u001b[36m579.7393\u001b[0m  1.9699\n",
      "     22      \u001b[36m524.1820\u001b[0m  1.9568\n",
      "     23      \u001b[36m511.8854\u001b[0m  1.9860\n",
      "     24      \u001b[36m488.0329\u001b[0m  1.9868\n",
      "     25      \u001b[36m471.9442\u001b[0m  1.9519\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m174064.8086\u001b[0m  1.9948\n",
      "      2    \u001b[36m45366.9361\u001b[0m  1.9561\n",
      "      3    \u001b[36m10427.2827\u001b[0m  1.9624\n",
      "      4    12231.2920  1.9584\n",
      "      5     \u001b[36m4087.5388\u001b[0m  1.9850\n",
      "      6     \u001b[36m2691.4885\u001b[0m  1.9622\n",
      "      7     2821.3442  1.9619\n",
      "      8     \u001b[36m1954.6350\u001b[0m  1.9616\n",
      "      9     \u001b[36m1422.4827\u001b[0m  1.9561\n",
      "     10     \u001b[36m1339.5474\u001b[0m  1.9619\n",
      "     11     \u001b[36m1158.9539\u001b[0m  1.9522\n",
      "     12     \u001b[36m1012.4914\u001b[0m  2.0170\n",
      "     13      \u001b[36m924.1147\u001b[0m  1.9681\n",
      "     14      \u001b[36m881.7629\u001b[0m  1.9552\n",
      "     15      \u001b[36m770.1111\u001b[0m  1.9669\n",
      "     16      \u001b[36m714.2545\u001b[0m  1.9671\n",
      "     17      716.1087  1.9589\n",
      "     18      \u001b[36m630.2864\u001b[0m  1.9736\n",
      "     19      652.4912  2.0012\n",
      "     20      656.9161  1.9804\n",
      "     21      \u001b[36m622.3958\u001b[0m  1.9588\n",
      "     22      \u001b[36m567.1662\u001b[0m  1.9635\n",
      "     23      \u001b[36m542.7110\u001b[0m  1.9686\n",
      "     24      \u001b[36m461.0686\u001b[0m  1.9645\n",
      "     25      464.7773  1.9594\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m320162.2292\u001b[0m  2.0595\n",
      "      2    \u001b[36m99489.7057\u001b[0m  1.9604\n",
      "      3    \u001b[36m54331.3312\u001b[0m  1.9634\n",
      "      4    \u001b[36m26737.6944\u001b[0m  1.9570\n",
      "      5     \u001b[36m7570.7833\u001b[0m  1.9479\n",
      "      6     9970.9268  1.9490\n",
      "      7     9124.0255  2.0029\n",
      "      8     \u001b[36m5020.3196\u001b[0m  1.9827\n",
      "      9     \u001b[36m3117.6924\u001b[0m  1.9765\n",
      "     10     3220.1975  1.9697\n",
      "     11     3358.5528  1.9714\n",
      "     12     \u001b[36m2686.1522\u001b[0m  1.9699\n",
      "     13     \u001b[36m2231.6848\u001b[0m  1.9647\n",
      "     14     \u001b[36m2102.8263\u001b[0m  1.9720\n",
      "     15     \u001b[36m1802.0530\u001b[0m  1.9662\n",
      "     16     \u001b[36m1700.8767\u001b[0m  1.9972\n",
      "     17     \u001b[36m1498.9857\u001b[0m  1.9392\n",
      "     18     \u001b[36m1491.9824\u001b[0m  1.9359\n",
      "     19     \u001b[36m1311.4566\u001b[0m  1.9426\n",
      "     20     \u001b[36m1181.4582\u001b[0m  1.9372\n",
      "     21     1183.9522  1.9361\n",
      "     22     \u001b[36m1121.7174\u001b[0m  1.9388\n",
      "     23      \u001b[36m982.0785\u001b[0m  1.9666\n",
      "     24      \u001b[36m974.9689\u001b[0m  1.9465\n",
      "     25      \u001b[36m928.3190\u001b[0m  1.9401\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m183175.6276\u001b[0m  2.0015\n",
      "      2    \u001b[36m40702.3704\u001b[0m  1.9641\n",
      "      3    \u001b[36m15631.1538\u001b[0m  1.9731\n",
      "      4    \u001b[36m15574.5481\u001b[0m  1.9533\n",
      "      5     \u001b[36m6038.2257\u001b[0m  2.0050\n",
      "      6     \u001b[36m3321.3114\u001b[0m  2.0229\n",
      "      7     3546.9664  1.9554\n",
      "      8     \u001b[36m2901.8293\u001b[0m  1.9650\n",
      "      9     \u001b[36m1926.1085\u001b[0m  1.9562\n",
      "     10     \u001b[36m1399.2748\u001b[0m  1.9567\n",
      "     11     \u001b[36m1151.9565\u001b[0m  1.9523\n",
      "     12     \u001b[36m1046.7663\u001b[0m  1.9651\n",
      "     13      \u001b[36m824.0234\u001b[0m  1.9609\n",
      "     14      \u001b[36m715.7022\u001b[0m  1.9503\n",
      "     15      \u001b[36m657.6628\u001b[0m  1.9507\n",
      "     16      \u001b[36m559.1519\u001b[0m  1.9556\n",
      "     17      \u001b[36m555.7732\u001b[0m  1.9434\n",
      "     18      \u001b[36m520.4149\u001b[0m  1.9483\n",
      "     19      \u001b[36m504.9054\u001b[0m  1.9485\n",
      "     20      \u001b[36m451.6999\u001b[0m  2.0267\n",
      "     21      452.3013  1.9958\n",
      "     22      469.0100  2.0358\n",
      "     23      \u001b[36m417.2597\u001b[0m  1.9724\n",
      "     24      \u001b[36m386.9004\u001b[0m  1.9567\n",
      "     25      \u001b[36m363.1648\u001b[0m  1.9577\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1    \u001b[36m96403.6374\u001b[0m  2.0448\n",
      "      2    \u001b[36m33527.2927\u001b[0m  1.9817\n",
      "      3    \u001b[36m11259.8779\u001b[0m  2.0078\n",
      "      4     \u001b[36m7692.1149\u001b[0m  1.9708\n",
      "      5     \u001b[36m2827.4532\u001b[0m  1.9531\n",
      "      6     2911.0524  1.9532\n",
      "      7     \u001b[36m1845.4280\u001b[0m  1.9521\n",
      "      8     \u001b[36m1197.8521\u001b[0m  1.9497\n",
      "      9     \u001b[36m1152.3813\u001b[0m  2.0027\n",
      "     10      \u001b[36m915.0182\u001b[0m  1.9554\n",
      "     11      \u001b[36m640.1727\u001b[0m  1.9554\n",
      "     12      643.7674  1.9508\n",
      "     13      \u001b[36m550.1188\u001b[0m  1.9591\n",
      "     14      584.7954  1.9584\n",
      "     15      \u001b[36m486.2523\u001b[0m  2.0155\n",
      "     16      \u001b[36m454.0671\u001b[0m  1.9935\n",
      "     17      \u001b[36m399.7000\u001b[0m  1.9741\n",
      "     18      \u001b[36m399.4250\u001b[0m  1.9357\n",
      "     19      \u001b[36m395.4294\u001b[0m  1.9429\n",
      "     20      \u001b[36m328.6384\u001b[0m  1.9442\n",
      "     21      353.8023  1.9339\n",
      "     22      \u001b[36m318.0166\u001b[0m  1.9487\n",
      "     23      \u001b[36m285.1998\u001b[0m  1.9581\n",
      "     24      \u001b[36m273.4542\u001b[0m  2.0113\n",
      "     25      291.4019  1.9618\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m159211.0451\u001b[0m  2.0107\n",
      "      2    \u001b[36m54700.4792\u001b[0m  1.9612\n",
      "      3    \u001b[36m21958.5030\u001b[0m  1.9627\n",
      "      4    \u001b[36m12757.5081\u001b[0m  1.9644\n",
      "      5     \u001b[36m2740.2955\u001b[0m  2.0002\n",
      "      6     2885.7472  1.9670\n",
      "      7     2951.2840  1.9739\n",
      "      8     \u001b[36m1937.9514\u001b[0m  1.9693\n",
      "      9     \u001b[36m1203.1241\u001b[0m  1.9660\n",
      "     10      \u001b[36m943.6053\u001b[0m  1.9635\n",
      "     11     1045.6166  1.9629\n",
      "     12      \u001b[36m804.4060\u001b[0m  1.9637\n",
      "     13      844.8988  2.0133\n",
      "     14      \u001b[36m775.3127\u001b[0m  1.9650\n",
      "     15      \u001b[36m669.5421\u001b[0m  1.9596\n",
      "     16      \u001b[36m660.6599\u001b[0m  1.9661\n",
      "     17      \u001b[36m587.3595\u001b[0m  1.9638\n",
      "     18      606.7565  1.9626\n",
      "     19      \u001b[36m522.7788\u001b[0m  1.9679\n",
      "     20      544.3913  1.9828\n",
      "     21      \u001b[36m510.3992\u001b[0m  1.9910\n",
      "     22      \u001b[36m503.2896\u001b[0m  1.9885\n",
      "     23      \u001b[36m450.6358\u001b[0m  1.9634\n",
      "     24      \u001b[36m408.4384\u001b[0m  1.9692\n",
      "     25      439.0763  1.9714\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1420458.7332\u001b[0m  2.0267\n",
      "      2   \u001b[36m216079.0854\u001b[0m  1.9800\n",
      "      3    \u001b[36m90103.5892\u001b[0m  1.9543\n",
      "      4     \u001b[36m9326.0660\u001b[0m  1.9692\n",
      "      5     \u001b[36m5200.5481\u001b[0m  1.9620\n",
      "      6     7988.3897  1.9697\n",
      "      7     7755.4224  1.9555\n",
      "      8     5686.3680  1.9605\n",
      "      9     \u001b[36m3953.3279\u001b[0m  2.0196\n",
      "     10     \u001b[36m3337.6413\u001b[0m  1.9663\n",
      "     11     \u001b[36m2194.3124\u001b[0m  1.9805\n",
      "     12     \u001b[36m2097.3390\u001b[0m  2.0519\n",
      "     13     \u001b[36m1860.3269\u001b[0m  1.9783\n",
      "     14     \u001b[36m1704.1169\u001b[0m  1.9670\n",
      "     15     \u001b[36m1556.6062\u001b[0m  2.0003\n",
      "     16     1599.7324  2.0594\n",
      "     17     \u001b[36m1361.0805\u001b[0m  2.0355\n",
      "     18     1440.0685  1.9711\n",
      "     19     \u001b[36m1337.0927\u001b[0m  1.9687\n",
      "     20     \u001b[36m1292.7416\u001b[0m  1.9614\n",
      "     21     \u001b[36m1215.7798\u001b[0m  1.9552\n",
      "     22     \u001b[36m1186.9436\u001b[0m  1.9542\n",
      "     23     \u001b[36m1067.5882\u001b[0m  1.9848\n",
      "     24     1068.3807  2.0049\n",
      "     25     1143.7961  2.0220\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m103503.4645\u001b[0m  2.0759\n",
      "      2    \u001b[36m38126.7472\u001b[0m  2.0147\n",
      "      3    \u001b[36m15830.3704\u001b[0m  1.9559\n",
      "      4     \u001b[36m6924.1968\u001b[0m  1.9571\n",
      "      5     \u001b[36m4497.2117\u001b[0m  2.0806\n",
      "      6     \u001b[36m3424.6963\u001b[0m  1.9698\n",
      "      7     \u001b[36m1582.6855\u001b[0m  1.9592\n",
      "      8     1597.3353  1.9734\n",
      "      9     \u001b[36m1039.9445\u001b[0m  1.9738\n",
      "     10      \u001b[36m852.2962\u001b[0m  1.9938\n",
      "     11      859.5895  1.9609\n",
      "     12      \u001b[36m723.4955\u001b[0m  1.9478\n",
      "     13      \u001b[36m602.9502\u001b[0m  2.0177\n",
      "     14      \u001b[36m538.6882\u001b[0m  1.9633\n",
      "     15      541.9324  1.9624\n",
      "     16      \u001b[36m523.2063\u001b[0m  1.9550\n",
      "     17      \u001b[36m471.8353\u001b[0m  1.9604\n",
      "     18      \u001b[36m446.2689\u001b[0m  1.9755\n",
      "     19      \u001b[36m410.0726\u001b[0m  1.9646\n",
      "     20      \u001b[36m352.2578\u001b[0m  1.9845\n",
      "     21      379.3316  1.9754\n",
      "     22      359.7862  1.9510\n",
      "     23      \u001b[36m325.1237\u001b[0m  1.9517\n",
      "     24      \u001b[36m320.7175\u001b[0m  1.9650\n",
      "     25      \u001b[36m301.0181\u001b[0m  1.9750\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m134137.7067\u001b[0m  2.0259\n",
      "      2     \u001b[36m9408.5975\u001b[0m  1.9879\n",
      "      3    11008.7322  1.9672\n",
      "      4     \u001b[36m5447.9465\u001b[0m  1.9723\n",
      "      5     \u001b[36m1482.1868\u001b[0m  1.9595\n",
      "      6      \u001b[36m984.4526\u001b[0m  1.9708\n",
      "      7     1155.9983  1.9689\n",
      "      8     1069.4214  1.9590\n",
      "      9      \u001b[36m681.6212\u001b[0m  2.0172\n",
      "     10      \u001b[36m506.4684\u001b[0m  1.9626\n",
      "     11      \u001b[36m333.1606\u001b[0m  1.9699\n",
      "     12      \u001b[36m317.7533\u001b[0m  1.9633\n",
      "     13      329.8760  1.9567\n",
      "     14      335.4123  1.9542\n",
      "     15      \u001b[36m295.9169\u001b[0m  1.9613\n",
      "     16      \u001b[36m261.2049\u001b[0m  1.9856\n",
      "     17      \u001b[36m244.8379\u001b[0m  1.9844\n",
      "     18      \u001b[36m213.8566\u001b[0m  1.9596\n",
      "     19      240.4824  1.9646\n",
      "     20      227.0099  1.9634\n",
      "     21      215.3536  1.9663\n",
      "     22      \u001b[36m177.0665\u001b[0m  1.9649\n",
      "     23      202.5750  1.9582\n",
      "     24      \u001b[36m173.9685\u001b[0m  1.9826\n",
      "     25      \u001b[36m166.6502\u001b[0m  1.9874\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m286227.2647\u001b[0m  2.0107\n",
      "      2    \u001b[36m25962.8239\u001b[0m  1.9667\n",
      "      3    36299.6081  1.9747\n",
      "      4    \u001b[36m11350.0415\u001b[0m  1.9649\n",
      "      5     \u001b[36m3014.3289\u001b[0m  2.0064\n",
      "      6     4853.4786  1.9948\n",
      "      7     4052.0506  2.0107\n",
      "      8     \u001b[36m2436.4350\u001b[0m  1.9684\n",
      "      9     \u001b[36m1410.2223\u001b[0m  1.9810\n",
      "     10     \u001b[36m1152.2355\u001b[0m  1.9773\n",
      "     11     \u001b[36m1062.4023\u001b[0m  1.9582\n",
      "     12      \u001b[36m958.5043\u001b[0m  1.9354\n",
      "     13      \u001b[36m937.3770\u001b[0m  1.9901\n",
      "     14      \u001b[36m860.4748\u001b[0m  1.9554\n",
      "     15      \u001b[36m727.9365\u001b[0m  1.9559\n",
      "     16      \u001b[36m672.9313\u001b[0m  1.9509\n",
      "     17      \u001b[36m612.5295\u001b[0m  1.9572\n",
      "     18      688.6286  1.9629\n",
      "     19      \u001b[36m582.4131\u001b[0m  1.9550\n",
      "     20      629.9812  1.9774\n",
      "     21      \u001b[36m543.4885\u001b[0m  1.9809\n",
      "     22      \u001b[36m510.0108\u001b[0m  1.9712\n",
      "     23      512.4726  1.9670\n",
      "     24      518.5275  1.9635\n",
      "     25      \u001b[36m439.0414\u001b[0m  1.9896\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1    \u001b[36m38782.0320\u001b[0m  1.9974\n",
      "      2     \u001b[36m7833.2919\u001b[0m  2.0356\n",
      "      3     \u001b[36m4677.2897\u001b[0m  1.9719\n",
      "      4     \u001b[36m1645.0452\u001b[0m  1.9679\n",
      "      5     \u001b[36m1532.0568\u001b[0m  1.9730\n",
      "      6      \u001b[36m817.0353\u001b[0m  1.9856\n",
      "      7      \u001b[36m455.9169\u001b[0m  1.9727\n",
      "      8      \u001b[36m332.0933\u001b[0m  1.9580\n",
      "      9      \u001b[36m323.3802\u001b[0m  1.9972\n",
      "     10      \u001b[36m249.9635\u001b[0m  1.9874\n",
      "     11      250.7635  1.9715\n",
      "     12      \u001b[36m204.5380\u001b[0m  1.9626\n",
      "     13      \u001b[36m195.6684\u001b[0m  1.9563\n",
      "     14      \u001b[36m176.2008\u001b[0m  1.9443\n",
      "     15      \u001b[36m168.9694\u001b[0m  1.9471\n",
      "     16      \u001b[36m150.9927\u001b[0m  1.9520\n",
      "     17      151.3870  1.9807\n",
      "     18      \u001b[36m124.0853\u001b[0m  1.9526\n",
      "     19      139.2014  1.9567\n",
      "     20      \u001b[36m113.4090\u001b[0m  1.9559\n",
      "     21      116.3807  1.9577\n",
      "     22      \u001b[36m106.7063\u001b[0m  1.9523\n",
      "     23      \u001b[36m101.6243\u001b[0m  1.9469\n",
      "     24       \u001b[36m87.5072\u001b[0m  1.9712\n",
      "     25       95.6935  1.9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-29 02:15:18,895]\u001b[0m Trial 8 finished with value: 151.23049332013142 and parameters: {'levels': 6, 'hidden_units': 17, 'kernel_size': 2, 'dropout': 0.1818640804157564, 'learning_rate': 0.004375517173207359}. Best is trial 2 with value: 125.41270930622379.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1976027.1656\u001b[0m  8.7352\n",
      "      2   \u001b[36m187634.8744\u001b[0m  8.7626\n",
      "      3    \u001b[36m15129.5326\u001b[0m  8.8283\n",
      "      4    35720.5074  8.7624\n",
      "      5    29953.3109  8.7993\n",
      "      6    16657.4608  8.6986\n",
      "      7     \u001b[36m8356.4901\u001b[0m  8.7695\n",
      "      8     \u001b[36m3755.1415\u001b[0m  8.7114\n",
      "      9     \u001b[36m1606.3406\u001b[0m  8.7171\n",
      "     10     \u001b[36m1184.2320\u001b[0m  8.7193\n",
      "     11     \u001b[36m1174.5101\u001b[0m  8.6941\n",
      "     12     1281.8829  8.7457\n",
      "     13     1298.6050  8.6803\n",
      "     14     1187.6919  8.7430\n",
      "     15     \u001b[36m1154.5224\u001b[0m  8.7647\n",
      "     16     \u001b[36m1082.3322\u001b[0m  8.7035\n",
      "     17      \u001b[36m973.7971\u001b[0m  8.8030\n",
      "     18      \u001b[36m916.1332\u001b[0m  8.7136\n",
      "     19      939.5716  8.7640\n",
      "     20      936.2608  8.7001\n",
      "     21      \u001b[36m899.7890\u001b[0m  8.7610\n",
      "     22      943.5654  8.7533\n",
      "     23      \u001b[36m852.3880\u001b[0m  8.6991\n",
      "     24      892.6066  8.7408\n",
      "     25      878.3542  8.6993\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1811653.2468\u001b[0m  8.7335\n",
      "      2   \u001b[36m587119.0612\u001b[0m  8.7358\n",
      "      3    \u001b[36m24480.6277\u001b[0m  8.6964\n",
      "      4    67767.1584  8.7413\n",
      "      5    50302.9347  8.7892\n",
      "      6    \u001b[36m23077.7854\u001b[0m  8.7013\n",
      "      7     \u001b[36m8737.6762\u001b[0m  8.7281\n",
      "      8     \u001b[36m2628.4711\u001b[0m  8.7825\n",
      "      9      \u001b[36m975.0871\u001b[0m  8.7524\n",
      "     10      \u001b[36m919.2598\u001b[0m  8.7497\n",
      "     11     1144.2419  8.6965\n",
      "     12     1123.0418  8.7559\n",
      "     13     1046.2471  8.6899\n",
      "     14      952.3961  8.7328\n",
      "     15      \u001b[36m784.0723\u001b[0m  8.7122\n",
      "     16      813.7824  8.7153\n",
      "     17      \u001b[36m655.1096\u001b[0m  8.7441\n",
      "     18      708.2978  8.6889\n",
      "     19      \u001b[36m651.7031\u001b[0m  8.7562\n",
      "     20      \u001b[36m627.5873\u001b[0m  8.7087\n",
      "     21      \u001b[36m625.6603\u001b[0m  8.7370\n",
      "     22      637.9612  8.7246\n",
      "     23      \u001b[36m614.8639\u001b[0m  8.7017\n",
      "     24      \u001b[36m612.2464\u001b[0m  8.7763\n",
      "     25      \u001b[36m602.1281\u001b[0m  8.7324\n",
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m793891.7843\u001b[0m  8.7310\n",
      "      2   \u001b[36m221142.2174\u001b[0m  8.7553\n",
      "      3    \u001b[36m12949.7136\u001b[0m  8.6985\n",
      "      4    32583.2505  8.7509\n",
      "      5    25283.6909  8.7581\n",
      "      6    \u001b[36m10550.3114\u001b[0m  8.6942\n",
      "      7     \u001b[36m3125.1567\u001b[0m  8.7688\n",
      "      8     \u001b[36m1825.9568\u001b[0m  8.7019\n",
      "      9     2251.9093  8.7449\n",
      "     10     2268.7174  8.7151\n",
      "     11     2051.5072  8.6863\n",
      "     12     \u001b[36m1526.9659\u001b[0m  8.7531\n",
      "     13     \u001b[36m1327.6009\u001b[0m  8.6990\n",
      "     14     \u001b[36m1165.6957\u001b[0m  8.7635\n",
      "     15     \u001b[36m1038.4809\u001b[0m  8.6791\n",
      "     16     1113.1615  8.7461\n",
      "     17     \u001b[36m1028.2710\u001b[0m  8.7692\n",
      "     18      \u001b[36m901.1935\u001b[0m  8.7042\n",
      "     19      \u001b[36m863.6054\u001b[0m  8.7703\n",
      "     20      895.0907  8.6976\n",
      "     21      877.4310  8.7967\n",
      "     22      \u001b[36m834.4876\u001b[0m  8.7123\n",
      "     23      \u001b[36m825.1974\u001b[0m  8.7115\n",
      "     24      \u001b[36m749.1359\u001b[0m  8.7450\n",
      "     25      751.1039  8.6844\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m604563.9643\u001b[0m  8.8427\n",
      "      2    \u001b[36m68764.4726\u001b[0m  8.7503\n",
      "      3    \u001b[36m48090.0144\u001b[0m  8.7257\n",
      "      4     \u001b[36m4795.5801\u001b[0m  8.7601\n",
      "      5     \u001b[36m2104.2964\u001b[0m  8.7280\n",
      "      6     4110.8088  8.7080\n",
      "      7     3682.3010  8.7527\n",
      "      8     2312.4891  8.6892\n",
      "      9     \u001b[36m1110.0497\u001b[0m  8.8098\n",
      "     10      \u001b[36m546.7665\u001b[0m  8.7421\n",
      "     11      \u001b[36m479.3205\u001b[0m  8.7242\n",
      "     12      \u001b[36m440.8390\u001b[0m  8.7364\n",
      "     13      460.4692  8.7027\n",
      "     14      \u001b[36m420.1004\u001b[0m  8.7502\n",
      "     15      \u001b[36m400.6059\u001b[0m  8.6974\n",
      "     16      \u001b[36m381.6726\u001b[0m  8.7548\n",
      "     17      401.8700  8.7507\n",
      "     18      394.3574  8.6944\n",
      "     19      \u001b[36m375.7061\u001b[0m  8.7372\n",
      "     20      \u001b[36m330.9737\u001b[0m  8.6687\n",
      "     21      342.3420  8.7270\n",
      "     22      331.5345  8.6726\n",
      "     23      341.3996  8.7585\n",
      "     24      342.2555  8.7515\n",
      "     25      \u001b[36m321.0725\u001b[0m  8.6826\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m9391544.9335\u001b[0m  8.7197\n",
      "      2   \u001b[36m396233.3704\u001b[0m  8.7349\n",
      "      3   \u001b[36m226890.2465\u001b[0m  8.6839\n",
      "      4    \u001b[36m48348.9189\u001b[0m  8.7216\n",
      "      5    \u001b[36m15044.5476\u001b[0m  8.7227\n",
      "      6     \u001b[36m6950.7575\u001b[0m  8.7299\n",
      "      7     \u001b[36m3994.4319\u001b[0m  8.7527\n",
      "      8     \u001b[36m2635.5040\u001b[0m  8.7035\n",
      "      9     \u001b[36m2073.8430\u001b[0m  8.7519\n",
      "     10     \u001b[36m1683.8762\u001b[0m  8.6980\n",
      "     11     \u001b[36m1436.1021\u001b[0m  8.8115\n",
      "     12     \u001b[36m1220.4441\u001b[0m  8.7566\n",
      "     13     \u001b[36m1142.9429\u001b[0m  8.7104\n",
      "     14     \u001b[36m1034.2062\u001b[0m  8.7530\n",
      "     15      \u001b[36m940.0327\u001b[0m  8.7093\n",
      "     16      \u001b[36m842.5582\u001b[0m  8.7589\n",
      "     17      863.5636  8.7406\n",
      "     18      \u001b[36m749.3665\u001b[0m  8.7403\n",
      "     19      809.1840  8.7755\n",
      "     20      \u001b[36m713.3771\u001b[0m  8.8289\n",
      "     21      \u001b[36m614.2979\u001b[0m  8.7634\n",
      "     22      637.2876  8.7089\n",
      "     23      \u001b[36m552.0699\u001b[0m  8.7409\n",
      "     24      \u001b[36m527.3072\u001b[0m  8.7262\n",
      "     25      \u001b[36m516.8947\u001b[0m  8.7242\n",
      "Training ['Sweden'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m951435.9528\u001b[0m  8.7490\n",
      "      2   \u001b[36m290341.2264\u001b[0m  8.7298\n",
      "      3     \u001b[36m5522.1269\u001b[0m  8.6878\n",
      "      4    10320.2447  8.7600\n",
      "      5    13293.5916  8.6919\n",
      "      6    10076.4471  8.7594\n",
      "      7     6331.9643  8.7535\n",
      "      8     \u001b[36m3628.6483\u001b[0m  8.6932\n",
      "      9     \u001b[36m1982.9737\u001b[0m  8.7374\n",
      "     10     \u001b[36m1113.9324\u001b[0m  8.7053\n",
      "     11      \u001b[36m648.9854\u001b[0m  8.7567\n",
      "     12      \u001b[36m430.1449\u001b[0m  8.7374\n",
      "     13      \u001b[36m316.0254\u001b[0m  8.7190\n",
      "     14      \u001b[36m227.5503\u001b[0m  8.7759\n",
      "     15      \u001b[36m214.4741\u001b[0m  8.6972\n",
      "     16      \u001b[36m204.0926\u001b[0m  8.7686\n",
      "     17      205.8212  8.6941\n",
      "     18      \u001b[36m190.9698\u001b[0m  8.7560\n",
      "     19      196.6046  8.7492\n",
      "     20      \u001b[36m188.0224\u001b[0m  8.6897\n",
      "     21      197.7585  8.7318\n",
      "     22      203.0202  8.6902\n",
      "     23      \u001b[36m183.2279\u001b[0m  8.7614\n",
      "     24      \u001b[36m178.9935\u001b[0m  8.7118\n",
      "     25      \u001b[36m169.3791\u001b[0m  8.7136\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m5280202.6702\u001b[0m  8.8409\n",
      "      2   \u001b[36m860619.3148\u001b[0m  8.7489\n",
      "      3   \u001b[36m119899.6892\u001b[0m  8.6895\n",
      "      4     \u001b[36m5819.3116\u001b[0m  8.7516\n",
      "      5    10310.3467  8.7788\n",
      "      6    15533.3753  8.7384\n",
      "      7    16579.1047  8.7404\n",
      "      8    13690.9354  8.6926\n",
      "      9    10192.2191  8.7586\n",
      "     10     6949.2575  8.6995\n",
      "     11     \u001b[36m4975.7831\u001b[0m  8.7690\n",
      "     12     \u001b[36m3207.8544\u001b[0m  8.7326\n",
      "     13     \u001b[36m2191.7948\u001b[0m  8.7162\n",
      "     14     \u001b[36m1953.9258\u001b[0m  8.7685\n",
      "     15     \u001b[36m1604.8286\u001b[0m  8.7939\n",
      "     16     \u001b[36m1538.8970\u001b[0m  8.7636\n",
      "     17     1547.0323  8.6882\n",
      "     18     1677.2267  8.7410\n",
      "     19     1552.7597  8.7587\n",
      "     20     1581.9957  8.6966\n",
      "     21     \u001b[36m1510.5474\u001b[0m  8.7499\n",
      "     22     \u001b[36m1413.3343\u001b[0m  8.6964\n",
      "     23     1430.3695  8.7631\n",
      "     24     1510.5428  8.7115\n",
      "     25     \u001b[36m1299.8101\u001b[0m  8.7200\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m747959.5370\u001b[0m  8.8177\n",
      "      2   \u001b[36m227246.4326\u001b[0m  8.7832\n",
      "      3    \u001b[36m56053.7048\u001b[0m  8.7901\n",
      "      4    \u001b[36m54800.3850\u001b[0m  8.7457\n",
      "      5    \u001b[36m10575.6945\u001b[0m  8.6907\n",
      "      6     \u001b[36m3152.0530\u001b[0m  8.7638\n",
      "      7     6828.4777  8.7461\n",
      "      8     6568.9843  8.7127\n",
      "      9     3359.0457  8.7577\n",
      "     10     \u001b[36m1724.0851\u001b[0m  8.7020\n",
      "     11     \u001b[36m1426.5742\u001b[0m  8.7536\n",
      "     12     1574.1885  8.6897\n",
      "     13     1572.1741  8.7537\n",
      "     14     \u001b[36m1402.5482\u001b[0m  8.7578\n",
      "     15     \u001b[36m1261.4132\u001b[0m  8.7118\n",
      "     16     \u001b[36m1106.3033\u001b[0m  8.7654\n",
      "     17     1206.3166  8.7049\n",
      "     18     \u001b[36m1088.7537\u001b[0m  8.7461\n",
      "     19     \u001b[36m1064.5521\u001b[0m  8.7054\n",
      "     20     1067.0484  8.7162\n",
      "     21     \u001b[36m1046.6731\u001b[0m  8.7581\n",
      "     22      \u001b[36m945.8326\u001b[0m  8.6873\n",
      "     23     1146.3667  8.7278\n",
      "     24      \u001b[36m938.9269\u001b[0m  8.7105\n",
      "     25      \u001b[36m924.2790\u001b[0m  8.7411\n",
      "Training ['Finland'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m2670542.0995\u001b[0m  8.7827\n",
      "      2   \u001b[36m326644.3917\u001b[0m  8.7696\n",
      "      3    \u001b[36m38014.7005\u001b[0m  8.6959\n",
      "      4    85133.4899  8.7461\n",
      "      5    48584.8464  8.7418\n",
      "      6    \u001b[36m13740.8253\u001b[0m  8.8290\n",
      "      7     \u001b[36m2905.5749\u001b[0m  8.7113\n",
      "      8     \u001b[36m2371.7135\u001b[0m  8.7158\n",
      "      9     3314.8033  8.7344\n",
      "     10     3751.8398  8.6976\n",
      "     11     3140.9565  8.7886\n",
      "     12     \u001b[36m2206.4675\u001b[0m  8.6865\n",
      "     13     \u001b[36m1702.8712\u001b[0m  8.7232\n",
      "     14     \u001b[36m1523.3997\u001b[0m  8.7546\n",
      "     15     \u001b[36m1368.1677\u001b[0m  8.6882\n",
      "     16     1441.5413  8.7392\n",
      "     17     1443.8278  8.7207\n",
      "     18     1435.6006  8.8240\n",
      "     19     \u001b[36m1323.9536\u001b[0m  8.7048\n",
      "     20     \u001b[36m1288.7684\u001b[0m  8.7262\n",
      "     21     1349.5101  8.8279\n",
      "     22     1362.7454  8.7432\n",
      "     23     \u001b[36m1242.8744\u001b[0m  8.7725\n",
      "     24     \u001b[36m1238.5643\u001b[0m  8.6919\n",
      "     25     \u001b[36m1209.6561\u001b[0m  8.7556\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1145574.8406\u001b[0m  8.8268\n",
      "      2   \u001b[36m140873.3306\u001b[0m  8.7178\n",
      "      3     \u001b[36m2893.8451\u001b[0m  8.7044\n",
      "      4     4224.7644  8.7556\n",
      "      5     5381.1095  8.6916\n",
      "      6     4883.9187  8.7876\n",
      "      7     3938.7743  8.7308\n",
      "      8     \u001b[36m2796.3604\u001b[0m  8.7775\n",
      "      9     \u001b[36m2006.8472\u001b[0m  8.7682\n",
      "     10     \u001b[36m1498.2427\u001b[0m  8.7057\n",
      "     11     \u001b[36m1105.0996\u001b[0m  8.7430\n",
      "     12      \u001b[36m809.2621\u001b[0m  8.7040\n",
      "     13      \u001b[36m611.2713\u001b[0m  8.7492\n",
      "     14      \u001b[36m429.1705\u001b[0m  8.7181\n",
      "     15      \u001b[36m367.1675\u001b[0m  8.7270\n",
      "     16      \u001b[36m293.6598\u001b[0m  8.7544\n",
      "     17      \u001b[36m232.3355\u001b[0m  8.7004\n",
      "     18      \u001b[36m203.7904\u001b[0m  8.7567\n",
      "     19      \u001b[36m184.5070\u001b[0m  8.7001\n",
      "     20      \u001b[36m165.6563\u001b[0m  8.9039\n",
      "     21      \u001b[36m163.3553\u001b[0m  8.7745\n",
      "     22      \u001b[36m154.6206\u001b[0m  8.7177\n",
      "     23      157.1931  8.7746\n",
      "     24      155.1160  8.7361\n",
      "     25      \u001b[36m147.2431\u001b[0m  8.7471\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1288179.2327\u001b[0m  8.7876\n",
      "      2   \u001b[36m423421.3543\u001b[0m  8.7711\n",
      "      3    \u001b[36m15943.2175\u001b[0m  8.7264\n",
      "      4    53386.6428  8.7415\n",
      "      5    45841.2440  8.7706\n",
      "      6    23283.8960  8.7729\n",
      "      7     \u001b[36m9479.3751\u001b[0m  8.6983\n",
      "      8     \u001b[36m3339.3146\u001b[0m  8.7919\n",
      "      9     \u001b[36m1212.4543\u001b[0m  8.7478\n",
      "     10      \u001b[36m847.4414\u001b[0m  8.6902\n",
      "     11     1045.0247  8.7454\n",
      "     12     1329.1507  8.6826\n",
      "     13     1198.2634  8.7505\n",
      "     14     1043.1433  8.7210\n",
      "     15      \u001b[36m790.1474\u001b[0m  8.7093\n",
      "     16      \u001b[36m729.8270\u001b[0m  8.7288\n",
      "     17      \u001b[36m724.8031\u001b[0m  8.6847\n",
      "     18      \u001b[36m677.6388\u001b[0m  8.7549\n",
      "     19      \u001b[36m635.4788\u001b[0m  8.7056\n",
      "     20      \u001b[36m621.7193\u001b[0m  8.7457\n",
      "     21      \u001b[36m601.0903\u001b[0m  8.7585\n",
      "     22      617.7345  8.7520\n",
      "     23      643.0420  8.7639\n",
      "     24      \u001b[36m588.6733\u001b[0m  8.7060\n",
      "     25      \u001b[36m572.4702\u001b[0m  8.7739\n",
      "Training ['Finland'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1170663.3102\u001b[0m  8.8293\n",
      "      2    \u001b[36m98225.6876\u001b[0m  8.7248\n",
      "      3    \u001b[36m89314.6668\u001b[0m  8.7642\n",
      "      4     \u001b[36m8797.3671\u001b[0m  8.7488\n",
      "      5     \u001b[36m2858.3448\u001b[0m  8.6881\n",
      "      6     6805.1022  8.7431\n",
      "      7     6733.7726  8.7021\n",
      "      8     4398.2564  8.7475\n",
      "      9     \u001b[36m2189.8461\u001b[0m  8.7200\n",
      "     10     \u001b[36m1130.9930\u001b[0m  8.7307\n",
      "     11      \u001b[36m841.0881\u001b[0m  8.7655\n",
      "     12      \u001b[36m699.5497\u001b[0m  8.6798\n",
      "     13      707.5674  8.7313\n",
      "     14      \u001b[36m680.9498\u001b[0m  8.6819\n",
      "     15      \u001b[36m671.5236\u001b[0m  8.7471\n",
      "     16      725.1325  8.7370\n",
      "     17      \u001b[36m638.3448\u001b[0m  8.7075\n",
      "     18      \u001b[36m586.0114\u001b[0m  8.7570\n",
      "     19      603.7068  8.6861\n",
      "     20      626.2049  8.8432\n",
      "     21      \u001b[36m536.1736\u001b[0m  8.7530\n",
      "     22      \u001b[36m526.9241\u001b[0m  8.7166\n",
      "     23      \u001b[36m523.5110\u001b[0m  8.7572\n",
      "     24      528.8762  8.7216\n",
      "     25      \u001b[36m516.1291\u001b[0m  8.7515\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m1321280.4986\u001b[0m  8.8235\n",
      "      2   \u001b[36m465831.6759\u001b[0m  8.6901\n",
      "      3    \u001b[36m73429.2326\u001b[0m  8.8110\n",
      "      4   108893.5821  8.7216\n",
      "      5    \u001b[36m26059.7896\u001b[0m  8.6733\n",
      "      6     \u001b[36m9276.8469\u001b[0m  8.8068\n",
      "      7    17471.3641  8.7922\n",
      "      8    13219.5054  8.7354\n",
      "      9     \u001b[36m6916.8939\u001b[0m  8.7192\n",
      "     10     \u001b[36m4047.9183\u001b[0m  8.7166\n",
      "     11     4527.4210  8.7452\n",
      "     12     4398.4529  8.7465\n",
      "     13     \u001b[36m3777.0330\u001b[0m  8.7421\n",
      "     14     \u001b[36m3361.2538\u001b[0m  8.7207\n",
      "     15     \u001b[36m3291.3943\u001b[0m  8.8021\n",
      "     16     \u001b[36m3039.2861\u001b[0m  8.7418\n",
      "     17     3159.7531  8.6994\n",
      "     18     \u001b[36m2819.3684\u001b[0m  8.7445\n",
      "     19     \u001b[36m2773.0363\u001b[0m  8.6978\n",
      "     20     \u001b[36m2475.9348\u001b[0m  8.7533\n",
      "     21     \u001b[36m2323.7848\u001b[0m  8.7070\n",
      "     22     2468.1269  8.7320\n",
      "     23     \u001b[36m2319.3929\u001b[0m  8.7472\n",
      "     24     \u001b[36m2118.1773\u001b[0m  8.7060\n",
      "     25     \u001b[36m2060.7950\u001b[0m  8.7470\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m3262706.2044\u001b[0m  8.7977\n",
      "      2   \u001b[36m439443.3455\u001b[0m  8.6961\n",
      "      3     \u001b[36m6373.9244\u001b[0m  8.7505\n",
      "      4    12736.3364  8.7009\n",
      "      5    16967.8896  8.7193\n",
      "      6    14838.1632  8.7461\n",
      "      7    11196.4605  8.7041\n",
      "      8     8159.0592  8.7407\n",
      "      9     \u001b[36m6266.1970\u001b[0m  8.6801\n",
      "     10     \u001b[36m4291.5542\u001b[0m  8.7494\n",
      "     11     \u001b[36m3055.2815\u001b[0m  8.7493\n",
      "     12     \u001b[36m2361.4969\u001b[0m  8.7354\n",
      "     13     \u001b[36m1800.7231\u001b[0m  8.7720\n",
      "     14     \u001b[36m1445.1402\u001b[0m  8.7210\n",
      "     15      \u001b[36m989.7238\u001b[0m  8.7485\n",
      "     16      \u001b[36m834.2002\u001b[0m  8.7020\n",
      "     17      \u001b[36m667.5606\u001b[0m  8.7304\n",
      "     18      \u001b[36m619.0041\u001b[0m  8.7579\n",
      "     19      \u001b[36m562.9542\u001b[0m  8.6924\n",
      "     20      \u001b[36m429.7262\u001b[0m  8.7572\n",
      "     21      436.8378  8.6914\n",
      "     22      \u001b[36m366.6363\u001b[0m  8.7505\n",
      "     23      381.2601  8.7390\n",
      "     24      \u001b[36m353.7923\u001b[0m  8.7053\n",
      "     25      \u001b[36m318.0209\u001b[0m  8.7488\n",
      "Training ['Norway'], ['KaggleMart'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m427362.1290\u001b[0m  8.8184\n",
      "      2    \u001b[36m87447.8968\u001b[0m  8.7538\n",
      "      3    \u001b[36m16642.9040\u001b[0m  8.7772\n",
      "      4    \u001b[36m12928.9604\u001b[0m  8.7010\n",
      "      5    13758.8235  8.7377\n",
      "      6     \u001b[36m3841.2911\u001b[0m  8.7580\n",
      "      7     \u001b[36m1284.2734\u001b[0m  8.6988\n",
      "      8     2155.3087  8.7425\n",
      "      9     2018.2318  8.6915\n",
      "     10     \u001b[36m1251.0314\u001b[0m  8.7488\n",
      "     11      \u001b[36m807.1590\u001b[0m  8.7574\n",
      "     12      \u001b[36m746.0935\u001b[0m  8.6988\n",
      "     13      804.9100  8.7608\n",
      "     14      \u001b[36m733.8192\u001b[0m  8.6976\n",
      "     15      \u001b[36m665.1256\u001b[0m  8.7568\n",
      "     16      \u001b[36m601.5272\u001b[0m  8.7439\n",
      "     17      \u001b[36m600.3459\u001b[0m  8.7628\n",
      "     18      \u001b[36m536.0399\u001b[0m  8.7971\n",
      "     19      578.1552  8.7656\n",
      "     20      564.5524  8.7395\n",
      "     21      \u001b[36m521.7697\u001b[0m  8.7404\n",
      "     22      \u001b[36m501.5681\u001b[0m  8.7623\n",
      "     23      513.2176  8.7687\n",
      "     24      506.1269  8.7369\n",
      "     25      515.6977  8.7748\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Mug']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m582923.9258\u001b[0m  8.8358\n",
      "      2    \u001b[36m29879.4956\u001b[0m  8.7047\n",
      "      3    53773.4329  8.7449\n",
      "      4    \u001b[36m13352.9719\u001b[0m  8.7319\n",
      "      5     \u001b[36m2007.8365\u001b[0m  8.7656\n",
      "      6     4576.9585  8.7502\n",
      "      7     5195.3139  8.6774\n",
      "      8     2890.9462  8.7462\n",
      "      9     \u001b[36m1328.9427\u001b[0m  8.6862\n",
      "     10     \u001b[36m1014.9515\u001b[0m  8.7383\n",
      "     11     1139.7683  8.6962\n",
      "     12     1148.6805  8.7182\n",
      "     13     1087.9010  8.7620\n",
      "     14      \u001b[36m850.6331\u001b[0m  8.7249\n",
      "     15      \u001b[36m826.3373\u001b[0m  8.7733\n",
      "     16      831.8368  8.7140\n",
      "     17      \u001b[36m779.3705\u001b[0m  8.7971\n",
      "     18      796.0255  8.8215\n",
      "     19      \u001b[36m759.7393\u001b[0m  8.7176\n",
      "     20      \u001b[36m690.7757\u001b[0m  8.7543\n",
      "     21      721.0497  8.7039\n",
      "     22      \u001b[36m657.4486\u001b[0m  8.7396\n",
      "     23      \u001b[36m655.2230\u001b[0m  8.7184\n",
      "     24      667.5970  8.7202\n",
      "     25      \u001b[36m611.1380\u001b[0m  8.7466\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Hat']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m682584.6916\u001b[0m  8.7977\n",
      "      2   \u001b[36m182407.8307\u001b[0m  8.6962\n",
      "      3     \u001b[36m7339.1445\u001b[0m  8.7500\n",
      "      4    17593.6346  8.7773\n",
      "      5    13798.5239  8.8122\n",
      "      6     7706.6918  8.7882\n",
      "      7     \u001b[36m3744.9614\u001b[0m  8.7095\n",
      "      8     \u001b[36m1739.2022\u001b[0m  8.7567\n",
      "      9      \u001b[36m911.8471\u001b[0m  8.7150\n",
      "     10      \u001b[36m578.9791\u001b[0m  8.7654\n",
      "     11      \u001b[36m335.1375\u001b[0m  8.7163\n",
      "     12      \u001b[36m298.8548\u001b[0m  8.7266\n",
      "     13      \u001b[36m269.6914\u001b[0m  8.7838\n",
      "     14      288.0521  8.6910\n",
      "     15      \u001b[36m262.1508\u001b[0m  8.7624\n",
      "     16      271.4407  8.6958\n",
      "     17      \u001b[36m260.3376\u001b[0m  8.7076\n",
      "     18      \u001b[36m256.4191\u001b[0m  8.7212\n",
      "     19      \u001b[36m234.0766\u001b[0m  8.6628\n",
      "     20      239.7389  8.7624\n",
      "     21      \u001b[36m225.2026\u001b[0m  8.6766\n",
      "     22      \u001b[36m224.1615\u001b[0m  8.7290\n",
      "     23      229.8902  8.6904\n",
      "     24      227.7147  8.7546\n",
      "     25      235.4675  8.7528\n",
      "Training ['Norway'], ['KaggleRama'], ['Kaggle Sticker']\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1  \u001b[36m9549701.3932\u001b[0m  8.8730\n",
      "      2   \u001b[36m463848.4453\u001b[0m  8.6919\n",
      "      3   \u001b[36m235921.6435\u001b[0m  8.7532\n",
      "      4    \u001b[36m26194.2808\u001b[0m  8.6919\n",
      "      5     \u001b[36m4269.9622\u001b[0m  8.7423\n",
      "      6     \u001b[36m1096.5273\u001b[0m  8.6972\n",
      "      7      \u001b[36m493.6063\u001b[0m  8.7200\n",
      "      8      \u001b[36m342.8070\u001b[0m  8.7992\n",
      "      9      \u001b[36m340.7266\u001b[0m  8.7218\n",
      "     10      349.7703  8.7342\n",
      "     11      \u001b[36m331.6966\u001b[0m  8.6821\n",
      "     12      344.3191  8.7519\n",
      "     13      \u001b[36m330.1243\u001b[0m  8.7428\n",
      "     14      364.2717  8.6921\n",
      "     15      354.2458  8.7562\n",
      "     16      \u001b[36m318.6248\u001b[0m  8.6942\n",
      "     17      323.2590  8.7261\n",
      "     18      327.5145  8.7153\n",
      "     19      \u001b[36m293.4634\u001b[0m  8.7511\n",
      "     20      321.9534  8.7396\n",
      "     21      313.6021  8.7393\n",
      "     22      303.7723  8.8812\n",
      "     23      300.9797  8.7730\n",
      "     24      \u001b[36m280.6786\u001b[0m  8.7522\n",
      "     25      283.0801  8.7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-29 03:22:36,662]\u001b[0m Trial 9 finished with value: 183.7844219789372 and parameters: {'levels': 12, 'hidden_units': 13, 'kernel_size': 6, 'dropout': 0.10934205586865593, 'learning_rate': 0.002870165242185818}. Best is trial 2 with value: 125.41270930622379.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ['Sweden'], ['KaggleMart'], ['Kaggle Mug']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2022-01-29 03:22:39,602]\u001b[0m Trial 10 failed because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 2.05 GiB (GPU 0; 7.79 GiB total capacity; 2.56 GiB already allocated; 1.46 GiB free; 2.68 GiB reserved in total by PyTorch)')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-66-c1e22e86d9b8>\", line 81, in objective\n",
      "    return skorch_trainer(model_kwargs=model_params, wandb_tracked=False)#, telegram=False)\n",
      "  File \"<ipython-input-59-a10dd6e2e2b8>\", line 147, in skorch_trainer\n",
      "    net.fit(X,y)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\", line 1215, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\", line 1174, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\", line 1087, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\", line 1122, in run_single_epoch\n",
      "    step = step_fn(batch, **fit_params)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\", line 1007, in train_step\n",
      "    self._step_optimizer(step_fn)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\", line 963, in _step_optimizer\n",
      "    optimizer.step(step_fn)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/torch/optim/adam.py\", line 66, in step\n",
      "    loss = closure()\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\", line 997, in step_fn\n",
      "    step = self.train_step_single(batch, **fit_params)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\", line 898, in train_step_single\n",
      "    loss.backward()\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/torch/tensor.py\", line 245, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/sf/anaconda3/envs/time/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 145, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.05 GiB (GPU 0; 7.79 GiB total capacity; 2.56 GiB already allocated; 1.46 GiB free; 2.68 GiB reserved in total by PyTorch)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.05 GiB (GPU 0; 7.79 GiB total capacity; 2.56 GiB already allocated; 1.46 GiB free; 2.68 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-c1e22e86d9b8>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, arch)\u001b[0m\n\u001b[1;32m     79\u001b[0m         }\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mskorch_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_tracked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, telegram=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-a10dd6e2e2b8>\u001b[0m in \u001b[0;36mskorch_trainer\u001b[0;34m(model, model_kwargs, tv_df, test_df, folds, countries, stores, products, random_seed, target, wandb_tracked, forecasting)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     )\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;31m#                     net.save_params(f_params=modelpath/f'20220128-TCN-country{country}-store{store}-product{product}-model_params.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/regressor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# this is actually a pylint bug:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# https://github.com/PyCQA/pylint/issues/1085\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralNetRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_train_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_epoch_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mon_epoch_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n\u001b[0m\u001b[1;32m   1088\u001b[0m                                   step_fn=self.train_step, **fit_params)\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mrun_single_epoch\u001b[0;34m(self, dataset, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_batch_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             batch_size = (get_len(batch[0]) if isinstance(batch, (tuple, list))\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\u001b[0m in \u001b[0;36m_step_optimizer\u001b[0;34m(self, step_fn)\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mstep_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero_grad_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m             \u001b[0mstep_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mtrain_step_single\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         return {\n\u001b[1;32m    900\u001b[0m             \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/time/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.05 GiB (GPU 0; 7.79 GiB total capacity; 2.56 GiB already allocated; 1.46 GiB free; 2.68 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for x in range(1, 500):\n",
    "    study.optimize(objective, n_trials = 1, callbacks = [wandbc], show_progress_bar=False)#, catch=(xgboost.core.XGBoostError,)) \n",
    "    dump(study, filename=studypath/f\"optuna_{arch}_study-{start_time}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "39c0a502-eb17-427b-83a7-4f3e96a501a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 605648... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dropout</td><td>▅▅▁▃▂▅█▅█▄</td></tr><tr><td>hidden_units</td><td>█▁█▃▂▂▂▃▄▃</td></tr><tr><td>kernel_size</td><td>▇██▅▃▅▁▁▁▅</td></tr><tr><td>learning_rate</td><td>▁▃▁▁▁▁█▁▁▁</td></tr><tr><td>levels</td><td>▅▂▁▃▆█▆█▂▇</td></tr><tr><td>trials_in_study</td><td>▁</td></tr><tr><td>value</td><td>██▁▅▇██▇▄█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_params</td><td>{'levels': 5, 'hidde...</td></tr><tr><td>dropout</td><td>0.10934</td></tr><tr><td>hidden_units</td><td>13</td></tr><tr><td>kernel_size</td><td>6</td></tr><tr><td>learning_rate</td><td>0.00287</td></tr><tr><td>levels</td><td>12</td></tr><tr><td>trials_in_study</td><td>11</td></tr><tr><td>value</td><td>183.78442</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">optuna_forecasting_20220128_210412</strong>: <a href=\"https://wandb.ai/hushifang/uncategorized/runs/3bbrnt0c\" target=\"_blank\">https://wandb.ai/hushifang/uncategorized/runs/3bbrnt0c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220128_210412-3bbrnt0c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'best_params': str(study.best_trial.params),\n",
    "#            'trials_in_run': len(study.trials),\n",
    "           'trials_in_study': len(study.trials)\n",
    "          })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "61f369d1-f1a9-405d-b7d6-eba5efaeb5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'levels': 5,\n",
       "  'hidden_units': 30,\n",
       "  'kernel_size': 9,\n",
       "  'dropout': 0.04246782213565523,\n",
       "  'learning_rate': 0.002820996133514492},\n",
       " 125.41270930622379)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params, study.best_trial.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dfb2eb48-f44b-43ab-9cd4-970ed7d7f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = datetime.now().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "edb698ce-e8b3-49b5-8839-cd02b147b3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 ms, sys: 0 ns, total: 2.07 ms\n",
      "Wall time: 2.12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# xgb_forecast_oof_preds, xgb_forecast_test_preds = gbm_trainer(arch='xgboost', model_kwargs=xgboost_params)\n",
    "# dump(xgb_forecast_oof_preds, predpath/f'{DATE}_xgboost_oof_preds.joblib')\n",
    "# dump(xgb_forecast_test_preds, predpath/f'{DATE}_xgboost_test_preds.joblib')\n",
    "xgb_forecast_oof_preds = load(predpath/f'{DATE}_xgboost_oof_preds.joblib')\n",
    "xgb_forecast_test_preds = load(predpath/f'{DATE}_xgboost_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7ccf6db9-3da2-4088-8eb6-0b5bae56c3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# xgb1701_forecast_oof_preds, xgb1701_forecast_test_preds = gbm_trainer(arch='xgboost', random_seed=1701, model_kwargs=xgboost_params)\n",
    "# dump(xgb1701_forecast_oof_preds, predpath/f'{DATE}_xgboost1701_oof_preds.joblib')\n",
    "# dump(xgb1701_forecast_test_preds, predpath/f'{DATE}_xgboost1701_test_preds.joblib')\n",
    "# xgb1701_forecast_oof_preds = load(predpath/'20220127_xgboost1701_oof_preds.joblib')\n",
    "# xgb1701_forecast_test_preds = load(predpath/'20220127_xgboost1701_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18508b4e-ecdb-4f12-a3ce-78221cb63ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# xgb41918_forecast_oof_preds, xgb41918_forecast_test_preds = gbm_trainer(arch='xgboost', random_seed=41918, model_kwargs=xgboost_params)\n",
    "# dump(xgb41918_forecast_oof_preds, predpath/f'{DATE}_xgboost41918_oof_preds.joblib')\n",
    "# dump(xgb41918_forecast_test_preds, predpath/f'{DATE}_xgboost41918_test_preds.joblib')\n",
    "# xgb41918_forecast_oof_preds = load(predpath/'20220127_xgboost41918_oof_preds.joblib')\n",
    "# xgb41918_forecast_test_preds = load(predpath/'20220127_xgboost41918_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e742b58-9e0a-4fed-b79f-c29105bba7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.3 ms, sys: 178 µs, total: 2.48 ms\n",
      "Wall time: 2.23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# lgb_forecast_oof_preds, lgb_forecast_test_preds = gbm_trainer(arch='lightgbm', model_kwargs=lightgbm_params)\n",
    "# dump(lgb_forecast_oof_preds, predpath/f'{DATE}_lightgbm_oof_preds.joblib')\n",
    "# dump(lgb_forecast_test_preds, predpath/f'{DATE}_lightgbm_test_preds.joblib')\n",
    "lgb_forecast_oof_preds = load(predpath/f'{DATE}_lightgbm_oof_preds.joblib')\n",
    "lgb_forecast_test_preds = load(predpath/f'{DATE}_lightgbm_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bca1d83f-533d-47ae-a7f1-652249a73931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.53 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# lgb1701_forecast_oof_preds, lgb1701_forecast_test_preds = gbm_trainer(arch='lightgbm', random_seed=1701, model_kwargs=lightgbm_params)\n",
    "# dump(lgb1701_forecast_oof_preds, predpath/f'{DATE}_lightgbm1701_oof_preds.joblib')\n",
    "# dump(lgb1701_forecast_test_preds, predpath/f'{DATE}_lightgbm1701_test_preds.joblib')\n",
    "# lgb1701_forecast_oof_preds = load(predpath/'20220127_lightgbm1701_oof_preds.joblib')\n",
    "# lgb1701_forecast_test_preds = load(predpath/'20220127_lightgbm1701_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "23f40c8f-50af-43a2-9e0f-3bdc0c044189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# lgb41918_forecast_oof_preds, lgb41918_forecast_test_preds = gbm_trainer(arch='lightgbm', random_seed=41918, model_kwargs=lightgbm_params)\n",
    "# dump(lgb41918_forecast_oof_preds, predpath/f'{DATE}_lightgbm41918_oof_preds.joblib')\n",
    "# dump(lgb41918_forecast_test_preds, predpath/f'{DATE}_lightgbm41918_test_preds.joblib')\n",
    "# lgb41918_forecast_oof_preds = load(predpath/f'{DATE}_lightgbm41918_oof_preds.joblib')\n",
    "# lgb41918_forecast_test_preds = load(predpath/f'{DATE}_lightgbm41918_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "efdf54a7-cba8-41cb-a0ad-eb6021a54998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 ms, sys: 154 µs, total: 2.15 ms\n",
      "Wall time: 2.12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# cat_forecast_oof_preds, cat_forecast_test_preds = gbm_trainer(arch='catboost', model_kwargs=catboost_params)\n",
    "# dump(cat_forecast_oof_preds, predpath/f'{DATE}_catboost_oof_preds.joblib')\n",
    "# dump(cat_forecast_test_preds, predpath/f'{DATE}_catboost_test_preds.joblib')\n",
    "\n",
    "cat_forecast_oof_preds = load(predpath/f'{DATE}_catboost_oof_preds.joblib')\n",
    "cat_forecast_test_preds = load(predpath/f'{DATE}_catboost_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c7c3b7a5-c8b8-4583-9ed2-fcde4356b13a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# cat1701_forecast_oof_preds, cat1701_forecast_test_preds = gbm_trainer(arch='catboost', random_seed=1701, model_kwargs=catboost_params)\n",
    "# dump(cat1701_forecast_oof_preds, predpath/f'{DATE}_catboost1701_oof_preds.joblib')\n",
    "# dump(cat1701_forecast_test_preds, predpath/f'{DATE}_catboost1701_test_preds.joblib')\n",
    "\n",
    "# cat1701_forecast_oof_preds = load(predpath/'20220127_catboost1701_oof_preds.joblib')\n",
    "# cat1701_forecast_test_preds = load(predpath/'20220127_catboost1701_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1f01448-3ef2-4a02-85a5-90f51f56d5af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# cat41918_forecast_oof_preds, cat41918_forecast_test_preds = gbm_trainer(arch='catboost', random_seed=41918, model_kwargs=catboost_params)\n",
    "# dump(cat41918_forecast_oof_preds, predpath/f'{DATE}_catboost41918_oof_preds.joblib')\n",
    "# dump(cat41918_forecast_test_preds, predpath/f'{DATE}_catboost41918_test_preds.joblib')\n",
    "\n",
    "# cat41918_forecast_oof_preds = load(predpath/'20220127_catboost41918_oof_preds.joblib')\n",
    "# cat41918_forecast_test_preds = load(predpath/'20220127_catboost41918_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "04c45b67-c595-4155-92c6-80828d312510",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 4.77 µs\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Sweden - KaggleMart - Kaggle Mug - Train SMAPE: 6.815541\n",
      "Validation Range [2018-01-01, 2019-01-01) - Sweden - KaggleMart - Kaggle Mug - Validation SMAPE: 8.011073\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Sweden - KaggleMart - Kaggle Hat - Train SMAPE: 7.210369\n",
      "Validation Range [2018-01-01, 2019-01-01) - Sweden - KaggleMart - Kaggle Hat - Validation SMAPE: 7.470548\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Sweden - KaggleMart - Kaggle Sticker - Train SMAPE: 7.056273\n",
      "Validation Range [2018-01-01, 2019-01-01) - Sweden - KaggleMart - Kaggle Sticker - Validation SMAPE: 7.264969\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Sweden - KaggleRama - Kaggle Mug - Train SMAPE: 6.716048\n",
      "Validation Range [2018-01-01, 2019-01-01) - Sweden - KaggleRama - Kaggle Mug - Validation SMAPE: 7.406634\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Sweden - KaggleRama - Kaggle Hat - Train SMAPE: 7.000068\n",
      "Validation Range [2018-01-01, 2019-01-01) - Sweden - KaggleRama - Kaggle Hat - Validation SMAPE: 7.162872\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Sweden - KaggleRama - Kaggle Sticker - Train SMAPE: 6.674554\n",
      "Validation Range [2018-01-01, 2019-01-01) - Sweden - KaggleRama - Kaggle Sticker - Validation SMAPE: 7.606732\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Finland - KaggleMart - Kaggle Mug - Train SMAPE: 7.224865\n",
      "Validation Range [2018-01-01, 2019-01-01) - Finland - KaggleMart - Kaggle Mug - Validation SMAPE: 7.034564\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Finland - KaggleMart - Kaggle Hat - Train SMAPE: 7.135125\n",
      "Validation Range [2018-01-01, 2019-01-01) - Finland - KaggleMart - Kaggle Hat - Validation SMAPE: 6.984664\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Finland - KaggleMart - Kaggle Sticker - Train SMAPE: 6.903014\n",
      "Validation Range [2018-01-01, 2019-01-01) - Finland - KaggleMart - Kaggle Sticker - Validation SMAPE: 6.821940\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Finland - KaggleRama - Kaggle Mug - Train SMAPE: 7.026900\n",
      "Validation Range [2018-01-01, 2019-01-01) - Finland - KaggleRama - Kaggle Mug - Validation SMAPE: 6.906350\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Finland - KaggleRama - Kaggle Hat - Train SMAPE: 7.069145\n",
      "Validation Range [2018-01-01, 2019-01-01) - Finland - KaggleRama - Kaggle Hat - Validation SMAPE: 7.116741\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Finland - KaggleRama - Kaggle Sticker - Train SMAPE: 7.058737\n",
      "Validation Range [2018-01-01, 2019-01-01) - Finland - KaggleRama - Kaggle Sticker - Validation SMAPE: 7.042075\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Norway - KaggleMart - Kaggle Mug - Train SMAPE: 7.287884\n",
      "Validation Range [2018-01-01, 2019-01-01) - Norway - KaggleMart - Kaggle Mug - Validation SMAPE: 6.890005\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Norway - KaggleMart - Kaggle Hat - Train SMAPE: 7.413405\n",
      "Validation Range [2018-01-01, 2019-01-01) - Norway - KaggleMart - Kaggle Hat - Validation SMAPE: 7.504373\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Norway - KaggleMart - Kaggle Sticker - Train SMAPE: 7.033793\n",
      "Validation Range [2018-01-01, 2019-01-01) - Norway - KaggleMart - Kaggle Sticker - Validation SMAPE: 7.441114\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Norway - KaggleRama - Kaggle Mug - Train SMAPE: 7.094054\n",
      "Validation Range [2018-01-01, 2019-01-01) - Norway - KaggleRama - Kaggle Mug - Validation SMAPE: 7.095961\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Norway - KaggleRama - Kaggle Hat - Train SMAPE: 7.359608\n",
      "Validation Range [2018-01-01, 2019-01-01) - Norway - KaggleRama - Kaggle Hat - Validation SMAPE: 7.178791\n",
      "\n",
      "\n",
      "Training Range [2015-01-01, 2018-01-01) - Norway - KaggleRama - Kaggle Sticker - Train SMAPE: 6.934721\n",
      "Validation Range [2018-01-01, 2019-01-01) - Norway - KaggleRama - Kaggle Sticker - Validation SMAPE: 7.135771\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/preds/20220129_prophet_test_preds.joblib']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "prophet_tv_preds, prophet_test_preds = prophet_trainer(target='num_sold')\n",
    "dump(prophet_tv_preds, predpath/f'{DATE}_prophet_oof_preds.joblib')\n",
    "dump(prophet_test_preds, predpath/f'{DATE}_prophet_test_preds.joblib')\n",
    "\n",
    "# prophet_tv_preds = load(predpath/'20220127_prophet_oof_preds.joblib')\n",
    "# prophet_test_preds = load(predpath/'20220127_prophet_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "671874c4-3fec-4bb1-acfd-ed377a7e58ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.53 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# neural_tv_preds, neural_test_preds = neuralprophet_trainer(target='num_sold')\n",
    "# dump(neural_tv_preds, predpath/f'{DATE}_neuralprophet_tv_preds.joblib')\n",
    "# dump(neural_test_preds, predpath/f'{DATE}_neuralprophet_test_preds.joblib')\n",
    "neural_tv_preds = load(predpath/f'{DATE}_neuralprophet_tv_preds.joblib')\n",
    "neural_test_preds = load(predpath/f'{DATE}_neuralprophet_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba8ec95a-702d-4977-a5a1-c0be6fd3f869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# ridge_tv_preds, ridge_test_preds = sklearn_trainer(estimator=Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26593c2b-48a8-4941-acba-b19b9e775c59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.61 s, sys: 135 ms, total: 5.75 s\n",
      "Wall time: 769 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_combo_tv_preds, ridge_combo_test_preds = sklearn_trainer(estimator=Ridge)#, by_combo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68dc0965-4453-4bb8-98ca-28f020cfc4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# linear_tv_preds, linear_test_preds = sklearn_trainer(estimator=LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9f3df316-714d-4f71-91d3-e3aef8285b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.98 s, sys: 200 ms, total: 7.18 s\n",
      "Wall time: 900 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "linear_combo_tv_preds, linear_combo_test_preds = sklearn_trainer(estimator=LinearRegression)#, by_combo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "58dbf5f8-68df-4893-b07f-9798fb046a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 s, sys: 63.8 ms, total: 2.26 s\n",
      "Wall time: 971 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "huber_combo_tv_preds, huber_combo_test_preds = sklearn_trainer(estimator=HuberRegressor)#, by_combo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9684657-1299-48c4-85c3-65f8943ba97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# mlp_combo_tv_preds, mlp_combo_test_preds = sklearn_trainer(estimator=MLPRegressor)#, by_combo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7fdadc1d-946c-42bd-ac0c-67805d248bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 756 ms, sys: 16 ms, total: 772 ms\n",
      "Wall time: 773 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lasso_combo_tv_preds, lasso_combo_test_preds = sklearn_trainer(estimator=Lasso)#), by_combo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af131a8d-bd65-42f1-b9b3-7ffe173d9fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyearth import Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1800e18b-c837-4f9e-9ee3-7438fa05655b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.62 ms, sys: 0 ns, total: 4.62 ms\n",
      "Wall time: 465 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# earth_combo_tv_preds, earth_combo_test_preds = sklearn_trainer(estimator=Earth), #by_combo=True)\n",
    "# mystery = sklearn_trainer(estimator=Earth)\n",
    "# earth_combo_tv_preds, earth_combo_test_preds = mystery\n",
    "# dump(earth_combo_tv_preds, predpath/f'{DATE}_earth_oof_preds.joblib')\n",
    "# dump(earth_combo_test_preds, predpath/f'{DATE}_earth_test_preds.joblib')\n",
    "\n",
    "earth_combo_tv_preds = load(predpath/f'20220127_earth_oof_preds.joblib')\n",
    "earth_combo_test_preds = load(predpath/f'20220127_earth_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f929d8e1-13c9-4a1c-a0d1-d29f5e7d218c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         360.672190\n",
       "1         584.851806\n",
       "2         138.401176\n",
       "3         562.479785\n",
       "4         944.584508\n",
       "            ...     \n",
       "26293     836.947940\n",
       "26294     254.287204\n",
       "26295    1058.174059\n",
       "26296    2036.549357\n",
       "26297     472.307718\n",
       "Name: model_forecast, Length: 26298, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earth_combo_tv_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "093e0c26-07f7-40f7-9709-654ae0710cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tv = orig_train_df['num_sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "03467410-5520-4a5b-9bb8-8ed743410ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.098824805150043\n",
      "4.7690929069301795\n",
      "4.1715986305416815\n",
      "18.660584396082328\n",
      "7.926662675729486\n",
      "15.11713989060885\n",
      "5.338351055481194\n",
      "6.006121995525318\n",
      "5.979081928638902\n",
      "5.946422999458977\n"
     ]
    }
   ],
   "source": [
    "for preds in [prophet_tv_preds, ridge_combo_tv_preds, linear_combo_tv_preds, huber_combo_tv_preds, neural_tv_preds, lasso_combo_tv_preds, earth_combo_tv_preds, xgb_forecast_oof_preds, lgb_forecast_oof_preds, cat_forecast_oof_preds]:\n",
    "    print(SMAPE(y_pred=preds, y_true=y_tv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d933b-7036-4667-9023-4d2aa6135433",
   "metadata": {},
   "source": [
    "Clearly, doing it by combination is the way to go. I think Ridge and LinearRegressor are definitely good to use; Prophet and NeuralProphet are worth including as well. Huber, probably not; MLPRegressor, definitely not barring the discovery of better hyperparams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115ef92-8be4-45d5-a153-4f1e4aedfc04",
   "metadata": {},
   "source": [
    "### Forecast Bundling\n",
    "Now, create an iterable collection consisting of all the forecaster predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "257a398a-c8c5-45ee-a426-c44b3779af05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_tv_preds = {\n",
    "    'prophet': prophet_tv_preds,\n",
    "    'neuralprophet': neural_tv_preds,\n",
    "    'ridge': ridge_combo_tv_preds,\n",
    "    'linear': linear_combo_tv_preds,\n",
    "#     'huber': huber_combo_tv_preds,\n",
    "#     'lasso': lasso_combo_tv_preds,\n",
    "    'earth': earth_combo_tv_preds,\n",
    "    'xgboost': xgb_forecast_oof_preds, \n",
    "    'lightgbm': lgb_forecast_oof_preds, \n",
    "    'catboost': cat_forecast_oof_preds,\n",
    "#     'xgb1701': xgb1701_forecast_oof_preds,\n",
    "#     'xgb41918': xgb41918_forecast_oof_preds,\n",
    "#     'lgb1701': lgb1701_forecast_oof_preds,\n",
    "#     'lgb41918': lgb41918_forecast_oof_preds,\n",
    "#     'cat1701': cat1701_forecast_oof_preds,\n",
    "#     'cat41918': cat41918_forecast_oof_preds\n",
    "}\n",
    "\n",
    "forecast_test_preds = {\n",
    "    'prophet': prophet_test_preds,\n",
    "    'neuralprophet': neural_test_preds,\n",
    "    'ridge': ridge_combo_test_preds,\n",
    "    'linear': linear_combo_test_preds,\n",
    "#     'huber': huber_combo_test_preds,\n",
    "#     'lasso': lasso_combo_test_preds,\n",
    "    'earth': earth_combo_test_preds,\n",
    "    'xgboost': xgb_forecast_test_preds, \n",
    "    'lightgbm': lgb_forecast_test_preds, \n",
    "    'catboost': cat_forecast_test_preds,\n",
    "#     'xgb1701': xgb1701_forecast_test_preds,\n",
    "#     'xgb41918': xgb41918_forecast_test_preds,\n",
    "#     'lgb1701': lgb1701_forecast_test_preds,\n",
    "#     'lgb41918': lgb41918_forecast_test_preds,\n",
    "#     'cat1701': cat1701_forecast_test_preds,\n",
    "#     'cat41918': cat41918_forecast_test_preds,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1304683-0d0b-4386-ad43-cac2a1a9eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_preds = pd.DataFrame({\n",
    "    'date': orig_train_df['date'],\n",
    "    'num_sold': orig_train_df['num_sold'],\n",
    "    **forecast_tv_preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e3134137-eb04-4b15-b1f9-01051867ee80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>329</td>\n",
       "      <td>346.560416</td>\n",
       "      <td>329.134521</td>\n",
       "      <td>282.947564</td>\n",
       "      <td>323.494168</td>\n",
       "      <td>360.672190</td>\n",
       "      <td>329.699150</td>\n",
       "      <td>337.386928</td>\n",
       "      <td>316.636303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>520</td>\n",
       "      <td>536.203586</td>\n",
       "      <td>458.008301</td>\n",
       "      <td>441.709447</td>\n",
       "      <td>502.785894</td>\n",
       "      <td>584.851806</td>\n",
       "      <td>489.933725</td>\n",
       "      <td>543.343607</td>\n",
       "      <td>508.221832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>146</td>\n",
       "      <td>143.412803</td>\n",
       "      <td>145.325577</td>\n",
       "      <td>121.161437</td>\n",
       "      <td>136.434389</td>\n",
       "      <td>138.401176</td>\n",
       "      <td>134.669764</td>\n",
       "      <td>167.039891</td>\n",
       "      <td>139.910672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>572</td>\n",
       "      <td>590.117165</td>\n",
       "      <td>552.571167</td>\n",
       "      <td>491.090363</td>\n",
       "      <td>557.025587</td>\n",
       "      <td>562.479785</td>\n",
       "      <td>571.549171</td>\n",
       "      <td>581.483568</td>\n",
       "      <td>552.913777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>911</td>\n",
       "      <td>939.673009</td>\n",
       "      <td>781.967285</td>\n",
       "      <td>781.160208</td>\n",
       "      <td>889.998292</td>\n",
       "      <td>944.584508</td>\n",
       "      <td>898.322803</td>\n",
       "      <td>942.031061</td>\n",
       "      <td>883.653559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>823</td>\n",
       "      <td>898.322121</td>\n",
       "      <td>669.418457</td>\n",
       "      <td>716.770601</td>\n",
       "      <td>844.702073</td>\n",
       "      <td>836.947940</td>\n",
       "      <td>771.448493</td>\n",
       "      <td>838.133988</td>\n",
       "      <td>812.599656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>250</td>\n",
       "      <td>253.512355</td>\n",
       "      <td>227.158142</td>\n",
       "      <td>204.201638</td>\n",
       "      <td>240.365029</td>\n",
       "      <td>254.287204</td>\n",
       "      <td>204.990275</td>\n",
       "      <td>247.800215</td>\n",
       "      <td>235.944831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1004</td>\n",
       "      <td>1039.635205</td>\n",
       "      <td>715.639648</td>\n",
       "      <td>818.296836</td>\n",
       "      <td>970.936534</td>\n",
       "      <td>1058.174059</td>\n",
       "      <td>877.666985</td>\n",
       "      <td>927.313209</td>\n",
       "      <td>907.747468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1441</td>\n",
       "      <td>1526.908216</td>\n",
       "      <td>980.234009</td>\n",
       "      <td>1237.753414</td>\n",
       "      <td>1455.732603</td>\n",
       "      <td>2036.549357</td>\n",
       "      <td>1330.550686</td>\n",
       "      <td>1458.114228</td>\n",
       "      <td>1409.321391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>388</td>\n",
       "      <td>463.705207</td>\n",
       "      <td>423.052612</td>\n",
       "      <td>367.633099</td>\n",
       "      <td>438.365858</td>\n",
       "      <td>472.307718</td>\n",
       "      <td>381.841982</td>\n",
       "      <td>426.105716</td>\n",
       "      <td>411.451216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  num_sold      prophet  neuralprophet        ridge  \\\n",
       "0      2015-01-01       329   346.560416     329.134521   282.947564   \n",
       "1      2015-01-01       520   536.203586     458.008301   441.709447   \n",
       "2      2015-01-01       146   143.412803     145.325577   121.161437   \n",
       "3      2015-01-01       572   590.117165     552.571167   491.090363   \n",
       "4      2015-01-01       911   939.673009     781.967285   781.160208   \n",
       "...           ...       ...          ...            ...          ...   \n",
       "26293  2018-12-31       823   898.322121     669.418457   716.770601   \n",
       "26294  2018-12-31       250   253.512355     227.158142   204.201638   \n",
       "26295  2018-12-31      1004  1039.635205     715.639648   818.296836   \n",
       "26296  2018-12-31      1441  1526.908216     980.234009  1237.753414   \n",
       "26297  2018-12-31       388   463.705207     423.052612   367.633099   \n",
       "\n",
       "            linear        earth      xgboost     lightgbm     catboost  \n",
       "0       323.494168   360.672190   329.699150   337.386928   316.636303  \n",
       "1       502.785894   584.851806   489.933725   543.343607   508.221832  \n",
       "2       136.434389   138.401176   134.669764   167.039891   139.910672  \n",
       "3       557.025587   562.479785   571.549171   581.483568   552.913777  \n",
       "4       889.998292   944.584508   898.322803   942.031061   883.653559  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "26293   844.702073   836.947940   771.448493   838.133988   812.599656  \n",
       "26294   240.365029   254.287204   204.990275   247.800215   235.944831  \n",
       "26295   970.936534  1058.174059   877.666985   927.313209   907.747468  \n",
       "26296  1455.732603  2036.549357  1330.550686  1458.114228  1409.321391  \n",
       "26297   438.365858   472.307718   381.841982   426.105716   411.451216  \n",
       "\n",
       "[26298 rows x 10 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "17daeb57-cc7a-4449-9257-437edec6f194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19728</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>405</td>\n",
       "      <td>383.365770</td>\n",
       "      <td>374.809906</td>\n",
       "      <td>344.131415</td>\n",
       "      <td>416.085371</td>\n",
       "      <td>382.162985</td>\n",
       "      <td>348.545834</td>\n",
       "      <td>375.494286</td>\n",
       "      <td>348.215962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19729</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>621</td>\n",
       "      <td>602.594228</td>\n",
       "      <td>539.082520</td>\n",
       "      <td>539.757025</td>\n",
       "      <td>639.857859</td>\n",
       "      <td>609.231013</td>\n",
       "      <td>556.343848</td>\n",
       "      <td>595.390836</td>\n",
       "      <td>558.046340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>176</td>\n",
       "      <td>161.735807</td>\n",
       "      <td>167.919098</td>\n",
       "      <td>147.830687</td>\n",
       "      <td>174.087468</td>\n",
       "      <td>167.839584</td>\n",
       "      <td>142.888377</td>\n",
       "      <td>164.004701</td>\n",
       "      <td>151.922397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>714</td>\n",
       "      <td>656.201669</td>\n",
       "      <td>635.251648</td>\n",
       "      <td>599.825712</td>\n",
       "      <td>716.678282</td>\n",
       "      <td>597.369896</td>\n",
       "      <td>619.101126</td>\n",
       "      <td>655.745034</td>\n",
       "      <td>608.753283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1043</td>\n",
       "      <td>1044.903702</td>\n",
       "      <td>940.582886</td>\n",
       "      <td>938.024944</td>\n",
       "      <td>1104.039407</td>\n",
       "      <td>1059.300177</td>\n",
       "      <td>948.854816</td>\n",
       "      <td>1041.963053</td>\n",
       "      <td>970.924371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>823</td>\n",
       "      <td>898.322121</td>\n",
       "      <td>669.418457</td>\n",
       "      <td>716.770601</td>\n",
       "      <td>844.702073</td>\n",
       "      <td>836.947940</td>\n",
       "      <td>771.448493</td>\n",
       "      <td>838.133988</td>\n",
       "      <td>812.599656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>250</td>\n",
       "      <td>253.512355</td>\n",
       "      <td>227.158142</td>\n",
       "      <td>204.201638</td>\n",
       "      <td>240.365029</td>\n",
       "      <td>254.287204</td>\n",
       "      <td>204.990275</td>\n",
       "      <td>247.800215</td>\n",
       "      <td>235.944831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1004</td>\n",
       "      <td>1039.635205</td>\n",
       "      <td>715.639648</td>\n",
       "      <td>818.296836</td>\n",
       "      <td>970.936534</td>\n",
       "      <td>1058.174059</td>\n",
       "      <td>877.666985</td>\n",
       "      <td>927.313209</td>\n",
       "      <td>907.747468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1441</td>\n",
       "      <td>1526.908216</td>\n",
       "      <td>980.234009</td>\n",
       "      <td>1237.753414</td>\n",
       "      <td>1455.732603</td>\n",
       "      <td>2036.549357</td>\n",
       "      <td>1330.550686</td>\n",
       "      <td>1458.114228</td>\n",
       "      <td>1409.321391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>388</td>\n",
       "      <td>463.705207</td>\n",
       "      <td>423.052612</td>\n",
       "      <td>367.633099</td>\n",
       "      <td>438.365858</td>\n",
       "      <td>472.307718</td>\n",
       "      <td>381.841982</td>\n",
       "      <td>426.105716</td>\n",
       "      <td>411.451216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  num_sold      prophet  neuralprophet        ridge  \\\n",
       "19728  2018-01-01       405   383.365770     374.809906   344.131415   \n",
       "19729  2018-01-01       621   602.594228     539.082520   539.757025   \n",
       "19730  2018-01-01       176   161.735807     167.919098   147.830687   \n",
       "19731  2018-01-01       714   656.201669     635.251648   599.825712   \n",
       "19732  2018-01-01      1043  1044.903702     940.582886   938.024944   \n",
       "...           ...       ...          ...            ...          ...   \n",
       "26293  2018-12-31       823   898.322121     669.418457   716.770601   \n",
       "26294  2018-12-31       250   253.512355     227.158142   204.201638   \n",
       "26295  2018-12-31      1004  1039.635205     715.639648   818.296836   \n",
       "26296  2018-12-31      1441  1526.908216     980.234009  1237.753414   \n",
       "26297  2018-12-31       388   463.705207     423.052612   367.633099   \n",
       "\n",
       "            linear        earth      xgboost     lightgbm     catboost  \n",
       "19728   416.085371   382.162985   348.545834   375.494286   348.215962  \n",
       "19729   639.857859   609.231013   556.343848   595.390836   558.046340  \n",
       "19730   174.087468   167.839584   142.888377   164.004701   151.922397  \n",
       "19731   716.678282   597.369896   619.101126   655.745034   608.753283  \n",
       "19732  1104.039407  1059.300177   948.854816  1041.963053   970.924371  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "26293   844.702073   836.947940   771.448493   838.133988   812.599656  \n",
       "26294   240.365029   254.287204   204.990275   247.800215   235.944831  \n",
       "26295   970.936534  1058.174059   877.666985   927.313209   907.747468  \n",
       "26296  1455.732603  2036.549357  1330.550686  1458.114228  1409.321391  \n",
       "26297   438.365858   472.307718   381.841982   426.105716   411.451216  \n",
       "\n",
       "[6570 rows x 10 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_forecast_preds = tv_preds[tv_preds['date'] > '2017-12-31']\n",
    "valid_forecast_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "71b4df91-1d40-496a-8ab0-90bfe3cd8970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_sold</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972938</td>\n",
       "      <td>0.961440</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.995402</td>\n",
       "      <td>0.984322</td>\n",
       "      <td>0.994093</td>\n",
       "      <td>0.994247</td>\n",
       "      <td>0.994582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prophet</th>\n",
       "      <td>0.972938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990336</td>\n",
       "      <td>0.987971</td>\n",
       "      <td>0.975207</td>\n",
       "      <td>0.972174</td>\n",
       "      <td>0.979712</td>\n",
       "      <td>0.979264</td>\n",
       "      <td>0.977482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralprophet</th>\n",
       "      <td>0.961440</td>\n",
       "      <td>0.990336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982021</td>\n",
       "      <td>0.962141</td>\n",
       "      <td>0.961384</td>\n",
       "      <td>0.968907</td>\n",
       "      <td>0.967780</td>\n",
       "      <td>0.965788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.987971</td>\n",
       "      <td>0.982021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.988859</td>\n",
       "      <td>0.996588</td>\n",
       "      <td>0.996172</td>\n",
       "      <td>0.995755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>0.995402</td>\n",
       "      <td>0.975207</td>\n",
       "      <td>0.962141</td>\n",
       "      <td>0.995283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987834</td>\n",
       "      <td>0.997142</td>\n",
       "      <td>0.997403</td>\n",
       "      <td>0.997497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earth</th>\n",
       "      <td>0.984322</td>\n",
       "      <td>0.972174</td>\n",
       "      <td>0.961384</td>\n",
       "      <td>0.988859</td>\n",
       "      <td>0.987834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991126</td>\n",
       "      <td>0.991210</td>\n",
       "      <td>0.991284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.994093</td>\n",
       "      <td>0.979712</td>\n",
       "      <td>0.968907</td>\n",
       "      <td>0.996588</td>\n",
       "      <td>0.997142</td>\n",
       "      <td>0.991126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.999165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>0.994247</td>\n",
       "      <td>0.979264</td>\n",
       "      <td>0.967780</td>\n",
       "      <td>0.996172</td>\n",
       "      <td>0.997403</td>\n",
       "      <td>0.991210</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost</th>\n",
       "      <td>0.994582</td>\n",
       "      <td>0.977482</td>\n",
       "      <td>0.965788</td>\n",
       "      <td>0.995755</td>\n",
       "      <td>0.997497</td>\n",
       "      <td>0.991284</td>\n",
       "      <td>0.999165</td>\n",
       "      <td>0.999206</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               num_sold   prophet  neuralprophet     ridge    linear  \\\n",
       "num_sold       1.000000  0.972938       0.961440  0.992042  0.995402   \n",
       "prophet        0.972938  1.000000       0.990336  0.987971  0.975207   \n",
       "neuralprophet  0.961440  0.990336       1.000000  0.982021  0.962141   \n",
       "ridge          0.992042  0.987971       0.982021  1.000000  0.995283   \n",
       "linear         0.995402  0.975207       0.962141  0.995283  1.000000   \n",
       "earth          0.984322  0.972174       0.961384  0.988859  0.987834   \n",
       "xgboost        0.994093  0.979712       0.968907  0.996588  0.997142   \n",
       "lightgbm       0.994247  0.979264       0.967780  0.996172  0.997403   \n",
       "catboost       0.994582  0.977482       0.965788  0.995755  0.997497   \n",
       "\n",
       "                  earth   xgboost  lightgbm  catboost  \n",
       "num_sold       0.984322  0.994093  0.994247  0.994582  \n",
       "prophet        0.972174  0.979712  0.979264  0.977482  \n",
       "neuralprophet  0.961384  0.968907  0.967780  0.965788  \n",
       "ridge          0.988859  0.996588  0.996172  0.995755  \n",
       "linear         0.987834  0.997142  0.997403  0.997497  \n",
       "earth          1.000000  0.991126  0.991210  0.991284  \n",
       "xgboost        0.991126  1.000000  0.999098  0.999165  \n",
       "lightgbm       0.991210  0.999098  1.000000  0.999206  \n",
       "catboost       0.991284  0.999165  0.999206  1.000000  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_forecast_preds.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d18d79d8-9700-465c-8c09-e9e7eaa7eec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_sold</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975093</td>\n",
       "      <td>0.960082</td>\n",
       "      <td>0.992276</td>\n",
       "      <td>0.995885</td>\n",
       "      <td>0.990016</td>\n",
       "      <td>0.992043</td>\n",
       "      <td>0.990737</td>\n",
       "      <td>0.991378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prophet</th>\n",
       "      <td>0.975093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985693</td>\n",
       "      <td>0.988571</td>\n",
       "      <td>0.977816</td>\n",
       "      <td>0.974833</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.979337</td>\n",
       "      <td>0.977567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralprophet</th>\n",
       "      <td>0.960082</td>\n",
       "      <td>0.985693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981114</td>\n",
       "      <td>0.962575</td>\n",
       "      <td>0.960241</td>\n",
       "      <td>0.967428</td>\n",
       "      <td>0.964121</td>\n",
       "      <td>0.961945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>0.992276</td>\n",
       "      <td>0.988571</td>\n",
       "      <td>0.981114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995616</td>\n",
       "      <td>0.991267</td>\n",
       "      <td>0.994990</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>0.993199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>0.995885</td>\n",
       "      <td>0.977816</td>\n",
       "      <td>0.962575</td>\n",
       "      <td>0.995616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993991</td>\n",
       "      <td>0.995060</td>\n",
       "      <td>0.993677</td>\n",
       "      <td>0.994107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earth</th>\n",
       "      <td>0.990016</td>\n",
       "      <td>0.974833</td>\n",
       "      <td>0.960241</td>\n",
       "      <td>0.991267</td>\n",
       "      <td>0.993991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989740</td>\n",
       "      <td>0.988106</td>\n",
       "      <td>0.987979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.992043</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.967428</td>\n",
       "      <td>0.994990</td>\n",
       "      <td>0.995060</td>\n",
       "      <td>0.989740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997452</td>\n",
       "      <td>0.997577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>0.990737</td>\n",
       "      <td>0.979337</td>\n",
       "      <td>0.964121</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>0.993677</td>\n",
       "      <td>0.988106</td>\n",
       "      <td>0.997452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost</th>\n",
       "      <td>0.991378</td>\n",
       "      <td>0.977567</td>\n",
       "      <td>0.961945</td>\n",
       "      <td>0.993199</td>\n",
       "      <td>0.994107</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.997577</td>\n",
       "      <td>0.998421</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               num_sold   prophet  neuralprophet     ridge    linear  \\\n",
       "num_sold       1.000000  0.975093       0.960082  0.992276  0.995885   \n",
       "prophet        0.975093  1.000000       0.985693  0.988571  0.977816   \n",
       "neuralprophet  0.960082  0.985693       1.000000  0.981114  0.962575   \n",
       "ridge          0.992276  0.988571       0.981114  1.000000  0.995616   \n",
       "linear         0.995885  0.977816       0.962575  0.995616  1.000000   \n",
       "earth          0.990016  0.974833       0.960241  0.991267  0.993991   \n",
       "xgboost        0.992043  0.980282       0.967428  0.994990  0.995060   \n",
       "lightgbm       0.990737  0.979337       0.964121  0.993590  0.993677   \n",
       "catboost       0.991378  0.977567       0.961945  0.993199  0.994107   \n",
       "\n",
       "                  earth   xgboost  lightgbm  catboost  \n",
       "num_sold       0.990016  0.992043  0.990737  0.991378  \n",
       "prophet        0.974833  0.980282  0.979337  0.977567  \n",
       "neuralprophet  0.960241  0.967428  0.964121  0.961945  \n",
       "ridge          0.991267  0.994990  0.993590  0.993199  \n",
       "linear         0.993991  0.995060  0.993677  0.994107  \n",
       "earth          1.000000  0.989740  0.988106  0.987979  \n",
       "xgboost        0.989740  1.000000  0.997452  0.997577  \n",
       "lightgbm       0.988106  0.997452  1.000000  0.998421  \n",
       "catboost       0.987979  0.997577  0.998421  1.000000  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_preds.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129e48f-dd45-4627-af08-1544cd69aa1b",
   "metadata": {},
   "source": [
    "So Ridge and Linear perform best, and are quite similar; Prophet and NeuralProphet are next best, and similar to one another; Huber is an outlier (and not so good). Lasso performs worst of all, but is closer to Prophet and NeuralProphet in performance than the others. It's closest of all to NeuralProphet, interestingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2a0d99ea-df90-4533-b1bf-6ed324c74235",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecast_preds = pd.DataFrame({\n",
    "    'date': orig_test_df['date'],\n",
    "#     'num_sold': orig_train_df['num_sold'],\n",
    "    **forecast_test_preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2bad0b95-51ee-46d0-9c2a-a2d724840cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>402.296576</td>\n",
       "      <td>383.543457</td>\n",
       "      <td>362.464248</td>\n",
       "      <td>393.690384</td>\n",
       "      <td>417.102143</td>\n",
       "      <td>358.401163</td>\n",
       "      <td>390.655390</td>\n",
       "      <td>376.761862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>631.642099</td>\n",
       "      <td>551.044983</td>\n",
       "      <td>566.459768</td>\n",
       "      <td>619.858987</td>\n",
       "      <td>693.844970</td>\n",
       "      <td>555.421264</td>\n",
       "      <td>613.560797</td>\n",
       "      <td>590.379813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>169.992094</td>\n",
       "      <td>173.065674</td>\n",
       "      <td>154.947737</td>\n",
       "      <td>167.245652</td>\n",
       "      <td>181.642807</td>\n",
       "      <td>148.552857</td>\n",
       "      <td>167.695509</td>\n",
       "      <td>161.606811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>684.559821</td>\n",
       "      <td>648.783691</td>\n",
       "      <td>621.668002</td>\n",
       "      <td>672.131457</td>\n",
       "      <td>594.166542</td>\n",
       "      <td>628.374452</td>\n",
       "      <td>677.405142</td>\n",
       "      <td>658.087114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1092.875362</td>\n",
       "      <td>965.683716</td>\n",
       "      <td>987.160922</td>\n",
       "      <td>1089.659829</td>\n",
       "      <td>1142.564632</td>\n",
       "      <td>966.886437</td>\n",
       "      <td>1079.936178</td>\n",
       "      <td>1027.408044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>917.085861</td>\n",
       "      <td>674.891174</td>\n",
       "      <td>711.025807</td>\n",
       "      <td>788.272434</td>\n",
       "      <td>830.606248</td>\n",
       "      <td>751.182192</td>\n",
       "      <td>814.329407</td>\n",
       "      <td>805.086966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>261.569590</td>\n",
       "      <td>232.082169</td>\n",
       "      <td>205.408865</td>\n",
       "      <td>225.353374</td>\n",
       "      <td>269.498973</td>\n",
       "      <td>207.247002</td>\n",
       "      <td>236.116164</td>\n",
       "      <td>231.138037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1065.400121</td>\n",
       "      <td>730.457275</td>\n",
       "      <td>828.788841</td>\n",
       "      <td>933.467881</td>\n",
       "      <td>1133.560974</td>\n",
       "      <td>862.219767</td>\n",
       "      <td>902.112953</td>\n",
       "      <td>902.193863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1568.407214</td>\n",
       "      <td>1005.392944</td>\n",
       "      <td>1241.035458</td>\n",
       "      <td>1376.010186</td>\n",
       "      <td>2627.470943</td>\n",
       "      <td>1296.892822</td>\n",
       "      <td>1418.421922</td>\n",
       "      <td>1397.420903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>476.122783</td>\n",
       "      <td>428.916565</td>\n",
       "      <td>369.001469</td>\n",
       "      <td>417.688712</td>\n",
       "      <td>507.799360</td>\n",
       "      <td>382.992893</td>\n",
       "      <td>394.010248</td>\n",
       "      <td>401.768782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      prophet  neuralprophet        ridge       linear  \\\n",
       "0     2019-01-01   402.296576     383.543457   362.464248   393.690384   \n",
       "1     2019-01-01   631.642099     551.044983   566.459768   619.858987   \n",
       "2     2019-01-01   169.992094     173.065674   154.947737   167.245652   \n",
       "3     2019-01-01   684.559821     648.783691   621.668002   672.131457   \n",
       "4     2019-01-01  1092.875362     965.683716   987.160922  1089.659829   \n",
       "...          ...          ...            ...          ...          ...   \n",
       "6565  2019-12-31   917.085861     674.891174   711.025807   788.272434   \n",
       "6566  2019-12-31   261.569590     232.082169   205.408865   225.353374   \n",
       "6567  2019-12-31  1065.400121     730.457275   828.788841   933.467881   \n",
       "6568  2019-12-31  1568.407214    1005.392944  1241.035458  1376.010186   \n",
       "6569  2019-12-31   476.122783     428.916565   369.001469   417.688712   \n",
       "\n",
       "            earth      xgboost     lightgbm     catboost  \n",
       "0      417.102143   358.401163   390.655390   376.761862  \n",
       "1      693.844970   555.421264   613.560797   590.379813  \n",
       "2      181.642807   148.552857   167.695509   161.606811  \n",
       "3      594.166542   628.374452   677.405142   658.087114  \n",
       "4     1142.564632   966.886437  1079.936178  1027.408044  \n",
       "...           ...          ...          ...          ...  \n",
       "6565   830.606248   751.182192   814.329407   805.086966  \n",
       "6566   269.498973   207.247002   236.116164   231.138037  \n",
       "6567  1133.560974   862.219767   902.112953   902.193863  \n",
       "6568  2627.470943  1296.892822  1418.421922  1397.420903  \n",
       "6569   507.799360   382.992893   394.010248   401.768782  \n",
       "\n",
       "[6570 rows x 9 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_forecast_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9de072-2501-4a4c-be5f-71e987006d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c0613e3-e18d-4890-ae72-7946d5a0fc3b",
   "metadata": {},
   "source": [
    "## Submission of Naive Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7ad86-1818-4304-92f4-1ef7d9236e4e",
   "metadata": {},
   "source": [
    "Something's up with the residuals; for now, let's just run a simple model on the original forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dcd47281-fcb5-471c-ad54-ee7eb952af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso()\n",
    "# model = Ridge() # not good\n",
    "# model = LinearRegression() # not good\n",
    "# model = CatBoostRegressor(**catboost_params) # very bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7dc15e90-ba40-4c20-8158-a053587a1b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>329</td>\n",
       "      <td>346.560416</td>\n",
       "      <td>329.134521</td>\n",
       "      <td>282.947564</td>\n",
       "      <td>323.494168</td>\n",
       "      <td>360.672190</td>\n",
       "      <td>329.699150</td>\n",
       "      <td>337.386928</td>\n",
       "      <td>316.636303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>520</td>\n",
       "      <td>536.203586</td>\n",
       "      <td>458.008301</td>\n",
       "      <td>441.709447</td>\n",
       "      <td>502.785894</td>\n",
       "      <td>584.851806</td>\n",
       "      <td>489.933725</td>\n",
       "      <td>543.343607</td>\n",
       "      <td>508.221832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>146</td>\n",
       "      <td>143.412803</td>\n",
       "      <td>145.325577</td>\n",
       "      <td>121.161437</td>\n",
       "      <td>136.434389</td>\n",
       "      <td>138.401176</td>\n",
       "      <td>134.669764</td>\n",
       "      <td>167.039891</td>\n",
       "      <td>139.910672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>572</td>\n",
       "      <td>590.117165</td>\n",
       "      <td>552.571167</td>\n",
       "      <td>491.090363</td>\n",
       "      <td>557.025587</td>\n",
       "      <td>562.479785</td>\n",
       "      <td>571.549171</td>\n",
       "      <td>581.483568</td>\n",
       "      <td>552.913777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>911</td>\n",
       "      <td>939.673009</td>\n",
       "      <td>781.967285</td>\n",
       "      <td>781.160208</td>\n",
       "      <td>889.998292</td>\n",
       "      <td>944.584508</td>\n",
       "      <td>898.322803</td>\n",
       "      <td>942.031061</td>\n",
       "      <td>883.653559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>823</td>\n",
       "      <td>898.322121</td>\n",
       "      <td>669.418457</td>\n",
       "      <td>716.770601</td>\n",
       "      <td>844.702073</td>\n",
       "      <td>836.947940</td>\n",
       "      <td>771.448493</td>\n",
       "      <td>838.133988</td>\n",
       "      <td>812.599656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>250</td>\n",
       "      <td>253.512355</td>\n",
       "      <td>227.158142</td>\n",
       "      <td>204.201638</td>\n",
       "      <td>240.365029</td>\n",
       "      <td>254.287204</td>\n",
       "      <td>204.990275</td>\n",
       "      <td>247.800215</td>\n",
       "      <td>235.944831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1004</td>\n",
       "      <td>1039.635205</td>\n",
       "      <td>715.639648</td>\n",
       "      <td>818.296836</td>\n",
       "      <td>970.936534</td>\n",
       "      <td>1058.174059</td>\n",
       "      <td>877.666985</td>\n",
       "      <td>927.313209</td>\n",
       "      <td>907.747468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1441</td>\n",
       "      <td>1526.908216</td>\n",
       "      <td>980.234009</td>\n",
       "      <td>1237.753414</td>\n",
       "      <td>1455.732603</td>\n",
       "      <td>2036.549357</td>\n",
       "      <td>1330.550686</td>\n",
       "      <td>1458.114228</td>\n",
       "      <td>1409.321391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>388</td>\n",
       "      <td>463.705207</td>\n",
       "      <td>423.052612</td>\n",
       "      <td>367.633099</td>\n",
       "      <td>438.365858</td>\n",
       "      <td>472.307718</td>\n",
       "      <td>381.841982</td>\n",
       "      <td>426.105716</td>\n",
       "      <td>411.451216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  num_sold      prophet  neuralprophet        ridge  \\\n",
       "0      2015-01-01       329   346.560416     329.134521   282.947564   \n",
       "1      2015-01-01       520   536.203586     458.008301   441.709447   \n",
       "2      2015-01-01       146   143.412803     145.325577   121.161437   \n",
       "3      2015-01-01       572   590.117165     552.571167   491.090363   \n",
       "4      2015-01-01       911   939.673009     781.967285   781.160208   \n",
       "...           ...       ...          ...            ...          ...   \n",
       "26293  2018-12-31       823   898.322121     669.418457   716.770601   \n",
       "26294  2018-12-31       250   253.512355     227.158142   204.201638   \n",
       "26295  2018-12-31      1004  1039.635205     715.639648   818.296836   \n",
       "26296  2018-12-31      1441  1526.908216     980.234009  1237.753414   \n",
       "26297  2018-12-31       388   463.705207     423.052612   367.633099   \n",
       "\n",
       "            linear        earth      xgboost     lightgbm     catboost  \n",
       "0       323.494168   360.672190   329.699150   337.386928   316.636303  \n",
       "1       502.785894   584.851806   489.933725   543.343607   508.221832  \n",
       "2       136.434389   138.401176   134.669764   167.039891   139.910672  \n",
       "3       557.025587   562.479785   571.549171   581.483568   552.913777  \n",
       "4       889.998292   944.584508   898.322803   942.031061   883.653559  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "26293   844.702073   836.947940   771.448493   838.133988   812.599656  \n",
       "26294   240.365029   254.287204   204.990275   247.800215   235.944831  \n",
       "26295   970.936534  1058.174059   877.666985   927.313209   907.747468  \n",
       "26296  1455.732603  2036.549357  1330.550686  1458.114228  1409.321391  \n",
       "26297   438.365858   472.307718   381.841982   426.105716   411.451216  \n",
       "\n",
       "[26298 rows x 10 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d6ecabad-3daf-4237-b01a-0b5662953801",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tv_preds.drop(columns=['num_sold'])\n",
    "y = tv_preds['num_sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0cd84ee5-93c3-4833-b348-1f07aa66a57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>402.296576</td>\n",
       "      <td>383.543457</td>\n",
       "      <td>362.464248</td>\n",
       "      <td>393.690384</td>\n",
       "      <td>417.102143</td>\n",
       "      <td>358.401163</td>\n",
       "      <td>390.655390</td>\n",
       "      <td>376.761862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>631.642099</td>\n",
       "      <td>551.044983</td>\n",
       "      <td>566.459768</td>\n",
       "      <td>619.858987</td>\n",
       "      <td>693.844970</td>\n",
       "      <td>555.421264</td>\n",
       "      <td>613.560797</td>\n",
       "      <td>590.379813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>169.992094</td>\n",
       "      <td>173.065674</td>\n",
       "      <td>154.947737</td>\n",
       "      <td>167.245652</td>\n",
       "      <td>181.642807</td>\n",
       "      <td>148.552857</td>\n",
       "      <td>167.695509</td>\n",
       "      <td>161.606811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>684.559821</td>\n",
       "      <td>648.783691</td>\n",
       "      <td>621.668002</td>\n",
       "      <td>672.131457</td>\n",
       "      <td>594.166542</td>\n",
       "      <td>628.374452</td>\n",
       "      <td>677.405142</td>\n",
       "      <td>658.087114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1092.875362</td>\n",
       "      <td>965.683716</td>\n",
       "      <td>987.160922</td>\n",
       "      <td>1089.659829</td>\n",
       "      <td>1142.564632</td>\n",
       "      <td>966.886437</td>\n",
       "      <td>1079.936178</td>\n",
       "      <td>1027.408044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>917.085861</td>\n",
       "      <td>674.891174</td>\n",
       "      <td>711.025807</td>\n",
       "      <td>788.272434</td>\n",
       "      <td>830.606248</td>\n",
       "      <td>751.182192</td>\n",
       "      <td>814.329407</td>\n",
       "      <td>805.086966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>261.569590</td>\n",
       "      <td>232.082169</td>\n",
       "      <td>205.408865</td>\n",
       "      <td>225.353374</td>\n",
       "      <td>269.498973</td>\n",
       "      <td>207.247002</td>\n",
       "      <td>236.116164</td>\n",
       "      <td>231.138037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1065.400121</td>\n",
       "      <td>730.457275</td>\n",
       "      <td>828.788841</td>\n",
       "      <td>933.467881</td>\n",
       "      <td>1133.560974</td>\n",
       "      <td>862.219767</td>\n",
       "      <td>902.112953</td>\n",
       "      <td>902.193863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1568.407214</td>\n",
       "      <td>1005.392944</td>\n",
       "      <td>1241.035458</td>\n",
       "      <td>1376.010186</td>\n",
       "      <td>2627.470943</td>\n",
       "      <td>1296.892822</td>\n",
       "      <td>1418.421922</td>\n",
       "      <td>1397.420903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>476.122783</td>\n",
       "      <td>428.916565</td>\n",
       "      <td>369.001469</td>\n",
       "      <td>417.688712</td>\n",
       "      <td>507.799360</td>\n",
       "      <td>382.992893</td>\n",
       "      <td>394.010248</td>\n",
       "      <td>401.768782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      prophet  neuralprophet        ridge       linear  \\\n",
       "0     2019-01-01   402.296576     383.543457   362.464248   393.690384   \n",
       "1     2019-01-01   631.642099     551.044983   566.459768   619.858987   \n",
       "2     2019-01-01   169.992094     173.065674   154.947737   167.245652   \n",
       "3     2019-01-01   684.559821     648.783691   621.668002   672.131457   \n",
       "4     2019-01-01  1092.875362     965.683716   987.160922  1089.659829   \n",
       "...          ...          ...            ...          ...          ...   \n",
       "6565  2019-12-31   917.085861     674.891174   711.025807   788.272434   \n",
       "6566  2019-12-31   261.569590     232.082169   205.408865   225.353374   \n",
       "6567  2019-12-31  1065.400121     730.457275   828.788841   933.467881   \n",
       "6568  2019-12-31  1568.407214    1005.392944  1241.035458  1376.010186   \n",
       "6569  2019-12-31   476.122783     428.916565   369.001469   417.688712   \n",
       "\n",
       "            earth      xgboost     lightgbm     catboost  \n",
       "0      417.102143   358.401163   390.655390   376.761862  \n",
       "1      693.844970   555.421264   613.560797   590.379813  \n",
       "2      181.642807   148.552857   167.695509   161.606811  \n",
       "3      594.166542   628.374452   677.405142   658.087114  \n",
       "4     1142.564632   966.886437  1079.936178  1027.408044  \n",
       "...           ...          ...          ...          ...  \n",
       "6565   830.606248   751.182192   814.329407   805.086966  \n",
       "6566   269.498973   207.247002   236.116164   231.138037  \n",
       "6567  1133.560974   862.219767   902.112953   902.193863  \n",
       "6568  2627.470943  1296.892822  1418.421922  1397.420903  \n",
       "6569   507.799360   382.992893   394.010248   401.768782  \n",
       "\n",
       "[6570 rows x 9 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_forecast_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7bef6ea2-5561-498c-8760-7b1a33fca878",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_forecast_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6674517b-f523-49b4-a1b6-6cdfb0a15eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['date'] = pd.to_datetime(X.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "163f756a-4bc1-4012-a85a-a9afec42e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['date'] = X['date'].map(dt.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "17c54022-ab1c-4bc2-88c6-00704bb71324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.drop(columns=['huber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ecdd9038-7bd0-4f50-9b60-9517ba9462a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso()"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eeee00a2-8854-4c2d-9722-ddbb48088a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "01f7ed08-cf51-4b05-91be-afaa1070e0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>402.296576</td>\n",
       "      <td>383.543457</td>\n",
       "      <td>362.464248</td>\n",
       "      <td>393.690384</td>\n",
       "      <td>417.102143</td>\n",
       "      <td>358.401163</td>\n",
       "      <td>390.655390</td>\n",
       "      <td>376.761862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>631.642099</td>\n",
       "      <td>551.044983</td>\n",
       "      <td>566.459768</td>\n",
       "      <td>619.858987</td>\n",
       "      <td>693.844970</td>\n",
       "      <td>555.421264</td>\n",
       "      <td>613.560797</td>\n",
       "      <td>590.379813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>169.992094</td>\n",
       "      <td>173.065674</td>\n",
       "      <td>154.947737</td>\n",
       "      <td>167.245652</td>\n",
       "      <td>181.642807</td>\n",
       "      <td>148.552857</td>\n",
       "      <td>167.695509</td>\n",
       "      <td>161.606811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>684.559821</td>\n",
       "      <td>648.783691</td>\n",
       "      <td>621.668002</td>\n",
       "      <td>672.131457</td>\n",
       "      <td>594.166542</td>\n",
       "      <td>628.374452</td>\n",
       "      <td>677.405142</td>\n",
       "      <td>658.087114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1092.875362</td>\n",
       "      <td>965.683716</td>\n",
       "      <td>987.160922</td>\n",
       "      <td>1089.659829</td>\n",
       "      <td>1142.564632</td>\n",
       "      <td>966.886437</td>\n",
       "      <td>1079.936178</td>\n",
       "      <td>1027.408044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>917.085861</td>\n",
       "      <td>674.891174</td>\n",
       "      <td>711.025807</td>\n",
       "      <td>788.272434</td>\n",
       "      <td>830.606248</td>\n",
       "      <td>751.182192</td>\n",
       "      <td>814.329407</td>\n",
       "      <td>805.086966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>261.569590</td>\n",
       "      <td>232.082169</td>\n",
       "      <td>205.408865</td>\n",
       "      <td>225.353374</td>\n",
       "      <td>269.498973</td>\n",
       "      <td>207.247002</td>\n",
       "      <td>236.116164</td>\n",
       "      <td>231.138037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1065.400121</td>\n",
       "      <td>730.457275</td>\n",
       "      <td>828.788841</td>\n",
       "      <td>933.467881</td>\n",
       "      <td>1133.560974</td>\n",
       "      <td>862.219767</td>\n",
       "      <td>902.112953</td>\n",
       "      <td>902.193863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1568.407214</td>\n",
       "      <td>1005.392944</td>\n",
       "      <td>1241.035458</td>\n",
       "      <td>1376.010186</td>\n",
       "      <td>2627.470943</td>\n",
       "      <td>1296.892822</td>\n",
       "      <td>1418.421922</td>\n",
       "      <td>1397.420903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>476.122783</td>\n",
       "      <td>428.916565</td>\n",
       "      <td>369.001469</td>\n",
       "      <td>417.688712</td>\n",
       "      <td>507.799360</td>\n",
       "      <td>382.992893</td>\n",
       "      <td>394.010248</td>\n",
       "      <td>401.768782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      prophet  neuralprophet        ridge       linear  \\\n",
       "0     2019-01-01   402.296576     383.543457   362.464248   393.690384   \n",
       "1     2019-01-01   631.642099     551.044983   566.459768   619.858987   \n",
       "2     2019-01-01   169.992094     173.065674   154.947737   167.245652   \n",
       "3     2019-01-01   684.559821     648.783691   621.668002   672.131457   \n",
       "4     2019-01-01  1092.875362     965.683716   987.160922  1089.659829   \n",
       "...          ...          ...            ...          ...          ...   \n",
       "6565  2019-12-31   917.085861     674.891174   711.025807   788.272434   \n",
       "6566  2019-12-31   261.569590     232.082169   205.408865   225.353374   \n",
       "6567  2019-12-31  1065.400121     730.457275   828.788841   933.467881   \n",
       "6568  2019-12-31  1568.407214    1005.392944  1241.035458  1376.010186   \n",
       "6569  2019-12-31   476.122783     428.916565   369.001469   417.688712   \n",
       "\n",
       "            earth      xgboost     lightgbm     catboost  \n",
       "0      417.102143   358.401163   390.655390   376.761862  \n",
       "1      693.844970   555.421264   613.560797   590.379813  \n",
       "2      181.642807   148.552857   167.695509   161.606811  \n",
       "3      594.166542   628.374452   677.405142   658.087114  \n",
       "4     1142.564632   966.886437  1079.936178  1027.408044  \n",
       "...           ...          ...          ...          ...  \n",
       "6565   830.606248   751.182192   814.329407   805.086966  \n",
       "6566   269.498973   207.247002   236.116164   231.138037  \n",
       "6567  1133.560974   862.219767   902.112953   902.193863  \n",
       "6568  2627.470943  1296.892822  1418.421922  1397.420903  \n",
       "6569   507.799360   382.992893   394.010248   401.768782  \n",
       "\n",
       "[6570 rows x 9 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0134bfd3-b6a8-484c-9f6b-ff8da56426ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['date'] = X['date'].map(dt.datetime.toordinal)\n",
    "X_test['date'] = pd.to_datetime(X_test.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b3daf1e5-12b7-4235-9331-7c72b23dbde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>402.296576</td>\n",
       "      <td>383.543457</td>\n",
       "      <td>362.464248</td>\n",
       "      <td>393.690384</td>\n",
       "      <td>417.102143</td>\n",
       "      <td>358.401163</td>\n",
       "      <td>390.655390</td>\n",
       "      <td>376.761862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>631.642099</td>\n",
       "      <td>551.044983</td>\n",
       "      <td>566.459768</td>\n",
       "      <td>619.858987</td>\n",
       "      <td>693.844970</td>\n",
       "      <td>555.421264</td>\n",
       "      <td>613.560797</td>\n",
       "      <td>590.379813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>169.992094</td>\n",
       "      <td>173.065674</td>\n",
       "      <td>154.947737</td>\n",
       "      <td>167.245652</td>\n",
       "      <td>181.642807</td>\n",
       "      <td>148.552857</td>\n",
       "      <td>167.695509</td>\n",
       "      <td>161.606811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>684.559821</td>\n",
       "      <td>648.783691</td>\n",
       "      <td>621.668002</td>\n",
       "      <td>672.131457</td>\n",
       "      <td>594.166542</td>\n",
       "      <td>628.374452</td>\n",
       "      <td>677.405142</td>\n",
       "      <td>658.087114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1092.875362</td>\n",
       "      <td>965.683716</td>\n",
       "      <td>987.160922</td>\n",
       "      <td>1089.659829</td>\n",
       "      <td>1142.564632</td>\n",
       "      <td>966.886437</td>\n",
       "      <td>1079.936178</td>\n",
       "      <td>1027.408044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>917.085861</td>\n",
       "      <td>674.891174</td>\n",
       "      <td>711.025807</td>\n",
       "      <td>788.272434</td>\n",
       "      <td>830.606248</td>\n",
       "      <td>751.182192</td>\n",
       "      <td>814.329407</td>\n",
       "      <td>805.086966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>261.569590</td>\n",
       "      <td>232.082169</td>\n",
       "      <td>205.408865</td>\n",
       "      <td>225.353374</td>\n",
       "      <td>269.498973</td>\n",
       "      <td>207.247002</td>\n",
       "      <td>236.116164</td>\n",
       "      <td>231.138037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1065.400121</td>\n",
       "      <td>730.457275</td>\n",
       "      <td>828.788841</td>\n",
       "      <td>933.467881</td>\n",
       "      <td>1133.560974</td>\n",
       "      <td>862.219767</td>\n",
       "      <td>902.112953</td>\n",
       "      <td>902.193863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1568.407214</td>\n",
       "      <td>1005.392944</td>\n",
       "      <td>1241.035458</td>\n",
       "      <td>1376.010186</td>\n",
       "      <td>2627.470943</td>\n",
       "      <td>1296.892822</td>\n",
       "      <td>1418.421922</td>\n",
       "      <td>1397.420903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>476.122783</td>\n",
       "      <td>428.916565</td>\n",
       "      <td>369.001469</td>\n",
       "      <td>417.688712</td>\n",
       "      <td>507.799360</td>\n",
       "      <td>382.992893</td>\n",
       "      <td>394.010248</td>\n",
       "      <td>401.768782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date      prophet  neuralprophet        ridge       linear  \\\n",
       "0    2019-01-01   402.296576     383.543457   362.464248   393.690384   \n",
       "1    2019-01-01   631.642099     551.044983   566.459768   619.858987   \n",
       "2    2019-01-01   169.992094     173.065674   154.947737   167.245652   \n",
       "3    2019-01-01   684.559821     648.783691   621.668002   672.131457   \n",
       "4    2019-01-01  1092.875362     965.683716   987.160922  1089.659829   \n",
       "...         ...          ...            ...          ...          ...   \n",
       "6565 2019-12-31   917.085861     674.891174   711.025807   788.272434   \n",
       "6566 2019-12-31   261.569590     232.082169   205.408865   225.353374   \n",
       "6567 2019-12-31  1065.400121     730.457275   828.788841   933.467881   \n",
       "6568 2019-12-31  1568.407214    1005.392944  1241.035458  1376.010186   \n",
       "6569 2019-12-31   476.122783     428.916565   369.001469   417.688712   \n",
       "\n",
       "            earth      xgboost     lightgbm     catboost  \n",
       "0      417.102143   358.401163   390.655390   376.761862  \n",
       "1      693.844970   555.421264   613.560797   590.379813  \n",
       "2      181.642807   148.552857   167.695509   161.606811  \n",
       "3      594.166542   628.374452   677.405142   658.087114  \n",
       "4     1142.564632   966.886437  1079.936178  1027.408044  \n",
       "...           ...          ...          ...          ...  \n",
       "6565   830.606248   751.182192   814.329407   805.086966  \n",
       "6566   269.498973   207.247002   236.116164   231.138037  \n",
       "6567  1133.560974   862.219767   902.112953   902.193863  \n",
       "6568  2627.470943  1296.892822  1418.421922  1397.420903  \n",
       "6569   507.799360   382.992893   394.010248   401.768782  \n",
       "\n",
       "[6570 rows x 9 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c33c0706-4bed-4fa0-a0ce-2e04f7246150",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['date'] = X_test['date'].map(dt.datetime.toordinal)\n",
    "# X_test = X_test.drop(columns=['huber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "60bb9478-04c4-4167-8ccb-6223089c240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2ff7fbb7-b8a9-4f37-99b3-961a3b05a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tv_preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d404fa9-65c7-4248-ab9d-f97c5b193967",
   "metadata": {},
   "source": [
    "So, with all of the model preds used, Lasso SMAPE is 4.15283882859097\n",
    "- Dropping Huber only, it's 4.152469194333937\n",
    "- Dropping Huber+Lasso it's 4.1525522971210345\n",
    "- Dropping Lasso only, it's 4.15285802958327\n",
    "\n",
    "So of these, definitely the best is dropping Huber only.\n",
    "\n",
    "With all the model preds used, Ridge SMAPE is 4.158465882203197 -- that's much worse.\n",
    "\n",
    "With all the model preds used, LinearRegression SMAPE is 4.158465902939169 -- also much worse.\n",
    "\n",
    "Using the default hyperparams for CatBoost is even worse: 4.31463984301381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2d268982-b473-4466-adf2-7921d40a9213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.126837360138765"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMAPE(y_pred=final_tv_preds, y_true=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f93c1-2124-4987-83b1-ae043b7ef428",
   "metadata": {},
   "source": [
    "SMAPE on all of them, with Lasso as final, Earth included, is 4.1531645410329565.\n",
    "\n",
    "Including GBMs too gets 4.141452782674763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "54fe91df-ad09-4965-9191-70bee1b65465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "46df5591-2b32-4bc5-835b-b438730d8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'num_sold'] = final_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "97421db6-3f46-40d1-ac57-379ee6a89de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26298</td>\n",
       "      <td>382.787974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26299</td>\n",
       "      <td>606.064213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26300</td>\n",
       "      <td>160.436047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26301</td>\n",
       "      <td>656.882507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26302</td>\n",
       "      <td>1064.638795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>32863</td>\n",
       "      <td>773.815626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>32864</td>\n",
       "      <td>216.365811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>32865</td>\n",
       "      <td>917.641178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>32866</td>\n",
       "      <td>1368.064945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>32867</td>\n",
       "      <td>403.024492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id     num_sold\n",
       "0      26298   382.787974\n",
       "1      26299   606.064213\n",
       "2      26300   160.436047\n",
       "3      26301   656.882507\n",
       "4      26302  1064.638795\n",
       "...      ...          ...\n",
       "6565   32863   773.815626\n",
       "6566   32864   216.365811\n",
       "6567   32865   917.641178\n",
       "6568   32866  1368.064945\n",
       "6569   32867   403.024492\n",
       "\n",
       "[6570 rows x 2 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "86e16bca-ded0-4a3b-880c-3cb8f5b7a3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26298</td>\n",
       "      <td>382.787974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26299</td>\n",
       "      <td>606.064213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26300</td>\n",
       "      <td>160.436047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26301</td>\n",
       "      <td>656.882507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26302</td>\n",
       "      <td>1064.638795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>32863</td>\n",
       "      <td>773.815626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>32864</td>\n",
       "      <td>216.365811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>32865</td>\n",
       "      <td>917.641178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>32866</td>\n",
       "      <td>1368.064945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>32867</td>\n",
       "      <td>403.024492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id     num_sold\n",
       "0      26298   382.787974\n",
       "1      26299   606.064213\n",
       "2      26300   160.436047\n",
       "3      26301   656.882507\n",
       "4      26302  1064.638795\n",
       "...      ...          ...\n",
       "6565   32863   773.815626\n",
       "6566   32864   216.365811\n",
       "6567   32865   917.641178\n",
       "6568   32866  1368.064945\n",
       "6569   32867   403.024492\n",
       "\n",
       "[6570 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "018c6883-318b-4dc0-8171-f3a131c586ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['num_sold'] = sample_df['num_sold'].apply(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "890928ad-0241-420a-aa0f-685b6c61367e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26298</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26299</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26300</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26301</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26302</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  num_sold\n",
       "0   26298       383\n",
       "1   26299       606\n",
       "2   26300       160\n",
       "3   26301       657\n",
       "4   26302      1065"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3a6db176-6c50-466a-aaa3-167dc586f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(subpath/f\"20220129_new_tmw_fe_forecasts(all)-huber-lasso+old_earth+gbms_lasso_preds_rounded.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1c5c1-21a7-4baf-936a-acb46efa451e",
   "metadata": {},
   "source": [
    "### Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc5287-ab0f-4fa2-9461-06e1a2958cb8",
   "metadata": {},
   "source": [
    "At this point, I have two DataFrames containing predictions from the forecasting models (which try to learn trends): \n",
    "1. `valid_forecast_preds`\n",
    "2. `test_forecast_preds`\n",
    "\n",
    "Both still contain the features `'date'` (having a datetime type) and the validation preds contain `num_sold`.\n",
    "\n",
    "The goal now will be to iteratively generate a final prediction DataFrame containing all possible combinations of the forecasting model predictions and residual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0c4b00bf-0ab4-4f7c-a3ab-9ca923e5d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# omitting the GBMs, since they're effectively full-stack\n",
    "forecast_models = [\n",
    "    'prophet', \n",
    "    'neuralprophet', \n",
    "    'ridge', \n",
    "    'linear', \n",
    "#     'huber', \n",
    "#     'lasso', \n",
    "    'earth'\n",
    "] # models to use to provide basis for residual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "58e4125f-99c4-49d1-a6ed-f0bf26c76393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "47e6ec23-4a69-4862-ab61-25949945cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(encoded_all_df, datapath/'encoded_train+testset_with_gdp+teckmengwong-time-features+transformed-target-for-train.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eebd4acf-8940-493b-90f8-26d04cb2d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_train_df = encoded_all_df[:len(train_df)]\n",
    "# hybrid_valid_df = encoded_all_df[len(train_df): len(train_df)+len(valid_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9040538b-351a-4d0f-8a6c-03394d8aa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_test_df = encoded_all_df[len(train_df)+len(valid_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2879f3ec-7e9e-4b39-9060-8b4546e71686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04403cf9-708c-4d7c-8470-899780c58fd6",
   "metadata": {},
   "source": [
    "Note that the `tv_df` still contains both transformed targets and `num_sold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "abe1053a-ecc9-49db-92bc-145b642557f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>gdp</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>wd4</th>\n",
       "      <th>wd56</th>\n",
       "      <th>...</th>\n",
       "      <th>easter50</th>\n",
       "      <th>easter51</th>\n",
       "      <th>easter52</th>\n",
       "      <th>easter53</th>\n",
       "      <th>easter54</th>\n",
       "      <th>easter55</th>\n",
       "      <th>easter56</th>\n",
       "      <th>easter57</th>\n",
       "      <th>easter58</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>329.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.738239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>520.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.196010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>146.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.925788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>572.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.291321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>911.0</td>\n",
       "      <td>5.461456</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.756724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>823.0</td>\n",
       "      <td>6.321586</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.477861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>250.0</td>\n",
       "      <td>6.321586</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.286366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>6.321586</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.676652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>6.321586</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.037997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>388.0</td>\n",
       "      <td>6.321586</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.725910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  country       store         product  num_sold       gdp  \\\n",
       "0     2015-01-01  Finland  KaggleMart      Kaggle Mug     329.0  5.461456   \n",
       "1     2015-01-01  Finland  KaggleMart      Kaggle Hat     520.0  5.461456   \n",
       "2     2015-01-01  Finland  KaggleMart  Kaggle Sticker     146.0  5.461456   \n",
       "3     2015-01-01  Finland  KaggleRama      Kaggle Mug     572.0  5.461456   \n",
       "4     2015-01-01  Finland  KaggleRama      Kaggle Hat     911.0  5.461456   \n",
       "...          ...      ...         ...             ...       ...       ...   \n",
       "26293 2018-12-31   Sweden  KaggleMart      Kaggle Hat     823.0  6.321586   \n",
       "26294 2018-12-31   Sweden  KaggleMart  Kaggle Sticker     250.0  6.321586   \n",
       "26295 2018-12-31   Sweden  KaggleRama      Kaggle Mug    1004.0  6.321586   \n",
       "26296 2018-12-31   Sweden  KaggleRama      Kaggle Hat    1441.0  6.321586   \n",
       "26297 2018-12-31   Sweden  KaggleRama  Kaggle Sticker     388.0  6.321586   \n",
       "\n",
       "       month  season    wd4   wd56  ...  easter50  easter51  easter52  \\\n",
       "0          1       1  False  False  ...     False     False     False   \n",
       "1          1       1  False  False  ...     False     False     False   \n",
       "2          1       1  False  False  ...     False     False     False   \n",
       "3          1       1  False  False  ...     False     False     False   \n",
       "4          1       1  False  False  ...     False     False     False   \n",
       "...      ...     ...    ...    ...  ...       ...       ...       ...   \n",
       "26293     12       1  False  False  ...     False     False     False   \n",
       "26294     12       1  False  False  ...     False     False     False   \n",
       "26295     12       1  False  False  ...     False     False     False   \n",
       "26296     12       1  False  False  ...     False     False     False   \n",
       "26297     12       1  False  False  ...     False     False     False   \n",
       "\n",
       "       easter53  easter54  easter55  easter56  easter57  easter58    target  \n",
       "0         False     False     False     False     False     False  3.738239  \n",
       "1         False     False     False     False     False     False  4.196010  \n",
       "2         False     False     False     False     False     False  2.925788  \n",
       "3         False     False     False     False     False     False  4.291321  \n",
       "4         False     False     False     False     False     False  4.756724  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "26293     False     False     False     False     False     False  4.477861  \n",
       "26294     False     False     False     False     False     False  3.286366  \n",
       "26295     False     False     False     False     False     False  4.676652  \n",
       "26296     False     False     False     False     False     False  5.037997  \n",
       "26297     False     False     False     False     False     False  3.725910  \n",
       "\n",
       "[26298 rows x 246 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ead87f7c-1cf5-4292-96a0-9909c630c8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>gdp</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>wd4</th>\n",
       "      <th>wd56</th>\n",
       "      <th>...</th>\n",
       "      <th>easter50</th>\n",
       "      <th>easter51</th>\n",
       "      <th>easter52</th>\n",
       "      <th>easter53</th>\n",
       "      <th>easter54</th>\n",
       "      <th>easter55</th>\n",
       "      <th>easter56</th>\n",
       "      <th>easter57</th>\n",
       "      <th>easter58</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.597614</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.597614</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.597614</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.597614</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.597614</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Sticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.282042</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  country       store         product  num_sold       gdp  \\\n",
       "0    2019-01-01  Finland  KaggleMart      Kaggle Mug       NaN  5.597614   \n",
       "1    2019-01-01  Finland  KaggleMart      Kaggle Hat       NaN  5.597614   \n",
       "2    2019-01-01  Finland  KaggleMart  Kaggle Sticker       NaN  5.597614   \n",
       "3    2019-01-01  Finland  KaggleRama      Kaggle Mug       NaN  5.597614   \n",
       "4    2019-01-01  Finland  KaggleRama      Kaggle Hat       NaN  5.597614   \n",
       "...         ...      ...         ...             ...       ...       ...   \n",
       "6565 2019-12-31   Sweden  KaggleMart      Kaggle Hat       NaN  6.282042   \n",
       "6566 2019-12-31   Sweden  KaggleMart  Kaggle Sticker       NaN  6.282042   \n",
       "6567 2019-12-31   Sweden  KaggleRama      Kaggle Mug       NaN  6.282042   \n",
       "6568 2019-12-31   Sweden  KaggleRama      Kaggle Hat       NaN  6.282042   \n",
       "6569 2019-12-31   Sweden  KaggleRama  Kaggle Sticker       NaN  6.282042   \n",
       "\n",
       "      month  season    wd4   wd56  ...  easter50  easter51  easter52  \\\n",
       "0         1       1  False  False  ...     False     False     False   \n",
       "1         1       1  False  False  ...     False     False     False   \n",
       "2         1       1  False  False  ...     False     False     False   \n",
       "3         1       1  False  False  ...     False     False     False   \n",
       "4         1       1  False  False  ...     False     False     False   \n",
       "...     ...     ...    ...    ...  ...       ...       ...       ...   \n",
       "6565     12       1  False  False  ...     False     False     False   \n",
       "6566     12       1  False  False  ...     False     False     False   \n",
       "6567     12       1  False  False  ...     False     False     False   \n",
       "6568     12       1  False  False  ...     False     False     False   \n",
       "6569     12       1  False  False  ...     False     False     False   \n",
       "\n",
       "      easter53  easter54  easter55  easter56  easter57  easter58  target  \n",
       "0        False     False     False     False     False     False     NaN  \n",
       "1        False     False     False     False     False     False     NaN  \n",
       "2        False     False     False     False     False     False     NaN  \n",
       "3        False     False     False     False     False     False     NaN  \n",
       "4        False     False     False     False     False     False     NaN  \n",
       "...        ...       ...       ...       ...       ...       ...     ...  \n",
       "6565     False     False     False     False     False     False     NaN  \n",
       "6566     False     False     False     False     False     False     NaN  \n",
       "6567     False     False     False     False     False     False     NaN  \n",
       "6568     False     False     False     False     False     False     NaN  \n",
       "6569     False     False     False     False     False     False     NaN  \n",
       "\n",
       "[6570 rows x 246 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8825c-4f51-4d27-89af-6b5416a0f98c",
   "metadata": {},
   "source": [
    "One question: should the full `tv_df` be passed as data, or only the validation set? I think it's better to supply as much data as possible, so I'll use the whole thing for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "03445ac7-a189-40f8-a42b-2e57001df451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6b36b5a4-4c73-45ed-acb6-6b283a43d24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26298"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00188415-c7de-448d-afad-69c6bc853480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26298"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c91fb31b-b595-4905-966f-9bb5229e738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b888c8e0-081b-497c-b771-0a7ef0eccd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527db03d-15fe-41bc-96ee-e7a99868c0c7",
   "metadata": {},
   "source": [
    "#### Residual Trainer \n",
    "(assumes Scikit-Learn API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b19a872c-4e83-4f2e-a979-1a004f0eeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_trainer(estimator, model_kwargs={}, forecast_models=forecast_models,\n",
    "                     tv_df=tv_df, test_df=test_df,\n",
    "                     tv_forecast_preds=tv_preds, test_forecast_preds=test_forecast_preds):\n",
    "#     df_2018 = tv_df[tv_df['date'] > '2017-12-31']\n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, tv_df = label_encoder(df_train) # should leave broader scope's tv_df alone\n",
    "    _, test_df = label_encoder(df_test) # should leave broader scope's test_df alone\n",
    "    del df_train, df_test\n",
    "    \n",
    "    residual_tv_df = pd.DataFrame({\n",
    "        'date': orig_train_df['date'],\n",
    "        'num_sold': orig_train_df['num_sold']\n",
    "    })\n",
    "    print(len(residual_tv_df))\n",
    "    \n",
    "    residual_test_df = pd.DataFrame({\n",
    "        'date': orig_test_df['date'],\n",
    "    })\n",
    "    \n",
    "    # getting rid of unneeded 'target' feature, also num_sold since we're only interested in predicting the residual\n",
    "    # following @ambrosm lightgbm nb and leaving date in for GroupKFold, but unsure of this (or if I should drop it later)\n",
    "    tv_df = tv_df.drop(columns=['target', 'num_sold'])#, 'date'])\n",
    "    test_df = test_df.drop(columns=['target', 'num_sold'])#, 'date'])\n",
    "    \n",
    "    kfolds= GroupKFold(n_splits=4)\n",
    "    \n",
    "    test_fold_preds = {}\n",
    "    \n",
    "    for forecast_model in forecast_models:\n",
    "        print(f\"Working with forecasts from {forecast_model}...\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        forecast = tv_forecast_preds[forecast_model] # pull out the predictions on the t-v sets for a given estimator\n",
    "        residuals = residual_tv_df['num_sold'] - forecast # get the residuals for the given model's forecast\n",
    "#         residual_df[f'actual_{forecast_model}_residual'] = residuals # may not need to put this in there\n",
    "#         tv_df['residual'] = residuals # residuals will rotate in and out of this feature\n",
    "        \n",
    "        \n",
    "        X_test = test_df.drop(columns=['date'])\n",
    "#         print(\"y.shape is \", y.shape)\n",
    "        \n",
    "        # prepare for Group K-Fold cross-val; below from @ambrosm LightGBM notebook\n",
    "        oof_preds = pd.Series(0, index=tv_df.index)\n",
    "#         test_preds_df = pd.DataFrame({\n",
    "#             'date': test_df['date']\n",
    "#         })\n",
    "        score_list = []\n",
    "#         params['seed'] = 1\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfolds.split(tv_df, groups=tv_df.date.dt.year)):\n",
    "            print(\"-----------------------------------------------------\")\n",
    "            print(f\"FOLD {fold}\")\n",
    "            X = tv_df.iloc[train_idx].drop(columns=['date'])\n",
    "            y = residuals.iloc[train_idx]#['date']\n",
    "            X_valid = tv_df.iloc[val_idx].drop(columns=['date'])\n",
    "            y_valid = residuals.iloc[val_idx]#['date']\n",
    "            \n",
    "            model = estimator(**model_kwargs)\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            residual_valid_preds = model.predict(X_valid)\n",
    "            residual_test_preds = model.predict(X_test)\n",
    "            \n",
    "            oof_preds[val_idx] = residual_valid_preds\n",
    "#             test_preds_df[f'{forecast_model}_fold_{fold}_preds'] = residual_test_preds\n",
    "#             residual_test_df = residual_test_df.join(test_preds_df)\n",
    "#             smape = SMAPE(y_pred=residual_valid_preds, y_true=y_valid.values)\n",
    "#             print(f\"SMAPE: {smape}\")\n",
    "            rmse = math.sqrt(mean_squared_error(y_pred=residual_valid_preds, y_true=y_valid.values))\n",
    "            print(f\"RMSE: {rmse}\")\n",
    "            test_fold_preds[f'{forecast_model}_{fold}_residual_preds'] = residual_test_preds\n",
    "        residual_tv_df[f'{forecast_model}_oof_residual_preds'] = oof_preds\n",
    "#         residual_test_df = residual_test_df.join(test_preds_df)\n",
    "    return residual_tv_df, test_fold_preds #residual_test_df\n",
    "        \n",
    "            \n",
    "            \n",
    "#             model, smape = fit_model(X_tr, X_va, run=0, fold=fold)\n",
    "\n",
    "# print(f\"Average SMAPE: {sum(score_list) / len(score_list):.5f}\")\n",
    "# with open('oof.pickle', 'wb') as handle: pickle.dump(oof, handle)\n",
    "#         X = tv_df.drop(columns=['residual'])\n",
    "#         y = tv_df['residual']\n",
    "        \n",
    "#         tv_\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "#     y_valid = valid_df['num_sold']\n",
    "#     valid_df = valid_df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dd79291a-e0db-47ac-937e-c8973318f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282e7b8-a7f7-4b7c-b3dd-18c0bc4a1f3f",
   "metadata": {},
   "source": [
    "#### Residual model hyperparams\n",
    "These are inspired by the best hyperparams for the full-stack GBM models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f894553e-497a-4335-b955-3ca63074ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'eval_metric': ['mae', 'mape'],\n",
    "    'sampling_method': 'gradient_based',\n",
    "    'seed': 42,\n",
    "    'grow_policy': 'lossguide',\n",
    "    'learning_rate': .05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.2,\n",
    "#     'max_leaves': 255,\n",
    "    'lambda': 0.05,\n",
    "    'alpha': 0.01,\n",
    "    'n_estimators': 4200,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'colsample_bytree': 0.98,\n",
    "    'min_child_weight': 7,\n",
    "    'gamma': 0.1\n",
    "#     'verbose': True,\n",
    "}\n",
    "\n",
    "\n",
    "lightgbm_params = {\n",
    "    'objective': 'mse',\n",
    "    'random_state': 42,\n",
    "    'device_type': 'cpu',\n",
    "    'n_jobs': -1,\n",
    "#                 eval_metric='auc',\n",
    "#     'device_type': 'gpu',\n",
    "#     'max_bin': 63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     'gpu_use_dp': False,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.035,\n",
    "    'subsample': .96,\n",
    "    'n_estimators': 6050,\n",
    "    'reg_alpha': 0.009,\n",
    "    'min_child_samples': 24,\n",
    "    'num_leaves': 235,\n",
    "    'colsample_bytree': 0.92,\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'task_type':'GPU',\n",
    "    'silent':True,\n",
    "    'random_state':42,\n",
    "    'iterations': 10500,\n",
    "    'learning_rate': 0.07,\n",
    "    'random_strength':44,\n",
    "    'od_wait': 261,\n",
    "    'reg_lambda': 35.67,\n",
    "    'border_count': 57,\n",
    "    'min_child_samples': 19,\n",
    "    'leaf_estimation_iterations': 2,\n",
    "    'max_depth': 3\n",
    "}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ad4ba-04df-47d6-bc2a-8e57c1d81856",
   "metadata": {
    "tags": []
   },
   "source": [
    "XGBoost SMAPEs with `n_estimators=500`:\n",
    "\n",
    "```\n",
    "26298\n",
    "Working with forecasts from prophet...\n",
    "-----------------------------------------------------\n",
    "-----------------------------------------------------\n",
    "FOLD 0\n",
    "RMSE: 44.22169948290956\n",
    "-----------------------------------------------------\n",
    "FOLD 1\n",
    "RMSE: 42.2884194618039\n",
    "-----------------------------------------------------\n",
    "FOLD 2\n",
    "RMSE: 38.87851134388903\n",
    "-----------------------------------------------------\n",
    "FOLD 3\n",
    "RMSE: 39.14401892347087\n",
    "Working with forecasts from neuralprophet...\n",
    "-----------------------------------------------------\n",
    "-----------------------------------------------------\n",
    "FOLD 0\n",
    "RMSE: 71.59711327241503\n",
    "-----------------------------------------------------\n",
    "FOLD 1\n",
    "RMSE: 50.410812085025206\n",
    "-----------------------------------------------------\n",
    "FOLD 2\n",
    "RMSE: 56.30623610755547\n",
    "-----------------------------------------------------\n",
    "FOLD 3\n",
    "RMSE: 71.87002649034811\n",
    "Working with forecasts from ridge...\n",
    "-----------------------------------------------------\n",
    "-----------------------------------------------------\n",
    "FOLD 0\n",
    "RMSE: 29.951196797722172\n",
    "-----------------------------------------------------\n",
    "FOLD 1\n",
    "RMSE: 33.87710734178301\n",
    "-----------------------------------------------------\n",
    "FOLD 2\n",
    "RMSE: 30.3425528497887\n",
    "-----------------------------------------------------\n",
    "FOLD 3\n",
    "RMSE: 30.908377053367243\n",
    "Working with forecasts from linear...\n",
    "-----------------------------------------------------\n",
    "-----------------------------------------------------\n",
    "FOLD 0\n",
    "RMSE: 25.909733767351145\n",
    "-----------------------------------------------------\n",
    "FOLD 1\n",
    "RMSE: 32.55539418287357\n",
    "-----------------------------------------------------\n",
    "FOLD 2\n",
    "RMSE: 28.02573800531867\n",
    "-----------------------------------------------------\n",
    "FOLD 3\n",
    "RMSE: 27.512157750399876\n",
    "Working with forecasts from huber...\n",
    "-----------------------------------------------------\n",
    "-----------------------------------------------------\n",
    "FOLD 0\n",
    "RMSE: 43.8980188030959\n",
    "-----------------------------------------------------\n",
    "FOLD 1\n",
    "RMSE: 55.40375581530791\n",
    "-----------------------------------------------------\n",
    "FOLD 2\n",
    "RMSE: 46.37734856211295\n",
    "-----------------------------------------------------\n",
    "FOLD 3\n",
    "RMSE: 41.743947334147386\n",
    "Working with forecasts from lasso...\n",
    "-----------------------------------------------------\n",
    "-----------------------------------------------------\n",
    "FOLD 0\n",
    "RMSE: 42.212939444847706\n",
    "-----------------------------------------------------\n",
    "FOLD 1\n",
    "RMSE: 58.6828784233263\n",
    "-----------------------------------------------------\n",
    "FOLD 2\n",
    "RMSE: 47.91103516420516\n",
    "-----------------------------------------------------\n",
    "FOLD 3\n",
    "RMSE: 41.89282012115221\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71150a-f321-4384-a2ed-8ee6c2e49490",
   "metadata": {},
   "source": [
    "#### Residual model training / loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f9f495a4-121d-4a0d-b8e0-74486c89224e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 ms, sys: 4.02 ms, total: 5.25 ms\n",
      "Wall time: 5.23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# xgb_tv_df, xgb_test_fold_preds = residual_trainer(estimator=XGBRegressor, model_kwargs=xgboost_params,)\n",
    "# dump(xgb_tv_df, predpath/'20220127_-huber-lasso+earth_residual_xgboost_oof_preds.joblib')\n",
    "# dump(xgb_test_fold_preds, predpath/'20220125_-huber-lasso+earth_residual_xgboost_test_preds.joblib')\n",
    "xgb_tv_df = load(predpath/'20220127_-huber-lasso+earth_residual_xgboost_oof_preds.joblib')\n",
    "xgb_test_fold_preds = load(predpath/'20220125_-huber-lasso+earth_residual_xgboost_test_preds.joblib')\n",
    "# loading the 500-estimator version, which is universally better\n",
    "# xgb_tv_df = load(predpath/'20220124_residual_oof_xgboost_preds.joblib')\n",
    "# xgb_test_fold_preds = load(predpath/'20220124_residual_test_xgboost_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8ef97520-5580-40a3-9a3f-50870f058dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fd76fcae-43a6-4c0e-a480-e3dfe69d17c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.56 ms, sys: 0 ns, total: 4.56 ms\n",
      "Wall time: 4.46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# lgb_tv_df, lgb_test_fold_preds = residual_trainer(estimator=LGBMRegressor, model_kwargs=lightgbm_params,)\n",
    "# dump(lgb_tv_df, predpath/'20220127_-huber-lasso+earth_residual_lightgbm_oof_preds.joblib')\n",
    "# dump(lgb_test_fold_preds, predpath/'20220127_-huber-lasso+earth_residual_lightgbm_test_preds.joblib')\n",
    "\n",
    "lgb_tv_df = load(predpath/'20220127_-huber-lasso+earth_residual_lightgbm_oof_preds.joblib')\n",
    "lgb_test_fold_preds = load(predpath/'20220127_-huber-lasso+earth_residual_lightgbm_test_preds.joblib')\n",
    "# lgb_tv_df = load(predpath/'20220124_residual_oof_residual_lightgbm_preds.joblib')\n",
    "# lgb_test_fold_preds = load(predpath/'20220124_residual_test_residual_lightgbm_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9880e97d-f8b2-4f37-b86b-34398de4c68a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.93 ms, sys: 7 µs, total: 4.94 ms\n",
      "Wall time: 4.64 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cat_tv_df, cat_test_fold_preds = residual_trainer(estimator=CatBoostRegressor, model_kwargs=catboost_params,)\n",
    "# dump(cat_tv_df, predpath/'20220127_-huber-lasso+earth_residual_catboost_oof_preds.joblib')\n",
    "# dump(cat_test_df, predpath/'20220127_-huber-lasso+earth_residual_catboost_test_preds.joblib')\n",
    "\n",
    "cat_tv_df = load(predpath/'20220127_-huber-lasso+earth_residual_catboost_oof_preds.joblib')\n",
    "cat_test_fold_preds = load(predpath/'20220127_-huber-lasso+earth_residual_catboost_test_preds.joblib')\n",
    "\n",
    "# dump(cat_tv_df, predpath/'20220125_+earth_residual_oof_residual_catboost_preds.joblib')\n",
    "# dump(cat_test_fold_preds, predpath/'20220125_+earth_residual_test_residual_catboost_preds.joblib')\n",
    "# cat_tv_df = load(predpath/'20220124_residual_oof_residual_catboost_preds.joblib')\n",
    "# cat_test_fold_preds = load(predpath/'20220124_residual_test_residual_catboost_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5a8913b7-c900-4e87-8f47-9640cd3bab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(cat_test_fold_preds, predpath/'20220127_-huber-lasso+earth_residual_catboost_test_preds.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e911f-070d-494d-afcf-ad880e1e2d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dd50763d-658c-46d3-bcdb-018e61f6f089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# ridge_tv_df, ridge_test_fold_preds = residual_trainer(estimator=Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0085b3ef-6efd-4bfe-a70f-89d97a5a5363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# lasso_tv_df, lasso_test_fold_preds = residual_trainer(estimator=Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6dceb449-46ee-4ce6-9736-7549f7b5192d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# linear_tv_df, linear_test_fold_preds = residual_trainer(estimator=LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "082ba17d-b909-4d13-91d1-166fa0b0f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(valid_forecast_preds, predpath/'20220124_forecast_valid2018_preds.joblib')\n",
    "# dump(test_forecast_preds, predpath/'20220124_forecast_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea69bb6-001b-478c-9497-b21c91d521f5",
   "metadata": {},
   "source": [
    "#### Assembling together residual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "12ac91d1-2234-4b47-bedf-4a266b82cc67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tv_dfs = {\n",
    "    'xgb': xgb_tv_df, \n",
    "    'lgb': lgb_tv_df, \n",
    "    'cat': cat_tv_df,\n",
    "#     'ridge': ridge_tv_df,\n",
    "#     'lasso': lasso_tv_df,\n",
    "#     'linear': linear_tv_df,\n",
    "}\n",
    "\n",
    "for arch in tv_dfs.keys():\n",
    "    for forecast_model in forecast_models:\n",
    "        tv_dfs[arch][f'{forecast_model}_pred'] = tv_preds[forecast_model]\n",
    "        tv_dfs[arch][f'{forecast_model}_residual'] = tv_dfs[arch]['num_sold'] - tv_preds[forecast_model] #- xgb_tv_df['num_sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "437c3330-0afa-4635-b986-671a741af3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_tv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "06263943-6c13-48f4-a475-273c610cafb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet_oof_residual_preds</th>\n",
       "      <th>neuralprophet_oof_residual_preds</th>\n",
       "      <th>ridge_oof_residual_preds</th>\n",
       "      <th>linear_oof_residual_preds</th>\n",
       "      <th>earth_oof_residual_preds</th>\n",
       "      <th>prophet_pred</th>\n",
       "      <th>prophet_residual</th>\n",
       "      <th>neuralprophet_pred</th>\n",
       "      <th>neuralprophet_residual</th>\n",
       "      <th>ridge_pred</th>\n",
       "      <th>ridge_residual</th>\n",
       "      <th>linear_pred</th>\n",
       "      <th>linear_residual</th>\n",
       "      <th>earth_pred</th>\n",
       "      <th>earth_residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>329</td>\n",
       "      <td>-11.155267</td>\n",
       "      <td>-3.433359</td>\n",
       "      <td>41.070728</td>\n",
       "      <td>2.167291</td>\n",
       "      <td>38.581459</td>\n",
       "      <td>346.560416</td>\n",
       "      <td>-17.560416</td>\n",
       "      <td>329.134521</td>\n",
       "      <td>-0.134521</td>\n",
       "      <td>283.272258</td>\n",
       "      <td>45.727742</td>\n",
       "      <td>324.188332</td>\n",
       "      <td>4.811668</td>\n",
       "      <td>360.672190</td>\n",
       "      <td>-31.672190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>520</td>\n",
       "      <td>-28.441416</td>\n",
       "      <td>5.382236</td>\n",
       "      <td>61.150066</td>\n",
       "      <td>-2.865466</td>\n",
       "      <td>13.708234</td>\n",
       "      <td>536.203586</td>\n",
       "      <td>-16.203586</td>\n",
       "      <td>458.008301</td>\n",
       "      <td>61.991699</td>\n",
       "      <td>439.771734</td>\n",
       "      <td>80.228266</td>\n",
       "      <td>503.081569</td>\n",
       "      <td>16.918431</td>\n",
       "      <td>584.851806</td>\n",
       "      <td>-64.851806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>146</td>\n",
       "      <td>-6.331515</td>\n",
       "      <td>2.279078</td>\n",
       "      <td>17.805452</td>\n",
       "      <td>3.196031</td>\n",
       "      <td>12.222075</td>\n",
       "      <td>143.412803</td>\n",
       "      <td>2.587197</td>\n",
       "      <td>145.325577</td>\n",
       "      <td>0.674423</td>\n",
       "      <td>120.885683</td>\n",
       "      <td>25.114317</td>\n",
       "      <td>136.667932</td>\n",
       "      <td>9.332068</td>\n",
       "      <td>138.401176</td>\n",
       "      <td>7.598824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>572</td>\n",
       "      <td>6.812320</td>\n",
       "      <td>-0.307026</td>\n",
       "      <td>69.837616</td>\n",
       "      <td>-4.111918</td>\n",
       "      <td>61.455334</td>\n",
       "      <td>590.117165</td>\n",
       "      <td>-18.117165</td>\n",
       "      <td>552.571167</td>\n",
       "      <td>19.428833</td>\n",
       "      <td>488.810710</td>\n",
       "      <td>83.189290</td>\n",
       "      <td>555.531957</td>\n",
       "      <td>16.468043</td>\n",
       "      <td>562.479785</td>\n",
       "      <td>9.520215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>911</td>\n",
       "      <td>0.980131</td>\n",
       "      <td>81.692017</td>\n",
       "      <td>118.799637</td>\n",
       "      <td>-7.382885</td>\n",
       "      <td>17.165312</td>\n",
       "      <td>939.673009</td>\n",
       "      <td>-28.673009</td>\n",
       "      <td>781.967285</td>\n",
       "      <td>129.032715</td>\n",
       "      <td>771.701702</td>\n",
       "      <td>139.298298</td>\n",
       "      <td>890.469201</td>\n",
       "      <td>20.530799</td>\n",
       "      <td>944.584508</td>\n",
       "      <td>-33.584508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>823</td>\n",
       "      <td>-55.787830</td>\n",
       "      <td>259.895264</td>\n",
       "      <td>140.673294</td>\n",
       "      <td>-8.261147</td>\n",
       "      <td>-16.538561</td>\n",
       "      <td>898.322121</td>\n",
       "      <td>-75.322121</td>\n",
       "      <td>669.418457</td>\n",
       "      <td>153.581543</td>\n",
       "      <td>723.714830</td>\n",
       "      <td>99.285170</td>\n",
       "      <td>846.597603</td>\n",
       "      <td>-23.597603</td>\n",
       "      <td>836.947940</td>\n",
       "      <td>-13.947940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>250</td>\n",
       "      <td>2.033622</td>\n",
       "      <td>24.113207</td>\n",
       "      <td>50.021725</td>\n",
       "      <td>9.767989</td>\n",
       "      <td>1.243830</td>\n",
       "      <td>253.512355</td>\n",
       "      <td>-3.512355</td>\n",
       "      <td>227.158142</td>\n",
       "      <td>22.841858</td>\n",
       "      <td>205.880079</td>\n",
       "      <td>44.119921</td>\n",
       "      <td>241.048951</td>\n",
       "      <td>8.951049</td>\n",
       "      <td>254.287204</td>\n",
       "      <td>-4.287204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1004</td>\n",
       "      <td>-47.085091</td>\n",
       "      <td>349.409943</td>\n",
       "      <td>150.556046</td>\n",
       "      <td>-3.797827</td>\n",
       "      <td>-20.398714</td>\n",
       "      <td>1039.635205</td>\n",
       "      <td>-35.635205</td>\n",
       "      <td>715.639648</td>\n",
       "      <td>288.360352</td>\n",
       "      <td>832.192362</td>\n",
       "      <td>171.807638</td>\n",
       "      <td>975.785339</td>\n",
       "      <td>28.214661</td>\n",
       "      <td>1058.174059</td>\n",
       "      <td>-54.174059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1441</td>\n",
       "      <td>-21.645599</td>\n",
       "      <td>593.559570</td>\n",
       "      <td>221.251251</td>\n",
       "      <td>-4.062232</td>\n",
       "      <td>-15.556402</td>\n",
       "      <td>1526.908216</td>\n",
       "      <td>-85.908216</td>\n",
       "      <td>980.234009</td>\n",
       "      <td>460.765991</td>\n",
       "      <td>1255.885410</td>\n",
       "      <td>185.114590</td>\n",
       "      <td>1468.776593</td>\n",
       "      <td>-27.776593</td>\n",
       "      <td>2036.549357</td>\n",
       "      <td>-595.549357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>388</td>\n",
       "      <td>-36.886063</td>\n",
       "      <td>105.492065</td>\n",
       "      <td>73.411644</td>\n",
       "      <td>1.753363</td>\n",
       "      <td>13.074070</td>\n",
       "      <td>463.705207</td>\n",
       "      <td>-75.705207</td>\n",
       "      <td>423.052612</td>\n",
       "      <td>-35.052612</td>\n",
       "      <td>373.542094</td>\n",
       "      <td>14.457906</td>\n",
       "      <td>441.128842</td>\n",
       "      <td>-53.128842</td>\n",
       "      <td>472.307718</td>\n",
       "      <td>-84.307718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  num_sold  prophet_oof_residual_preds  \\\n",
       "0      2015-01-01       329                  -11.155267   \n",
       "1      2015-01-01       520                  -28.441416   \n",
       "2      2015-01-01       146                   -6.331515   \n",
       "3      2015-01-01       572                    6.812320   \n",
       "4      2015-01-01       911                    0.980131   \n",
       "...           ...       ...                         ...   \n",
       "26293  2018-12-31       823                  -55.787830   \n",
       "26294  2018-12-31       250                    2.033622   \n",
       "26295  2018-12-31      1004                  -47.085091   \n",
       "26296  2018-12-31      1441                  -21.645599   \n",
       "26297  2018-12-31       388                  -36.886063   \n",
       "\n",
       "       neuralprophet_oof_residual_preds  ridge_oof_residual_preds  \\\n",
       "0                             -3.433359                 41.070728   \n",
       "1                              5.382236                 61.150066   \n",
       "2                              2.279078                 17.805452   \n",
       "3                             -0.307026                 69.837616   \n",
       "4                             81.692017                118.799637   \n",
       "...                                 ...                       ...   \n",
       "26293                        259.895264                140.673294   \n",
       "26294                         24.113207                 50.021725   \n",
       "26295                        349.409943                150.556046   \n",
       "26296                        593.559570                221.251251   \n",
       "26297                        105.492065                 73.411644   \n",
       "\n",
       "       linear_oof_residual_preds  earth_oof_residual_preds  prophet_pred  \\\n",
       "0                       2.167291                 38.581459    346.560416   \n",
       "1                      -2.865466                 13.708234    536.203586   \n",
       "2                       3.196031                 12.222075    143.412803   \n",
       "3                      -4.111918                 61.455334    590.117165   \n",
       "4                      -7.382885                 17.165312    939.673009   \n",
       "...                          ...                       ...           ...   \n",
       "26293                  -8.261147                -16.538561    898.322121   \n",
       "26294                   9.767989                  1.243830    253.512355   \n",
       "26295                  -3.797827                -20.398714   1039.635205   \n",
       "26296                  -4.062232                -15.556402   1526.908216   \n",
       "26297                   1.753363                 13.074070    463.705207   \n",
       "\n",
       "       prophet_residual  neuralprophet_pred  neuralprophet_residual  \\\n",
       "0            -17.560416          329.134521               -0.134521   \n",
       "1            -16.203586          458.008301               61.991699   \n",
       "2              2.587197          145.325577                0.674423   \n",
       "3            -18.117165          552.571167               19.428833   \n",
       "4            -28.673009          781.967285              129.032715   \n",
       "...                 ...                 ...                     ...   \n",
       "26293        -75.322121          669.418457              153.581543   \n",
       "26294         -3.512355          227.158142               22.841858   \n",
       "26295        -35.635205          715.639648              288.360352   \n",
       "26296        -85.908216          980.234009              460.765991   \n",
       "26297        -75.705207          423.052612              -35.052612   \n",
       "\n",
       "        ridge_pred  ridge_residual  linear_pred  linear_residual   earth_pred  \\\n",
       "0       283.272258       45.727742   324.188332         4.811668   360.672190   \n",
       "1       439.771734       80.228266   503.081569        16.918431   584.851806   \n",
       "2       120.885683       25.114317   136.667932         9.332068   138.401176   \n",
       "3       488.810710       83.189290   555.531957        16.468043   562.479785   \n",
       "4       771.701702      139.298298   890.469201        20.530799   944.584508   \n",
       "...            ...             ...          ...              ...          ...   \n",
       "26293   723.714830       99.285170   846.597603       -23.597603   836.947940   \n",
       "26294   205.880079       44.119921   241.048951         8.951049   254.287204   \n",
       "26295   832.192362      171.807638   975.785339        28.214661  1058.174059   \n",
       "26296  1255.885410      185.114590  1468.776593       -27.776593  2036.549357   \n",
       "26297   373.542094       14.457906   441.128842       -53.128842   472.307718   \n",
       "\n",
       "       earth_residual  \n",
       "0          -31.672190  \n",
       "1          -64.851806  \n",
       "2            7.598824  \n",
       "3            9.520215  \n",
       "4          -33.584508  \n",
       "...               ...  \n",
       "26293      -13.947940  \n",
       "26294       -4.287204  \n",
       "26295      -54.174059  \n",
       "26296     -595.549357  \n",
       "26297      -84.307718  \n",
       "\n",
       "[26298 rows x 17 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dfs['xgb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a8afcd17-bd33-4e8a-a663-901f20a3cc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For xgb...\n",
      "\n",
      "Before residual adjustment, SMAPE for prophet was 7.098824805150043. Final SMAPE for prophet is 6.026884283769224.\n",
      "Before residual adjustment, SMAPE for neuralprophet was 7.926662675729486. Final SMAPE for neuralprophet is 8.772834447498484.\n",
      "Before residual adjustment, SMAPE for ridge was 4.781041939812455. Final SMAPE for ridge is 5.289615055217569.\n",
      "Before residual adjustment, SMAPE for linear was 4.175573461450916. Final SMAPE for linear is 5.040366554565315.\n",
      "Before residual adjustment, SMAPE for earth was 5.338351055481194. Final SMAPE for earth is 6.029509457078209.\n",
      "\n",
      "For lgb...\n",
      "\n",
      "Before residual adjustment, SMAPE for prophet was 7.098824805150043. Final SMAPE for prophet is 6.122509629258212.\n",
      "Before residual adjustment, SMAPE for neuralprophet was 7.926662675729486. Final SMAPE for neuralprophet is 8.460500050162052.\n",
      "Before residual adjustment, SMAPE for ridge was 4.781041939812455. Final SMAPE for ridge is 4.876170971167097.\n",
      "Before residual adjustment, SMAPE for linear was 4.175573461450916. Final SMAPE for linear is 4.611225161526616.\n",
      "Before residual adjustment, SMAPE for earth was 5.338351055481194. Final SMAPE for earth is 6.176815191347562.\n",
      "\n",
      "For cat...\n",
      "\n",
      "Before residual adjustment, SMAPE for prophet was 7.098824805150043. Final SMAPE for prophet is 5.632460495389126.\n",
      "Before residual adjustment, SMAPE for neuralprophet was 7.926662675729486. Final SMAPE for neuralprophet is 8.08082530428997.\n",
      "Before residual adjustment, SMAPE for ridge was 4.781041939812455. Final SMAPE for ridge is 4.69133021716973.\n",
      "Before residual adjustment, SMAPE for linear was 4.175573461450916. Final SMAPE for linear is 4.486422338269691.\n",
      "Before residual adjustment, SMAPE for earth was 5.338351055481194. Final SMAPE for earth is 6.080372573807768.\n"
     ]
    }
   ],
   "source": [
    "for arch in tv_dfs.keys(): # xgb, then lgb, then cat\n",
    "    print(f\"\\nFor {arch}...\\n\")\n",
    "    for forecast_model in forecast_models:\n",
    "        \n",
    "#         if arch == 'xgb':\n",
    "#             adjusted_smape = SMAPE(y_pred=tv_dfs[arch][f'{forecast_model}_pred']+tv_dfs[arch][f'{forecast_model}_oof_preds'], y_true=tv_dfs[arch]['num_sold'])\n",
    "#         else:\n",
    "        adjusted_smape = SMAPE(y_pred=tv_dfs[arch][f'{forecast_model}_pred']+tv_dfs[arch][f'{forecast_model}_oof_residual_preds'], y_true=tv_dfs[arch]['num_sold'])\n",
    "        original_smape = SMAPE(y_pred=tv_dfs[arch][f'{forecast_model}_pred'], y_true=tv_dfs[arch]['num_sold'])\n",
    "        print(f'Before residual adjustment, SMAPE for {forecast_model} was {original_smape}. Final SMAPE for {forecast_model} is {adjusted_smape}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b8405bb2-f1eb-4adb-a227-007a9f08f799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>329</td>\n",
       "      <td>346.560416</td>\n",
       "      <td>329.134521</td>\n",
       "      <td>283.272258</td>\n",
       "      <td>324.188332</td>\n",
       "      <td>360.672190</td>\n",
       "      <td>343.250478</td>\n",
       "      <td>331.496226</td>\n",
       "      <td>326.054356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>520</td>\n",
       "      <td>536.203586</td>\n",
       "      <td>458.008301</td>\n",
       "      <td>439.771734</td>\n",
       "      <td>503.081569</td>\n",
       "      <td>584.851806</td>\n",
       "      <td>510.616623</td>\n",
       "      <td>514.576867</td>\n",
       "      <td>509.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>146</td>\n",
       "      <td>143.412803</td>\n",
       "      <td>145.325577</td>\n",
       "      <td>120.885683</td>\n",
       "      <td>136.667932</td>\n",
       "      <td>138.401176</td>\n",
       "      <td>148.959080</td>\n",
       "      <td>146.429925</td>\n",
       "      <td>144.894609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>572</td>\n",
       "      <td>590.117165</td>\n",
       "      <td>552.571167</td>\n",
       "      <td>488.810710</td>\n",
       "      <td>555.531957</td>\n",
       "      <td>562.479785</td>\n",
       "      <td>596.463984</td>\n",
       "      <td>584.653899</td>\n",
       "      <td>569.038022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>911</td>\n",
       "      <td>939.673009</td>\n",
       "      <td>781.967285</td>\n",
       "      <td>771.701702</td>\n",
       "      <td>890.469201</td>\n",
       "      <td>944.584508</td>\n",
       "      <td>907.745110</td>\n",
       "      <td>888.842963</td>\n",
       "      <td>882.892654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>823</td>\n",
       "      <td>898.322121</td>\n",
       "      <td>669.418457</td>\n",
       "      <td>723.714830</td>\n",
       "      <td>846.597603</td>\n",
       "      <td>836.947940</td>\n",
       "      <td>830.633157</td>\n",
       "      <td>814.597001</td>\n",
       "      <td>825.377960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>250</td>\n",
       "      <td>253.512355</td>\n",
       "      <td>227.158142</td>\n",
       "      <td>205.880079</td>\n",
       "      <td>241.048951</td>\n",
       "      <td>254.287204</td>\n",
       "      <td>241.115195</td>\n",
       "      <td>234.409908</td>\n",
       "      <td>245.208674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1004</td>\n",
       "      <td>1039.635205</td>\n",
       "      <td>715.639648</td>\n",
       "      <td>832.192362</td>\n",
       "      <td>975.785339</td>\n",
       "      <td>1058.174059</td>\n",
       "      <td>898.057926</td>\n",
       "      <td>900.745370</td>\n",
       "      <td>915.158956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1441</td>\n",
       "      <td>1526.908216</td>\n",
       "      <td>980.234009</td>\n",
       "      <td>1255.885410</td>\n",
       "      <td>1468.776593</td>\n",
       "      <td>2036.549357</td>\n",
       "      <td>1397.507711</td>\n",
       "      <td>1419.978895</td>\n",
       "      <td>1401.414363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>388</td>\n",
       "      <td>463.705207</td>\n",
       "      <td>423.052612</td>\n",
       "      <td>373.542094</td>\n",
       "      <td>441.128842</td>\n",
       "      <td>472.307718</td>\n",
       "      <td>415.256775</td>\n",
       "      <td>417.253845</td>\n",
       "      <td>421.897675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  num_sold      prophet  neuralprophet        ridge  \\\n",
       "0      2015-01-01       329   346.560416     329.134521   283.272258   \n",
       "1      2015-01-01       520   536.203586     458.008301   439.771734   \n",
       "2      2015-01-01       146   143.412803     145.325577   120.885683   \n",
       "3      2015-01-01       572   590.117165     552.571167   488.810710   \n",
       "4      2015-01-01       911   939.673009     781.967285   771.701702   \n",
       "...           ...       ...          ...            ...          ...   \n",
       "26293  2018-12-31       823   898.322121     669.418457   723.714830   \n",
       "26294  2018-12-31       250   253.512355     227.158142   205.880079   \n",
       "26295  2018-12-31      1004  1039.635205     715.639648   832.192362   \n",
       "26296  2018-12-31      1441  1526.908216     980.234009  1255.885410   \n",
       "26297  2018-12-31       388   463.705207     423.052612   373.542094   \n",
       "\n",
       "            linear        earth      xgboost     lightgbm     catboost  \n",
       "0       324.188332   360.672190   343.250478   331.496226   326.054356  \n",
       "1       503.081569   584.851806   510.616623   514.576867   509.005936  \n",
       "2       136.667932   138.401176   148.959080   146.429925   144.894609  \n",
       "3       555.531957   562.479785   596.463984   584.653899   569.038022  \n",
       "4       890.469201   944.584508   907.745110   888.842963   882.892654  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "26293   846.597603   836.947940   830.633157   814.597001   825.377960  \n",
       "26294   241.048951   254.287204   241.115195   234.409908   245.208674  \n",
       "26295   975.785339  1058.174059   898.057926   900.745370   915.158956  \n",
       "26296  1468.776593  2036.549357  1397.507711  1419.978895  1401.414363  \n",
       "26297   441.128842   472.307718   415.256775   417.253845   421.897675  \n",
       "\n",
       "[26298 rows x 10 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11373ca-ce60-4aac-8363-a6d4de3a0e4b",
   "metadata": {},
   "source": [
    "## Submission of Adjusted XGB Naivish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b693a516-a1c6-47ed-ab44-001dd950d1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb\n",
      "\n",
      "lgb\n",
      "\n",
      "cat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oof_features = ['']\n",
    "adjusted_oof_preds = pd.DataFrame({\n",
    "    'date': orig_train_df['date'],\n",
    "})\n",
    "\n",
    "tv_dfs_dict = {\n",
    "    'xgb': xgb_tv_df,\n",
    "    'lgb': lgb_tv_df,\n",
    "    'cat': cat_tv_df,\n",
    "}\n",
    "\n",
    "for arch in tv_dfs_dict.keys():\n",
    "    print(f\"{arch}\\n\")\n",
    "    for forecast_model in forecast_models:\n",
    "#         if arch == 'xgb':\n",
    "#                         adjusted_oof_preds[f\"{arch}+{forecast_model}\"] = tv_dfs_dict[arch][f'{forecast_model}_pred']+tv_dfs_dict[arch][f'{forecast_model}_oof_preds']\n",
    "#         else:\n",
    "        adjusted_oof_preds[f\"{arch}+{forecast_model}\"] = tv_dfs_dict[arch][f'{forecast_model}_pred']+tv_dfs_dict[arch][f'{forecast_model}_oof_residual_preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "faee6e18-e3a4-4b9a-902b-1b2dfeabd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_preds_only = tv_preds.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f64d17a5-a2e3-4d42-b792-9651b229feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_oof_preds_selective = adjusted_oof_preds.drop(columns=[\n",
    "                                                                'xgb+neuralprophet', \n",
    "#                                                                 'xgb+ridge', \n",
    "#                                                                 'xgb+linear', \n",
    "                                                                'xgb+earth',\n",
    "                                                                'lgb+neuralprophet', \n",
    "#                                                                 'lgb+ridge', \n",
    "#                                                                 'lgb+linear', \n",
    "                                                                'lgb+earth',\n",
    "                                                                'cat+neuralprophet', \n",
    "#                                                                 'cat+linear', \n",
    "                                                                'cat+earth'\n",
    "                                                               ]   \n",
    "                                                      )\n",
    "                                                      \n",
    "# tv_preds_only = tv_preds.drop(columns=['date', 'num_sold'])\n",
    "adjusted_oof_preds_expanded = adjusted_oof_preds.join(tv_preds_only)#.drop(columns=['date', 'num_sold'], inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fc347367-ce28-4988-b2e3-183488ae6e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>xgb+prophet</th>\n",
       "      <th>xgb+ridge</th>\n",
       "      <th>xgb+linear</th>\n",
       "      <th>lgb+prophet</th>\n",
       "      <th>lgb+ridge</th>\n",
       "      <th>lgb+linear</th>\n",
       "      <th>cat+prophet</th>\n",
       "      <th>cat+ridge</th>\n",
       "      <th>cat+linear</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>335.405149</td>\n",
       "      <td>324.342987</td>\n",
       "      <td>326.355622</td>\n",
       "      <td>353.816288</td>\n",
       "      <td>315.109073</td>\n",
       "      <td>320.781832</td>\n",
       "      <td>342.162795</td>\n",
       "      <td>308.871192</td>\n",
       "      <td>319.407647</td>\n",
       "      <td>329</td>\n",
       "      <td>346.560416</td>\n",
       "      <td>329.134521</td>\n",
       "      <td>283.272258</td>\n",
       "      <td>324.188332</td>\n",
       "      <td>360.672190</td>\n",
       "      <td>343.250478</td>\n",
       "      <td>331.496226</td>\n",
       "      <td>326.054356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>507.762170</td>\n",
       "      <td>500.921801</td>\n",
       "      <td>500.216103</td>\n",
       "      <td>502.099511</td>\n",
       "      <td>502.309507</td>\n",
       "      <td>497.514037</td>\n",
       "      <td>505.468074</td>\n",
       "      <td>494.362517</td>\n",
       "      <td>498.780657</td>\n",
       "      <td>520</td>\n",
       "      <td>536.203586</td>\n",
       "      <td>458.008301</td>\n",
       "      <td>439.771734</td>\n",
       "      <td>503.081569</td>\n",
       "      <td>584.851806</td>\n",
       "      <td>510.616623</td>\n",
       "      <td>514.576867</td>\n",
       "      <td>509.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>137.081288</td>\n",
       "      <td>138.691135</td>\n",
       "      <td>139.863963</td>\n",
       "      <td>161.207516</td>\n",
       "      <td>137.984103</td>\n",
       "      <td>133.714742</td>\n",
       "      <td>149.287358</td>\n",
       "      <td>134.114025</td>\n",
       "      <td>135.374514</td>\n",
       "      <td>146</td>\n",
       "      <td>143.412803</td>\n",
       "      <td>145.325577</td>\n",
       "      <td>120.885683</td>\n",
       "      <td>136.667932</td>\n",
       "      <td>138.401176</td>\n",
       "      <td>148.959080</td>\n",
       "      <td>146.429925</td>\n",
       "      <td>144.894609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>596.929485</td>\n",
       "      <td>558.648326</td>\n",
       "      <td>551.420039</td>\n",
       "      <td>577.112455</td>\n",
       "      <td>555.405898</td>\n",
       "      <td>552.703990</td>\n",
       "      <td>576.479862</td>\n",
       "      <td>555.407912</td>\n",
       "      <td>550.887544</td>\n",
       "      <td>572</td>\n",
       "      <td>590.117165</td>\n",
       "      <td>552.571167</td>\n",
       "      <td>488.810710</td>\n",
       "      <td>555.531957</td>\n",
       "      <td>562.479785</td>\n",
       "      <td>596.463984</td>\n",
       "      <td>584.653899</td>\n",
       "      <td>569.038022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>940.653140</td>\n",
       "      <td>890.501339</td>\n",
       "      <td>883.086316</td>\n",
       "      <td>934.119140</td>\n",
       "      <td>893.043397</td>\n",
       "      <td>882.242292</td>\n",
       "      <td>919.623890</td>\n",
       "      <td>886.321175</td>\n",
       "      <td>881.045421</td>\n",
       "      <td>911</td>\n",
       "      <td>939.673009</td>\n",
       "      <td>781.967285</td>\n",
       "      <td>771.701702</td>\n",
       "      <td>890.469201</td>\n",
       "      <td>944.584508</td>\n",
       "      <td>907.745110</td>\n",
       "      <td>888.842963</td>\n",
       "      <td>882.892654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>842.534291</td>\n",
       "      <td>864.388124</td>\n",
       "      <td>838.336456</td>\n",
       "      <td>924.978028</td>\n",
       "      <td>855.545431</td>\n",
       "      <td>839.722064</td>\n",
       "      <td>905.346443</td>\n",
       "      <td>841.013156</td>\n",
       "      <td>845.194035</td>\n",
       "      <td>823</td>\n",
       "      <td>898.322121</td>\n",
       "      <td>669.418457</td>\n",
       "      <td>723.714830</td>\n",
       "      <td>846.597603</td>\n",
       "      <td>836.947940</td>\n",
       "      <td>830.633157</td>\n",
       "      <td>814.597001</td>\n",
       "      <td>825.377960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>255.545977</td>\n",
       "      <td>255.901804</td>\n",
       "      <td>250.816940</td>\n",
       "      <td>189.916929</td>\n",
       "      <td>261.353502</td>\n",
       "      <td>241.161049</td>\n",
       "      <td>206.318911</td>\n",
       "      <td>244.486973</td>\n",
       "      <td>242.723551</td>\n",
       "      <td>250</td>\n",
       "      <td>253.512355</td>\n",
       "      <td>227.158142</td>\n",
       "      <td>205.880079</td>\n",
       "      <td>241.048951</td>\n",
       "      <td>254.287204</td>\n",
       "      <td>241.115195</td>\n",
       "      <td>234.409908</td>\n",
       "      <td>245.208674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>992.550115</td>\n",
       "      <td>982.748408</td>\n",
       "      <td>971.987512</td>\n",
       "      <td>1006.865069</td>\n",
       "      <td>980.987703</td>\n",
       "      <td>965.731059</td>\n",
       "      <td>1031.550009</td>\n",
       "      <td>956.716929</td>\n",
       "      <td>972.733624</td>\n",
       "      <td>1004</td>\n",
       "      <td>1039.635205</td>\n",
       "      <td>715.639648</td>\n",
       "      <td>832.192362</td>\n",
       "      <td>975.785339</td>\n",
       "      <td>1058.174059</td>\n",
       "      <td>898.057926</td>\n",
       "      <td>900.745370</td>\n",
       "      <td>915.158956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1505.262616</td>\n",
       "      <td>1477.136661</td>\n",
       "      <td>1464.714361</td>\n",
       "      <td>1644.013144</td>\n",
       "      <td>1473.934875</td>\n",
       "      <td>1460.599837</td>\n",
       "      <td>1614.987267</td>\n",
       "      <td>1448.806267</td>\n",
       "      <td>1466.727788</td>\n",
       "      <td>1441</td>\n",
       "      <td>1526.908216</td>\n",
       "      <td>980.234009</td>\n",
       "      <td>1255.885410</td>\n",
       "      <td>1468.776593</td>\n",
       "      <td>2036.549357</td>\n",
       "      <td>1397.507711</td>\n",
       "      <td>1419.978895</td>\n",
       "      <td>1401.414363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>426.819145</td>\n",
       "      <td>446.953738</td>\n",
       "      <td>442.882205</td>\n",
       "      <td>397.987550</td>\n",
       "      <td>455.860021</td>\n",
       "      <td>440.576937</td>\n",
       "      <td>426.220568</td>\n",
       "      <td>434.771205</td>\n",
       "      <td>441.414946</td>\n",
       "      <td>388</td>\n",
       "      <td>463.705207</td>\n",
       "      <td>423.052612</td>\n",
       "      <td>373.542094</td>\n",
       "      <td>441.128842</td>\n",
       "      <td>472.307718</td>\n",
       "      <td>415.256775</td>\n",
       "      <td>417.253845</td>\n",
       "      <td>421.897675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  xgb+prophet    xgb+ridge   xgb+linear  lgb+prophet  \\\n",
       "0      2015-01-01   335.405149   324.342987   326.355622   353.816288   \n",
       "1      2015-01-01   507.762170   500.921801   500.216103   502.099511   \n",
       "2      2015-01-01   137.081288   138.691135   139.863963   161.207516   \n",
       "3      2015-01-01   596.929485   558.648326   551.420039   577.112455   \n",
       "4      2015-01-01   940.653140   890.501339   883.086316   934.119140   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "26293  2018-12-31   842.534291   864.388124   838.336456   924.978028   \n",
       "26294  2018-12-31   255.545977   255.901804   250.816940   189.916929   \n",
       "26295  2018-12-31   992.550115   982.748408   971.987512  1006.865069   \n",
       "26296  2018-12-31  1505.262616  1477.136661  1464.714361  1644.013144   \n",
       "26297  2018-12-31   426.819145   446.953738   442.882205   397.987550   \n",
       "\n",
       "         lgb+ridge   lgb+linear  cat+prophet    cat+ridge   cat+linear  \\\n",
       "0       315.109073   320.781832   342.162795   308.871192   319.407647   \n",
       "1       502.309507   497.514037   505.468074   494.362517   498.780657   \n",
       "2       137.984103   133.714742   149.287358   134.114025   135.374514   \n",
       "3       555.405898   552.703990   576.479862   555.407912   550.887544   \n",
       "4       893.043397   882.242292   919.623890   886.321175   881.045421   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "26293   855.545431   839.722064   905.346443   841.013156   845.194035   \n",
       "26294   261.353502   241.161049   206.318911   244.486973   242.723551   \n",
       "26295   980.987703   965.731059  1031.550009   956.716929   972.733624   \n",
       "26296  1473.934875  1460.599837  1614.987267  1448.806267  1466.727788   \n",
       "26297   455.860021   440.576937   426.220568   434.771205   441.414946   \n",
       "\n",
       "       num_sold      prophet  neuralprophet        ridge       linear  \\\n",
       "0           329   346.560416     329.134521   283.272258   324.188332   \n",
       "1           520   536.203586     458.008301   439.771734   503.081569   \n",
       "2           146   143.412803     145.325577   120.885683   136.667932   \n",
       "3           572   590.117165     552.571167   488.810710   555.531957   \n",
       "4           911   939.673009     781.967285   771.701702   890.469201   \n",
       "...         ...          ...            ...          ...          ...   \n",
       "26293       823   898.322121     669.418457   723.714830   846.597603   \n",
       "26294       250   253.512355     227.158142   205.880079   241.048951   \n",
       "26295      1004  1039.635205     715.639648   832.192362   975.785339   \n",
       "26296      1441  1526.908216     980.234009  1255.885410  1468.776593   \n",
       "26297       388   463.705207     423.052612   373.542094   441.128842   \n",
       "\n",
       "             earth      xgboost     lightgbm     catboost  \n",
       "0       360.672190   343.250478   331.496226   326.054356  \n",
       "1       584.851806   510.616623   514.576867   509.005936  \n",
       "2       138.401176   148.959080   146.429925   144.894609  \n",
       "3       562.479785   596.463984   584.653899   569.038022  \n",
       "4       944.584508   907.745110   888.842963   882.892654  \n",
       "...            ...          ...          ...          ...  \n",
       "26293   836.947940   830.633157   814.597001   825.377960  \n",
       "26294   254.287204   241.115195   234.409908   245.208674  \n",
       "26295  1058.174059   898.057926   900.745370   915.158956  \n",
       "26296  2036.549357  1397.507711  1419.978895  1401.414363  \n",
       "26297   472.307718   415.256775   417.253845   421.897675  \n",
       "\n",
       "[26298 rows x 19 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_oof_preds_selective_expanded = adjusted_oof_preds_selective.join(tv_preds_only)\n",
    "adjusted_oof_preds_selective_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "48458840-a707-43a0-b244-af39df214fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>xgb+prophet</th>\n",
       "      <th>xgb+neuralprophet</th>\n",
       "      <th>xgb+ridge</th>\n",
       "      <th>xgb+linear</th>\n",
       "      <th>xgb+earth</th>\n",
       "      <th>lgb+prophet</th>\n",
       "      <th>lgb+neuralprophet</th>\n",
       "      <th>lgb+ridge</th>\n",
       "      <th>lgb+linear</th>\n",
       "      <th>...</th>\n",
       "      <th>cat+earth</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>335.405149</td>\n",
       "      <td>325.701162</td>\n",
       "      <td>324.342987</td>\n",
       "      <td>326.355622</td>\n",
       "      <td>399.253649</td>\n",
       "      <td>353.816288</td>\n",
       "      <td>303.237210</td>\n",
       "      <td>315.109073</td>\n",
       "      <td>320.781832</td>\n",
       "      <td>...</td>\n",
       "      <td>378.548520</td>\n",
       "      <td>329</td>\n",
       "      <td>346.560416</td>\n",
       "      <td>329.134521</td>\n",
       "      <td>283.272258</td>\n",
       "      <td>324.188332</td>\n",
       "      <td>360.672190</td>\n",
       "      <td>343.250478</td>\n",
       "      <td>331.496226</td>\n",
       "      <td>326.054356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>507.762170</td>\n",
       "      <td>463.390537</td>\n",
       "      <td>500.921801</td>\n",
       "      <td>500.216103</td>\n",
       "      <td>598.560040</td>\n",
       "      <td>502.099511</td>\n",
       "      <td>423.579452</td>\n",
       "      <td>502.309507</td>\n",
       "      <td>497.514037</td>\n",
       "      <td>...</td>\n",
       "      <td>592.709651</td>\n",
       "      <td>520</td>\n",
       "      <td>536.203586</td>\n",
       "      <td>458.008301</td>\n",
       "      <td>439.771734</td>\n",
       "      <td>503.081569</td>\n",
       "      <td>584.851806</td>\n",
       "      <td>510.616623</td>\n",
       "      <td>514.576867</td>\n",
       "      <td>509.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>137.081288</td>\n",
       "      <td>147.604655</td>\n",
       "      <td>138.691135</td>\n",
       "      <td>139.863963</td>\n",
       "      <td>150.623251</td>\n",
       "      <td>161.207516</td>\n",
       "      <td>152.517721</td>\n",
       "      <td>137.984103</td>\n",
       "      <td>133.714742</td>\n",
       "      <td>...</td>\n",
       "      <td>122.792138</td>\n",
       "      <td>146</td>\n",
       "      <td>143.412803</td>\n",
       "      <td>145.325577</td>\n",
       "      <td>120.885683</td>\n",
       "      <td>136.667932</td>\n",
       "      <td>138.401176</td>\n",
       "      <td>148.959080</td>\n",
       "      <td>146.429925</td>\n",
       "      <td>144.894609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>596.929485</td>\n",
       "      <td>552.264141</td>\n",
       "      <td>558.648326</td>\n",
       "      <td>551.420039</td>\n",
       "      <td>623.935119</td>\n",
       "      <td>577.112455</td>\n",
       "      <td>547.578908</td>\n",
       "      <td>555.405898</td>\n",
       "      <td>552.703990</td>\n",
       "      <td>...</td>\n",
       "      <td>610.328935</td>\n",
       "      <td>572</td>\n",
       "      <td>590.117165</td>\n",
       "      <td>552.571167</td>\n",
       "      <td>488.810710</td>\n",
       "      <td>555.531957</td>\n",
       "      <td>562.479785</td>\n",
       "      <td>596.463984</td>\n",
       "      <td>584.653899</td>\n",
       "      <td>569.038022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>940.653140</td>\n",
       "      <td>863.659302</td>\n",
       "      <td>890.501339</td>\n",
       "      <td>883.086316</td>\n",
       "      <td>961.749819</td>\n",
       "      <td>934.119140</td>\n",
       "      <td>918.265905</td>\n",
       "      <td>893.043397</td>\n",
       "      <td>882.242292</td>\n",
       "      <td>...</td>\n",
       "      <td>962.248311</td>\n",
       "      <td>911</td>\n",
       "      <td>939.673009</td>\n",
       "      <td>781.967285</td>\n",
       "      <td>771.701702</td>\n",
       "      <td>890.469201</td>\n",
       "      <td>944.584508</td>\n",
       "      <td>907.745110</td>\n",
       "      <td>888.842963</td>\n",
       "      <td>882.892654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>842.534291</td>\n",
       "      <td>929.313721</td>\n",
       "      <td>864.388124</td>\n",
       "      <td>838.336456</td>\n",
       "      <td>820.409379</td>\n",
       "      <td>924.978028</td>\n",
       "      <td>968.214591</td>\n",
       "      <td>855.545431</td>\n",
       "      <td>839.722064</td>\n",
       "      <td>...</td>\n",
       "      <td>834.502360</td>\n",
       "      <td>823</td>\n",
       "      <td>898.322121</td>\n",
       "      <td>669.418457</td>\n",
       "      <td>723.714830</td>\n",
       "      <td>846.597603</td>\n",
       "      <td>836.947940</td>\n",
       "      <td>830.633157</td>\n",
       "      <td>814.597001</td>\n",
       "      <td>825.377960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>255.545977</td>\n",
       "      <td>251.271349</td>\n",
       "      <td>255.901804</td>\n",
       "      <td>250.816940</td>\n",
       "      <td>255.531034</td>\n",
       "      <td>189.916929</td>\n",
       "      <td>244.527543</td>\n",
       "      <td>261.353502</td>\n",
       "      <td>241.161049</td>\n",
       "      <td>...</td>\n",
       "      <td>256.669182</td>\n",
       "      <td>250</td>\n",
       "      <td>253.512355</td>\n",
       "      <td>227.158142</td>\n",
       "      <td>205.880079</td>\n",
       "      <td>241.048951</td>\n",
       "      <td>254.287204</td>\n",
       "      <td>241.115195</td>\n",
       "      <td>234.409908</td>\n",
       "      <td>245.208674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>992.550115</td>\n",
       "      <td>1065.049591</td>\n",
       "      <td>982.748408</td>\n",
       "      <td>971.987512</td>\n",
       "      <td>1037.775345</td>\n",
       "      <td>1006.865069</td>\n",
       "      <td>1031.111582</td>\n",
       "      <td>980.987703</td>\n",
       "      <td>965.731059</td>\n",
       "      <td>...</td>\n",
       "      <td>1014.550436</td>\n",
       "      <td>1004</td>\n",
       "      <td>1039.635205</td>\n",
       "      <td>715.639648</td>\n",
       "      <td>832.192362</td>\n",
       "      <td>975.785339</td>\n",
       "      <td>1058.174059</td>\n",
       "      <td>898.057926</td>\n",
       "      <td>900.745370</td>\n",
       "      <td>915.158956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1505.262616</td>\n",
       "      <td>1573.793579</td>\n",
       "      <td>1477.136661</td>\n",
       "      <td>1464.714361</td>\n",
       "      <td>2020.992955</td>\n",
       "      <td>1644.013144</td>\n",
       "      <td>1589.240447</td>\n",
       "      <td>1473.934875</td>\n",
       "      <td>1460.599837</td>\n",
       "      <td>...</td>\n",
       "      <td>2042.430342</td>\n",
       "      <td>1441</td>\n",
       "      <td>1526.908216</td>\n",
       "      <td>980.234009</td>\n",
       "      <td>1255.885410</td>\n",
       "      <td>1468.776593</td>\n",
       "      <td>2036.549357</td>\n",
       "      <td>1397.507711</td>\n",
       "      <td>1419.978895</td>\n",
       "      <td>1401.414363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>426.819145</td>\n",
       "      <td>528.544678</td>\n",
       "      <td>446.953738</td>\n",
       "      <td>442.882205</td>\n",
       "      <td>485.381788</td>\n",
       "      <td>397.987550</td>\n",
       "      <td>533.676805</td>\n",
       "      <td>455.860021</td>\n",
       "      <td>440.576937</td>\n",
       "      <td>...</td>\n",
       "      <td>466.961430</td>\n",
       "      <td>388</td>\n",
       "      <td>463.705207</td>\n",
       "      <td>423.052612</td>\n",
       "      <td>373.542094</td>\n",
       "      <td>441.128842</td>\n",
       "      <td>472.307718</td>\n",
       "      <td>415.256775</td>\n",
       "      <td>417.253845</td>\n",
       "      <td>421.897675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  xgb+prophet  xgb+neuralprophet    xgb+ridge   xgb+linear  \\\n",
       "0      2015-01-01   335.405149         325.701162   324.342987   326.355622   \n",
       "1      2015-01-01   507.762170         463.390537   500.921801   500.216103   \n",
       "2      2015-01-01   137.081288         147.604655   138.691135   139.863963   \n",
       "3      2015-01-01   596.929485         552.264141   558.648326   551.420039   \n",
       "4      2015-01-01   940.653140         863.659302   890.501339   883.086316   \n",
       "...           ...          ...                ...          ...          ...   \n",
       "26293  2018-12-31   842.534291         929.313721   864.388124   838.336456   \n",
       "26294  2018-12-31   255.545977         251.271349   255.901804   250.816940   \n",
       "26295  2018-12-31   992.550115        1065.049591   982.748408   971.987512   \n",
       "26296  2018-12-31  1505.262616        1573.793579  1477.136661  1464.714361   \n",
       "26297  2018-12-31   426.819145         528.544678   446.953738   442.882205   \n",
       "\n",
       "         xgb+earth  lgb+prophet  lgb+neuralprophet    lgb+ridge   lgb+linear  \\\n",
       "0       399.253649   353.816288         303.237210   315.109073   320.781832   \n",
       "1       598.560040   502.099511         423.579452   502.309507   497.514037   \n",
       "2       150.623251   161.207516         152.517721   137.984103   133.714742   \n",
       "3       623.935119   577.112455         547.578908   555.405898   552.703990   \n",
       "4       961.749819   934.119140         918.265905   893.043397   882.242292   \n",
       "...            ...          ...                ...          ...          ...   \n",
       "26293   820.409379   924.978028         968.214591   855.545431   839.722064   \n",
       "26294   255.531034   189.916929         244.527543   261.353502   241.161049   \n",
       "26295  1037.775345  1006.865069        1031.111582   980.987703   965.731059   \n",
       "26296  2020.992955  1644.013144        1589.240447  1473.934875  1460.599837   \n",
       "26297   485.381788   397.987550         533.676805   455.860021   440.576937   \n",
       "\n",
       "       ...    cat+earth  num_sold      prophet  neuralprophet        ridge  \\\n",
       "0      ...   378.548520       329   346.560416     329.134521   283.272258   \n",
       "1      ...   592.709651       520   536.203586     458.008301   439.771734   \n",
       "2      ...   122.792138       146   143.412803     145.325577   120.885683   \n",
       "3      ...   610.328935       572   590.117165     552.571167   488.810710   \n",
       "4      ...   962.248311       911   939.673009     781.967285   771.701702   \n",
       "...    ...          ...       ...          ...            ...          ...   \n",
       "26293  ...   834.502360       823   898.322121     669.418457   723.714830   \n",
       "26294  ...   256.669182       250   253.512355     227.158142   205.880079   \n",
       "26295  ...  1014.550436      1004  1039.635205     715.639648   832.192362   \n",
       "26296  ...  2042.430342      1441  1526.908216     980.234009  1255.885410   \n",
       "26297  ...   466.961430       388   463.705207     423.052612   373.542094   \n",
       "\n",
       "            linear        earth      xgboost     lightgbm     catboost  \n",
       "0       324.188332   360.672190   343.250478   331.496226   326.054356  \n",
       "1       503.081569   584.851806   510.616623   514.576867   509.005936  \n",
       "2       136.667932   138.401176   148.959080   146.429925   144.894609  \n",
       "3       555.531957   562.479785   596.463984   584.653899   569.038022  \n",
       "4       890.469201   944.584508   907.745110   888.842963   882.892654  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "26293   846.597603   836.947940   830.633157   814.597001   825.377960  \n",
       "26294   241.048951   254.287204   241.115195   234.409908   245.208674  \n",
       "26295   975.785339  1058.174059   898.057926   900.745370   915.158956  \n",
       "26296  1468.776593  2036.549357  1397.507711  1419.978895  1401.414363  \n",
       "26297   441.128842   472.307718   415.256775   417.253845   421.897675  \n",
       "\n",
       "[26298 rows x 25 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_oof_preds_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3d66508f-d427-4e13-8c6a-f1e80545b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_preds_df = pd.DataFrame(xgb_test_fold_preds)\n",
    "lgb_test_preds_df = pd.DataFrame(lgb_test_fold_preds)\n",
    "cat_test_preds_df = pd.DataFrame(cat_test_fold_preds)\n",
    "\n",
    "test_dfs_dict = {\n",
    "    'xgb': xgb_test_preds_df,\n",
    "    'lgb': lgb_test_preds_df,\n",
    "    'cat': cat_test_preds_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b6104382-e280-4734-904c-ac833f76e3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prophet_0_residual_preds</th>\n",
       "      <th>prophet_1_residual_preds</th>\n",
       "      <th>prophet_2_residual_preds</th>\n",
       "      <th>prophet_3_residual_preds</th>\n",
       "      <th>neuralprophet_0_residual_preds</th>\n",
       "      <th>neuralprophet_1_residual_preds</th>\n",
       "      <th>neuralprophet_2_residual_preds</th>\n",
       "      <th>neuralprophet_3_residual_preds</th>\n",
       "      <th>ridge_0_residual_preds</th>\n",
       "      <th>ridge_1_residual_preds</th>\n",
       "      <th>ridge_2_residual_preds</th>\n",
       "      <th>ridge_3_residual_preds</th>\n",
       "      <th>linear_0_residual_preds</th>\n",
       "      <th>linear_1_residual_preds</th>\n",
       "      <th>linear_2_residual_preds</th>\n",
       "      <th>linear_3_residual_preds</th>\n",
       "      <th>earth_0_residual_preds</th>\n",
       "      <th>earth_1_residual_preds</th>\n",
       "      <th>earth_2_residual_preds</th>\n",
       "      <th>earth_3_residual_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.033663</td>\n",
       "      <td>-21.819101</td>\n",
       "      <td>-20.461470</td>\n",
       "      <td>4.619490</td>\n",
       "      <td>-21.076921</td>\n",
       "      <td>3.158182</td>\n",
       "      <td>0.283048</td>\n",
       "      <td>11.436007</td>\n",
       "      <td>41.045731</td>\n",
       "      <td>43.582924</td>\n",
       "      <td>39.203148</td>\n",
       "      <td>52.930038</td>\n",
       "      <td>11.043393</td>\n",
       "      <td>-10.374799</td>\n",
       "      <td>-3.198138</td>\n",
       "      <td>11.198472</td>\n",
       "      <td>6.389496</td>\n",
       "      <td>12.277696</td>\n",
       "      <td>9.860520</td>\n",
       "      <td>44.748325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.953610</td>\n",
       "      <td>-32.106422</td>\n",
       "      <td>-29.866251</td>\n",
       "      <td>-13.119456</td>\n",
       "      <td>23.624197</td>\n",
       "      <td>17.082817</td>\n",
       "      <td>12.703603</td>\n",
       "      <td>25.939672</td>\n",
       "      <td>79.355980</td>\n",
       "      <td>60.213955</td>\n",
       "      <td>57.458580</td>\n",
       "      <td>68.211830</td>\n",
       "      <td>13.730322</td>\n",
       "      <td>-5.473454</td>\n",
       "      <td>-6.398356</td>\n",
       "      <td>-0.480903</td>\n",
       "      <td>15.075122</td>\n",
       "      <td>-1.671091</td>\n",
       "      <td>-11.525816</td>\n",
       "      <td>26.900377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.117981</td>\n",
       "      <td>-17.535273</td>\n",
       "      <td>-14.463274</td>\n",
       "      <td>-1.783982</td>\n",
       "      <td>-6.031038</td>\n",
       "      <td>9.711327</td>\n",
       "      <td>-7.441924</td>\n",
       "      <td>5.877184</td>\n",
       "      <td>7.184831</td>\n",
       "      <td>9.987694</td>\n",
       "      <td>8.546426</td>\n",
       "      <td>18.454214</td>\n",
       "      <td>1.068279</td>\n",
       "      <td>-9.214305</td>\n",
       "      <td>-4.012601</td>\n",
       "      <td>6.126245</td>\n",
       "      <td>-0.055850</td>\n",
       "      <td>3.943094</td>\n",
       "      <td>5.865566</td>\n",
       "      <td>11.813078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.838993</td>\n",
       "      <td>-29.329393</td>\n",
       "      <td>-4.876259</td>\n",
       "      <td>19.027905</td>\n",
       "      <td>-15.885752</td>\n",
       "      <td>-12.469327</td>\n",
       "      <td>3.310822</td>\n",
       "      <td>14.986485</td>\n",
       "      <td>75.993629</td>\n",
       "      <td>66.369682</td>\n",
       "      <td>75.367096</td>\n",
       "      <td>75.453247</td>\n",
       "      <td>4.767874</td>\n",
       "      <td>-20.277174</td>\n",
       "      <td>13.027990</td>\n",
       "      <td>1.270042</td>\n",
       "      <td>58.526283</td>\n",
       "      <td>44.996883</td>\n",
       "      <td>42.497440</td>\n",
       "      <td>72.461845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-26.509970</td>\n",
       "      <td>-16.158661</td>\n",
       "      <td>-10.763204</td>\n",
       "      <td>10.099065</td>\n",
       "      <td>65.650642</td>\n",
       "      <td>121.963394</td>\n",
       "      <td>108.657166</td>\n",
       "      <td>100.493484</td>\n",
       "      <td>117.317764</td>\n",
       "      <td>127.078529</td>\n",
       "      <td>131.368958</td>\n",
       "      <td>118.615768</td>\n",
       "      <td>-13.505404</td>\n",
       "      <td>-6.629692</td>\n",
       "      <td>11.166040</td>\n",
       "      <td>-11.286895</td>\n",
       "      <td>8.115604</td>\n",
       "      <td>12.812317</td>\n",
       "      <td>7.845061</td>\n",
       "      <td>29.399580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>-99.123657</td>\n",
       "      <td>-71.305107</td>\n",
       "      <td>-59.250942</td>\n",
       "      <td>-10.330985</td>\n",
       "      <td>138.355927</td>\n",
       "      <td>196.987167</td>\n",
       "      <td>150.624374</td>\n",
       "      <td>262.165802</td>\n",
       "      <td>101.768959</td>\n",
       "      <td>119.761124</td>\n",
       "      <td>108.255318</td>\n",
       "      <td>130.089218</td>\n",
       "      <td>-19.740946</td>\n",
       "      <td>-3.063937</td>\n",
       "      <td>-4.934291</td>\n",
       "      <td>15.305479</td>\n",
       "      <td>6.367306</td>\n",
       "      <td>-12.736743</td>\n",
       "      <td>-19.809973</td>\n",
       "      <td>-51.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>-16.493229</td>\n",
       "      <td>-16.453074</td>\n",
       "      <td>-14.665970</td>\n",
       "      <td>13.453952</td>\n",
       "      <td>-38.068619</td>\n",
       "      <td>8.731948</td>\n",
       "      <td>4.070496</td>\n",
       "      <td>54.360695</td>\n",
       "      <td>42.092594</td>\n",
       "      <td>35.633297</td>\n",
       "      <td>36.126293</td>\n",
       "      <td>46.406498</td>\n",
       "      <td>15.607967</td>\n",
       "      <td>12.405040</td>\n",
       "      <td>10.690983</td>\n",
       "      <td>13.768815</td>\n",
       "      <td>7.738034</td>\n",
       "      <td>5.247971</td>\n",
       "      <td>19.875986</td>\n",
       "      <td>-1.179993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>-87.356529</td>\n",
       "      <td>-57.674362</td>\n",
       "      <td>-53.783466</td>\n",
       "      <td>20.081490</td>\n",
       "      <td>210.766830</td>\n",
       "      <td>250.296646</td>\n",
       "      <td>213.821228</td>\n",
       "      <td>337.864075</td>\n",
       "      <td>139.503235</td>\n",
       "      <td>132.930740</td>\n",
       "      <td>140.493744</td>\n",
       "      <td>161.274094</td>\n",
       "      <td>6.022972</td>\n",
       "      <td>-0.797151</td>\n",
       "      <td>1.846847</td>\n",
       "      <td>38.112049</td>\n",
       "      <td>7.979291</td>\n",
       "      <td>3.600235</td>\n",
       "      <td>-11.092898</td>\n",
       "      <td>-10.844264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>-75.296303</td>\n",
       "      <td>-29.718555</td>\n",
       "      <td>-70.626648</td>\n",
       "      <td>28.712942</td>\n",
       "      <td>445.830048</td>\n",
       "      <td>494.277771</td>\n",
       "      <td>396.313354</td>\n",
       "      <td>517.456787</td>\n",
       "      <td>199.091965</td>\n",
       "      <td>202.822571</td>\n",
       "      <td>184.382248</td>\n",
       "      <td>201.019592</td>\n",
       "      <td>-11.430958</td>\n",
       "      <td>0.492131</td>\n",
       "      <td>-22.447880</td>\n",
       "      <td>-5.213744</td>\n",
       "      <td>-32.852684</td>\n",
       "      <td>-4.504761</td>\n",
       "      <td>-79.522964</td>\n",
       "      <td>-133.354050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>-69.658989</td>\n",
       "      <td>-46.492012</td>\n",
       "      <td>-72.104233</td>\n",
       "      <td>-46.330444</td>\n",
       "      <td>3.368485</td>\n",
       "      <td>49.085247</td>\n",
       "      <td>-2.452202</td>\n",
       "      <td>62.458782</td>\n",
       "      <td>51.297211</td>\n",
       "      <td>58.019791</td>\n",
       "      <td>38.285564</td>\n",
       "      <td>38.643036</td>\n",
       "      <td>-6.316245</td>\n",
       "      <td>3.485101</td>\n",
       "      <td>-27.769218</td>\n",
       "      <td>-28.430882</td>\n",
       "      <td>1.084858</td>\n",
       "      <td>21.966936</td>\n",
       "      <td>-1.669499</td>\n",
       "      <td>-19.059151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prophet_0_residual_preds  prophet_1_residual_preds  \\\n",
       "0                   -17.033663                -21.819101   \n",
       "1                    -7.953610                -32.106422   \n",
       "2                    -3.117981                -17.535273   \n",
       "3                   -17.838993                -29.329393   \n",
       "4                   -26.509970                -16.158661   \n",
       "...                        ...                       ...   \n",
       "6565                -99.123657                -71.305107   \n",
       "6566                -16.493229                -16.453074   \n",
       "6567                -87.356529                -57.674362   \n",
       "6568                -75.296303                -29.718555   \n",
       "6569                -69.658989                -46.492012   \n",
       "\n",
       "      prophet_2_residual_preds  prophet_3_residual_preds  \\\n",
       "0                   -20.461470                  4.619490   \n",
       "1                   -29.866251                -13.119456   \n",
       "2                   -14.463274                 -1.783982   \n",
       "3                    -4.876259                 19.027905   \n",
       "4                   -10.763204                 10.099065   \n",
       "...                        ...                       ...   \n",
       "6565                -59.250942                -10.330985   \n",
       "6566                -14.665970                 13.453952   \n",
       "6567                -53.783466                 20.081490   \n",
       "6568                -70.626648                 28.712942   \n",
       "6569                -72.104233                -46.330444   \n",
       "\n",
       "      neuralprophet_0_residual_preds  neuralprophet_1_residual_preds  \\\n",
       "0                         -21.076921                        3.158182   \n",
       "1                          23.624197                       17.082817   \n",
       "2                          -6.031038                        9.711327   \n",
       "3                         -15.885752                      -12.469327   \n",
       "4                          65.650642                      121.963394   \n",
       "...                              ...                             ...   \n",
       "6565                      138.355927                      196.987167   \n",
       "6566                      -38.068619                        8.731948   \n",
       "6567                      210.766830                      250.296646   \n",
       "6568                      445.830048                      494.277771   \n",
       "6569                        3.368485                       49.085247   \n",
       "\n",
       "      neuralprophet_2_residual_preds  neuralprophet_3_residual_preds  \\\n",
       "0                           0.283048                       11.436007   \n",
       "1                          12.703603                       25.939672   \n",
       "2                          -7.441924                        5.877184   \n",
       "3                           3.310822                       14.986485   \n",
       "4                         108.657166                      100.493484   \n",
       "...                              ...                             ...   \n",
       "6565                      150.624374                      262.165802   \n",
       "6566                        4.070496                       54.360695   \n",
       "6567                      213.821228                      337.864075   \n",
       "6568                      396.313354                      517.456787   \n",
       "6569                       -2.452202                       62.458782   \n",
       "\n",
       "      ridge_0_residual_preds  ridge_1_residual_preds  ridge_2_residual_preds  \\\n",
       "0                  41.045731               43.582924               39.203148   \n",
       "1                  79.355980               60.213955               57.458580   \n",
       "2                   7.184831                9.987694                8.546426   \n",
       "3                  75.993629               66.369682               75.367096   \n",
       "4                 117.317764              127.078529              131.368958   \n",
       "...                      ...                     ...                     ...   \n",
       "6565              101.768959              119.761124              108.255318   \n",
       "6566               42.092594               35.633297               36.126293   \n",
       "6567              139.503235              132.930740              140.493744   \n",
       "6568              199.091965              202.822571              184.382248   \n",
       "6569               51.297211               58.019791               38.285564   \n",
       "\n",
       "      ridge_3_residual_preds  linear_0_residual_preds  \\\n",
       "0                  52.930038                11.043393   \n",
       "1                  68.211830                13.730322   \n",
       "2                  18.454214                 1.068279   \n",
       "3                  75.453247                 4.767874   \n",
       "4                 118.615768               -13.505404   \n",
       "...                      ...                      ...   \n",
       "6565              130.089218               -19.740946   \n",
       "6566               46.406498                15.607967   \n",
       "6567              161.274094                 6.022972   \n",
       "6568              201.019592               -11.430958   \n",
       "6569               38.643036                -6.316245   \n",
       "\n",
       "      linear_1_residual_preds  linear_2_residual_preds  \\\n",
       "0                  -10.374799                -3.198138   \n",
       "1                   -5.473454                -6.398356   \n",
       "2                   -9.214305                -4.012601   \n",
       "3                  -20.277174                13.027990   \n",
       "4                   -6.629692                11.166040   \n",
       "...                       ...                      ...   \n",
       "6565                -3.063937                -4.934291   \n",
       "6566                12.405040                10.690983   \n",
       "6567                -0.797151                 1.846847   \n",
       "6568                 0.492131               -22.447880   \n",
       "6569                 3.485101               -27.769218   \n",
       "\n",
       "      linear_3_residual_preds  earth_0_residual_preds  earth_1_residual_preds  \\\n",
       "0                   11.198472                6.389496               12.277696   \n",
       "1                   -0.480903               15.075122               -1.671091   \n",
       "2                    6.126245               -0.055850                3.943094   \n",
       "3                    1.270042               58.526283               44.996883   \n",
       "4                  -11.286895                8.115604               12.812317   \n",
       "...                       ...                     ...                     ...   \n",
       "6565                15.305479                6.367306              -12.736743   \n",
       "6566                13.768815                7.738034                5.247971   \n",
       "6567                38.112049                7.979291                3.600235   \n",
       "6568                -5.213744              -32.852684               -4.504761   \n",
       "6569               -28.430882                1.084858               21.966936   \n",
       "\n",
       "      earth_2_residual_preds  earth_3_residual_preds  \n",
       "0                   9.860520               44.748325  \n",
       "1                 -11.525816               26.900377  \n",
       "2                   5.865566               11.813078  \n",
       "3                  42.497440               72.461845  \n",
       "4                   7.845061               29.399580  \n",
       "...                      ...                     ...  \n",
       "6565              -19.809973              -51.501400  \n",
       "6566               19.875986               -1.179993  \n",
       "6567              -11.092898              -10.844264  \n",
       "6568              -79.522964             -133.354050  \n",
       "6569               -1.669499              -19.059151  \n",
       "\n",
       "[6570 rows x 20 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_test_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "56ad36be-b9c7-453a-9f52-fe051d524539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prophet_0_residual_preds</th>\n",
       "      <th>prophet_1_residual_preds</th>\n",
       "      <th>prophet_2_residual_preds</th>\n",
       "      <th>prophet_3_residual_preds</th>\n",
       "      <th>neuralprophet_0_residual_preds</th>\n",
       "      <th>neuralprophet_1_residual_preds</th>\n",
       "      <th>neuralprophet_2_residual_preds</th>\n",
       "      <th>neuralprophet_3_residual_preds</th>\n",
       "      <th>ridge_0_residual_preds</th>\n",
       "      <th>ridge_1_residual_preds</th>\n",
       "      <th>ridge_2_residual_preds</th>\n",
       "      <th>ridge_3_residual_preds</th>\n",
       "      <th>linear_0_residual_preds</th>\n",
       "      <th>linear_1_residual_preds</th>\n",
       "      <th>linear_2_residual_preds</th>\n",
       "      <th>linear_3_residual_preds</th>\n",
       "      <th>earth_0_residual_preds</th>\n",
       "      <th>earth_1_residual_preds</th>\n",
       "      <th>earth_2_residual_preds</th>\n",
       "      <th>earth_3_residual_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.417380</td>\n",
       "      <td>-61.508158</td>\n",
       "      <td>23.100475</td>\n",
       "      <td>34.776261</td>\n",
       "      <td>14.015606</td>\n",
       "      <td>-67.475469</td>\n",
       "      <td>24.595720</td>\n",
       "      <td>17.191331</td>\n",
       "      <td>42.890087</td>\n",
       "      <td>29.066765</td>\n",
       "      <td>51.724626</td>\n",
       "      <td>50.557733</td>\n",
       "      <td>-25.086643</td>\n",
       "      <td>-6.080084</td>\n",
       "      <td>-21.387555</td>\n",
       "      <td>-18.124663</td>\n",
       "      <td>1.457896</td>\n",
       "      <td>12.600928</td>\n",
       "      <td>5.106946</td>\n",
       "      <td>27.319493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.405302</td>\n",
       "      <td>-49.418148</td>\n",
       "      <td>-10.470536</td>\n",
       "      <td>-8.375791</td>\n",
       "      <td>68.692888</td>\n",
       "      <td>-1.427907</td>\n",
       "      <td>73.688546</td>\n",
       "      <td>27.369757</td>\n",
       "      <td>79.817463</td>\n",
       "      <td>63.870581</td>\n",
       "      <td>76.631124</td>\n",
       "      <td>75.445836</td>\n",
       "      <td>-13.752755</td>\n",
       "      <td>1.513884</td>\n",
       "      <td>-18.366406</td>\n",
       "      <td>-21.613810</td>\n",
       "      <td>-1.088331</td>\n",
       "      <td>2.642132</td>\n",
       "      <td>-12.250419</td>\n",
       "      <td>4.065161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.998711</td>\n",
       "      <td>-25.421969</td>\n",
       "      <td>30.043784</td>\n",
       "      <td>39.225742</td>\n",
       "      <td>14.824780</td>\n",
       "      <td>-30.362008</td>\n",
       "      <td>23.805578</td>\n",
       "      <td>24.665745</td>\n",
       "      <td>26.499811</td>\n",
       "      <td>11.961173</td>\n",
       "      <td>23.278174</td>\n",
       "      <td>24.414215</td>\n",
       "      <td>-10.134384</td>\n",
       "      <td>-3.428975</td>\n",
       "      <td>-14.746318</td>\n",
       "      <td>-10.119366</td>\n",
       "      <td>2.341616</td>\n",
       "      <td>14.972660</td>\n",
       "      <td>-1.494605</td>\n",
       "      <td>11.773964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.633334</td>\n",
       "      <td>-62.285334</td>\n",
       "      <td>18.500085</td>\n",
       "      <td>11.427457</td>\n",
       "      <td>44.873675</td>\n",
       "      <td>-14.800776</td>\n",
       "      <td>69.548091</td>\n",
       "      <td>47.822503</td>\n",
       "      <td>78.918727</td>\n",
       "      <td>67.520122</td>\n",
       "      <td>101.831534</td>\n",
       "      <td>85.488041</td>\n",
       "      <td>-37.633571</td>\n",
       "      <td>-6.703128</td>\n",
       "      <td>-17.667744</td>\n",
       "      <td>-29.790984</td>\n",
       "      <td>34.890573</td>\n",
       "      <td>15.067120</td>\n",
       "      <td>40.353713</td>\n",
       "      <td>54.879569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.107800</td>\n",
       "      <td>-11.843453</td>\n",
       "      <td>15.149475</td>\n",
       "      <td>17.086193</td>\n",
       "      <td>200.071190</td>\n",
       "      <td>175.228765</td>\n",
       "      <td>229.800475</td>\n",
       "      <td>207.703967</td>\n",
       "      <td>130.247401</td>\n",
       "      <td>131.723874</td>\n",
       "      <td>152.688608</td>\n",
       "      <td>133.778653</td>\n",
       "      <td>-39.749207</td>\n",
       "      <td>-2.612528</td>\n",
       "      <td>-18.533138</td>\n",
       "      <td>-36.136083</td>\n",
       "      <td>0.623418</td>\n",
       "      <td>0.111770</td>\n",
       "      <td>4.338212</td>\n",
       "      <td>14.705306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>-34.769136</td>\n",
       "      <td>26.655907</td>\n",
       "      <td>-30.118321</td>\n",
       "      <td>31.958759</td>\n",
       "      <td>224.130400</td>\n",
       "      <td>298.796134</td>\n",
       "      <td>157.606998</td>\n",
       "      <td>286.654485</td>\n",
       "      <td>121.092300</td>\n",
       "      <td>131.830601</td>\n",
       "      <td>111.756038</td>\n",
       "      <td>139.203643</td>\n",
       "      <td>-0.500225</td>\n",
       "      <td>-6.875539</td>\n",
       "      <td>-6.603869</td>\n",
       "      <td>5.567733</td>\n",
       "      <td>4.104113</td>\n",
       "      <td>-3.600418</td>\n",
       "      <td>-17.342312</td>\n",
       "      <td>-21.125879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>-35.256477</td>\n",
       "      <td>-63.595426</td>\n",
       "      <td>-56.630480</td>\n",
       "      <td>-13.999866</td>\n",
       "      <td>-4.979816</td>\n",
       "      <td>17.369401</td>\n",
       "      <td>-28.662670</td>\n",
       "      <td>20.092102</td>\n",
       "      <td>50.968948</td>\n",
       "      <td>55.473423</td>\n",
       "      <td>46.283016</td>\n",
       "      <td>51.097698</td>\n",
       "      <td>7.056347</td>\n",
       "      <td>0.112098</td>\n",
       "      <td>6.443867</td>\n",
       "      <td>8.402282</td>\n",
       "      <td>13.542317</td>\n",
       "      <td>13.475763</td>\n",
       "      <td>4.569655</td>\n",
       "      <td>8.916277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>-28.233504</td>\n",
       "      <td>-32.770136</td>\n",
       "      <td>-30.336255</td>\n",
       "      <td>41.386606</td>\n",
       "      <td>298.183710</td>\n",
       "      <td>315.471933</td>\n",
       "      <td>224.615620</td>\n",
       "      <td>357.176083</td>\n",
       "      <td>160.322462</td>\n",
       "      <td>148.795341</td>\n",
       "      <td>144.805298</td>\n",
       "      <td>175.504044</td>\n",
       "      <td>18.861284</td>\n",
       "      <td>-10.054280</td>\n",
       "      <td>12.646866</td>\n",
       "      <td>18.732640</td>\n",
       "      <td>-14.994401</td>\n",
       "      <td>-14.018352</td>\n",
       "      <td>-0.518033</td>\n",
       "      <td>-25.635188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>30.104592</td>\n",
       "      <td>117.104928</td>\n",
       "      <td>46.801813</td>\n",
       "      <td>156.275600</td>\n",
       "      <td>521.559478</td>\n",
       "      <td>609.006438</td>\n",
       "      <td>414.563545</td>\n",
       "      <td>634.022354</td>\n",
       "      <td>197.029132</td>\n",
       "      <td>218.049465</td>\n",
       "      <td>190.814035</td>\n",
       "      <td>225.541757</td>\n",
       "      <td>-8.550275</td>\n",
       "      <td>-8.176756</td>\n",
       "      <td>-11.061050</td>\n",
       "      <td>-6.942820</td>\n",
       "      <td>-56.755817</td>\n",
       "      <td>-12.915289</td>\n",
       "      <td>-54.205188</td>\n",
       "      <td>-87.964328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>-66.304123</td>\n",
       "      <td>-65.717657</td>\n",
       "      <td>-86.227708</td>\n",
       "      <td>-52.871250</td>\n",
       "      <td>50.974641</td>\n",
       "      <td>110.624193</td>\n",
       "      <td>-2.337548</td>\n",
       "      <td>69.678150</td>\n",
       "      <td>63.319989</td>\n",
       "      <td>82.317927</td>\n",
       "      <td>56.934777</td>\n",
       "      <td>59.055496</td>\n",
       "      <td>-3.964434</td>\n",
       "      <td>-0.551906</td>\n",
       "      <td>-7.848817</td>\n",
       "      <td>-11.176501</td>\n",
       "      <td>-18.913369</td>\n",
       "      <td>17.067617</td>\n",
       "      <td>6.937332</td>\n",
       "      <td>-10.428272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prophet_0_residual_preds  prophet_1_residual_preds  \\\n",
       "0                     8.417380                -61.508158   \n",
       "1                    -0.405302                -49.418148   \n",
       "2                    17.998711                -25.421969   \n",
       "3                    -6.633334                -62.285334   \n",
       "4                    16.107800                -11.843453   \n",
       "...                        ...                       ...   \n",
       "6565                -34.769136                 26.655907   \n",
       "6566                -35.256477                -63.595426   \n",
       "6567                -28.233504                -32.770136   \n",
       "6568                 30.104592                117.104928   \n",
       "6569                -66.304123                -65.717657   \n",
       "\n",
       "      prophet_2_residual_preds  prophet_3_residual_preds  \\\n",
       "0                    23.100475                 34.776261   \n",
       "1                   -10.470536                 -8.375791   \n",
       "2                    30.043784                 39.225742   \n",
       "3                    18.500085                 11.427457   \n",
       "4                    15.149475                 17.086193   \n",
       "...                        ...                       ...   \n",
       "6565                -30.118321                 31.958759   \n",
       "6566                -56.630480                -13.999866   \n",
       "6567                -30.336255                 41.386606   \n",
       "6568                 46.801813                156.275600   \n",
       "6569                -86.227708                -52.871250   \n",
       "\n",
       "      neuralprophet_0_residual_preds  neuralprophet_1_residual_preds  \\\n",
       "0                          14.015606                      -67.475469   \n",
       "1                          68.692888                       -1.427907   \n",
       "2                          14.824780                      -30.362008   \n",
       "3                          44.873675                      -14.800776   \n",
       "4                         200.071190                      175.228765   \n",
       "...                              ...                             ...   \n",
       "6565                      224.130400                      298.796134   \n",
       "6566                       -4.979816                       17.369401   \n",
       "6567                      298.183710                      315.471933   \n",
       "6568                      521.559478                      609.006438   \n",
       "6569                       50.974641                      110.624193   \n",
       "\n",
       "      neuralprophet_2_residual_preds  neuralprophet_3_residual_preds  \\\n",
       "0                          24.595720                       17.191331   \n",
       "1                          73.688546                       27.369757   \n",
       "2                          23.805578                       24.665745   \n",
       "3                          69.548091                       47.822503   \n",
       "4                         229.800475                      207.703967   \n",
       "...                              ...                             ...   \n",
       "6565                      157.606998                      286.654485   \n",
       "6566                      -28.662670                       20.092102   \n",
       "6567                      224.615620                      357.176083   \n",
       "6568                      414.563545                      634.022354   \n",
       "6569                       -2.337548                       69.678150   \n",
       "\n",
       "      ridge_0_residual_preds  ridge_1_residual_preds  ridge_2_residual_preds  \\\n",
       "0                  42.890087               29.066765               51.724626   \n",
       "1                  79.817463               63.870581               76.631124   \n",
       "2                  26.499811               11.961173               23.278174   \n",
       "3                  78.918727               67.520122              101.831534   \n",
       "4                 130.247401              131.723874              152.688608   \n",
       "...                      ...                     ...                     ...   \n",
       "6565              121.092300              131.830601              111.756038   \n",
       "6566               50.968948               55.473423               46.283016   \n",
       "6567              160.322462              148.795341              144.805298   \n",
       "6568              197.029132              218.049465              190.814035   \n",
       "6569               63.319989               82.317927               56.934777   \n",
       "\n",
       "      ridge_3_residual_preds  linear_0_residual_preds  \\\n",
       "0                  50.557733               -25.086643   \n",
       "1                  75.445836               -13.752755   \n",
       "2                  24.414215               -10.134384   \n",
       "3                  85.488041               -37.633571   \n",
       "4                 133.778653               -39.749207   \n",
       "...                      ...                      ...   \n",
       "6565              139.203643                -0.500225   \n",
       "6566               51.097698                 7.056347   \n",
       "6567              175.504044                18.861284   \n",
       "6568              225.541757                -8.550275   \n",
       "6569               59.055496                -3.964434   \n",
       "\n",
       "      linear_1_residual_preds  linear_2_residual_preds  \\\n",
       "0                   -6.080084               -21.387555   \n",
       "1                    1.513884               -18.366406   \n",
       "2                   -3.428975               -14.746318   \n",
       "3                   -6.703128               -17.667744   \n",
       "4                   -2.612528               -18.533138   \n",
       "...                       ...                      ...   \n",
       "6565                -6.875539                -6.603869   \n",
       "6566                 0.112098                 6.443867   \n",
       "6567               -10.054280                12.646866   \n",
       "6568                -8.176756               -11.061050   \n",
       "6569                -0.551906                -7.848817   \n",
       "\n",
       "      linear_3_residual_preds  earth_0_residual_preds  earth_1_residual_preds  \\\n",
       "0                  -18.124663                1.457896               12.600928   \n",
       "1                  -21.613810               -1.088331                2.642132   \n",
       "2                  -10.119366                2.341616               14.972660   \n",
       "3                  -29.790984               34.890573               15.067120   \n",
       "4                  -36.136083                0.623418                0.111770   \n",
       "...                       ...                     ...                     ...   \n",
       "6565                 5.567733                4.104113               -3.600418   \n",
       "6566                 8.402282               13.542317               13.475763   \n",
       "6567                18.732640              -14.994401              -14.018352   \n",
       "6568                -6.942820              -56.755817              -12.915289   \n",
       "6569               -11.176501              -18.913369               17.067617   \n",
       "\n",
       "      earth_2_residual_preds  earth_3_residual_preds  \n",
       "0                   5.106946               27.319493  \n",
       "1                 -12.250419                4.065161  \n",
       "2                  -1.494605               11.773964  \n",
       "3                  40.353713               54.879569  \n",
       "4                   4.338212               14.705306  \n",
       "...                      ...                     ...  \n",
       "6565              -17.342312              -21.125879  \n",
       "6566                4.569655                8.916277  \n",
       "6567               -0.518033              -25.635188  \n",
       "6568              -54.205188              -87.964328  \n",
       "6569                6.937332              -10.428272  \n",
       "\n",
       "[6570 rows x 20 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_test_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "85152ca2-aefb-442f-b671-e90c68b8815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_oof_preds['num_sold'] = orig_train_df['num_sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f48ec564-6644-4950-b786-52f0937a49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbms_and_adjusted_oof_preds = adjusted_oof_preds.copy()\n",
    "gbms_and_adjusted_oof_preds['xgb'] = xgb_forecast_oof_preds\n",
    "gbms_and_adjusted_oof_preds['lgb'] = lgb_forecast_oof_preds\n",
    "gbms_and_adjusted_oof_preds['cat'] = cat_forecast_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c64c45ae-13eb-4e07-a663-26a5116423f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>xgb+prophet</th>\n",
       "      <th>xgb+neuralprophet</th>\n",
       "      <th>xgb+ridge</th>\n",
       "      <th>xgb+linear</th>\n",
       "      <th>xgb+earth</th>\n",
       "      <th>lgb+prophet</th>\n",
       "      <th>lgb+neuralprophet</th>\n",
       "      <th>lgb+ridge</th>\n",
       "      <th>lgb+linear</th>\n",
       "      <th>lgb+earth</th>\n",
       "      <th>cat+prophet</th>\n",
       "      <th>cat+neuralprophet</th>\n",
       "      <th>cat+ridge</th>\n",
       "      <th>cat+linear</th>\n",
       "      <th>cat+earth</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>xgb</th>\n",
       "      <th>lgb</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>335.405149</td>\n",
       "      <td>325.701162</td>\n",
       "      <td>324.342987</td>\n",
       "      <td>326.355622</td>\n",
       "      <td>399.253649</td>\n",
       "      <td>353.816288</td>\n",
       "      <td>303.237210</td>\n",
       "      <td>315.109073</td>\n",
       "      <td>320.781832</td>\n",
       "      <td>360.017842</td>\n",
       "      <td>342.162795</td>\n",
       "      <td>297.299018</td>\n",
       "      <td>308.871192</td>\n",
       "      <td>319.407647</td>\n",
       "      <td>378.548520</td>\n",
       "      <td>329</td>\n",
       "      <td>343.250478</td>\n",
       "      <td>331.496226</td>\n",
       "      <td>326.054356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>507.762170</td>\n",
       "      <td>463.390537</td>\n",
       "      <td>500.921801</td>\n",
       "      <td>500.216103</td>\n",
       "      <td>598.560040</td>\n",
       "      <td>502.099511</td>\n",
       "      <td>423.579452</td>\n",
       "      <td>502.309507</td>\n",
       "      <td>497.514037</td>\n",
       "      <td>575.971519</td>\n",
       "      <td>505.468074</td>\n",
       "      <td>453.105968</td>\n",
       "      <td>494.362517</td>\n",
       "      <td>498.780657</td>\n",
       "      <td>592.709651</td>\n",
       "      <td>520</td>\n",
       "      <td>510.616623</td>\n",
       "      <td>514.576867</td>\n",
       "      <td>509.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>137.081288</td>\n",
       "      <td>147.604655</td>\n",
       "      <td>138.691135</td>\n",
       "      <td>139.863963</td>\n",
       "      <td>150.623251</td>\n",
       "      <td>161.207516</td>\n",
       "      <td>152.517721</td>\n",
       "      <td>137.984103</td>\n",
       "      <td>133.714742</td>\n",
       "      <td>126.248183</td>\n",
       "      <td>149.287358</td>\n",
       "      <td>153.054598</td>\n",
       "      <td>134.114025</td>\n",
       "      <td>135.374514</td>\n",
       "      <td>122.792138</td>\n",
       "      <td>146</td>\n",
       "      <td>148.959080</td>\n",
       "      <td>146.429925</td>\n",
       "      <td>144.894609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>596.929485</td>\n",
       "      <td>552.264141</td>\n",
       "      <td>558.648326</td>\n",
       "      <td>551.420039</td>\n",
       "      <td>623.935119</td>\n",
       "      <td>577.112455</td>\n",
       "      <td>547.578908</td>\n",
       "      <td>555.405898</td>\n",
       "      <td>552.703990</td>\n",
       "      <td>582.508689</td>\n",
       "      <td>576.479862</td>\n",
       "      <td>554.053877</td>\n",
       "      <td>555.407912</td>\n",
       "      <td>550.887544</td>\n",
       "      <td>610.328935</td>\n",
       "      <td>572</td>\n",
       "      <td>596.463984</td>\n",
       "      <td>584.653899</td>\n",
       "      <td>569.038022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>940.653140</td>\n",
       "      <td>863.659302</td>\n",
       "      <td>890.501339</td>\n",
       "      <td>883.086316</td>\n",
       "      <td>961.749819</td>\n",
       "      <td>934.119140</td>\n",
       "      <td>918.265905</td>\n",
       "      <td>893.043397</td>\n",
       "      <td>882.242292</td>\n",
       "      <td>937.060549</td>\n",
       "      <td>919.623890</td>\n",
       "      <td>887.265360</td>\n",
       "      <td>886.321175</td>\n",
       "      <td>881.045421</td>\n",
       "      <td>962.248311</td>\n",
       "      <td>911</td>\n",
       "      <td>907.745110</td>\n",
       "      <td>888.842963</td>\n",
       "      <td>882.892654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>842.534291</td>\n",
       "      <td>929.313721</td>\n",
       "      <td>864.388124</td>\n",
       "      <td>838.336456</td>\n",
       "      <td>820.409379</td>\n",
       "      <td>924.978028</td>\n",
       "      <td>968.214591</td>\n",
       "      <td>855.545431</td>\n",
       "      <td>839.722064</td>\n",
       "      <td>833.347522</td>\n",
       "      <td>905.346443</td>\n",
       "      <td>910.465738</td>\n",
       "      <td>841.013156</td>\n",
       "      <td>845.194035</td>\n",
       "      <td>834.502360</td>\n",
       "      <td>823</td>\n",
       "      <td>830.633157</td>\n",
       "      <td>814.597001</td>\n",
       "      <td>825.377960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>255.545977</td>\n",
       "      <td>251.271349</td>\n",
       "      <td>255.901804</td>\n",
       "      <td>250.816940</td>\n",
       "      <td>255.531034</td>\n",
       "      <td>189.916929</td>\n",
       "      <td>244.527543</td>\n",
       "      <td>261.353502</td>\n",
       "      <td>241.161049</td>\n",
       "      <td>267.762967</td>\n",
       "      <td>206.318911</td>\n",
       "      <td>223.127072</td>\n",
       "      <td>244.486973</td>\n",
       "      <td>242.723551</td>\n",
       "      <td>256.669182</td>\n",
       "      <td>250</td>\n",
       "      <td>241.115195</td>\n",
       "      <td>234.409908</td>\n",
       "      <td>245.208674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>992.550115</td>\n",
       "      <td>1065.049591</td>\n",
       "      <td>982.748408</td>\n",
       "      <td>971.987512</td>\n",
       "      <td>1037.775345</td>\n",
       "      <td>1006.865069</td>\n",
       "      <td>1031.111582</td>\n",
       "      <td>980.987703</td>\n",
       "      <td>965.731059</td>\n",
       "      <td>1044.155707</td>\n",
       "      <td>1031.550009</td>\n",
       "      <td>986.525329</td>\n",
       "      <td>956.716929</td>\n",
       "      <td>972.733624</td>\n",
       "      <td>1014.550436</td>\n",
       "      <td>1004</td>\n",
       "      <td>898.057926</td>\n",
       "      <td>900.745370</td>\n",
       "      <td>915.158956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1505.262616</td>\n",
       "      <td>1573.793579</td>\n",
       "      <td>1477.136661</td>\n",
       "      <td>1464.714361</td>\n",
       "      <td>2020.992955</td>\n",
       "      <td>1644.013144</td>\n",
       "      <td>1589.240447</td>\n",
       "      <td>1473.934875</td>\n",
       "      <td>1460.599837</td>\n",
       "      <td>2023.634069</td>\n",
       "      <td>1614.987267</td>\n",
       "      <td>1489.943014</td>\n",
       "      <td>1448.806267</td>\n",
       "      <td>1466.727788</td>\n",
       "      <td>2042.430342</td>\n",
       "      <td>1441</td>\n",
       "      <td>1397.507711</td>\n",
       "      <td>1419.978895</td>\n",
       "      <td>1401.414363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>426.819145</td>\n",
       "      <td>528.544678</td>\n",
       "      <td>446.953738</td>\n",
       "      <td>442.882205</td>\n",
       "      <td>485.381788</td>\n",
       "      <td>397.987550</td>\n",
       "      <td>533.676805</td>\n",
       "      <td>455.860021</td>\n",
       "      <td>440.576937</td>\n",
       "      <td>489.375335</td>\n",
       "      <td>426.220568</td>\n",
       "      <td>499.272707</td>\n",
       "      <td>434.771205</td>\n",
       "      <td>441.414946</td>\n",
       "      <td>466.961430</td>\n",
       "      <td>388</td>\n",
       "      <td>415.256775</td>\n",
       "      <td>417.253845</td>\n",
       "      <td>421.897675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  xgb+prophet  xgb+neuralprophet    xgb+ridge   xgb+linear  \\\n",
       "0      2015-01-01   335.405149         325.701162   324.342987   326.355622   \n",
       "1      2015-01-01   507.762170         463.390537   500.921801   500.216103   \n",
       "2      2015-01-01   137.081288         147.604655   138.691135   139.863963   \n",
       "3      2015-01-01   596.929485         552.264141   558.648326   551.420039   \n",
       "4      2015-01-01   940.653140         863.659302   890.501339   883.086316   \n",
       "...           ...          ...                ...          ...          ...   \n",
       "26293  2018-12-31   842.534291         929.313721   864.388124   838.336456   \n",
       "26294  2018-12-31   255.545977         251.271349   255.901804   250.816940   \n",
       "26295  2018-12-31   992.550115        1065.049591   982.748408   971.987512   \n",
       "26296  2018-12-31  1505.262616        1573.793579  1477.136661  1464.714361   \n",
       "26297  2018-12-31   426.819145         528.544678   446.953738   442.882205   \n",
       "\n",
       "         xgb+earth  lgb+prophet  lgb+neuralprophet    lgb+ridge   lgb+linear  \\\n",
       "0       399.253649   353.816288         303.237210   315.109073   320.781832   \n",
       "1       598.560040   502.099511         423.579452   502.309507   497.514037   \n",
       "2       150.623251   161.207516         152.517721   137.984103   133.714742   \n",
       "3       623.935119   577.112455         547.578908   555.405898   552.703990   \n",
       "4       961.749819   934.119140         918.265905   893.043397   882.242292   \n",
       "...            ...          ...                ...          ...          ...   \n",
       "26293   820.409379   924.978028         968.214591   855.545431   839.722064   \n",
       "26294   255.531034   189.916929         244.527543   261.353502   241.161049   \n",
       "26295  1037.775345  1006.865069        1031.111582   980.987703   965.731059   \n",
       "26296  2020.992955  1644.013144        1589.240447  1473.934875  1460.599837   \n",
       "26297   485.381788   397.987550         533.676805   455.860021   440.576937   \n",
       "\n",
       "         lgb+earth  cat+prophet  cat+neuralprophet    cat+ridge   cat+linear  \\\n",
       "0       360.017842   342.162795         297.299018   308.871192   319.407647   \n",
       "1       575.971519   505.468074         453.105968   494.362517   498.780657   \n",
       "2       126.248183   149.287358         153.054598   134.114025   135.374514   \n",
       "3       582.508689   576.479862         554.053877   555.407912   550.887544   \n",
       "4       937.060549   919.623890         887.265360   886.321175   881.045421   \n",
       "...            ...          ...                ...          ...          ...   \n",
       "26293   833.347522   905.346443         910.465738   841.013156   845.194035   \n",
       "26294   267.762967   206.318911         223.127072   244.486973   242.723551   \n",
       "26295  1044.155707  1031.550009         986.525329   956.716929   972.733624   \n",
       "26296  2023.634069  1614.987267        1489.943014  1448.806267  1466.727788   \n",
       "26297   489.375335   426.220568         499.272707   434.771205   441.414946   \n",
       "\n",
       "         cat+earth  num_sold          xgb          lgb          cat  \n",
       "0       378.548520       329   343.250478   331.496226   326.054356  \n",
       "1       592.709651       520   510.616623   514.576867   509.005936  \n",
       "2       122.792138       146   148.959080   146.429925   144.894609  \n",
       "3       610.328935       572   596.463984   584.653899   569.038022  \n",
       "4       962.248311       911   907.745110   888.842963   882.892654  \n",
       "...            ...       ...          ...          ...          ...  \n",
       "26293   834.502360       823   830.633157   814.597001   825.377960  \n",
       "26294   256.669182       250   241.115195   234.409908   245.208674  \n",
       "26295  1014.550436      1004   898.057926   900.745370   915.158956  \n",
       "26296  2042.430342      1441  1397.507711  1419.978895  1401.414363  \n",
       "26297   466.961430       388   415.256775   417.253845   421.897675  \n",
       "\n",
       "[26298 rows x 20 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbms_and_adjusted_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0366c787-dfdf-470d-8831-b7815e858c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1fa9120b-48b6-48cc-9c19-a4102027fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adjusted_oof_preds.drop(columns=['num_sold'])\n",
    "y = adjusted_oof_preds['num_sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ff5c4158-3be1-44db-a1d0-123745a46403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>xgb+prophet</th>\n",
       "      <th>xgb+neuralprophet</th>\n",
       "      <th>xgb+ridge</th>\n",
       "      <th>xgb+linear</th>\n",
       "      <th>xgb+earth</th>\n",
       "      <th>lgb+prophet</th>\n",
       "      <th>lgb+neuralprophet</th>\n",
       "      <th>lgb+ridge</th>\n",
       "      <th>lgb+linear</th>\n",
       "      <th>lgb+earth</th>\n",
       "      <th>cat+prophet</th>\n",
       "      <th>cat+neuralprophet</th>\n",
       "      <th>cat+ridge</th>\n",
       "      <th>cat+linear</th>\n",
       "      <th>cat+earth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>-13.673685</td>\n",
       "      <td>-1.549922</td>\n",
       "      <td>44.190460</td>\n",
       "      <td>2.167232</td>\n",
       "      <td>18.319008</td>\n",
       "      <td>1.196489</td>\n",
       "      <td>-2.918203</td>\n",
       "      <td>43.559803</td>\n",
       "      <td>-17.669736</td>\n",
       "      <td>11.621316</td>\n",
       "      <td>0.955770</td>\n",
       "      <td>-4.563203</td>\n",
       "      <td>40.854708</td>\n",
       "      <td>-17.643456</td>\n",
       "      <td>14.989288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>-20.761435</td>\n",
       "      <td>19.837572</td>\n",
       "      <td>66.310081</td>\n",
       "      <td>0.344402</td>\n",
       "      <td>7.194648</td>\n",
       "      <td>-17.167444</td>\n",
       "      <td>42.080821</td>\n",
       "      <td>73.941251</td>\n",
       "      <td>-13.054772</td>\n",
       "      <td>-1.657864</td>\n",
       "      <td>-19.494950</td>\n",
       "      <td>31.390587</td>\n",
       "      <td>73.837511</td>\n",
       "      <td>-14.630494</td>\n",
       "      <td>-6.618691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>-9.225127</td>\n",
       "      <td>0.528887</td>\n",
       "      <td>11.043291</td>\n",
       "      <td>-1.508096</td>\n",
       "      <td>5.391472</td>\n",
       "      <td>15.461567</td>\n",
       "      <td>8.233524</td>\n",
       "      <td>21.538343</td>\n",
       "      <td>-9.607261</td>\n",
       "      <td>6.898409</td>\n",
       "      <td>17.580708</td>\n",
       "      <td>21.197725</td>\n",
       "      <td>20.440282</td>\n",
       "      <td>-9.436740</td>\n",
       "      <td>-2.127463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>-8.254186</td>\n",
       "      <td>-2.514443</td>\n",
       "      <td>73.295914</td>\n",
       "      <td>-0.302817</td>\n",
       "      <td>54.620613</td>\n",
       "      <td>-9.747781</td>\n",
       "      <td>36.860873</td>\n",
       "      <td>83.439606</td>\n",
       "      <td>-22.948857</td>\n",
       "      <td>36.297744</td>\n",
       "      <td>-10.562935</td>\n",
       "      <td>21.469288</td>\n",
       "      <td>80.422080</td>\n",
       "      <td>-22.815989</td>\n",
       "      <td>44.181862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>-10.833193</td>\n",
       "      <td>99.191177</td>\n",
       "      <td>123.595261</td>\n",
       "      <td>-5.063988</td>\n",
       "      <td>14.543140</td>\n",
       "      <td>9.125004</td>\n",
       "      <td>203.201099</td>\n",
       "      <td>137.109634</td>\n",
       "      <td>-24.257739</td>\n",
       "      <td>4.944676</td>\n",
       "      <td>-7.329994</td>\n",
       "      <td>152.278801</td>\n",
       "      <td>131.605603</td>\n",
       "      <td>-22.844062</td>\n",
       "      <td>5.667452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>-60.002674</td>\n",
       "      <td>187.033325</td>\n",
       "      <td>114.968658</td>\n",
       "      <td>-3.108424</td>\n",
       "      <td>-19.420202</td>\n",
       "      <td>-1.568198</td>\n",
       "      <td>241.797005</td>\n",
       "      <td>125.970645</td>\n",
       "      <td>-2.102975</td>\n",
       "      <td>-9.491124</td>\n",
       "      <td>-33.207248</td>\n",
       "      <td>221.947118</td>\n",
       "      <td>115.450504</td>\n",
       "      <td>-2.479457</td>\n",
       "      <td>-7.796854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>-8.539580</td>\n",
       "      <td>7.273630</td>\n",
       "      <td>40.064671</td>\n",
       "      <td>13.118201</td>\n",
       "      <td>7.920500</td>\n",
       "      <td>-42.370562</td>\n",
       "      <td>0.954754</td>\n",
       "      <td>50.955771</td>\n",
       "      <td>5.503649</td>\n",
       "      <td>10.126003</td>\n",
       "      <td>-56.678126</td>\n",
       "      <td>-6.542535</td>\n",
       "      <td>41.053457</td>\n",
       "      <td>4.860815</td>\n",
       "      <td>10.829827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>-44.683212</td>\n",
       "      <td>253.187195</td>\n",
       "      <td>143.550446</td>\n",
       "      <td>11.296180</td>\n",
       "      <td>-2.589409</td>\n",
       "      <td>-12.488322</td>\n",
       "      <td>298.861837</td>\n",
       "      <td>157.356786</td>\n",
       "      <td>10.046627</td>\n",
       "      <td>-13.791494</td>\n",
       "      <td>-21.789688</td>\n",
       "      <td>260.933804</td>\n",
       "      <td>137.885133</td>\n",
       "      <td>7.177214</td>\n",
       "      <td>-31.752921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>-36.732143</td>\n",
       "      <td>463.469482</td>\n",
       "      <td>196.829102</td>\n",
       "      <td>-9.650113</td>\n",
       "      <td>-62.558617</td>\n",
       "      <td>87.571733</td>\n",
       "      <td>544.787954</td>\n",
       "      <td>207.858597</td>\n",
       "      <td>-8.682725</td>\n",
       "      <td>-52.960155</td>\n",
       "      <td>36.306836</td>\n",
       "      <td>475.671818</td>\n",
       "      <td>189.814122</td>\n",
       "      <td>-6.990601</td>\n",
       "      <td>-46.545445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>-58.646420</td>\n",
       "      <td>28.115078</td>\n",
       "      <td>46.561401</td>\n",
       "      <td>-14.757811</td>\n",
       "      <td>0.580786</td>\n",
       "      <td>-67.780184</td>\n",
       "      <td>57.234859</td>\n",
       "      <td>65.407047</td>\n",
       "      <td>-5.885414</td>\n",
       "      <td>-1.334173</td>\n",
       "      <td>-70.569336</td>\n",
       "      <td>50.739855</td>\n",
       "      <td>54.734686</td>\n",
       "      <td>-4.370621</td>\n",
       "      <td>-12.433902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  xgb+prophet  xgb+neuralprophet   xgb+ridge  xgb+linear  \\\n",
       "0     2019-01-01   -13.673685          -1.549922   44.190460    2.167232   \n",
       "1     2019-01-01   -20.761435          19.837572   66.310081    0.344402   \n",
       "2     2019-01-01    -9.225127           0.528887   11.043291   -1.508096   \n",
       "3     2019-01-01    -8.254186          -2.514443   73.295914   -0.302817   \n",
       "4     2019-01-01   -10.833193          99.191177  123.595261   -5.063988   \n",
       "...          ...          ...                ...         ...         ...   \n",
       "6565  2019-12-31   -60.002674         187.033325  114.968658   -3.108424   \n",
       "6566  2019-12-31    -8.539580           7.273630   40.064671   13.118201   \n",
       "6567  2019-12-31   -44.683212         253.187195  143.550446   11.296180   \n",
       "6568  2019-12-31   -36.732143         463.469482  196.829102   -9.650113   \n",
       "6569  2019-12-31   -58.646420          28.115078   46.561401  -14.757811   \n",
       "\n",
       "      xgb+earth  lgb+prophet  lgb+neuralprophet   lgb+ridge  lgb+linear  \\\n",
       "0     18.319008     1.196489          -2.918203   43.559803  -17.669736   \n",
       "1      7.194648   -17.167444          42.080821   73.941251  -13.054772   \n",
       "2      5.391472    15.461567           8.233524   21.538343   -9.607261   \n",
       "3     54.620613    -9.747781          36.860873   83.439606  -22.948857   \n",
       "4     14.543140     9.125004         203.201099  137.109634  -24.257739   \n",
       "...         ...          ...                ...         ...         ...   \n",
       "6565 -19.420202    -1.568198         241.797005  125.970645   -2.102975   \n",
       "6566   7.920500   -42.370562           0.954754   50.955771    5.503649   \n",
       "6567  -2.589409   -12.488322         298.861837  157.356786   10.046627   \n",
       "6568 -62.558617    87.571733         544.787954  207.858597   -8.682725   \n",
       "6569   0.580786   -67.780184          57.234859   65.407047   -5.885414   \n",
       "\n",
       "      lgb+earth  cat+prophet  cat+neuralprophet   cat+ridge  cat+linear  \\\n",
       "0     11.621316     0.955770          -4.563203   40.854708  -17.643456   \n",
       "1     -1.657864   -19.494950          31.390587   73.837511  -14.630494   \n",
       "2      6.898409    17.580708          21.197725   20.440282   -9.436740   \n",
       "3     36.297744   -10.562935          21.469288   80.422080  -22.815989   \n",
       "4      4.944676    -7.329994         152.278801  131.605603  -22.844062   \n",
       "...         ...          ...                ...         ...         ...   \n",
       "6565  -9.491124   -33.207248         221.947118  115.450504   -2.479457   \n",
       "6566  10.126003   -56.678126          -6.542535   41.053457    4.860815   \n",
       "6567 -13.791494   -21.789688         260.933804  137.885133    7.177214   \n",
       "6568 -52.960155    36.306836         475.671818  189.814122   -6.990601   \n",
       "6569  -1.334173   -70.569336          50.739855   54.734686   -4.370621   \n",
       "\n",
       "      cat+earth  \n",
       "0     14.989288  \n",
       "1     -6.618691  \n",
       "2     -2.127463  \n",
       "3     44.181862  \n",
       "4      5.667452  \n",
       "...         ...  \n",
       "6565  -7.796854  \n",
       "6566  10.829827  \n",
       "6567 -31.752921  \n",
       "6568 -46.545445  \n",
       "6569 -12.433902  \n",
       "\n",
       "[6570 rows x 16 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_residual_test_preds = pd.DataFrame({\n",
    "    'date': orig_test_df['date']\n",
    "})\n",
    "\n",
    "# tv_dfs_dict = {\n",
    "#     'xgb': xgb_tv_df,\n",
    "#     'lgb': lgb_tv_df,\n",
    "#     'cat': cat_tv_df,\n",
    "# }\n",
    "\n",
    "for arch in test_dfs_dict.keys():#['xgb', 'lgb', 'cat']: #prophet_0_residual_preds\n",
    "    for forecast_model in forecast_models:\n",
    "        adjusted_residual_test_preds[f'{arch}+{forecast_model}'] = sum([test_dfs_dict[arch][f'{forecast_model}_{i}_residual_preds'] for i in range(4)]) / 4\n",
    "adjusted_residual_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4fc0f6ea-a9a6-4237-80aa-ec8d08e5e9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>737060</td>\n",
       "      <td>402.296576</td>\n",
       "      <td>383.543457</td>\n",
       "      <td>359.817081</td>\n",
       "      <td>392.328390</td>\n",
       "      <td>417.102143</td>\n",
       "      <td>349.759459</td>\n",
       "      <td>373.034564</td>\n",
       "      <td>379.702323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>737060</td>\n",
       "      <td>631.642099</td>\n",
       "      <td>551.044983</td>\n",
       "      <td>560.783324</td>\n",
       "      <td>620.092467</td>\n",
       "      <td>693.844970</td>\n",
       "      <td>549.911123</td>\n",
       "      <td>578.563016</td>\n",
       "      <td>589.688285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>737060</td>\n",
       "      <td>169.992094</td>\n",
       "      <td>173.065674</td>\n",
       "      <td>153.552940</td>\n",
       "      <td>166.788184</td>\n",
       "      <td>181.642807</td>\n",
       "      <td>152.988182</td>\n",
       "      <td>165.419045</td>\n",
       "      <td>168.116727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>737060</td>\n",
       "      <td>684.559821</td>\n",
       "      <td>648.783691</td>\n",
       "      <td>622.237644</td>\n",
       "      <td>675.674104</td>\n",
       "      <td>594.166542</td>\n",
       "      <td>622.932486</td>\n",
       "      <td>651.424049</td>\n",
       "      <td>668.232707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>737060</td>\n",
       "      <td>1092.875362</td>\n",
       "      <td>965.683716</td>\n",
       "      <td>969.065554</td>\n",
       "      <td>1084.980801</td>\n",
       "      <td>1142.564632</td>\n",
       "      <td>943.483740</td>\n",
       "      <td>994.810045</td>\n",
       "      <td>1033.224703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>737424</td>\n",
       "      <td>917.085861</td>\n",
       "      <td>674.891174</td>\n",
       "      <td>742.331990</td>\n",
       "      <td>797.964290</td>\n",
       "      <td>830.606248</td>\n",
       "      <td>780.494827</td>\n",
       "      <td>809.243998</td>\n",
       "      <td>804.655222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>737424</td>\n",
       "      <td>261.569590</td>\n",
       "      <td>232.082169</td>\n",
       "      <td>210.920805</td>\n",
       "      <td>225.576610</td>\n",
       "      <td>269.498973</td>\n",
       "      <td>229.878546</td>\n",
       "      <td>235.835586</td>\n",
       "      <td>239.325998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>737424</td>\n",
       "      <td>1065.400121</td>\n",
       "      <td>730.457275</td>\n",
       "      <td>852.091174</td>\n",
       "      <td>925.788409</td>\n",
       "      <td>1133.560974</td>\n",
       "      <td>872.192736</td>\n",
       "      <td>899.608764</td>\n",
       "      <td>895.495983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>737424</td>\n",
       "      <td>1568.407214</td>\n",
       "      <td>1005.392944</td>\n",
       "      <td>1286.814471</td>\n",
       "      <td>1397.071983</td>\n",
       "      <td>2627.470943</td>\n",
       "      <td>1358.919453</td>\n",
       "      <td>1380.445879</td>\n",
       "      <td>1356.416644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>737424</td>\n",
       "      <td>476.122783</td>\n",
       "      <td>428.916565</td>\n",
       "      <td>382.800781</td>\n",
       "      <td>417.035222</td>\n",
       "      <td>507.799360</td>\n",
       "      <td>378.966547</td>\n",
       "      <td>407.419732</td>\n",
       "      <td>404.913007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      prophet  neuralprophet        ridge       linear  \\\n",
       "0     737060   402.296576     383.543457   359.817081   392.328390   \n",
       "1     737060   631.642099     551.044983   560.783324   620.092467   \n",
       "2     737060   169.992094     173.065674   153.552940   166.788184   \n",
       "3     737060   684.559821     648.783691   622.237644   675.674104   \n",
       "4     737060  1092.875362     965.683716   969.065554  1084.980801   \n",
       "...      ...          ...            ...          ...          ...   \n",
       "6565  737424   917.085861     674.891174   742.331990   797.964290   \n",
       "6566  737424   261.569590     232.082169   210.920805   225.576610   \n",
       "6567  737424  1065.400121     730.457275   852.091174   925.788409   \n",
       "6568  737424  1568.407214    1005.392944  1286.814471  1397.071983   \n",
       "6569  737424   476.122783     428.916565   382.800781   417.035222   \n",
       "\n",
       "            earth      xgboost     lightgbm     catboost  \n",
       "0      417.102143   349.759459   373.034564   379.702323  \n",
       "1      693.844970   549.911123   578.563016   589.688285  \n",
       "2      181.642807   152.988182   165.419045   168.116727  \n",
       "3      594.166542   622.932486   651.424049   668.232707  \n",
       "4     1142.564632   943.483740   994.810045  1033.224703  \n",
       "...           ...          ...          ...          ...  \n",
       "6565   830.606248   780.494827   809.243998   804.655222  \n",
       "6566   269.498973   229.878546   235.835586   239.325998  \n",
       "6567  1133.560974   872.192736   899.608764   895.495983  \n",
       "6568  2627.470943  1358.919453  1380.445879  1356.416644  \n",
       "6569   507.799360   378.966547   407.419732   404.913007  \n",
       "\n",
       "[6570 rows x 9 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_forecast_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9d0262d1-55c2-485f-892f-6cb8fd5e0035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/preds/20220127_residual_test_predictions+earth_averaged_by_fold_using_GBMs.joblib']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(adjusted_residual_test_preds, predpath/'20220127_residual_test_predictions+earth_averaged_by_fold_using_GBMs.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f507c36a-41d8-4afb-926a-b2774574b377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/preds/20220127_final_oof_predictions_forecast+Earth+GBMs-residuals.joblib']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(adjusted_oof_preds, predpath/'20220127_final_oof_predictions_forecast+Earth+GBMs-residuals.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6682497d-1ced-4c03-b1e2-9c2aa68ee694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted_test_residual_preds = adjusted_test_preds.copy() # fixing the confusing name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5ec1a3a0-b34e-46d7-85ef-7af8d259a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_test_preds = pd.DataFrame({\n",
    "    'date': orig_test_df['date']\n",
    "})\n",
    "\n",
    "for arch in ['xgb', 'lgb', 'cat']: # ['xgb', 'lgb', 'cat']:\n",
    "    for forecast_model in forecast_models:\n",
    "        adjusted_test_preds[f'{arch}+{forecast_model}'] = test_forecast_preds[f'{forecast_model}'] + adjusted_residual_test_preds[f'{arch}+{forecast_model}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b7ca7176-8960-4ded-932c-84d64f28bf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>xgb+prophet</th>\n",
       "      <th>xgb+neuralprophet</th>\n",
       "      <th>xgb+ridge</th>\n",
       "      <th>xgb+linear</th>\n",
       "      <th>xgb+earth</th>\n",
       "      <th>lgb+prophet</th>\n",
       "      <th>lgb+neuralprophet</th>\n",
       "      <th>lgb+ridge</th>\n",
       "      <th>lgb+linear</th>\n",
       "      <th>lgb+earth</th>\n",
       "      <th>cat+prophet</th>\n",
       "      <th>cat+neuralprophet</th>\n",
       "      <th>cat+ridge</th>\n",
       "      <th>cat+linear</th>\n",
       "      <th>cat+earth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>388.622891</td>\n",
       "      <td>381.993536</td>\n",
       "      <td>404.007541</td>\n",
       "      <td>394.495622</td>\n",
       "      <td>435.421150</td>\n",
       "      <td>403.493065</td>\n",
       "      <td>380.625254</td>\n",
       "      <td>403.376884</td>\n",
       "      <td>374.658654</td>\n",
       "      <td>428.723458</td>\n",
       "      <td>403.252346</td>\n",
       "      <td>378.980254</td>\n",
       "      <td>400.671789</td>\n",
       "      <td>374.684934</td>\n",
       "      <td>432.091431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>610.880664</td>\n",
       "      <td>570.882555</td>\n",
       "      <td>627.093405</td>\n",
       "      <td>620.436869</td>\n",
       "      <td>701.039618</td>\n",
       "      <td>614.474655</td>\n",
       "      <td>593.125804</td>\n",
       "      <td>634.724575</td>\n",
       "      <td>607.037695</td>\n",
       "      <td>692.187106</td>\n",
       "      <td>612.147149</td>\n",
       "      <td>582.435570</td>\n",
       "      <td>634.620835</td>\n",
       "      <td>605.461973</td>\n",
       "      <td>687.226279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>160.766967</td>\n",
       "      <td>173.594561</td>\n",
       "      <td>164.596231</td>\n",
       "      <td>165.280088</td>\n",
       "      <td>187.034279</td>\n",
       "      <td>185.453661</td>\n",
       "      <td>181.299197</td>\n",
       "      <td>175.091284</td>\n",
       "      <td>157.180923</td>\n",
       "      <td>188.541216</td>\n",
       "      <td>187.572802</td>\n",
       "      <td>194.263398</td>\n",
       "      <td>173.993223</td>\n",
       "      <td>157.351444</td>\n",
       "      <td>179.515344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>676.305636</td>\n",
       "      <td>646.269248</td>\n",
       "      <td>695.533558</td>\n",
       "      <td>675.371287</td>\n",
       "      <td>648.787155</td>\n",
       "      <td>674.812040</td>\n",
       "      <td>685.644565</td>\n",
       "      <td>705.677250</td>\n",
       "      <td>652.725247</td>\n",
       "      <td>630.464286</td>\n",
       "      <td>673.996886</td>\n",
       "      <td>670.252980</td>\n",
       "      <td>702.659724</td>\n",
       "      <td>652.858116</td>\n",
       "      <td>638.348403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1082.042169</td>\n",
       "      <td>1064.874893</td>\n",
       "      <td>1092.660815</td>\n",
       "      <td>1079.916814</td>\n",
       "      <td>1157.107772</td>\n",
       "      <td>1102.000366</td>\n",
       "      <td>1168.884815</td>\n",
       "      <td>1106.175188</td>\n",
       "      <td>1060.723062</td>\n",
       "      <td>1147.509309</td>\n",
       "      <td>1085.545368</td>\n",
       "      <td>1117.962517</td>\n",
       "      <td>1100.671157</td>\n",
       "      <td>1062.136740</td>\n",
       "      <td>1148.232084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>857.083187</td>\n",
       "      <td>861.924500</td>\n",
       "      <td>857.300648</td>\n",
       "      <td>794.855865</td>\n",
       "      <td>811.186046</td>\n",
       "      <td>915.517664</td>\n",
       "      <td>916.688179</td>\n",
       "      <td>868.302635</td>\n",
       "      <td>795.861315</td>\n",
       "      <td>821.115124</td>\n",
       "      <td>883.878613</td>\n",
       "      <td>896.838292</td>\n",
       "      <td>857.782493</td>\n",
       "      <td>795.484833</td>\n",
       "      <td>822.809394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>253.030010</td>\n",
       "      <td>239.355799</td>\n",
       "      <td>250.985476</td>\n",
       "      <td>238.694811</td>\n",
       "      <td>277.419473</td>\n",
       "      <td>219.199028</td>\n",
       "      <td>233.036923</td>\n",
       "      <td>261.876577</td>\n",
       "      <td>231.080258</td>\n",
       "      <td>279.624976</td>\n",
       "      <td>204.891464</td>\n",
       "      <td>225.539633</td>\n",
       "      <td>251.974263</td>\n",
       "      <td>230.437425</td>\n",
       "      <td>280.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1020.716908</td>\n",
       "      <td>983.644470</td>\n",
       "      <td>995.641620</td>\n",
       "      <td>937.084589</td>\n",
       "      <td>1130.971565</td>\n",
       "      <td>1052.911798</td>\n",
       "      <td>1029.319112</td>\n",
       "      <td>1009.447960</td>\n",
       "      <td>935.835037</td>\n",
       "      <td>1119.769480</td>\n",
       "      <td>1043.610433</td>\n",
       "      <td>991.391079</td>\n",
       "      <td>989.976308</td>\n",
       "      <td>932.965624</td>\n",
       "      <td>1101.808053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1531.675071</td>\n",
       "      <td>1468.862427</td>\n",
       "      <td>1483.643573</td>\n",
       "      <td>1387.421869</td>\n",
       "      <td>2564.912326</td>\n",
       "      <td>1655.978947</td>\n",
       "      <td>1550.180898</td>\n",
       "      <td>1494.673068</td>\n",
       "      <td>1388.389257</td>\n",
       "      <td>2574.510788</td>\n",
       "      <td>1604.714050</td>\n",
       "      <td>1481.064763</td>\n",
       "      <td>1476.628593</td>\n",
       "      <td>1390.081381</td>\n",
       "      <td>2580.925498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>417.476363</td>\n",
       "      <td>457.031643</td>\n",
       "      <td>429.362182</td>\n",
       "      <td>402.277412</td>\n",
       "      <td>508.380146</td>\n",
       "      <td>408.342599</td>\n",
       "      <td>486.151424</td>\n",
       "      <td>448.207828</td>\n",
       "      <td>411.149808</td>\n",
       "      <td>506.465187</td>\n",
       "      <td>405.553447</td>\n",
       "      <td>479.656420</td>\n",
       "      <td>437.535467</td>\n",
       "      <td>412.664601</td>\n",
       "      <td>495.365458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  xgb+prophet  xgb+neuralprophet    xgb+ridge   xgb+linear  \\\n",
       "0     2019-01-01   388.622891         381.993536   404.007541   394.495622   \n",
       "1     2019-01-01   610.880664         570.882555   627.093405   620.436869   \n",
       "2     2019-01-01   160.766967         173.594561   164.596231   165.280088   \n",
       "3     2019-01-01   676.305636         646.269248   695.533558   675.371287   \n",
       "4     2019-01-01  1082.042169        1064.874893  1092.660815  1079.916814   \n",
       "...          ...          ...                ...          ...          ...   \n",
       "6565  2019-12-31   857.083187         861.924500   857.300648   794.855865   \n",
       "6566  2019-12-31   253.030010         239.355799   250.985476   238.694811   \n",
       "6567  2019-12-31  1020.716908         983.644470   995.641620   937.084589   \n",
       "6568  2019-12-31  1531.675071        1468.862427  1483.643573  1387.421869   \n",
       "6569  2019-12-31   417.476363         457.031643   429.362182   402.277412   \n",
       "\n",
       "        xgb+earth  lgb+prophet  lgb+neuralprophet    lgb+ridge   lgb+linear  \\\n",
       "0      435.421150   403.493065         380.625254   403.376884   374.658654   \n",
       "1      701.039618   614.474655         593.125804   634.724575   607.037695   \n",
       "2      187.034279   185.453661         181.299197   175.091284   157.180923   \n",
       "3      648.787155   674.812040         685.644565   705.677250   652.725247   \n",
       "4     1157.107772  1102.000366        1168.884815  1106.175188  1060.723062   \n",
       "...           ...          ...                ...          ...          ...   \n",
       "6565   811.186046   915.517664         916.688179   868.302635   795.861315   \n",
       "6566   277.419473   219.199028         233.036923   261.876577   231.080258   \n",
       "6567  1130.971565  1052.911798        1029.319112  1009.447960   935.835037   \n",
       "6568  2564.912326  1655.978947        1550.180898  1494.673068  1388.389257   \n",
       "6569   508.380146   408.342599         486.151424   448.207828   411.149808   \n",
       "\n",
       "        lgb+earth  cat+prophet  cat+neuralprophet    cat+ridge   cat+linear  \\\n",
       "0      428.723458   403.252346         378.980254   400.671789   374.684934   \n",
       "1      692.187106   612.147149         582.435570   634.620835   605.461973   \n",
       "2      188.541216   187.572802         194.263398   173.993223   157.351444   \n",
       "3      630.464286   673.996886         670.252980   702.659724   652.858116   \n",
       "4     1147.509309  1085.545368        1117.962517  1100.671157  1062.136740   \n",
       "...           ...          ...                ...          ...          ...   \n",
       "6565   821.115124   883.878613         896.838292   857.782493   795.484833   \n",
       "6566   279.624976   204.891464         225.539633   251.974263   230.437425   \n",
       "6567  1119.769480  1043.610433         991.391079   989.976308   932.965624   \n",
       "6568  2574.510788  1604.714050        1481.064763  1476.628593  1390.081381   \n",
       "6569   506.465187   405.553447         479.656420   437.535467   412.664601   \n",
       "\n",
       "        cat+earth  \n",
       "0      432.091431  \n",
       "1      687.226279  \n",
       "2      179.515344  \n",
       "3      638.348403  \n",
       "4     1148.232084  \n",
       "...           ...  \n",
       "6565   822.809394  \n",
       "6566   280.328800  \n",
       "6567  1101.808053  \n",
       "6568  2580.925498  \n",
       "6569   495.365458  \n",
       "\n",
       "[6570 rows x 16 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c5836041-daa9-4a49-b75c-4ddb22513618",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecast_preds_only = test_forecast_preds.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b71f2584-0cb6-48cc-a6ee-9179936ca59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_test_preds_selective = adjusted_test_preds.drop(columns=[\n",
    "                                                                'xgb+neuralprophet', \n",
    "#                                                                 'xgb+ridge', \n",
    "#                                                                 'xgb+linear', \n",
    "                                                                'xgb+earth',\n",
    "                                                                'lgb+neuralprophet', \n",
    "#                                                                 'lgb+ridge', \n",
    "#                                                                 'lgb+linear', \n",
    "                                                                'lgb+earth',\n",
    "                                                                'cat+neuralprophet', \n",
    "#                                                                 'cat+linear', \n",
    "                                                                'cat+earth'\n",
    "                                                               ]   \n",
    "                                                      )\n",
    "                                                      \n",
    "# tv_preds_only = tv_preds.drop(columns=['date', 'num_sold'])\n",
    "adjusted_test_preds_expanded = adjusted_test_preds.join(test_forecast_preds_only)#_only)#.drop(columns=['date', 'num_sold'], inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ae9fa446-c59a-446a-9541-f11e7a22bbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>xgb+prophet</th>\n",
       "      <th>xgb+ridge</th>\n",
       "      <th>xgb+linear</th>\n",
       "      <th>lgb+prophet</th>\n",
       "      <th>lgb+ridge</th>\n",
       "      <th>lgb+linear</th>\n",
       "      <th>cat+prophet</th>\n",
       "      <th>cat+ridge</th>\n",
       "      <th>cat+linear</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>388.622891</td>\n",
       "      <td>404.007541</td>\n",
       "      <td>394.495622</td>\n",
       "      <td>403.493065</td>\n",
       "      <td>403.376884</td>\n",
       "      <td>374.658654</td>\n",
       "      <td>403.252346</td>\n",
       "      <td>400.671789</td>\n",
       "      <td>374.684934</td>\n",
       "      <td>402.296576</td>\n",
       "      <td>383.543457</td>\n",
       "      <td>359.817081</td>\n",
       "      <td>392.328390</td>\n",
       "      <td>417.102143</td>\n",
       "      <td>349.759459</td>\n",
       "      <td>373.034564</td>\n",
       "      <td>379.702323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>610.880664</td>\n",
       "      <td>627.093405</td>\n",
       "      <td>620.436869</td>\n",
       "      <td>614.474655</td>\n",
       "      <td>634.724575</td>\n",
       "      <td>607.037695</td>\n",
       "      <td>612.147149</td>\n",
       "      <td>634.620835</td>\n",
       "      <td>605.461973</td>\n",
       "      <td>631.642099</td>\n",
       "      <td>551.044983</td>\n",
       "      <td>560.783324</td>\n",
       "      <td>620.092467</td>\n",
       "      <td>693.844970</td>\n",
       "      <td>549.911123</td>\n",
       "      <td>578.563016</td>\n",
       "      <td>589.688285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>160.766967</td>\n",
       "      <td>164.596231</td>\n",
       "      <td>165.280088</td>\n",
       "      <td>185.453661</td>\n",
       "      <td>175.091284</td>\n",
       "      <td>157.180923</td>\n",
       "      <td>187.572802</td>\n",
       "      <td>173.993223</td>\n",
       "      <td>157.351444</td>\n",
       "      <td>169.992094</td>\n",
       "      <td>173.065674</td>\n",
       "      <td>153.552940</td>\n",
       "      <td>166.788184</td>\n",
       "      <td>181.642807</td>\n",
       "      <td>152.988182</td>\n",
       "      <td>165.419045</td>\n",
       "      <td>168.116727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>676.305636</td>\n",
       "      <td>695.533558</td>\n",
       "      <td>675.371287</td>\n",
       "      <td>674.812040</td>\n",
       "      <td>705.677250</td>\n",
       "      <td>652.725247</td>\n",
       "      <td>673.996886</td>\n",
       "      <td>702.659724</td>\n",
       "      <td>652.858116</td>\n",
       "      <td>684.559821</td>\n",
       "      <td>648.783691</td>\n",
       "      <td>622.237644</td>\n",
       "      <td>675.674104</td>\n",
       "      <td>594.166542</td>\n",
       "      <td>622.932486</td>\n",
       "      <td>651.424049</td>\n",
       "      <td>668.232707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1082.042169</td>\n",
       "      <td>1092.660815</td>\n",
       "      <td>1079.916814</td>\n",
       "      <td>1102.000366</td>\n",
       "      <td>1106.175188</td>\n",
       "      <td>1060.723062</td>\n",
       "      <td>1085.545368</td>\n",
       "      <td>1100.671157</td>\n",
       "      <td>1062.136740</td>\n",
       "      <td>1092.875362</td>\n",
       "      <td>965.683716</td>\n",
       "      <td>969.065554</td>\n",
       "      <td>1084.980801</td>\n",
       "      <td>1142.564632</td>\n",
       "      <td>943.483740</td>\n",
       "      <td>994.810045</td>\n",
       "      <td>1033.224703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>857.083187</td>\n",
       "      <td>857.300648</td>\n",
       "      <td>794.855865</td>\n",
       "      <td>915.517664</td>\n",
       "      <td>868.302635</td>\n",
       "      <td>795.861315</td>\n",
       "      <td>883.878613</td>\n",
       "      <td>857.782493</td>\n",
       "      <td>795.484833</td>\n",
       "      <td>917.085861</td>\n",
       "      <td>674.891174</td>\n",
       "      <td>742.331990</td>\n",
       "      <td>797.964290</td>\n",
       "      <td>830.606248</td>\n",
       "      <td>780.494827</td>\n",
       "      <td>809.243998</td>\n",
       "      <td>804.655222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>253.030010</td>\n",
       "      <td>250.985476</td>\n",
       "      <td>238.694811</td>\n",
       "      <td>219.199028</td>\n",
       "      <td>261.876577</td>\n",
       "      <td>231.080258</td>\n",
       "      <td>204.891464</td>\n",
       "      <td>251.974263</td>\n",
       "      <td>230.437425</td>\n",
       "      <td>261.569590</td>\n",
       "      <td>232.082169</td>\n",
       "      <td>210.920805</td>\n",
       "      <td>225.576610</td>\n",
       "      <td>269.498973</td>\n",
       "      <td>229.878546</td>\n",
       "      <td>235.835586</td>\n",
       "      <td>239.325998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1020.716908</td>\n",
       "      <td>995.641620</td>\n",
       "      <td>937.084589</td>\n",
       "      <td>1052.911798</td>\n",
       "      <td>1009.447960</td>\n",
       "      <td>935.835037</td>\n",
       "      <td>1043.610433</td>\n",
       "      <td>989.976308</td>\n",
       "      <td>932.965624</td>\n",
       "      <td>1065.400121</td>\n",
       "      <td>730.457275</td>\n",
       "      <td>852.091174</td>\n",
       "      <td>925.788409</td>\n",
       "      <td>1133.560974</td>\n",
       "      <td>872.192736</td>\n",
       "      <td>899.608764</td>\n",
       "      <td>895.495983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1531.675071</td>\n",
       "      <td>1483.643573</td>\n",
       "      <td>1387.421869</td>\n",
       "      <td>1655.978947</td>\n",
       "      <td>1494.673068</td>\n",
       "      <td>1388.389257</td>\n",
       "      <td>1604.714050</td>\n",
       "      <td>1476.628593</td>\n",
       "      <td>1390.081381</td>\n",
       "      <td>1568.407214</td>\n",
       "      <td>1005.392944</td>\n",
       "      <td>1286.814471</td>\n",
       "      <td>1397.071983</td>\n",
       "      <td>2627.470943</td>\n",
       "      <td>1358.919453</td>\n",
       "      <td>1380.445879</td>\n",
       "      <td>1356.416644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>417.476363</td>\n",
       "      <td>429.362182</td>\n",
       "      <td>402.277412</td>\n",
       "      <td>408.342599</td>\n",
       "      <td>448.207828</td>\n",
       "      <td>411.149808</td>\n",
       "      <td>405.553447</td>\n",
       "      <td>437.535467</td>\n",
       "      <td>412.664601</td>\n",
       "      <td>476.122783</td>\n",
       "      <td>428.916565</td>\n",
       "      <td>382.800781</td>\n",
       "      <td>417.035222</td>\n",
       "      <td>507.799360</td>\n",
       "      <td>378.966547</td>\n",
       "      <td>407.419732</td>\n",
       "      <td>404.913007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  xgb+prophet    xgb+ridge   xgb+linear  lgb+prophet  \\\n",
       "0     2019-01-01   388.622891   404.007541   394.495622   403.493065   \n",
       "1     2019-01-01   610.880664   627.093405   620.436869   614.474655   \n",
       "2     2019-01-01   160.766967   164.596231   165.280088   185.453661   \n",
       "3     2019-01-01   676.305636   695.533558   675.371287   674.812040   \n",
       "4     2019-01-01  1082.042169  1092.660815  1079.916814  1102.000366   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "6565  2019-12-31   857.083187   857.300648   794.855865   915.517664   \n",
       "6566  2019-12-31   253.030010   250.985476   238.694811   219.199028   \n",
       "6567  2019-12-31  1020.716908   995.641620   937.084589  1052.911798   \n",
       "6568  2019-12-31  1531.675071  1483.643573  1387.421869  1655.978947   \n",
       "6569  2019-12-31   417.476363   429.362182   402.277412   408.342599   \n",
       "\n",
       "        lgb+ridge   lgb+linear  cat+prophet    cat+ridge   cat+linear  \\\n",
       "0      403.376884   374.658654   403.252346   400.671789   374.684934   \n",
       "1      634.724575   607.037695   612.147149   634.620835   605.461973   \n",
       "2      175.091284   157.180923   187.572802   173.993223   157.351444   \n",
       "3      705.677250   652.725247   673.996886   702.659724   652.858116   \n",
       "4     1106.175188  1060.723062  1085.545368  1100.671157  1062.136740   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "6565   868.302635   795.861315   883.878613   857.782493   795.484833   \n",
       "6566   261.876577   231.080258   204.891464   251.974263   230.437425   \n",
       "6567  1009.447960   935.835037  1043.610433   989.976308   932.965624   \n",
       "6568  1494.673068  1388.389257  1604.714050  1476.628593  1390.081381   \n",
       "6569   448.207828   411.149808   405.553447   437.535467   412.664601   \n",
       "\n",
       "          prophet  neuralprophet        ridge       linear        earth  \\\n",
       "0      402.296576     383.543457   359.817081   392.328390   417.102143   \n",
       "1      631.642099     551.044983   560.783324   620.092467   693.844970   \n",
       "2      169.992094     173.065674   153.552940   166.788184   181.642807   \n",
       "3      684.559821     648.783691   622.237644   675.674104   594.166542   \n",
       "4     1092.875362     965.683716   969.065554  1084.980801  1142.564632   \n",
       "...           ...            ...          ...          ...          ...   \n",
       "6565   917.085861     674.891174   742.331990   797.964290   830.606248   \n",
       "6566   261.569590     232.082169   210.920805   225.576610   269.498973   \n",
       "6567  1065.400121     730.457275   852.091174   925.788409  1133.560974   \n",
       "6568  1568.407214    1005.392944  1286.814471  1397.071983  2627.470943   \n",
       "6569   476.122783     428.916565   382.800781   417.035222   507.799360   \n",
       "\n",
       "          xgboost     lightgbm     catboost  \n",
       "0      349.759459   373.034564   379.702323  \n",
       "1      549.911123   578.563016   589.688285  \n",
       "2      152.988182   165.419045   168.116727  \n",
       "3      622.932486   651.424049   668.232707  \n",
       "4      943.483740   994.810045  1033.224703  \n",
       "...           ...          ...          ...  \n",
       "6565   780.494827   809.243998   804.655222  \n",
       "6566   229.878546   235.835586   239.325998  \n",
       "6567   872.192736   899.608764   895.495983  \n",
       "6568  1358.919453  1380.445879  1356.416644  \n",
       "6569   378.966547   407.419732   404.913007  \n",
       "\n",
       "[6570 rows x 18 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_test_preds_selective_expanded = adjusted_test_preds_selective.join(test_forecast_preds_only)\n",
    "adjusted_test_preds_selective_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f244cde-4b8e-4b50-b91d-ef3bc6b5ee9b",
   "metadata": {},
   "source": [
    "### Final Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "00765c6a-53a6-4d2d-9690-27657dbe8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = adjusted_oof_preds\n",
    "# X = adjusted_oof_preds_selective\n",
    "# X = adjusted_oof_preds_expanded\n",
    "X = adjusted_oof_preds_selective_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a522aedd-d830-4775-9516-18dcfc5d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['num_sold']\n",
    "X = X.drop(columns=['num_sold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c9625e53-042f-49fa-922b-9b44f8af20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = adjusted_test_preds\n",
    "# X_test = adjusted_test_preds_selective\n",
    "# X_test = adjusted_test_preds_expanded\n",
    "X_test = adjusted_test_preds_selective_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d196a654-2a9a-4341-b65d-2f8d757ebb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['date'] = pd.to_datetime(X.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8df52a5a-5079-4aa8-ada5-6ea37411fbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso()"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['date'] = X['date'].map(dt.datetime.toordinal)\n",
    "model.fit(X,y) # this is the lasso regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b2bb1a3e-89f7-4bb7-8620-4319f6d130a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['date'] = X['date'].map(dt.datetime.toordinal)\n",
    "X_test['date'] = pd.to_datetime(X_test.date)\n",
    "X_test['date'] = X_test['date'].map(dt.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "67177067-8ed7-421d-acde-33ad7c242e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>xgb+prophet</th>\n",
       "      <th>xgb+ridge</th>\n",
       "      <th>xgb+linear</th>\n",
       "      <th>lgb+prophet</th>\n",
       "      <th>lgb+ridge</th>\n",
       "      <th>lgb+linear</th>\n",
       "      <th>cat+prophet</th>\n",
       "      <th>cat+ridge</th>\n",
       "      <th>cat+linear</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>735599</td>\n",
       "      <td>335.405149</td>\n",
       "      <td>324.342987</td>\n",
       "      <td>326.355622</td>\n",
       "      <td>353.816288</td>\n",
       "      <td>315.109073</td>\n",
       "      <td>320.781832</td>\n",
       "      <td>342.162795</td>\n",
       "      <td>308.871192</td>\n",
       "      <td>319.407647</td>\n",
       "      <td>346.560416</td>\n",
       "      <td>329.134521</td>\n",
       "      <td>283.272258</td>\n",
       "      <td>324.188332</td>\n",
       "      <td>360.672190</td>\n",
       "      <td>343.250478</td>\n",
       "      <td>331.496226</td>\n",
       "      <td>326.054356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>735599</td>\n",
       "      <td>507.762170</td>\n",
       "      <td>500.921801</td>\n",
       "      <td>500.216103</td>\n",
       "      <td>502.099511</td>\n",
       "      <td>502.309507</td>\n",
       "      <td>497.514037</td>\n",
       "      <td>505.468074</td>\n",
       "      <td>494.362517</td>\n",
       "      <td>498.780657</td>\n",
       "      <td>536.203586</td>\n",
       "      <td>458.008301</td>\n",
       "      <td>439.771734</td>\n",
       "      <td>503.081569</td>\n",
       "      <td>584.851806</td>\n",
       "      <td>510.616623</td>\n",
       "      <td>514.576867</td>\n",
       "      <td>509.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>735599</td>\n",
       "      <td>137.081288</td>\n",
       "      <td>138.691135</td>\n",
       "      <td>139.863963</td>\n",
       "      <td>161.207516</td>\n",
       "      <td>137.984103</td>\n",
       "      <td>133.714742</td>\n",
       "      <td>149.287358</td>\n",
       "      <td>134.114025</td>\n",
       "      <td>135.374514</td>\n",
       "      <td>143.412803</td>\n",
       "      <td>145.325577</td>\n",
       "      <td>120.885683</td>\n",
       "      <td>136.667932</td>\n",
       "      <td>138.401176</td>\n",
       "      <td>148.959080</td>\n",
       "      <td>146.429925</td>\n",
       "      <td>144.894609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>735599</td>\n",
       "      <td>596.929485</td>\n",
       "      <td>558.648326</td>\n",
       "      <td>551.420039</td>\n",
       "      <td>577.112455</td>\n",
       "      <td>555.405898</td>\n",
       "      <td>552.703990</td>\n",
       "      <td>576.479862</td>\n",
       "      <td>555.407912</td>\n",
       "      <td>550.887544</td>\n",
       "      <td>590.117165</td>\n",
       "      <td>552.571167</td>\n",
       "      <td>488.810710</td>\n",
       "      <td>555.531957</td>\n",
       "      <td>562.479785</td>\n",
       "      <td>596.463984</td>\n",
       "      <td>584.653899</td>\n",
       "      <td>569.038022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>735599</td>\n",
       "      <td>940.653140</td>\n",
       "      <td>890.501339</td>\n",
       "      <td>883.086316</td>\n",
       "      <td>934.119140</td>\n",
       "      <td>893.043397</td>\n",
       "      <td>882.242292</td>\n",
       "      <td>919.623890</td>\n",
       "      <td>886.321175</td>\n",
       "      <td>881.045421</td>\n",
       "      <td>939.673009</td>\n",
       "      <td>781.967285</td>\n",
       "      <td>771.701702</td>\n",
       "      <td>890.469201</td>\n",
       "      <td>944.584508</td>\n",
       "      <td>907.745110</td>\n",
       "      <td>888.842963</td>\n",
       "      <td>882.892654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>737059</td>\n",
       "      <td>842.534291</td>\n",
       "      <td>864.388124</td>\n",
       "      <td>838.336456</td>\n",
       "      <td>924.978028</td>\n",
       "      <td>855.545431</td>\n",
       "      <td>839.722064</td>\n",
       "      <td>905.346443</td>\n",
       "      <td>841.013156</td>\n",
       "      <td>845.194035</td>\n",
       "      <td>898.322121</td>\n",
       "      <td>669.418457</td>\n",
       "      <td>723.714830</td>\n",
       "      <td>846.597603</td>\n",
       "      <td>836.947940</td>\n",
       "      <td>830.633157</td>\n",
       "      <td>814.597001</td>\n",
       "      <td>825.377960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>737059</td>\n",
       "      <td>255.545977</td>\n",
       "      <td>255.901804</td>\n",
       "      <td>250.816940</td>\n",
       "      <td>189.916929</td>\n",
       "      <td>261.353502</td>\n",
       "      <td>241.161049</td>\n",
       "      <td>206.318911</td>\n",
       "      <td>244.486973</td>\n",
       "      <td>242.723551</td>\n",
       "      <td>253.512355</td>\n",
       "      <td>227.158142</td>\n",
       "      <td>205.880079</td>\n",
       "      <td>241.048951</td>\n",
       "      <td>254.287204</td>\n",
       "      <td>241.115195</td>\n",
       "      <td>234.409908</td>\n",
       "      <td>245.208674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>737059</td>\n",
       "      <td>992.550115</td>\n",
       "      <td>982.748408</td>\n",
       "      <td>971.987512</td>\n",
       "      <td>1006.865069</td>\n",
       "      <td>980.987703</td>\n",
       "      <td>965.731059</td>\n",
       "      <td>1031.550009</td>\n",
       "      <td>956.716929</td>\n",
       "      <td>972.733624</td>\n",
       "      <td>1039.635205</td>\n",
       "      <td>715.639648</td>\n",
       "      <td>832.192362</td>\n",
       "      <td>975.785339</td>\n",
       "      <td>1058.174059</td>\n",
       "      <td>898.057926</td>\n",
       "      <td>900.745370</td>\n",
       "      <td>915.158956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>737059</td>\n",
       "      <td>1505.262616</td>\n",
       "      <td>1477.136661</td>\n",
       "      <td>1464.714361</td>\n",
       "      <td>1644.013144</td>\n",
       "      <td>1473.934875</td>\n",
       "      <td>1460.599837</td>\n",
       "      <td>1614.987267</td>\n",
       "      <td>1448.806267</td>\n",
       "      <td>1466.727788</td>\n",
       "      <td>1526.908216</td>\n",
       "      <td>980.234009</td>\n",
       "      <td>1255.885410</td>\n",
       "      <td>1468.776593</td>\n",
       "      <td>2036.549357</td>\n",
       "      <td>1397.507711</td>\n",
       "      <td>1419.978895</td>\n",
       "      <td>1401.414363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>737059</td>\n",
       "      <td>426.819145</td>\n",
       "      <td>446.953738</td>\n",
       "      <td>442.882205</td>\n",
       "      <td>397.987550</td>\n",
       "      <td>455.860021</td>\n",
       "      <td>440.576937</td>\n",
       "      <td>426.220568</td>\n",
       "      <td>434.771205</td>\n",
       "      <td>441.414946</td>\n",
       "      <td>463.705207</td>\n",
       "      <td>423.052612</td>\n",
       "      <td>373.542094</td>\n",
       "      <td>441.128842</td>\n",
       "      <td>472.307718</td>\n",
       "      <td>415.256775</td>\n",
       "      <td>417.253845</td>\n",
       "      <td>421.897675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26298 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  xgb+prophet    xgb+ridge   xgb+linear  lgb+prophet  \\\n",
       "0      735599   335.405149   324.342987   326.355622   353.816288   \n",
       "1      735599   507.762170   500.921801   500.216103   502.099511   \n",
       "2      735599   137.081288   138.691135   139.863963   161.207516   \n",
       "3      735599   596.929485   558.648326   551.420039   577.112455   \n",
       "4      735599   940.653140   890.501339   883.086316   934.119140   \n",
       "...       ...          ...          ...          ...          ...   \n",
       "26293  737059   842.534291   864.388124   838.336456   924.978028   \n",
       "26294  737059   255.545977   255.901804   250.816940   189.916929   \n",
       "26295  737059   992.550115   982.748408   971.987512  1006.865069   \n",
       "26296  737059  1505.262616  1477.136661  1464.714361  1644.013144   \n",
       "26297  737059   426.819145   446.953738   442.882205   397.987550   \n",
       "\n",
       "         lgb+ridge   lgb+linear  cat+prophet    cat+ridge   cat+linear  \\\n",
       "0       315.109073   320.781832   342.162795   308.871192   319.407647   \n",
       "1       502.309507   497.514037   505.468074   494.362517   498.780657   \n",
       "2       137.984103   133.714742   149.287358   134.114025   135.374514   \n",
       "3       555.405898   552.703990   576.479862   555.407912   550.887544   \n",
       "4       893.043397   882.242292   919.623890   886.321175   881.045421   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "26293   855.545431   839.722064   905.346443   841.013156   845.194035   \n",
       "26294   261.353502   241.161049   206.318911   244.486973   242.723551   \n",
       "26295   980.987703   965.731059  1031.550009   956.716929   972.733624   \n",
       "26296  1473.934875  1460.599837  1614.987267  1448.806267  1466.727788   \n",
       "26297   455.860021   440.576937   426.220568   434.771205   441.414946   \n",
       "\n",
       "           prophet  neuralprophet        ridge       linear        earth  \\\n",
       "0       346.560416     329.134521   283.272258   324.188332   360.672190   \n",
       "1       536.203586     458.008301   439.771734   503.081569   584.851806   \n",
       "2       143.412803     145.325577   120.885683   136.667932   138.401176   \n",
       "3       590.117165     552.571167   488.810710   555.531957   562.479785   \n",
       "4       939.673009     781.967285   771.701702   890.469201   944.584508   \n",
       "...            ...            ...          ...          ...          ...   \n",
       "26293   898.322121     669.418457   723.714830   846.597603   836.947940   \n",
       "26294   253.512355     227.158142   205.880079   241.048951   254.287204   \n",
       "26295  1039.635205     715.639648   832.192362   975.785339  1058.174059   \n",
       "26296  1526.908216     980.234009  1255.885410  1468.776593  2036.549357   \n",
       "26297   463.705207     423.052612   373.542094   441.128842   472.307718   \n",
       "\n",
       "           xgboost     lightgbm     catboost  \n",
       "0       343.250478   331.496226   326.054356  \n",
       "1       510.616623   514.576867   509.005936  \n",
       "2       148.959080   146.429925   144.894609  \n",
       "3       596.463984   584.653899   569.038022  \n",
       "4       907.745110   888.842963   882.892654  \n",
       "...            ...          ...          ...  \n",
       "26293   830.633157   814.597001   825.377960  \n",
       "26294   241.115195   234.409908   245.208674  \n",
       "26295   898.057926   900.745370   915.158956  \n",
       "26296  1397.507711  1419.978895  1401.414363  \n",
       "26297   415.256775   417.253845   421.897675  \n",
       "\n",
       "[26298 rows x 18 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "41b0fef6-3509-4d47-b452-18936dfb2487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>xgb+prophet</th>\n",
       "      <th>xgb+ridge</th>\n",
       "      <th>xgb+linear</th>\n",
       "      <th>lgb+prophet</th>\n",
       "      <th>lgb+ridge</th>\n",
       "      <th>lgb+linear</th>\n",
       "      <th>cat+prophet</th>\n",
       "      <th>cat+ridge</th>\n",
       "      <th>cat+linear</th>\n",
       "      <th>prophet</th>\n",
       "      <th>neuralprophet</th>\n",
       "      <th>ridge</th>\n",
       "      <th>linear</th>\n",
       "      <th>earth</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>737060</td>\n",
       "      <td>388.622891</td>\n",
       "      <td>404.007541</td>\n",
       "      <td>394.495622</td>\n",
       "      <td>403.493065</td>\n",
       "      <td>403.376884</td>\n",
       "      <td>374.658654</td>\n",
       "      <td>403.252346</td>\n",
       "      <td>400.671789</td>\n",
       "      <td>374.684934</td>\n",
       "      <td>402.296576</td>\n",
       "      <td>383.543457</td>\n",
       "      <td>359.817081</td>\n",
       "      <td>392.328390</td>\n",
       "      <td>417.102143</td>\n",
       "      <td>349.759459</td>\n",
       "      <td>373.034564</td>\n",
       "      <td>379.702323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>737060</td>\n",
       "      <td>610.880664</td>\n",
       "      <td>627.093405</td>\n",
       "      <td>620.436869</td>\n",
       "      <td>614.474655</td>\n",
       "      <td>634.724575</td>\n",
       "      <td>607.037695</td>\n",
       "      <td>612.147149</td>\n",
       "      <td>634.620835</td>\n",
       "      <td>605.461973</td>\n",
       "      <td>631.642099</td>\n",
       "      <td>551.044983</td>\n",
       "      <td>560.783324</td>\n",
       "      <td>620.092467</td>\n",
       "      <td>693.844970</td>\n",
       "      <td>549.911123</td>\n",
       "      <td>578.563016</td>\n",
       "      <td>589.688285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>737060</td>\n",
       "      <td>160.766967</td>\n",
       "      <td>164.596231</td>\n",
       "      <td>165.280088</td>\n",
       "      <td>185.453661</td>\n",
       "      <td>175.091284</td>\n",
       "      <td>157.180923</td>\n",
       "      <td>187.572802</td>\n",
       "      <td>173.993223</td>\n",
       "      <td>157.351444</td>\n",
       "      <td>169.992094</td>\n",
       "      <td>173.065674</td>\n",
       "      <td>153.552940</td>\n",
       "      <td>166.788184</td>\n",
       "      <td>181.642807</td>\n",
       "      <td>152.988182</td>\n",
       "      <td>165.419045</td>\n",
       "      <td>168.116727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>737060</td>\n",
       "      <td>676.305636</td>\n",
       "      <td>695.533558</td>\n",
       "      <td>675.371287</td>\n",
       "      <td>674.812040</td>\n",
       "      <td>705.677250</td>\n",
       "      <td>652.725247</td>\n",
       "      <td>673.996886</td>\n",
       "      <td>702.659724</td>\n",
       "      <td>652.858116</td>\n",
       "      <td>684.559821</td>\n",
       "      <td>648.783691</td>\n",
       "      <td>622.237644</td>\n",
       "      <td>675.674104</td>\n",
       "      <td>594.166542</td>\n",
       "      <td>622.932486</td>\n",
       "      <td>651.424049</td>\n",
       "      <td>668.232707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>737060</td>\n",
       "      <td>1082.042169</td>\n",
       "      <td>1092.660815</td>\n",
       "      <td>1079.916814</td>\n",
       "      <td>1102.000366</td>\n",
       "      <td>1106.175188</td>\n",
       "      <td>1060.723062</td>\n",
       "      <td>1085.545368</td>\n",
       "      <td>1100.671157</td>\n",
       "      <td>1062.136740</td>\n",
       "      <td>1092.875362</td>\n",
       "      <td>965.683716</td>\n",
       "      <td>969.065554</td>\n",
       "      <td>1084.980801</td>\n",
       "      <td>1142.564632</td>\n",
       "      <td>943.483740</td>\n",
       "      <td>994.810045</td>\n",
       "      <td>1033.224703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>737424</td>\n",
       "      <td>857.083187</td>\n",
       "      <td>857.300648</td>\n",
       "      <td>794.855865</td>\n",
       "      <td>915.517664</td>\n",
       "      <td>868.302635</td>\n",
       "      <td>795.861315</td>\n",
       "      <td>883.878613</td>\n",
       "      <td>857.782493</td>\n",
       "      <td>795.484833</td>\n",
       "      <td>917.085861</td>\n",
       "      <td>674.891174</td>\n",
       "      <td>742.331990</td>\n",
       "      <td>797.964290</td>\n",
       "      <td>830.606248</td>\n",
       "      <td>780.494827</td>\n",
       "      <td>809.243998</td>\n",
       "      <td>804.655222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>737424</td>\n",
       "      <td>253.030010</td>\n",
       "      <td>250.985476</td>\n",
       "      <td>238.694811</td>\n",
       "      <td>219.199028</td>\n",
       "      <td>261.876577</td>\n",
       "      <td>231.080258</td>\n",
       "      <td>204.891464</td>\n",
       "      <td>251.974263</td>\n",
       "      <td>230.437425</td>\n",
       "      <td>261.569590</td>\n",
       "      <td>232.082169</td>\n",
       "      <td>210.920805</td>\n",
       "      <td>225.576610</td>\n",
       "      <td>269.498973</td>\n",
       "      <td>229.878546</td>\n",
       "      <td>235.835586</td>\n",
       "      <td>239.325998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>737424</td>\n",
       "      <td>1020.716908</td>\n",
       "      <td>995.641620</td>\n",
       "      <td>937.084589</td>\n",
       "      <td>1052.911798</td>\n",
       "      <td>1009.447960</td>\n",
       "      <td>935.835037</td>\n",
       "      <td>1043.610433</td>\n",
       "      <td>989.976308</td>\n",
       "      <td>932.965624</td>\n",
       "      <td>1065.400121</td>\n",
       "      <td>730.457275</td>\n",
       "      <td>852.091174</td>\n",
       "      <td>925.788409</td>\n",
       "      <td>1133.560974</td>\n",
       "      <td>872.192736</td>\n",
       "      <td>899.608764</td>\n",
       "      <td>895.495983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>737424</td>\n",
       "      <td>1531.675071</td>\n",
       "      <td>1483.643573</td>\n",
       "      <td>1387.421869</td>\n",
       "      <td>1655.978947</td>\n",
       "      <td>1494.673068</td>\n",
       "      <td>1388.389257</td>\n",
       "      <td>1604.714050</td>\n",
       "      <td>1476.628593</td>\n",
       "      <td>1390.081381</td>\n",
       "      <td>1568.407214</td>\n",
       "      <td>1005.392944</td>\n",
       "      <td>1286.814471</td>\n",
       "      <td>1397.071983</td>\n",
       "      <td>2627.470943</td>\n",
       "      <td>1358.919453</td>\n",
       "      <td>1380.445879</td>\n",
       "      <td>1356.416644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>737424</td>\n",
       "      <td>417.476363</td>\n",
       "      <td>429.362182</td>\n",
       "      <td>402.277412</td>\n",
       "      <td>408.342599</td>\n",
       "      <td>448.207828</td>\n",
       "      <td>411.149808</td>\n",
       "      <td>405.553447</td>\n",
       "      <td>437.535467</td>\n",
       "      <td>412.664601</td>\n",
       "      <td>476.122783</td>\n",
       "      <td>428.916565</td>\n",
       "      <td>382.800781</td>\n",
       "      <td>417.035222</td>\n",
       "      <td>507.799360</td>\n",
       "      <td>378.966547</td>\n",
       "      <td>407.419732</td>\n",
       "      <td>404.913007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  xgb+prophet    xgb+ridge   xgb+linear  lgb+prophet    lgb+ridge  \\\n",
       "0     737060   388.622891   404.007541   394.495622   403.493065   403.376884   \n",
       "1     737060   610.880664   627.093405   620.436869   614.474655   634.724575   \n",
       "2     737060   160.766967   164.596231   165.280088   185.453661   175.091284   \n",
       "3     737060   676.305636   695.533558   675.371287   674.812040   705.677250   \n",
       "4     737060  1082.042169  1092.660815  1079.916814  1102.000366  1106.175188   \n",
       "...      ...          ...          ...          ...          ...          ...   \n",
       "6565  737424   857.083187   857.300648   794.855865   915.517664   868.302635   \n",
       "6566  737424   253.030010   250.985476   238.694811   219.199028   261.876577   \n",
       "6567  737424  1020.716908   995.641620   937.084589  1052.911798  1009.447960   \n",
       "6568  737424  1531.675071  1483.643573  1387.421869  1655.978947  1494.673068   \n",
       "6569  737424   417.476363   429.362182   402.277412   408.342599   448.207828   \n",
       "\n",
       "       lgb+linear  cat+prophet    cat+ridge   cat+linear      prophet  \\\n",
       "0      374.658654   403.252346   400.671789   374.684934   402.296576   \n",
       "1      607.037695   612.147149   634.620835   605.461973   631.642099   \n",
       "2      157.180923   187.572802   173.993223   157.351444   169.992094   \n",
       "3      652.725247   673.996886   702.659724   652.858116   684.559821   \n",
       "4     1060.723062  1085.545368  1100.671157  1062.136740  1092.875362   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "6565   795.861315   883.878613   857.782493   795.484833   917.085861   \n",
       "6566   231.080258   204.891464   251.974263   230.437425   261.569590   \n",
       "6567   935.835037  1043.610433   989.976308   932.965624  1065.400121   \n",
       "6568  1388.389257  1604.714050  1476.628593  1390.081381  1568.407214   \n",
       "6569   411.149808   405.553447   437.535467   412.664601   476.122783   \n",
       "\n",
       "      neuralprophet        ridge       linear        earth      xgboost  \\\n",
       "0        383.543457   359.817081   392.328390   417.102143   349.759459   \n",
       "1        551.044983   560.783324   620.092467   693.844970   549.911123   \n",
       "2        173.065674   153.552940   166.788184   181.642807   152.988182   \n",
       "3        648.783691   622.237644   675.674104   594.166542   622.932486   \n",
       "4        965.683716   969.065554  1084.980801  1142.564632   943.483740   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "6565     674.891174   742.331990   797.964290   830.606248   780.494827   \n",
       "6566     232.082169   210.920805   225.576610   269.498973   229.878546   \n",
       "6567     730.457275   852.091174   925.788409  1133.560974   872.192736   \n",
       "6568    1005.392944  1286.814471  1397.071983  2627.470943  1358.919453   \n",
       "6569     428.916565   382.800781   417.035222   507.799360   378.966547   \n",
       "\n",
       "         lightgbm     catboost  \n",
       "0      373.034564   379.702323  \n",
       "1      578.563016   589.688285  \n",
       "2      165.419045   168.116727  \n",
       "3      651.424049   668.232707  \n",
       "4      994.810045  1033.224703  \n",
       "...           ...          ...  \n",
       "6565   809.243998   804.655222  \n",
       "6566   235.835586   239.325998  \n",
       "6567   899.608764   895.495983  \n",
       "6568  1380.445879  1356.416644  \n",
       "6569   407.419732   404.913007  \n",
       "\n",
       "[6570 rows x 18 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "74e10c70-9e61-436b-ae41-3b6ccacd5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X.columns:\n",
    "    if col not in X_test.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "27698b74-5d49-480c-bcee-56e055378030",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "77d7f19f-ea3f-4047-9cef-6b034d7d252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_tv_preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399b3ce-d536-42d7-93e6-33feb155d432",
   "metadata": {},
   "source": [
    "Naive SMAPE was 4.152715109788985"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8cb02-37f7-4962-9794-2b767d84ecdd",
   "metadata": {},
   "source": [
    "Below had been 4.384919101043075 with XGBoost included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3b2b186f-0a9f-4747-bc91-fce6fef552c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.34354975241337"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMAPE(y_pred=lasso_tv_preds, y_true=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb03e02-064d-4408-a744-09377b3fea44",
   "metadata": {},
   "source": [
    "Below had been 4.27663983822924 with XGBoost included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d6327110-14ab-44b1-a2e7-45d06338c4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.138474820600068"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X,y)\n",
    "ridge_test_preds = ridge.predict(X_test)\n",
    "ridge_tv_preds = ridge.predict(X)\n",
    "SMAPE(y_pred=ridge_tv_preds, y_true=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "668396b5-2e10-47d5-a0c4-2492c68dc01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "73cb734a-e9fd-4782-a9cc-a93577f311fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'num_sold'] = ridge_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "746817e6-0989-4166-8311-63b95c95c654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26298</td>\n",
       "      <td>394.012434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26299</td>\n",
       "      <td>617.972594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26300</td>\n",
       "      <td>166.368232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26301</td>\n",
       "      <td>676.177680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26302</td>\n",
       "      <td>1085.404238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id     num_sold\n",
       "0   26298   394.012434\n",
       "1   26299   617.972594\n",
       "2   26300   166.368232\n",
       "3   26301   676.177680\n",
       "4   26302  1085.404238"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6f0b610a-3c6b-4036-9eac-f840d5164db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(subpath/f\"20220127_specialmix_forecasts+Earth-Big3-GBMs-selective_residuals-some_passthru-ridge_preds.csv\", index=False)\n",
    "# sample_df.to_csv(subpath/f\"{wandb_config['name']}_3level-X_orig+KMeans8+synth-GBM-stack_ensemble_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ee2f8d52-714e-48a3-a57f-75a1a34674a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df = pd.read_csv(subpath/'LB-4.40223__20220124_forecasts_lasso_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4c6a098b-2880-4cff-ab7a-f003aa627100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26298</td>\n",
       "      <td>394.012434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26299</td>\n",
       "      <td>617.972594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26300</td>\n",
       "      <td>166.368232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26301</td>\n",
       "      <td>676.177680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26302</td>\n",
       "      <td>1085.404238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>32863</td>\n",
       "      <td>798.566362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>32864</td>\n",
       "      <td>216.971152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>32865</td>\n",
       "      <td>923.824266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>32866</td>\n",
       "      <td>1413.388152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>32867</td>\n",
       "      <td>413.079709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id     num_sold\n",
       "0      26298   394.012434\n",
       "1      26299   617.972594\n",
       "2      26300   166.368232\n",
       "3      26301   676.177680\n",
       "4      26302  1085.404238\n",
       "...      ...          ...\n",
       "6565   32863   798.566362\n",
       "6566   32864   216.971152\n",
       "6567   32865   923.824266\n",
       "6568   32866  1413.388152\n",
       "6569   32867   413.079709\n",
       "\n",
       "[6570 rows x 2 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d92fe031-e6f4-4843-94f5-7f76f257d5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26298</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26299</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26300</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26301</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26302</td>\n",
       "      <td>1085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>32863</td>\n",
       "      <td>799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>32864</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>32865</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>32866</td>\n",
       "      <td>1413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>32867</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id  num_sold\n",
       "0      26298       394\n",
       "1      26299       618\n",
       "2      26300       166\n",
       "3      26301       676\n",
       "4      26302      1085\n",
       "...      ...       ...\n",
       "6565   32863       799\n",
       "6566   32864       217\n",
       "6567   32865       924\n",
       "6568   32866      1413\n",
       "6569   32867       413\n",
       "\n",
       "[6570 rows x 2 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['num_sold'] = sample_df['num_sold'].apply(round)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6a93a0c7-02fc-4bd1-8fa1-70d21e88b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(subpath/'20220127_specialmix_forecasts+Earth-Big3-GBMs-selective_residuals-some_passthru-ridge_preds_rounded.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
