{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e815d3-d755-4fa2-85a5-ea4df4948fcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# widedeep_20211120\n",
    "I want to start playing around with some NN architectures. Eventually, I want to try some straight PyTorch, but for starters, I'll use `widedeep`. As a scaler, I'll use RankGauss; I won't (yet) do any feature reduction or selection. I also won't (yet) use `cleanlab`, though I may try it in the future, either via the wrapper they suggest or via `skorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843e3531-f950-4701-9330-07960ae9a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "COLAB = False # will trigger manual installation of packages\n",
    "USE_GPU = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a61aa18-6ef3-41d1-bb75-08d1f766dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "import gc; gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d84988-5ddb-40d9-bc12-826012acb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"saint_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf1e65-6447-47b9-88f2-8d02bbc29af0",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee16c791-548b-4616-9c82-ea76001e4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler #StandardScaler #, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n",
    "\n",
    "# from BorutaShap import BorutaShap\n",
    "from gauss_rank_scaler import GaussRankScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e9bc11-cfbd-4526-9bbb-95308c52a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAINT import TabAttention # from the official SAINT implementation as of 20211118, https://github.com/somepago/saint/blob/main/models/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b139207-5ea6-464d-922e-e4d0a398062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "from pytorch_widedeep.metrics import Accuracy\n",
    "from torchmetrics import AUROC\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55072fd2-dcc0-4169-a6f8-54d9b76ab432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71932e1-2e32-4474-acbc-3cbe63ce993e",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880b9f33-b517-40ef-a3e6-049dcc52a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/nov2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "#     datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')  \n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath]:\n",
    "        pth.mkdir(exist_ok=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1905a9de-cd7c-4a73-a39e-b42a8297785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "892c1dbb-a47d-4838-a1ed-074b96a362f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60cd4c5f-d848-4e4c-9fa6-097961ac8cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'X_orig.feather'),\n",
    "    'target_source': str(datapath/'y_orig.joblib'),\n",
    "    'test_source': str(datapath/'X_test_orig-no_scaling.feather'),\n",
    "    'scaler': str(GaussRankScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "X = pd.read_feather(dataset_params['train_source'])# load(dataset_params['train_source'])\n",
    "y = load(dataset_params['target_source'])\n",
    "X_test = pd.read_feather(dataset_params['test_source']) #load(dataset_params['test_source'])\n",
    "\n",
    "dataset_params['feature_count'] = X.shape[1]\n",
    "dataset_params['instance_count'] = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df908c8-d40b-4882-b5b9-f64de46c128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 114.44 Mb (75.0% reduction)\n",
      "Mem. usage decreased to 103.00 Mb (75.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# decrease memory footprint\n",
    "X = reduce_memory_usage(X)\n",
    "X_test = reduce_memory_usage(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d647e8af-e065-4ea8-aa6c-5d158791904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "exmodel_config = {\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "#     'random_state': SEED,\n",
    "#     'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "#     'subsample': 1,\n",
    "    'cross_val_strategy': None, # None for holdout, or the relevant sklearn class\n",
    "    'kfolds': 1, # if 1, that means just doing holdout\n",
    "    'test_size': 0.2,\n",
    "    **dataset_params\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5402d98-7bcd-4e46-b85d-6b0129ca6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['widedeep', 'deeplearning'],\n",
    "    'notes': \"Trying a variety of widedeep models, to see if I can get any working properly.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b9135-660e-4cbf-b769-7e75127f0cc3",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "Inspired a bit by Laurent Pourchot's Aug2021 Tabular Playground entry, I'm going to try to generate two versions of the dataset: a categorical one, using bins, and then (for now) a GaussRankScaled one. In the future, I might add further variations, e.g. with feature reduction via PCA and perhaps also UMAP and also denoising; I might also try other normalizations, e.g. Quantile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ed94a-3b27-412b-804e-587094e1d27f",
   "metadata": {},
   "source": [
    "## Binning (Generating wide cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abaf19a7-159f-466f-a5c2-d3fa57443af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h/t Laurent Pourchot https://www.kaggle.com/pourchot/in-python-tabular-denoising-residual-network/\n",
    "\n",
    "# 100 bins for the bins head of the NN (i.e. percentiles):\n",
    "X_bins = np.zeros((X.shape[0],X.shape[1])) # he used all available data for the first tuple entry, but I'll start like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cea9fa43-df35-4e21-8987-54b74abe65b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9d371e0-4c69-475f-b10d-45f5e7bf44cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X.shape[1]): # assumes X is a pd.DataFrame\n",
    "    X_bins[:,i] = pd.qcut(X.iloc[:,i],X.shape[1],labels=False,duplicates = 'drop')\n",
    "# blabeled = X_bins[:X.shape[0],:]\n",
    "# bunlabeled = X_ins[X.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a90e547-e32e-4bc2-8f9f-9e28c7afe183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[52., 70., 49., ..., 37., 64., 88.],\n",
       "       [56., 35., 33., ..., 48., 10., 28.],\n",
       "       [28., 31., 70., ..., 40., 79., 85.],\n",
       "       ...,\n",
       "       [94., 11., 31., ..., 16., 64., 46.],\n",
       "       [80., 75., 89., ..., 58., 51., 17.],\n",
       "       [64., 67., 56., ..., 37., 44., 41.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "792c729c-8943-4783-a0f9-66379eb0a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bins = X_bins.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2384fe26-50a9-427b-b42c-a03d26d5b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bins = pd.DataFrame(X_bins, index=X.index, columns=[f'rkd_f{col}' for col in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c60393d7-d6e7-468b-8892-044d54061b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rkd_f0</th>\n",
       "      <th>rkd_f1</th>\n",
       "      <th>rkd_f2</th>\n",
       "      <th>rkd_f3</th>\n",
       "      <th>rkd_f4</th>\n",
       "      <th>rkd_f5</th>\n",
       "      <th>rkd_f6</th>\n",
       "      <th>rkd_f7</th>\n",
       "      <th>rkd_f8</th>\n",
       "      <th>rkd_f9</th>\n",
       "      <th>...</th>\n",
       "      <th>rkd_f90</th>\n",
       "      <th>rkd_f91</th>\n",
       "      <th>rkd_f92</th>\n",
       "      <th>rkd_f93</th>\n",
       "      <th>rkd_f94</th>\n",
       "      <th>rkd_f95</th>\n",
       "      <th>rkd_f96</th>\n",
       "      <th>rkd_f97</th>\n",
       "      <th>rkd_f98</th>\n",
       "      <th>rkd_f99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>49</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "      <td>23</td>\n",
       "      <td>68</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "      <td>83</td>\n",
       "      <td>37</td>\n",
       "      <td>64</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>69</td>\n",
       "      <td>95</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>24</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>70</td>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "      <td>60</td>\n",
       "      <td>94</td>\n",
       "      <td>75</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>79</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>89</td>\n",
       "      <td>34</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>73</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>79</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rkd_f0  rkd_f1  rkd_f2  rkd_f3  rkd_f4  rkd_f5  rkd_f6  rkd_f7  rkd_f8  \\\n",
       "0      52      70      49      60      61      23      68      44      42   \n",
       "1      56      35      33      63      69      95      22       1      88   \n",
       "2      28      31      70      41      29      60      94      75      36   \n",
       "3      11       7      89      34      18      30      48      87      88   \n",
       "4      15      73      55      40      24      41      78       3      66   \n",
       "\n",
       "   rkd_f9  ...  rkd_f90  rkd_f91  rkd_f92  rkd_f93  rkd_f94  rkd_f95  rkd_f96  \\\n",
       "0      20  ...       18       22       22        6       44       52       83   \n",
       "1      36  ...       90       70       24       94       87       97        2   \n",
       "2      48  ...       86       94       58       30       76       10        0   \n",
       "3      83  ...        6       70       11        7       49       35       10   \n",
       "4      33  ...       20       40        4       86       79       32       20   \n",
       "\n",
       "   rkd_f97  rkd_f98  rkd_f99  \n",
       "0       37       64       88  \n",
       "1       48       10       28  \n",
       "2       40       79       85  \n",
       "3       37       39       41  \n",
       "4       19       13       58  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e9c95-cc52-4f5f-9c83-02a0c8fa1a59",
   "metadata": {},
   "source": [
    "## Normalizing (Preprocessing Deep Cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56993a51-da51-40d0-9e7f-756534220098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "scaler = GaussRankScaler()\n",
    "X_gauss = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d22bfb6d-6a53-4341-aa21-a9679754a76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106628</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>132.7500</td>\n",
       "      <td>3.183594</td>\n",
       "      <td>0.081970</td>\n",
       "      <td>1.188477</td>\n",
       "      <td>3.732422</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.099609</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>-0.011719</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>1.978516</td>\n",
       "      <td>0.085999</td>\n",
       "      <td>0.240479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.673828</td>\n",
       "      <td>76.5625</td>\n",
       "      <td>3.378906</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>1.275391</td>\n",
       "      <td>-0.471436</td>\n",
       "      <td>4.546875</td>\n",
       "      <td>0.037720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135864</td>\n",
       "      <td>3.460938</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.124878</td>\n",
       "      <td>0.154053</td>\n",
       "      <td>0.606934</td>\n",
       "      <td>-0.267822</td>\n",
       "      <td>2.578125</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.024719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036316</td>\n",
       "      <td>1.497070</td>\n",
       "      <td>233.5000</td>\n",
       "      <td>2.195312</td>\n",
       "      <td>0.026917</td>\n",
       "      <td>3.126953</td>\n",
       "      <td>5.058594</td>\n",
       "      <td>3.849609</td>\n",
       "      <td>1.801758</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>4.882812</td>\n",
       "      <td>0.085205</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.116089</td>\n",
       "      <td>-0.001689</td>\n",
       "      <td>-0.520020</td>\n",
       "      <td>2.140625</td>\n",
       "      <td>0.124451</td>\n",
       "      <td>0.148193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014076</td>\n",
       "      <td>0.245972</td>\n",
       "      <td>780.0000</td>\n",
       "      <td>1.890625</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>2.697266</td>\n",
       "      <td>4.515625</td>\n",
       "      <td>4.503906</td>\n",
       "      <td>0.123474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015350</td>\n",
       "      <td>3.474609</td>\n",
       "      <td>-0.017105</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>0.040009</td>\n",
       "      <td>0.044861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003260</td>\n",
       "      <td>3.714844</td>\n",
       "      <td>156.1250</td>\n",
       "      <td>2.148438</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>2.097656</td>\n",
       "      <td>4.156250</td>\n",
       "      <td>-0.038239</td>\n",
       "      <td>3.371094</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013779</td>\n",
       "      <td>1.910156</td>\n",
       "      <td>-0.042938</td>\n",
       "      <td>0.105591</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>0.037506</td>\n",
       "      <td>1.043945</td>\n",
       "      <td>1.075195</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.072815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0        f1        f2        f3        f4        f5        f6  \\\n",
       "0  0.106628  3.593750  132.7500  3.183594  0.081970  1.188477  3.732422   \n",
       "1  0.125000  1.673828   76.5625  3.378906  0.099426  5.093750  1.275391   \n",
       "2  0.036316  1.497070  233.5000  2.195312  0.026917  3.126953  5.058594   \n",
       "3 -0.014076  0.245972  780.0000  1.890625  0.006947  1.531250  2.697266   \n",
       "4 -0.003260  3.714844  156.1250  2.148438  0.018280  2.097656  4.156250   \n",
       "\n",
       "         f7        f8        f9  ...       f90       f91       f92       f93  \\\n",
       "0  2.265625  2.099609  0.012329  ...  0.010742  1.098633  0.013329 -0.011719   \n",
       "1 -0.471436  4.546875  0.037720  ...  0.135864  3.460938  0.017059  0.124878   \n",
       "2  3.849609  1.801758  0.057007  ...  0.117310  4.882812  0.085205  0.032410   \n",
       "3  4.515625  4.503906  0.123474  ... -0.015350  3.474609 -0.017105 -0.008102   \n",
       "4 -0.038239  3.371094  0.034180  ...  0.013779  1.910156 -0.042938  0.105591   \n",
       "\n",
       "        f94       f95       f96       f97       f98       f99  \n",
       "0  0.052765  0.065430  4.210938  1.978516  0.085999  0.240479  \n",
       "1  0.154053  0.606934 -0.267822  2.578125 -0.020874  0.024719  \n",
       "2  0.116089 -0.001689 -0.520020  2.140625  0.124451  0.148193  \n",
       "3  0.062012  0.041199  0.511719  1.968750  0.040009  0.044861  \n",
       "4  0.125122  0.037506  1.043945  1.075195 -0.012817  0.072815  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d0e852b-42e8-459d-b721-8b740f990ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X_gauss = pd.DataFrame(X_gauss, columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba5b34c0-9cd8-4c31-8f2b-407479812bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503864</td>\n",
       "      <td>1.170148</td>\n",
       "      <td>0.497627</td>\n",
       "      <td>1.061322</td>\n",
       "      <td>0.497063</td>\n",
       "      <td>0.697261</td>\n",
       "      <td>1.178046</td>\n",
       "      <td>0.918892</td>\n",
       "      <td>0.898030</td>\n",
       "      <td>0.264610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313791</td>\n",
       "      <td>0.697968</td>\n",
       "      <td>0.228935</td>\n",
       "      <td>-0.745756</td>\n",
       "      <td>0.410898</td>\n",
       "      <td>0.511767</td>\n",
       "      <td>1.301817</td>\n",
       "      <td>0.886265</td>\n",
       "      <td>0.491337</td>\n",
       "      <td>0.715292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.532606</td>\n",
       "      <td>0.817177</td>\n",
       "      <td>0.414955</td>\n",
       "      <td>1.099940</td>\n",
       "      <td>0.523378</td>\n",
       "      <td>1.430099</td>\n",
       "      <td>0.707201</td>\n",
       "      <td>-1.001742</td>\n",
       "      <td>1.361183</td>\n",
       "      <td>0.395362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712643</td>\n",
       "      <td>1.133297</td>\n",
       "      <td>0.258559</td>\n",
       "      <td>0.941823</td>\n",
       "      <td>0.550487</td>\n",
       "      <td>0.937882</td>\n",
       "      <td>-0.799881</td>\n",
       "      <td>0.975537</td>\n",
       "      <td>-0.595291</td>\n",
       "      <td>0.375001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.364383</td>\n",
       "      <td>0.777697</td>\n",
       "      <td>0.581712</td>\n",
       "      <td>0.902238</td>\n",
       "      <td>0.358373</td>\n",
       "      <td>1.046707</td>\n",
       "      <td>1.450572</td>\n",
       "      <td>1.213861</td>\n",
       "      <td>0.833888</td>\n",
       "      <td>0.448396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684576</td>\n",
       "      <td>1.420673</td>\n",
       "      <td>0.444982</td>\n",
       "      <td>0.597667</td>\n",
       "      <td>0.514780</td>\n",
       "      <td>-0.345367</td>\n",
       "      <td>-1.076664</td>\n",
       "      <td>0.911155</td>\n",
       "      <td>0.547897</td>\n",
       "      <td>0.627946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.552709</td>\n",
       "      <td>0.361766</td>\n",
       "      <td>0.800397</td>\n",
       "      <td>0.847488</td>\n",
       "      <td>0.205250</td>\n",
       "      <td>0.767239</td>\n",
       "      <td>0.975344</td>\n",
       "      <td>1.338971</td>\n",
       "      <td>1.352906</td>\n",
       "      <td>0.554931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.698743</td>\n",
       "      <td>1.136185</td>\n",
       "      <td>-0.622266</td>\n",
       "      <td>-0.671410</td>\n",
       "      <td>0.436283</td>\n",
       "      <td>0.439042</td>\n",
       "      <td>0.506585</td>\n",
       "      <td>0.883681</td>\n",
       "      <td>0.397615</td>\n",
       "      <td>0.451311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357576</td>\n",
       "      <td>1.199284</td>\n",
       "      <td>0.516476</td>\n",
       "      <td>0.895768</td>\n",
       "      <td>0.313432</td>\n",
       "      <td>0.890002</td>\n",
       "      <td>1.273778</td>\n",
       "      <td>-0.348193</td>\n",
       "      <td>1.110106</td>\n",
       "      <td>0.385873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349392</td>\n",
       "      <td>0.873186</td>\n",
       "      <td>-0.772866</td>\n",
       "      <td>0.873466</td>\n",
       "      <td>0.528289</td>\n",
       "      <td>0.427691</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.684102</td>\n",
       "      <td>-0.528382</td>\n",
       "      <td>0.520949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0        f1        f2        f3        f4        f5        f6  \\\n",
       "0  0.503864  1.170148  0.497627  1.061322  0.497063  0.697261  1.178046   \n",
       "1  0.532606  0.817177  0.414955  1.099940  0.523378  1.430099  0.707201   \n",
       "2  0.364383  0.777697  0.581712  0.902238  0.358373  1.046707  1.450572   \n",
       "3 -0.552709  0.361766  0.800397  0.847488  0.205250  0.767239  0.975344   \n",
       "4 -0.357576  1.199284  0.516476  0.895768  0.313432  0.890002  1.273778   \n",
       "\n",
       "         f7        f8        f9  ...       f90       f91       f92       f93  \\\n",
       "0  0.918892  0.898030  0.264610  ...  0.313791  0.697968  0.228935 -0.745756   \n",
       "1 -1.001742  1.361183  0.395362  ...  0.712643  1.133297  0.258559  0.941823   \n",
       "2  1.213861  0.833888  0.448396  ...  0.684576  1.420673  0.444982  0.597667   \n",
       "3  1.338971  1.352906  0.554931  ... -0.698743  1.136185 -0.622266 -0.671410   \n",
       "4 -0.348193  1.110106  0.385873  ...  0.349392  0.873186 -0.772866  0.873466   \n",
       "\n",
       "        f94       f95       f96       f97       f98       f99  \n",
       "0  0.410898  0.511767  1.301817  0.886265  0.491337  0.715292  \n",
       "1  0.550487  0.937882 -0.799881  0.975537 -0.595291  0.375001  \n",
       "2  0.514780 -0.345367 -1.076664  0.911155  0.547897  0.627946  \n",
       "3  0.436283  0.439042  0.506585  0.883681  0.397615  0.451311  \n",
       "4  0.528289  0.427691  0.679960  0.684102 -0.528382  0.520949  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gauss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ca6ea-9581-43c1-ba23-7cb1c5a1e430",
   "metadata": {},
   "source": [
    "## Preparing Data for WideDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9048c9b1-8bd1-427b-a06f-f951532bd5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X_pre = X_gauss.join(X_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "399bfc3a-85a1-4aea-b035-6d6577e3fa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>rkd_f90</th>\n",
       "      <th>rkd_f91</th>\n",
       "      <th>rkd_f92</th>\n",
       "      <th>rkd_f93</th>\n",
       "      <th>rkd_f94</th>\n",
       "      <th>rkd_f95</th>\n",
       "      <th>rkd_f96</th>\n",
       "      <th>rkd_f97</th>\n",
       "      <th>rkd_f98</th>\n",
       "      <th>rkd_f99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503864</td>\n",
       "      <td>1.170148</td>\n",
       "      <td>0.497627</td>\n",
       "      <td>1.061322</td>\n",
       "      <td>0.497063</td>\n",
       "      <td>0.697261</td>\n",
       "      <td>1.178046</td>\n",
       "      <td>0.918892</td>\n",
       "      <td>0.898030</td>\n",
       "      <td>0.264610</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "      <td>83</td>\n",
       "      <td>37</td>\n",
       "      <td>64</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.532606</td>\n",
       "      <td>0.817177</td>\n",
       "      <td>0.414955</td>\n",
       "      <td>1.099940</td>\n",
       "      <td>0.523378</td>\n",
       "      <td>1.430099</td>\n",
       "      <td>0.707201</td>\n",
       "      <td>-1.001742</td>\n",
       "      <td>1.361183</td>\n",
       "      <td>0.395362</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>24</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.364383</td>\n",
       "      <td>0.777697</td>\n",
       "      <td>0.581712</td>\n",
       "      <td>0.902238</td>\n",
       "      <td>0.358373</td>\n",
       "      <td>1.046707</td>\n",
       "      <td>1.450572</td>\n",
       "      <td>1.213861</td>\n",
       "      <td>0.833888</td>\n",
       "      <td>0.448396</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>79</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.552709</td>\n",
       "      <td>0.361766</td>\n",
       "      <td>0.800397</td>\n",
       "      <td>0.847488</td>\n",
       "      <td>0.205250</td>\n",
       "      <td>0.767239</td>\n",
       "      <td>0.975344</td>\n",
       "      <td>1.338971</td>\n",
       "      <td>1.352906</td>\n",
       "      <td>0.554931</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357576</td>\n",
       "      <td>1.199284</td>\n",
       "      <td>0.516476</td>\n",
       "      <td>0.895768</td>\n",
       "      <td>0.313432</td>\n",
       "      <td>0.890002</td>\n",
       "      <td>1.273778</td>\n",
       "      <td>-0.348193</td>\n",
       "      <td>1.110106</td>\n",
       "      <td>0.385873</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>79</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0        f1        f2        f3        f4        f5        f6  \\\n",
       "0  0.503864  1.170148  0.497627  1.061322  0.497063  0.697261  1.178046   \n",
       "1  0.532606  0.817177  0.414955  1.099940  0.523378  1.430099  0.707201   \n",
       "2  0.364383  0.777697  0.581712  0.902238  0.358373  1.046707  1.450572   \n",
       "3 -0.552709  0.361766  0.800397  0.847488  0.205250  0.767239  0.975344   \n",
       "4 -0.357576  1.199284  0.516476  0.895768  0.313432  0.890002  1.273778   \n",
       "\n",
       "         f7        f8        f9  ...  rkd_f90  rkd_f91  rkd_f92  rkd_f93  \\\n",
       "0  0.918892  0.898030  0.264610  ...       18       22       22        6   \n",
       "1 -1.001742  1.361183  0.395362  ...       90       70       24       94   \n",
       "2  1.213861  0.833888  0.448396  ...       86       94       58       30   \n",
       "3  1.338971  1.352906  0.554931  ...        6       70       11        7   \n",
       "4 -0.348193  1.110106  0.385873  ...       20       40        4       86   \n",
       "\n",
       "   rkd_f94  rkd_f95  rkd_f96  rkd_f97  rkd_f98  rkd_f99  \n",
       "0       44       52       83       37       64       88  \n",
       "1       87       97        2       48       10       28  \n",
       "2       76       10        0       40       79       85  \n",
       "3       49       35       10       37       39       41  \n",
       "4       79       32       20       19       13       58  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "900f0618-c6f2-4b25-8718-6a907e2c93b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "cont_cols = X_pre.iloc[:,:100].columns\n",
    "wide_cols = X_pre.iloc[:, 100:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ef65f9b-1558-499d-8cd5-9d3d2380fc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n",
       "       'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n",
       "       'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n",
       "       'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40',\n",
       "       'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n",
       "       'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n",
       "       'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n",
       "       'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n",
       "       'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n",
       "       'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86942c-fbce-4fb7-9a29-f94d16e325df",
   "metadata": {},
   "source": [
    "- So here, I've stored the column names in a list-like object (i.e. a pandas `Index`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d7dcf9d-b3d8-4b1e-9263-b635a5b6f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "X_wide = wide_preprocessor.fit_transform(X_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "886ff48e-4b2a-439d-9426-95320d20d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:179: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n"
     ]
    }
   ],
   "source": [
    "# tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=False,embed_cols=wide_cols)\n",
    "tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=True,embed_cols=wide_cols)\n",
    "X_tab = tab_preprocessor.fit_transform(X_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21ddbb81-b823-487f-ace2-a268d39c5f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600000, 100), (600000, 200))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide.shape, X_tab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e4ade-b953-49c9-b06d-1999ef207461",
   "metadata": {},
   "source": [
    "- So at this point, I've created two parallel versions of the dataset -- both contain all the instances, but the wide version contains only the categorical (binned) values, and the other contains both; the `embed_cols=` kwarg specifies that an embedding should be generated on the basis of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f436a532-9346-46f4-97a4-bbdd7c9e35d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_tab[10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260b075-733a-439b-8c8b-8cfebb419ce1",
   "metadata": {},
   "source": [
    "- Note that the ones bound for embedding go first. \n",
    "- Not sure about the values they've generated -- it's possible that the WideDeep library's preprocessing is inappropriate post-binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b715656-27ac-4c63-9657-0a5c3602c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed6b9686-7e7d-4eba-9ffd-047ea6aea4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchinfo.summary(wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0576ee77-078b-45f4-afa9-1260f9cbd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pre.loc[:, X_bins.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52ea3309-3abd-4dda-b3d8-f1b0264168d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide_train = np.array(X_wide_train)\n",
    "# X_tab_train = np.array(X_tab_train)\n",
    "# X_wide_valid = np.array(X_wide_valid)\n",
    "# X_tab_valid = np.array(X_tab_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ca6d90a-41f2-4b5c-bd6d-3d6b53d2ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeptabular = TabMlp(continuous_cols=cont_cols, column_idx=tab_preprocessor.column_idx)\n",
    "deeptabular = SAINT(continuous_cols=cont_cols, column_idx=tab_preprocessor.column_idx,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4380cbfb-88f8-4e87-9b13-f90cdf60683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7b554a1-17f2-4793-83d3-1cb9c4879e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "WideDeep                                                --\n",
       "├─Wide: 1-1                                             --\n",
       "│    └─Embedding: 2-1                                   10,001\n",
       "├─Sequential: 1-2                                       --\n",
       "│    └─SAINT: 2-2                                       --\n",
       "│    │    └─CatAndContEmbeddings: 3-1                   6,400\n",
       "│    │    └─Sequential: 3-2                             245,842,752\n",
       "│    │    └─MLP: 3-3                                    122,899,200\n",
       "│    └─Linear: 2-3                                      6,401\n",
       "================================================================================\n",
       "Total params: 368,764,754\n",
       "Trainable params: 368,764,754\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0dfd77e6-24e3-4d6c-9699-0448960c82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_deep_train, X_deep_valid\n",
    "X_wide_train, X_wide_valid, y_train, y_valid = train_test_split(X_wide, y, test_size=0.2, random_state=42)\n",
    "X_tab_train, X_tab_valid, _, _ = train_test_split(X_tab, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0501c3e7-e501-4479-973b-490480d1f13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((480000, 100), (480000, 200))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_train.shape, X_tab_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2ce37f7-1ff7-4237-a6bf-7360488294e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_wide_train = pd.DataFrame(X_wide_train, columns=wide_cols)\n",
    "# X_wide_valid = pd.DataFrame(X_wide_valid, columns=wide_cols)\n",
    "type(y_train)\n",
    "# type(X_wide_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e1ac1e4-6ce8-4a7b-9d20-76f276a401fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "147cc654-0d81-461f-9aa2-7d6488332009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 10000/10000 [23:26<00:00,  7.11it/s, loss=2.01, metrics={'acc': 0.5144}]\n",
      "epoch 2: 100%|██████████| 10000/10000 [23:11<00:00,  7.18it/s, loss=1.29, metrics={'acc': 0.5568}]\n",
      "epoch 3: 100%|██████████| 10000/10000 [22:04<00:00,  7.55it/s, loss=0.843, metrics={'acc': 0.6108}]\n",
      "epoch 4: 100%|██████████| 10000/10000 [21:56<00:00,  7.60it/s, loss=0.654, metrics={'acc': 0.6656}]\n",
      "epoch 5: 100%|██████████| 10000/10000 [21:48<00:00,  7.65it/s, loss=0.613, metrics={'acc': 0.6947}]\n",
      "epoch 6: 100%|██████████| 10000/10000 [22:10<00:00,  7.51it/s, loss=0.607, metrics={'acc': 0.7019}]\n",
      "epoch 7: 100%|██████████| 10000/10000 [22:26<00:00,  7.43it/s, loss=0.607, metrics={'acc': 0.702}]\n",
      "epoch 8: 100%|██████████| 10000/10000 [22:50<00:00,  7.29it/s, loss=0.607, metrics={'acc': 0.7023}]\n",
      "epoch 9: 100%|██████████| 10000/10000 [23:04<00:00,  7.22it/s, loss=0.607, metrics={'acc': 0.7023}]\n",
      "epoch 10: 100%|██████████| 10000/10000 [22:07<00:00,  7.53it/s, loss=0.607, metrics={'acc': 0.7023}]\n",
      "epoch 11: 100%|██████████| 10000/10000 [21:39<00:00,  7.69it/s, loss=0.607, metrics={'acc': 0.7019}]\n",
      "epoch 12: 100%|██████████| 10000/10000 [21:49<00:00,  7.64it/s, loss=0.607, metrics={'acc': 0.702}]\n",
      "epoch 13: 100%|██████████| 10000/10000 [23:27<00:00,  7.10it/s, loss=0.607, metrics={'acc': 0.7018}]\n",
      "epoch 14: 100%|██████████| 10000/10000 [23:09<00:00,  7.20it/s, loss=0.607, metrics={'acc': 0.7022}]\n",
      "epoch 15: 100%|██████████| 10000/10000 [23:03<00:00,  7.23it/s, loss=0.607, metrics={'acc': 0.7019}]\n",
      "epoch 16: 100%|██████████| 10000/10000 [22:58<00:00,  7.25it/s, loss=0.607, metrics={'acc': 0.7019}]\n",
      "epoch 17: 100%|██████████| 10000/10000 [22:53<00:00,  7.28it/s, loss=0.607, metrics={'acc': 0.7025}]\n",
      "epoch 18: 100%|██████████| 10000/10000 [22:50<00:00,  7.30it/s, loss=0.607, metrics={'acc': 0.7027}]\n",
      "epoch 19: 100%|██████████| 10000/10000 [22:42<00:00,  7.34it/s, loss=0.607, metrics={'acc': 0.702}]\n",
      "epoch 20: 100%|██████████| 10000/10000 [22:40<00:00,  7.35it/s, loss=0.607, metrics={'acc': 0.7016}]\n",
      "epoch 21: 100%|██████████| 10000/10000 [22:43<00:00,  7.33it/s, loss=0.607, metrics={'acc': 0.7023}]\n",
      "epoch 22: 100%|██████████| 10000/10000 [22:44<00:00,  7.33it/s, loss=0.607, metrics={'acc': 0.702}]\n",
      "epoch 23: 100%|██████████| 10000/10000 [22:42<00:00,  7.34it/s, loss=0.607, metrics={'acc': 0.7018}]\n",
      "epoch 24: 100%|██████████| 10000/10000 [22:42<00:00,  7.34it/s, loss=0.607, metrics={'acc': 0.7023}]\n",
      "epoch 25: 100%|██████████| 10000/10000 [22:35<00:00,  7.38it/s, loss=0.607, metrics={'acc': 0.7023}]\n",
      "predict: 100%|█████████▉| 2500/2501 [00:38<00:00, 64.33it/s]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "wide_opt = AdamW(model.wide.parameters(), lr=0.1)\n",
    "deep_opt = AdamW(model.deeptabular.parameters(), lr=0.1)\n",
    "\n",
    "wide_sch = OneCycleLR(optimizer=wide_opt, max_lr=0.01, steps_per_epoch=X_wide_train.shape[0], epochs=n_epochs)\n",
    "deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_tab_train.shape[0], epochs=n_epochs)\n",
    "\n",
    "optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    LRHistory(n_epochs=n_epochs), \n",
    "]\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(model=model, \n",
    "                  objective='binary', \n",
    "                  metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                  seed=42, \n",
    "                  optimizers=optimizers,\n",
    "                  callbacks=callbacks\n",
    "                 )\n",
    "\n",
    "#             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "trainer.fit( # this is where problem is beginning\n",
    "    X_wide=X_wide_train,\n",
    "    X_tab=X_tab_train,\n",
    "    target=y_train,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=48, # default value is 32; 1028 works for TabMLP\n",
    "#                 val_split=0.2, # no need for this\n",
    ")\n",
    "\n",
    "y_valid_preds = trainer.predict_proba(X_wide=X_wide_valid, X_tab=X_tab_valid, batch_size=1024)[:,1]\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ceb8c11f-36ac-44a6-b20a-4a2f061cbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(path=datapath/'saint_20211121_weights_25epochs', save_state_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5135908f-8cf2-4895-9826-baec612052ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:   0%|          | 0/469 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 314.00 MiB (GPU 0; 7.79 GiB total capacity; 6.21 GiB already allocated; 215.81 MiB free; 6.53 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a5d8b5745017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer.fit( # this is where problem is beginning\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mX_wide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_wide_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_tab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_tab_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ] = alias\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/training/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_wide, X_tab, X_text, X_img, X_train, X_val, val_split, target, n_epochs, validation_freq, batch_size, custom_dataloader, finetune, finetune_epochs, finetune_max_lr, finetune_deeptabular_gradual, finetune_deeptabular_max_lr, finetune_deeptabular_layers, finetune_deeptext_gradual, finetune_deeptext_max_lr, finetune_deeptext_layers, finetune_deepimage_gradual, finetune_deepimage_max_lr, finetune_deepimage_layers, finetune_routine, stop_after_finetuning, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargett\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                     \u001b[0mtrain_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargett\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mprint_loss_and_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/training/trainer.py\u001b[0m in \u001b[0;36m_train_step\u001b[0;34m(self, data, target, batch_idx)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tabnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_sparse\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/models/wide_deep.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_deephead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwide_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwide_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     def _build_deephead(\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/models/wide_deep.py\u001b[0m in \u001b[0;36m_forward_deep\u001b[0;34m(self, X, wide_out)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mwide_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mwide_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptabular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"deeptabular\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mwide_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"deeptext\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/models/transformers/saint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx_cat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_cls_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/models/transformers/_encoders.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_attn_addnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_attn_ff_addnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_attn_ff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b n d -> 1 b (n d)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/models/transformers/_attention_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, sublayer)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/models/transformers/_attention_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_Q, X_KV)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b h s l, b h l d -> b h s d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b h s d -> b s (h d)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 314.00 MiB (GPU 0; 7.79 GiB total capacity; 6.21 GiB already allocated; 215.81 MiB free; 6.53 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.fit( # this is where problem is beginning\n",
    "    X_wide=X_wide_train,\n",
    "    X_tab=X_tab_train,\n",
    "    target=y_train,\n",
    "    n_epochs=2,\n",
    "    batch_size=1024, # default value is 32\n",
    "#                 val_split=0.2, # no need for this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "901753ca-aa88-4eb4-902a-dfd090c46cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 469/469 [00:05<00:00, 82.63it/s, loss=0.575, metrics={'acc': 0.7156}]\n",
      "epoch 2: 100%|██████████| 469/469 [00:05<00:00, 86.69it/s, loss=0.574, metrics={'acc': 0.7171}]\n",
      "epoch 3: 100%|██████████| 469/469 [00:05<00:00, 82.69it/s, loss=0.574, metrics={'acc': 0.7172}]\n",
      "epoch 4: 100%|██████████| 469/469 [00:05<00:00, 79.52it/s, loss=0.573, metrics={'acc': 0.7177}]\n",
      "epoch 5: 100%|██████████| 469/469 [00:05<00:00, 82.21it/s, loss=0.573, metrics={'acc': 0.7172}]\n",
      "epoch 6: 100%|██████████| 469/469 [00:05<00:00, 80.59it/s, loss=0.573, metrics={'acc': 0.7175}]\n",
      "epoch 7: 100%|██████████| 469/469 [00:05<00:00, 82.45it/s, loss=0.573, metrics={'acc': 0.7178}]\n",
      "epoch 8: 100%|██████████| 469/469 [00:05<00:00, 84.08it/s, loss=0.572, metrics={'acc': 0.7171}] \n",
      "epoch 9: 100%|██████████| 469/469 [00:05<00:00, 82.23it/s, loss=0.572, metrics={'acc': 0.7179}]\n",
      "epoch 10: 100%|██████████| 469/469 [00:05<00:00, 79.12it/s, loss=0.572, metrics={'acc': 0.7184}]\n",
      "epoch 11: 100%|██████████| 469/469 [00:05<00:00, 82.39it/s, loss=0.571, metrics={'acc': 0.7185}]\n",
      "epoch 12: 100%|██████████| 469/469 [00:05<00:00, 80.07it/s, loss=0.571, metrics={'acc': 0.7183}]\n",
      "epoch 13: 100%|██████████| 469/469 [00:05<00:00, 80.23it/s, loss=0.571, metrics={'acc': 0.7182}]\n",
      "epoch 14: 100%|██████████| 469/469 [00:05<00:00, 80.69it/s, loss=0.571, metrics={'acc': 0.718}] \n",
      "epoch 15: 100%|██████████| 469/469 [00:05<00:00, 80.54it/s, loss=0.571, metrics={'acc': 0.718}] \n",
      "epoch 16: 100%|██████████| 469/469 [00:05<00:00, 80.50it/s, loss=0.571, metrics={'acc': 0.7189}]\n",
      "epoch 17: 100%|██████████| 469/469 [00:05<00:00, 79.89it/s, loss=0.57, metrics={'acc': 0.7186}] \n",
      "epoch 18: 100%|██████████| 469/469 [00:05<00:00, 84.31it/s, loss=0.571, metrics={'acc': 0.7188}]\n",
      "epoch 19: 100%|██████████| 469/469 [00:05<00:00, 80.81it/s, loss=0.57, metrics={'acc': 0.7191}] \n",
      "epoch 20: 100%|██████████| 469/469 [00:05<00:00, 82.16it/s, loss=0.57, metrics={'acc': 0.7193}] \n",
      "epoch 21: 100%|██████████| 469/469 [00:05<00:00, 82.64it/s, loss=0.569, metrics={'acc': 0.7193}]\n",
      "epoch 22: 100%|██████████| 469/469 [00:05<00:00, 79.98it/s, loss=0.569, metrics={'acc': 0.7193}]\n",
      "epoch 23: 100%|██████████| 469/469 [00:05<00:00, 80.20it/s, loss=0.569, metrics={'acc': 0.7193}]\n",
      "epoch 24: 100%|██████████| 469/469 [00:05<00:00, 82.49it/s, loss=0.569, metrics={'acc': 0.72}]  \n",
      "epoch 25: 100%|██████████| 469/469 [00:05<00:00, 81.39it/s, loss=0.569, metrics={'acc': 0.7198}]\n",
      "epoch 26: 100%|██████████| 469/469 [00:05<00:00, 81.84it/s, loss=0.568, metrics={'acc': 0.7201}]\n",
      "epoch 27: 100%|██████████| 469/469 [00:05<00:00, 82.41it/s, loss=0.569, metrics={'acc': 0.72}]  \n",
      "epoch 28: 100%|██████████| 469/469 [00:05<00:00, 84.32it/s, loss=0.568, metrics={'acc': 0.7201}]\n",
      "epoch 29: 100%|██████████| 469/469 [00:05<00:00, 83.23it/s, loss=0.568, metrics={'acc': 0.72}]  \n",
      "epoch 30: 100%|██████████| 469/469 [00:05<00:00, 81.83it/s, loss=0.568, metrics={'acc': 0.7198}]\n",
      "epoch 31: 100%|██████████| 469/469 [00:05<00:00, 81.95it/s, loss=0.568, metrics={'acc': 0.7205}]\n",
      "epoch 32: 100%|██████████| 469/469 [00:05<00:00, 84.29it/s, loss=0.567, metrics={'acc': 0.7202}]\n",
      "epoch 33: 100%|██████████| 469/469 [00:05<00:00, 83.48it/s, loss=0.567, metrics={'acc': 0.7204}]\n",
      "epoch 34: 100%|██████████| 469/469 [00:05<00:00, 80.11it/s, loss=0.567, metrics={'acc': 0.7202}]\n",
      "epoch 35: 100%|██████████| 469/469 [00:05<00:00, 82.41it/s, loss=0.567, metrics={'acc': 0.7207}]\n",
      "epoch 36: 100%|██████████| 469/469 [00:05<00:00, 85.77it/s, loss=0.567, metrics={'acc': 0.7211}]\n",
      "epoch 37: 100%|██████████| 469/469 [00:05<00:00, 86.72it/s, loss=0.567, metrics={'acc': 0.7206}] \n",
      "epoch 38: 100%|██████████| 469/469 [00:05<00:00, 83.05it/s, loss=0.566, metrics={'acc': 0.7213}]\n",
      "epoch 39: 100%|██████████| 469/469 [00:05<00:00, 81.63it/s, loss=0.566, metrics={'acc': 0.7212}]\n",
      "epoch 40: 100%|██████████| 469/469 [00:05<00:00, 80.16it/s, loss=0.566, metrics={'acc': 0.7212}]\n",
      "epoch 41: 100%|██████████| 469/469 [00:05<00:00, 80.89it/s, loss=0.566, metrics={'acc': 0.721}] \n",
      "epoch 42: 100%|██████████| 469/469 [00:05<00:00, 82.11it/s, loss=0.566, metrics={'acc': 0.7207}]\n",
      "epoch 43: 100%|██████████| 469/469 [00:05<00:00, 82.14it/s, loss=0.566, metrics={'acc': 0.7211}]\n",
      "epoch 44: 100%|██████████| 469/469 [00:05<00:00, 84.12it/s, loss=0.566, metrics={'acc': 0.7209}]\n",
      "epoch 45: 100%|██████████| 469/469 [00:05<00:00, 81.31it/s, loss=0.566, metrics={'acc': 0.7218}]\n",
      "epoch 46: 100%|██████████| 469/469 [00:05<00:00, 82.77it/s, loss=0.566, metrics={'acc': 0.7214}]\n",
      "epoch 47: 100%|██████████| 469/469 [00:05<00:00, 82.61it/s, loss=0.565, metrics={'acc': 0.7222}]\n",
      "epoch 48: 100%|██████████| 469/469 [00:05<00:00, 85.50it/s, loss=0.565, metrics={'acc': 0.7212}]\n",
      "epoch 49: 100%|██████████| 469/469 [00:05<00:00, 86.67it/s, loss=0.565, metrics={'acc': 0.722}] \n",
      "epoch 50: 100%|██████████| 469/469 [00:05<00:00, 84.69it/s, loss=0.564, metrics={'acc': 0.7221}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit( # this is where problem is beginning\n",
    "    X_wide=X_wide_train,\n",
    "    X_tab=X_tab_train,\n",
    "    target=y_train,\n",
    "    n_epochs=50,\n",
    "    batch_size=1024, # default value is 32\n",
    "#                 val_split=0.2, # no need for this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5693d1e-0d02-47cf-b05e-3219e144cbb7",
   "metadata": {},
   "source": [
    "# WideDeep Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcec515-3fee-4578-8d28-d0909a5502c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-ef23d42afe3c>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ef23d42afe3c>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    random_state=42, wandb_tracked=True):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def trainer(model, X_wide = X_bins, X_deep=X_gauss, y=y, exmodel_config=exmodel_config, wandb_config=wandb_config, random_state=42, wandb_tracked=True):\n",
    "    \"\"\"\n",
    "    Simple trainer wrapper for widedeep models, with holdout\n",
    "    \"\"\"\n",
    "    # concatenate together wide and deep data\n",
    "    X = X_wide.join(X_deep)\n",
    "    \n",
    "    wide = Wide()\n",
    "    \n",
    "    deeptabular = TabMlp(\n",
    "        mlp_hidden_dims=[64,32],\n",
    "        continuous_cols=X_deep.columns,\n",
    "    # scaling with GaussRankScaler, before doing holdout split\n",
    "    # scaler = GaussRankScaler(X)\n",
    "    # scaler.fit_transform(X)\n",
    "    \n",
    "    # skipping denoising for now\n",
    "    \n",
    "    # holdout split\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)    \n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(params)\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88368160-d477-4b54-8835-b782c8e272df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b759437-c961-4903-8fb2-a7a92b5ed3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required keyword-only arguments: 'categories', 'num_continuous', 'dim', 'depth', and 'heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9f3e64378f09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaint_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 5 required keyword-only arguments: 'categories', 'num_continuous', 'dim', 'depth', and 'heads'"
     ]
    }
   ],
   "source": [
    "saint_model = TabAttention(categories=None, num_continuous=X.shape[1], dim=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3ab351d-2c44-454d-a8b6-171f470a1685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-5c6737837e8e>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-5c6737837e8e>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    for_transformer=False, # change if using a Transformer-based model\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def cross_validate_pytorch_model(arch:str, X, y, X_test, params:dict={}, start_fold=0, \n",
    "                         exmodel_config=exmodel_config, wandb_config=wandb_config, \n",
    "                         random_state=42, shuffle_kfolds=True, wandb_tracked=True, encode_cats=False):\n",
    "    \"\"\"\n",
    "    Modification of the `cross_validate_model` function used in my stacking notebooks, customized to the dataset and to deep learning approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare for k-fold cross-validation; random-state here is notebook-wide, not per-model\n",
    "    # shuffle on the initial sets, but not subsequently -- performing the same operation twice means a very different dataset\n",
    "    if shuffle_kfolds:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=SEED)\n",
    "    else:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=False)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(params)\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )   \n",
    "    \n",
    "    # initialize lists for out-of-fold preds and ground truth\n",
    "    oof_preds, oof_y = [], []\n",
    "    \n",
    "    # initialize a numpy.ndarray containing the fold-model's preds for test set\n",
    "    test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "#         print(f\"type(train_ids) = {type(train_ids)} and train_ids.shape = {train_ids.shape}\")\n",
    "#         print(f\"type(valid_ids) = {type(valid_ids)} and train_ids.shape = {valid_ids.shape}\")\n",
    "        if fold < start_fold: # skip folds that are already trained\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"FOLD {fold}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "            print(f\"y_train shape is {y_train.shape}, y_valid shape is {y_valid.shape}\")\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                # X_train = pd.DataFrame(X_train, columns=\n",
    "            else:\n",
    "                X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:] # bc need pandas.DataFrames for ce\n",
    "            \n",
    "            # print(f\"X_train shape is {X_train.shape}\")\n",
    "            # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "            # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "            # scaling\n",
    "            # scaler = GaussRankScaler()\n",
    "            # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "            # X_valid = pd.DataFrame(scaler.transform(X_valid), columns=X.columns)\n",
    "            # X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "            \n",
    "            # print(\"Scaling complete\")\n",
    "            # print(f\"X_train shape is {X_train.shape}\")\n",
    "            # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "            # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "            # embedding & library-specific preprocessing\n",
    "            tab_preprocessor = TabPreprocessor(\n",
    "                # scale=False, # because GaussRank scaling already occurred\n",
    "                scale=True\n",
    "                for_transformer=False, # change if using a Transformer-based model\n",
    "                continuous_cols=X.columns,\n",
    "                # continuous_cols=range(X.shape[1]), # since it'll be working on a numpy.ndarray\n",
    "                auto_embed_dim=True, # uses fastai's rule of thumb\n",
    "            )#, embed_cols=embed_cols, )\n",
    "            X_train = tab_preprocessor.fit_transform(X_train)   \n",
    "            X_valid = tab_preprocessor.transform(X_valid)\n",
    "            X_test = tab_preprocessor.transform(X_test)\n",
    "            \n",
    "            print(\"Tab preprocessing complete.\")\n",
    "            print(f\"Type of X_train is {type(X_train)}\")\n",
    "            # print(f\"X_train shape is {X_train.shape}\")\n",
    "            # print(f\"X_valid shape is {X_valid.shape}\")\n",
    "            # print(f\"X_test shape is {X_test.shape}\")\n",
    "            \n",
    "            # define model\n",
    "            deeptabular = TabMlp(\n",
    "                mlp_hidden_dims=[64,32],\n",
    "                column_idx=tab_preprocessor.column_idx,\n",
    "            #     embed_input=tab_preprocessor.embeddings_input,\n",
    "                # continuous_cols=range(X.shape[1]), # since it'll be working on a numpy.ndarray\n",
    "                continuous_cols=X.columns,\n",
    "            )\n",
    "\n",
    "            n_epochs = 30\n",
    "\n",
    "            model = WideDeep(wide=None, deeptabular=deeptabular)\n",
    "\n",
    "            # pytorch hyperparams\n",
    "            deep_opt = AdamW(model.parameters(), lr=0.1)\n",
    "\n",
    "            # deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_train_tab.shape[0], epochs=n_epochs)\n",
    "\n",
    "            # optimizers = {'deeptabular': deep_opt }\n",
    "            # lr_schedulers = {'deeptabular': deep_sch }\n",
    "\n",
    "\n",
    "            callbacks = [\n",
    "                LRHistory(n_epochs=n_epochs), \n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "            # trainer\n",
    "            trainer = Trainer(model=model, \n",
    "                              objective='binary', \n",
    "                              metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                              seed=random_state, \n",
    "                              optimizers=deep_opt,\n",
    "                              callbacks=callbacks\n",
    "                             )\n",
    "\n",
    "    #             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "            trainer.fit( # this is where problem is beginning\n",
    "                # X_wide=X_train_wide,\n",
    "                X_tab=np.array(X_train),\n",
    "                target=np.array(y_train),\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=1024, # default value is 32\n",
    "    #                 val_split=0.2, # no need for this\n",
    "            )\n",
    "\n",
    "            y_valid_preds = trainer.predict_proba(X_tab=np.array(X_valid), batch_size=1024)[:,1]\n",
    "\n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "\n",
    "\n",
    "            # test set inference\n",
    "            fold_test_preds = trainer.predict_proba(X_tab=np.array(X_test), batch_size=1024)[:,1]\n",
    "            test_preds += fold_test_preds\n",
    "            \n",
    "            print(f\"NaNs in y_valid_preds: {np.isnan(y_valid_preds).any()}\")\n",
    "            print(f\"NaNs in y_valid: {np.isnan(y_valid).any()}\")\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    #         valid_loss = log_loss(y_valid, y_pred)\n",
    "            # give the valid AUC score, for edification\n",
    "            fold_valid_auc = roc_auc_score(y_valid, y_valid_preds)\n",
    "            if wandb_tracked:\n",
    "                wandb.log({f'fold{fold}_valid_roc_auc': fold_valid_auc})\n",
    "            print(f\"Valid AUC for fold {fold} is {fold_valid_auc}\")   \n",
    "        # dump(model, Path(runpath/f\"{arch}_fold{fold}_rs{random_state}_model.joblib\"))\n",
    "\n",
    "    model_valid_auc = roc_auc_score(oof_y, oof_preds)\n",
    "    print(f\"Valid AUC score for {arch} model is {model_valid_auc}\")\n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_valid_auc': model_valid_auc,\n",
    "                   'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()),\n",
    "                   'model_seed': random_state,\n",
    "                  })\n",
    "        wandb.finish()\n",
    "    \n",
    "    # finalize test preds\n",
    "    test_preds /= exmodel_config['kfolds']\n",
    "    \n",
    "    # save OOF preds and test-set preds\n",
    "#     if 'widedeep' in arch:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "#     else:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "    if not (datapath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\").is_file():\n",
    "        dump(oof_y, predpath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\")\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         if 'widedeep' in arch:\n",
    "#         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "#                    'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()), \n",
    "#         #                    'model_params': str(model.get_params()),\n",
    "#         })\n",
    "# #         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "# # #                    'model_params': str(model.get_params()),\n",
    "# #                   })\n",
    "#         wandb.finish()\n",
    "    return oof_preds, test_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b49138e1-077c-41e2-be5d-bd7fcf2b8c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "---------------------------------------------------\n",
      "y_train shape is (480000,), y_valid shape is (120000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:179: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1:   2%|▏         | 8/469 [00:00<00:06, 72.72it/s, loss=0.852, metrics={'acc': 0.498}] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tab preprocessing complete.\n",
      "Type of X_train is <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 469/469 [00:05<00:00, 78.36it/s, loss=0.647, metrics={'acc': 0.6435}]\n",
      "epoch 2: 100%|██████████| 469/469 [00:06<00:00, 75.90it/s, loss=0.64, metrics={'acc': 0.6527}] \n",
      "epoch 3: 100%|██████████| 469/469 [00:06<00:00, 68.60it/s, loss=0.645, metrics={'acc': 0.6499}]\n",
      "epoch 4: 100%|██████████| 469/469 [00:06<00:00, 76.83it/s, loss=0.648, metrics={'acc': 0.6462}]\n",
      "epoch 5: 100%|██████████| 469/469 [00:06<00:00, 76.18it/s, loss=0.648, metrics={'acc': 0.6472}]\n",
      "epoch 6: 100%|██████████| 469/469 [00:06<00:00, 74.80it/s, loss=0.645, metrics={'acc': 0.65}]  \n",
      "epoch 7: 100%|██████████| 469/469 [00:05<00:00, 78.54it/s, loss=0.656, metrics={'acc': 0.634}] \n",
      "epoch 8: 100%|██████████| 469/469 [00:06<00:00, 72.74it/s, loss=0.693, metrics={'acc': 0.5112}]\n",
      "epoch 9: 100%|██████████| 469/469 [00:06<00:00, 74.71it/s, loss=0.691, metrics={'acc': 0.5116}]\n",
      "epoch 10: 100%|██████████| 469/469 [00:06<00:00, 76.26it/s, loss=0.699, metrics={'acc': 0.5246}]\n",
      "epoch 11: 100%|██████████| 469/469 [00:05<00:00, 79.13it/s, loss=0.693, metrics={'acc': 0.503}] \n",
      "epoch 12: 100%|██████████| 469/469 [00:06<00:00, 77.79it/s, loss=0.693, metrics={'acc': 0.5022}]\n",
      "epoch 13: 100%|██████████| 469/469 [00:06<00:00, 77.95it/s, loss=0.693, metrics={'acc': 0.5014}]\n",
      "epoch 14: 100%|██████████| 469/469 [00:05<00:00, 80.07it/s, loss=0.693, metrics={'acc': 0.5026}]\n",
      "epoch 15: 100%|██████████| 469/469 [00:06<00:00, 74.53it/s, loss=0.693, metrics={'acc': 0.5023}]\n",
      "epoch 16: 100%|██████████| 469/469 [00:05<00:00, 79.15it/s, loss=0.693, metrics={'acc': 0.5028}]\n",
      "epoch 17: 100%|██████████| 469/469 [00:05<00:00, 79.72it/s, loss=0.693, metrics={'acc': 0.502}] \n",
      "epoch 18: 100%|██████████| 469/469 [00:06<00:00, 77.65it/s, loss=0.693, metrics={'acc': 0.5016}]\n",
      "epoch 19: 100%|██████████| 469/469 [00:05<00:00, 81.81it/s, loss=0.693, metrics={'acc': 0.5019}]\n",
      "epoch 20: 100%|██████████| 469/469 [00:06<00:00, 76.76it/s, loss=0.693, metrics={'acc': 0.5015}]\n",
      "epoch 21: 100%|██████████| 469/469 [00:06<00:00, 74.97it/s, loss=0.693, metrics={'acc': 0.5017}]\n",
      "epoch 22: 100%|██████████| 469/469 [00:06<00:00, 78.01it/s, loss=0.693, metrics={'acc': 0.5032}]\n",
      "epoch 23: 100%|██████████| 469/469 [00:06<00:00, 74.61it/s, loss=0.693, metrics={'acc': 0.502}] \n",
      "epoch 24: 100%|██████████| 469/469 [00:06<00:00, 73.76it/s, loss=0.693, metrics={'acc': 0.5027}]\n",
      "epoch 25: 100%|██████████| 469/469 [00:06<00:00, 71.35it/s, loss=0.693, metrics={'acc': 0.5013}]\n",
      "epoch 26: 100%|██████████| 469/469 [00:06<00:00, 72.63it/s, loss=0.693, metrics={'acc': 0.5011}]\n",
      "epoch 27: 100%|██████████| 469/469 [00:06<00:00, 76.28it/s, loss=0.693, metrics={'acc': 0.5009}]\n",
      "epoch 28: 100%|██████████| 469/469 [00:06<00:00, 77.03it/s, loss=0.693, metrics={'acc': 0.5015}]\n",
      "epoch 29: 100%|██████████| 469/469 [00:05<00:00, 78.65it/s, loss=0.694, metrics={'acc': 0.5018}]\n",
      "epoch 30: 100%|██████████| 469/469 [00:05<00:00, 85.00it/s, loss=0.693, metrics={'acc': 0.502}] \n",
      "predict: 100%|██████████| 118/118 [00:00<00:00, 130.64it/s]\n",
      "predict: 100%|██████████| 528/528 [00:01<00:00, 271.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in y_valid_preds: True\n",
      "NaNs in y_valid: False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-75824e55d591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moof_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_pytorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'widedeep-TabMLP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_tracked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-b9f964242d10>\u001b[0m in \u001b[0;36mcross_validate_pytorch_model\u001b[0;34m(arch, X, y, X_test, params, start_fold, exmodel_config, wandb_config, random_state, shuffle_kfolds, wandb_tracked, encode_cats)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m#         valid_loss = log_loss(y_valid, y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# give the valid AUC score, for edification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mfold_valid_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwandb_tracked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf'fold{fold}_valid_roc_auc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfold_valid_auc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     if y_type == \"multiclass\" or (\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "oof_preds, test_preds = cross_validate_pytorch_model('widedeep-TabMLP', X, y, X_test, wandb_tracked=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
