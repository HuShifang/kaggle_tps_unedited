{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "Setting up a more robust baseline notebook, suitable for use with all of the \"Big Three\" (XGBoost, CatBoost, LightGBM) libraries and on either Google Colab or the local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7764e0c4-5be5-412c-ad23-6c7a98ebc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two manual flags (ex-config)\n",
    "COLAB = False\n",
    "USE_GPU = True\n",
    "# libraries = ['xgboost', 'lightgbm', 'catboost']\n",
    "libraries = ['pytorch-widedeep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41986fda-3d55-4610-959c-8c7ef3d1ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9819651e-fb74-4270-9e2e-a9da26e83549",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"stacking_manual_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74eeb0c2-22c7-44e7-97d3-fff4cc0b0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle Google Colab-specific library installation/updating\n",
    "if COLAB:\n",
    "    # much of the below inspired by or cribbed from the May 2021 Kaggle Tabular Playground winner, at \n",
    "    # https://colab.research.google.com/gist/academicsuspect/0aac7bd6e506f5f70295bfc9a3dc2250/tabular-may-baseline.ipynb?authuser=1#scrollTo=LJoVKJb5wN0L\n",
    "    \n",
    "    # Kaggle API for downloading the datasets\n",
    "#     !pip install --upgrade -q kaggle\n",
    "\n",
    "    # weights and biases\n",
    "    !pip install -qqqU wandb\n",
    "    \n",
    "    # Optuna for parameter search\n",
    "    !pip install -q optuna\n",
    "\n",
    "    # upgrade sklearn\n",
    "    !pip install --upgrade scikit-learn\n",
    "\n",
    "#     !pip install category_encoders\n",
    "    \n",
    "    if 'catboost' in libraries:\n",
    "        !pip install catboost\n",
    "    \n",
    "    if 'xgboost' in libraries:\n",
    "        if USE_GPU: \n",
    "            # this part is from https://github.com/rapidsai/gputreeshap/issues/24\n",
    "            !pip install cmake --upgrade\n",
    "            # !pip install sklearn --upgrade\n",
    "            !git clone --recursive https://github.com/dmlc/xgboost\n",
    "            %cd /content/xgboost\n",
    "            !mkdir build\n",
    "            %cd build\n",
    "            !cmake .. -DUSE_CUDA=ON\n",
    "            !make -j4\n",
    "            %cd /content/xgboost/python-package\n",
    "            !python setup.py install --use-cuda --use-nccl\n",
    "            !/opt/bin/nvidia-smi\n",
    "            !pip install shap\n",
    "        else:\n",
    "            !pip install --upgrade xgboost\n",
    "    if 'lightgbm' in libraries:\n",
    "        if USE_GPU:\n",
    "            # lighgbm gpu compatible\n",
    "            !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "            ! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;\n",
    "        else:\n",
    "            !pip install --upgrade lightgbm\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07bf4db8-1390-49ee-a9f0-a1bc35f9b9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "# import timm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler #, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c01f5342-6d54-426e-bc90-344f1bc87729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb256b07-38ce-4d52-bb21-fe193940b0e6",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66cf5cab-1730-4dcf-8adf-057aeab85a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is the code for reading the train.csv and converting it to a .feather file\n",
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b5a678-4d6a-47d3-810b-2d5f5e66add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/oct2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "#     datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')  \n",
    "    root = Path('/home/sf/code/kaggle/tabular_playgrounds/oct2021/')\n",
    "    datapath = root/'datasets'\n",
    "    edapath = root/'EDA'\n",
    "    modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    \n",
    "    for pth in [root, datapath, edapath, modelpath, predpath, subpath]:\n",
    "        pth.mkdir(exist_ok=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31e4ac30-f4be-40c5-94f4-b60314aab5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5a5ac-59e2-4078-bc52-b03b3bb1ae83",
   "metadata": {},
   "source": [
    "## Ex-Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9c5da9-05ce-4b9f-9322-8a34d8941bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "exmodel_config = {\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "#     'random_state': SEED,\n",
    "#     'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "#     'subsample': 1,\n",
    "    'cross_val_strategy': KFold, # None for holdout, or the relevant sklearn class\n",
    "    'kfolds': 5, # if 1, that means just doing holdout\n",
    "    'test_size': 0.2,\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ec3fe-da59-4d26-b635-52fd48d43d18",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1031ba-a76a-4433-a82e-9ba4f42f7d18",
   "metadata": {},
   "source": [
    "**TODO** Write some conditional logic here to automate it -- possibly as part of a sklearn.*pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d68b71-d3c8-48f6-a031-fccccdca1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if exmodel_config['scaler']:\n",
    "#     scaler = exmodel_config['scaler']()\n",
    "#     scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ea54c68-7feb-4747-87d1-74c3a93db267",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = datapath/'train.feather'\n",
    "df = pd.read_feather(path=train_source)\n",
    "df.index.name = 'id'\n",
    "y_train = df.target\n",
    "features = [x for x in df.columns if x != 'target']\n",
    "X_train = df[features]\n",
    "# X.index.name = 'id'\n",
    "# y.index.name = 'id'\n",
    "X = np.array(X_train)\n",
    "y = np.array(y_train)\n",
    "\n",
    "# del df, X_train, y_train\n",
    "\n",
    "\n",
    "# exmodel_config['feature_count'] = len(X.columns)\n",
    "exmodel_config['feature_count'] = X.shape[1]\n",
    "exmodel_config['instance_count'] = X.shape[0]\n",
    "\n",
    "# exmodel_config['feature_generator'] = None\n",
    "# exmodel_config['feature_generator'] = \"Summary statistics\"\n",
    "\n",
    "exmodel_config['train_source'] = str(train_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4b987bb-d8ec-40b2-81ae-f9d744372396",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source = datapath/'test.feather'\n",
    "exmodel_config['test_source'] = str(test_source)\n",
    "X_test = pd.read_feather(path=test_source)\n",
    "# X_test = X_test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbc9301d-8d38-437b-a936-483d7a05d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e044f-82ed-42ab-9868-00a79f974486",
   "metadata": {},
   "source": [
    "## Weights and Biases Run Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c37505-cfe2-45ce-ab39-f7527eccf2ca",
   "metadata": {},
   "source": [
    "Below is the configuration for a Weights and Biases (`wandb`) run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "148ae785-8208-4eaa-829e-53c2fb7b0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['baseline', 'deep-learning'],\n",
    "    'notes': \"Going to try getting pytorch-widedeep working, initially with TabMLP\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d9012-34f1-435a-ba16-4416e0d4a286",
   "metadata": {},
   "source": [
    "## Deep Learning Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912a62f-970a-48b4-b428-d886f2612fc2",
   "metadata": {},
   "source": [
    "Due to the importance of identifying categorical variables for deep learning on tabular data (namely, the generation of embeddings containing meaningful information about them), I'm going to try using `fastai`'s `cont_cat_split` on the original dataset (post-imputation and generation of features based on summary statistics) and then proceeding with the other transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23937ff4-043a-4496-b78a-f86e94085c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "113d4b09-4800-406b-b79f-cd36fce94c47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cardinalities = X_train.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98237c36-f7de-4344-863e-07ee0c6fad60",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f144    607935\n",
       "f169    606385\n",
       "f226    594319\n",
       "f207    582192\n",
       "f125    579880\n",
       "f237    578749\n",
       "f168    576993\n",
       "f72     573057\n",
       "f142    560152\n",
       "f225    558377\n",
       "f217    557482\n",
       "f224    557335\n",
       "f227    553207\n",
       "f174    542053\n",
       "f166    537722\n",
       "f215    535865\n",
       "f161    532853\n",
       "f23     531719\n",
       "f191    529250\n",
       "f184    513613\n",
       "f122    508674\n",
       "f94     504544\n",
       "f198    501658\n",
       "f173    500033\n",
       "f114    499970\n",
       "f179    497596\n",
       "f138    497263\n",
       "f157    492134\n",
       "f192    490036\n",
       "f141    489019\n",
       "f124    486410\n",
       "f58     482130\n",
       "f209    476044\n",
       "f193    465107\n",
       "f158    463093\n",
       "f171    461900\n",
       "f214    460571\n",
       "f163    453071\n",
       "f2      452401\n",
       "f146    451339\n",
       "f182    451296\n",
       "f219    449261\n",
       "f154    446224\n",
       "f162    445373\n",
       "f143    444524\n",
       "f91     439202\n",
       "f188    437466\n",
       "f139    436631\n",
       "f84     435536\n",
       "f206    431717\n",
       "f8      428659\n",
       "f90     426170\n",
       "f6      421827\n",
       "f223    420899\n",
       "f200    420271\n",
       "f160    419967\n",
       "f153    412726\n",
       "f195    412007\n",
       "f123    411379\n",
       "f93     410485\n",
       "f234    408673\n",
       "f194    403041\n",
       "f4      401939\n",
       "f152    399983\n",
       "f34     395805\n",
       "f126    390685\n",
       "f39     389564\n",
       "f92     386351\n",
       "f104    382619\n",
       "f1      374634\n",
       "f175    372110\n",
       "f120    371072\n",
       "f170    370815\n",
       "f98     367956\n",
       "f149    366339\n",
       "f86     365382\n",
       "f208    365225\n",
       "f110    362484\n",
       "f229    358098\n",
       "f222    355695\n",
       "f10     355303\n",
       "f130    352035\n",
       "f101    336272\n",
       "f62     332336\n",
       "f73     328802\n",
       "f24     327370\n",
       "f231    325664\n",
       "f85     325589\n",
       "f15     324840\n",
       "f33     324185\n",
       "f89     324090\n",
       "f77     322750\n",
       "f9      322640\n",
       "f32     320368\n",
       "f38     315602\n",
       "f16     313228\n",
       "f65     307414\n",
       "f112    305296\n",
       "f14     302868\n",
       "f105    302842\n",
       "f199    302838\n",
       "f68     301469\n",
       "f145    297079\n",
       "f115    295815\n",
       "f21     295742\n",
       "f40     294067\n",
       "f30     292484\n",
       "f97     290300\n",
       "f88     280220\n",
       "f109    277867\n",
       "f148    273701\n",
       "f61     268732\n",
       "f46     268629\n",
       "f31     266964\n",
       "f7      265619\n",
       "f111    265113\n",
       "f203    264588\n",
       "f78     260647\n",
       "f63     259988\n",
       "f228    258202\n",
       "f13     257319\n",
       "f67     251979\n",
       "f147    251131\n",
       "f189    250081\n",
       "f106    249788\n",
       "f59     246300\n",
       "f12     246156\n",
       "f60     245103\n",
       "f3      244851\n",
       "f241    244154\n",
       "f44     241770\n",
       "f5      241585\n",
       "f53     240013\n",
       "f25     239760\n",
       "f239    239593\n",
       "f197    236735\n",
       "f66     236730\n",
       "f17     231233\n",
       "f150    226675\n",
       "f20     226437\n",
       "f127    226315\n",
       "f56     225538\n",
       "f211    223364\n",
       "f27     223306\n",
       "f36     221671\n",
       "f176    216383\n",
       "f212    216199\n",
       "f185    215580\n",
       "f133    214341\n",
       "f69     214266\n",
       "f76     212732\n",
       "f236    212431\n",
       "f41     211246\n",
       "f100    209986\n",
       "f83     202494\n",
       "f201    199649\n",
       "f87     198478\n",
       "f54     197660\n",
       "f235    195796\n",
       "f45     195770\n",
       "f134    195021\n",
       "f118    189040\n",
       "f187    188558\n",
       "f79     188230\n",
       "f81     186070\n",
       "f26     183403\n",
       "f129    183006\n",
       "f48     182403\n",
       "f238    179040\n",
       "f55     177793\n",
       "f128    177314\n",
       "f213    175959\n",
       "f74     175223\n",
       "f186    174865\n",
       "f216    172307\n",
       "f37     171918\n",
       "f42     171131\n",
       "f119    171081\n",
       "f50     171010\n",
       "f205    165836\n",
       "f28     163596\n",
       "f18     163185\n",
       "f49     163089\n",
       "f196    161836\n",
       "f19     161302\n",
       "f95     160801\n",
       "f240    159347\n",
       "f57     157449\n",
       "f70     157423\n",
       "f172    154349\n",
       "f221    152666\n",
       "f156    152362\n",
       "f183    150273\n",
       "f47     147312\n",
       "f107    147096\n",
       "f190    146233\n",
       "f96     146100\n",
       "f155    145915\n",
       "f0      142054\n",
       "f11     141980\n",
       "f113    141591\n",
       "f102    140521\n",
       "f80     140510\n",
       "f202    135845\n",
       "f29     132977\n",
       "f64     132128\n",
       "f135    131425\n",
       "f132    129663\n",
       "f140    128487\n",
       "f164    125692\n",
       "f71     125690\n",
       "f35     123688\n",
       "f165    121485\n",
       "f52     120431\n",
       "f75     116914\n",
       "f178    116857\n",
       "f230    113192\n",
       "f180    109203\n",
       "f82     107320\n",
       "f131    104500\n",
       "f204    103718\n",
       "f136    101424\n",
       "f177    100548\n",
       "f151     90592\n",
       "f220     89007\n",
       "f121     87355\n",
       "f99      86896\n",
       "f181     86680\n",
       "f137     82171\n",
       "f51      78966\n",
       "f218     77116\n",
       "f103     76473\n",
       "f117     75104\n",
       "f232     74434\n",
       "f108     66767\n",
       "f210     65746\n",
       "f116     61662\n",
       "f167     61583\n",
       "f233     51815\n",
       "f159     50632\n",
       "f265         2\n",
       "f273         2\n",
       "f266         2\n",
       "f267         2\n",
       "f268         2\n",
       "f264         2\n",
       "f269         2\n",
       "f270         2\n",
       "f271         2\n",
       "f272         2\n",
       "f279         2\n",
       "f274         2\n",
       "f275         2\n",
       "f276         2\n",
       "f277         2\n",
       "f278         2\n",
       "f280         2\n",
       "f281         2\n",
       "f282         2\n",
       "f283         2\n",
       "f262         2\n",
       "f263         2\n",
       "f242         2\n",
       "f261         2\n",
       "f260         2\n",
       "f43          2\n",
       "f22          2\n",
       "f243         2\n",
       "f244         2\n",
       "f245         2\n",
       "f246         2\n",
       "f247         2\n",
       "f248         2\n",
       "f249         2\n",
       "f250         2\n",
       "f251         2\n",
       "f252         2\n",
       "f253         2\n",
       "f254         2\n",
       "f255         2\n",
       "f256         2\n",
       "f257         2\n",
       "f258         2\n",
       "f259         2\n",
       "f284         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardinalities.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1bdb0-12c5-44f0-b232-87af0fbdadfc",
   "metadata": {},
   "source": [
    "So we have several features with cardinalities in the range (50,000, 100,000), and a bunch of binary indicator variables. Jeremy Howard is opposed to treating features with more than 10k values as categorical, and gets nervous over cardinalities over 5k, so I think I'll just treat the card-2 variables as wide and the others as deep (for now); I might experiment with embeddings for the 50k-100k cardinality features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b2ffcd-49c8-4cf2-885e-a9dd0e640869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_card_embed = 10000\n",
    "# max_card_cat = 100000\n",
    "# exmodel_config['max_card_for_embedding'] = max_card_embed\n",
    "# exmodel_config['max_card_for_categorical'] = max_card_cat\n",
    "\n",
    "\n",
    "# X_orig = X.iloc[:, :118] # excluding summary, meta-statistics\n",
    "# X_meta = X.iloc[:, 118:] # including summary, meta-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53f9d86-dce7-46db-83c1-929dc1ec7ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exmodel_config['max_card_for_categorical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86b704d8-ec62-4d98-8410-1e636e492178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_card_features = [f for f in X.columns if X[f].nunique() <= 50000]\n",
    "# high_card_features = [f for f in X.columns if X[f].nunique() > 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0b2e8b-50da-4656-b14b-2af467387223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(low_card_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9d9f790-8b15-4011-ab42-2b342a455e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(high_card_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbd183-5501-4dcd-870e-b5f2eae70749",
   "metadata": {},
   "source": [
    "# WideDeep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5a9cb-9524-4d16-9a98-d00ea8f99f03",
   "metadata": {},
   "source": [
    "## (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f712681f-f201-4c6b-9983-bd95f02ecb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(datapath/\"adult.csv.zip\")\n",
    "# df[\"income_label\"] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "# df.drop(\"income\", axis=1, inplace=True)\n",
    "# df_train, df_test = train_test_split(df, test_size=0.2, stratify=df.income_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb4e450b-08f7-4e02-9a6d-233b7225466a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for f in df.columns:\n",
    "#     print(f\"{f}: {df[f].nunique()}\")\n",
    "#     print(f\"NaNs: {df[f].isna().sum()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3256738c-6bf9-4e63-8e69-82645bbe11f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Columns: 285 entries, f0 to f284\n",
      "dtypes: float64(240), int64(45)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f230f28-cf7a-4f1f-ab9c-3e0075cbebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide_cols_pre = [f for f in X.columns if X[f].nunique() <= max_card_cat and X[f].nunique() > 2]\n",
    "# wide_cols_onehot = [f for f in X.columns if X[f].nunique() == 2]\n",
    "wide_cols = [f for f in X_train.columns if X_train[f].nunique() == 2]\n",
    "cont_cols = [f for f in X_train.columns if X_train[f].nunique() > 2]\n",
    "# embed_cols = [f for f in X.columns if X[f].nunique() <= max_card_embed and X[f].nunique() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7854fea6-eeaf-46e3-95da-9aa349621f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e25261-89e9-411c-91d9-3a6b4f2d442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cols_np = np.where(X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d37360c-a8c0-4393-b616-f787ef46f7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 240)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wide_cols), len(cont_cols)#, len(embed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3be34d81-a953-4183-9fca-2d14e5cc50c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 21,\n",
       " 37,\n",
       " 42,\n",
       " 45,\n",
       " 47,\n",
       " 53,\n",
       " 58,\n",
       " 68,\n",
       " 70,\n",
       " 75,\n",
       " 80,\n",
       " 96,\n",
       " 98,\n",
       " 110,\n",
       " 114,\n",
       " 118,\n",
       " 119,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0622ca79-c708-4b94-b98d-613f16ff42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_np = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e400b37c-656a-41eb-b7be-9125484be23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06ebc1ab-0e32-4ae7-ab18-62fb345481cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "X_wide_pre = wide_preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e4f4953-f2bc-4d58-86ec-f7228b7e00cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 45)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9700db34-7b28-4e8c-82d2-63d009295fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,  77995, 161433, ..., 794314, 794316, 794318],\n",
       "       [     2,  77996, 161434, ..., 794314, 794316, 794318],\n",
       "       [     3,  77997, 161435, ..., 794314, 794316, 794318],\n",
       "       ...,\n",
       "       [     8,  78002, 161440, ..., 794314, 794316, 794318],\n",
       "       [     9,  78003, 161441, ..., 794314, 794316, 794318],\n",
       "       [    10,  78004, 161442, ..., 794314, 794316, 794318]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_wide_pre[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b40fbb61-9229-40b8-bc29-a4045521a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide_pre_df = pd.DataFrame(X_wide_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf8db2e8-2be9-4615-a2ce-1899d1ea965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>77995</td>\n",
       "      <td>161433</td>\n",
       "      <td>199700</td>\n",
       "      <td>281240</td>\n",
       "      <td>365284</td>\n",
       "      <td>452743</td>\n",
       "      <td>460202</td>\n",
       "      <td>536580</td>\n",
       "      <td>550629</td>\n",
       "      <td>581609</td>\n",
       "      <td>660306</td>\n",
       "      <td>693792</td>\n",
       "      <td>694221</td>\n",
       "      <td>736126</td>\n",
       "      <td>780964</td>\n",
       "      <td>793934</td>\n",
       "      <td>793949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>77996</td>\n",
       "      <td>161434</td>\n",
       "      <td>199701</td>\n",
       "      <td>281241</td>\n",
       "      <td>365285</td>\n",
       "      <td>452744</td>\n",
       "      <td>460203</td>\n",
       "      <td>536581</td>\n",
       "      <td>550630</td>\n",
       "      <td>581610</td>\n",
       "      <td>660307</td>\n",
       "      <td>693793</td>\n",
       "      <td>694222</td>\n",
       "      <td>736127</td>\n",
       "      <td>780965</td>\n",
       "      <td>793935</td>\n",
       "      <td>793950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>77997</td>\n",
       "      <td>161435</td>\n",
       "      <td>199702</td>\n",
       "      <td>281242</td>\n",
       "      <td>365286</td>\n",
       "      <td>452745</td>\n",
       "      <td>460204</td>\n",
       "      <td>536582</td>\n",
       "      <td>550631</td>\n",
       "      <td>581611</td>\n",
       "      <td>660308</td>\n",
       "      <td>693794</td>\n",
       "      <td>694223</td>\n",
       "      <td>736128</td>\n",
       "      <td>780966</td>\n",
       "      <td>793936</td>\n",
       "      <td>793951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>77998</td>\n",
       "      <td>161436</td>\n",
       "      <td>199703</td>\n",
       "      <td>281243</td>\n",
       "      <td>365287</td>\n",
       "      <td>452746</td>\n",
       "      <td>460205</td>\n",
       "      <td>536583</td>\n",
       "      <td>550632</td>\n",
       "      <td>581612</td>\n",
       "      <td>660309</td>\n",
       "      <td>693795</td>\n",
       "      <td>694224</td>\n",
       "      <td>736129</td>\n",
       "      <td>780967</td>\n",
       "      <td>793937</td>\n",
       "      <td>793952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>77999</td>\n",
       "      <td>161437</td>\n",
       "      <td>199704</td>\n",
       "      <td>281244</td>\n",
       "      <td>365288</td>\n",
       "      <td>452747</td>\n",
       "      <td>460206</td>\n",
       "      <td>536584</td>\n",
       "      <td>550633</td>\n",
       "      <td>581613</td>\n",
       "      <td>660310</td>\n",
       "      <td>693794</td>\n",
       "      <td>694225</td>\n",
       "      <td>736130</td>\n",
       "      <td>780968</td>\n",
       "      <td>793938</td>\n",
       "      <td>793953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1       2       3       4       5       6       7       8       9   \\\n",
       "0   1  77995  161433  199700  281240  365284  452743  460202  536580  550629   \n",
       "1   2  77996  161434  199701  281241  365285  452744  460203  536581  550630   \n",
       "2   3  77997  161435  199702  281242  365286  452745  460204  536582  550631   \n",
       "3   4  77998  161436  199703  281243  365287  452746  460205  536583  550632   \n",
       "4   5  77999  161437  199704  281244  365288  452747  460206  536584  550633   \n",
       "\n",
       "       10      11      12      13      14      15      16      17  \n",
       "0  581609  660306  693792  694221  736126  780964  793934  793949  \n",
       "1  581610  660307  693793  694222  736127  780965  793935  793950  \n",
       "2  581611  660308  693794  694223  736128  780966  793936  793951  \n",
       "3  581612  660309  693795  694224  736129  780967  793937  793952  \n",
       "4  581613  660310  693794  694225  736130  780968  793938  793953  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_wide_pre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "619e112c-4969-45eb-bc93-50fb2c079bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>7.814957</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>7.795214</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>7.846019</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>7.784252</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>7.80212</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>7.858285</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>7.799560</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>7.814957</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>7.795214</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>7.804170</td>\n",
       "      <td>7.809044</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>7.806991</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>7.801096</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>7.828890</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>7.858285</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         128       129       130       131       132       133       134  \\\n",
       "id                                                                         \n",
       "0  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "1  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "2  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "3  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "4  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "\n",
       "         135       136       137      138       139       140       141  \\\n",
       "id                                                                        \n",
       "0  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "1  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "2  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "3  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "4  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "\n",
       "         142      143       144       145       146       147       148  \\\n",
       "id                                                                        \n",
       "0  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "1  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "2   7.795214 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "3  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "4   7.795214 -0.12801 -0.127939 -0.127508  7.804170  7.809044 -0.128052   \n",
       "\n",
       "         149       150       151       152       153      154       155  \\\n",
       "id                                                                        \n",
       "0  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "1  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "2  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "3  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "4  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "\n",
       "         156       157       158       159       160       161       162  \\\n",
       "id                                                                         \n",
       "0  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "1  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "2  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "3  -0.127888 -0.127791 -0.128992 -0.128368  7.799560 -0.127182 -0.127546   \n",
       "4  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "\n",
       "         163       164       165       166      167       168       169  \\\n",
       "id                                                                        \n",
       "0  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "1  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "2  -0.127669  7.846019 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "3  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "4  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "\n",
       "         170       171       172       173       174       175      176  \\\n",
       "id                                                                        \n",
       "0  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "1  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "2  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "3  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "4  -0.128057  7.806991 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "\n",
       "         177       178       179       180      181       182       183  \\\n",
       "id                                                                        \n",
       "0  -0.128506  7.814957 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "1  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "2  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "3  -0.128506  7.814957 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "4  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "\n",
       "         184       185       186       187       188       189       190  \\\n",
       "id                                                                         \n",
       "0  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "1  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "2  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "3  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "4  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "\n",
       "         191       192       193       194       195       196       197  \\\n",
       "id                                                                         \n",
       "0  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "1  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "2  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745  7.784252 -0.127242   \n",
       "3  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "4  -0.128574 -0.127884 -0.127884  7.801096 -0.128745 -0.128464 -0.127242   \n",
       "\n",
       "        198       199       200      201       202       203       204  \\\n",
       "id                                                                       \n",
       "0  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "1  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "2  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "3  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "4  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "\n",
       "         205       206       207       208       209       210       211  \\\n",
       "id                                                                         \n",
       "0  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "1  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "2  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "3  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "4  -0.128031  7.828890 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "\n",
       "         212       213      214       215       216      217       218  \\\n",
       "id                                                                       \n",
       "0  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "1  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "2  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015  7.80212 -0.128275   \n",
       "3  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "4  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "\n",
       "         219       220       221       222       223       224       225  \\\n",
       "id                                                                         \n",
       "0  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "1  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "2  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339  7.858285 -0.127352   \n",
       "3  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "4  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339  7.858285 -0.127352   \n",
       "\n",
       "         226       227      228       229       230      231       232  \\\n",
       "id                                                                       \n",
       "0  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "1  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "2  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "3  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "4  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "\n",
       "         233       234       235       236       237       238       239  \\\n",
       "id                                                                         \n",
       "0  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "1  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "2  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "3  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "4  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "\n",
       "         240       241       242      243       244      245  \n",
       "id                                                            \n",
       "0  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "1  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "2  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "3  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "4  -0.127119 -0.127985 -0.128494 -0.12862  7.821398 -0.12703  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.loc[:, wide_cols_onehot].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4ad554a-9bff-402c-a202-b9059e9d7c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# X_wide = X_wide_pre_df.join(X.loc[:,wide_cols_onehot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23289b-824b-4384-84ad-783671d680d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da2a03e5-fa67-4bcd-bd82-257d07fbf13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wide = X_wide_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4e3a43e-4052-4461-bd36-ba8400145eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eaac21f-fbb0-4d37-a70f-16dcb116192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols)#, embed_cols=embed_cols, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7a2446c-4182-45be-a5c2-06d450f578c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 957919 entries, 0 to 957918\n",
      "Columns: 246 entries, 0 to 245\n",
      "dtypes: float64(246)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "# X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64cf66a6-7382-4358-9617-1f10cda0af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tab = tab_preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9825519e-8aa8-498b-aca6-1ba1a98592c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 240)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14d819db-4b99-4f60-98be-4dc6a323c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptabular = TabMlp(\n",
    "    mlp_hidden_dims=[64,32],\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "#     embed_input=tab_preprocessor.embeddings_input,\n",
    "    continuous_cols=cont_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7306cc9-5e5e-49a1-b6cc-2e8d789f9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3a253cf-51d3-4b59-97ae-a6b7e060265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide = np.array(X_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de8d01a0-3f3b-4e8e-861b-c9da6a8db682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 45)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7800573b-d576-4687-9917-cf4f6a2d19f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 240)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dade972-be94-44b6-8b48-1b53c57c6b19",
   "metadata": {},
   "source": [
    "<!-- 39774, 758737, 552968 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af89e657-f704-43fd-b76f-1055ccb229ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy], seed=42, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fe7e338-d8c6-44ed-8e51-32f5c35e622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2abd8b0-994f-4cc1-aa4e-6ea2e20049ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|| 782/782 [00:10<00:00, 76.13it/s, loss=0.697, metrics={'acc': 0.6645}]\n",
      "valid: 100%|| 196/196 [00:02<00:00, 80.95it/s, loss=0.515, metrics={'acc': 0.7484}] \n",
      "epoch 2: 100%|| 782/782 [00:09<00:00, 81.18it/s, loss=0.501, metrics={'acc': 0.7569}]\n",
      "valid: 100%|| 196/196 [00:02<00:00, 79.99it/s, loss=0.487, metrics={'acc': 0.7639}] \n",
      "epoch 3: 100%|| 782/782 [00:09<00:00, 78.82it/s, loss=0.492, metrics={'acc': 0.7614}]\n",
      "valid: 100%|| 196/196 [00:02<00:00, 78.82it/s, loss=0.485, metrics={'acc': 0.7639}] \n",
      "epoch 4: 100%|| 782/782 [00:10<00:00, 77.82it/s, loss=0.49, metrics={'acc': 0.7621}] \n",
      "valid: 100%|| 196/196 [00:01<00:00, 101.71it/s, loss=0.484, metrics={'acc': 0.7649}]\n",
      "epoch 5: 100%|| 782/782 [00:09<00:00, 81.42it/s, loss=0.488, metrics={'acc': 0.7628}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 101.85it/s, loss=0.484, metrics={'acc': 0.765}] \n",
      "epoch 6: 100%|| 782/782 [00:09<00:00, 81.99it/s, loss=0.488, metrics={'acc': 0.7628}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 102.36it/s, loss=0.484, metrics={'acc': 0.7648}]\n",
      "epoch 7: 100%|| 782/782 [00:09<00:00, 83.21it/s, loss=0.487, metrics={'acc': 0.7634}] \n",
      "valid: 100%|| 196/196 [00:01<00:00, 100.29it/s, loss=0.484, metrics={'acc': 0.765}] \n",
      "epoch 8: 100%|| 782/782 [00:09<00:00, 81.24it/s, loss=0.486, metrics={'acc': 0.764}] \n",
      "valid: 100%|| 196/196 [00:01<00:00, 105.45it/s, loss=0.484, metrics={'acc': 0.7647}]\n",
      "epoch 9: 100%|| 782/782 [00:09<00:00, 82.83it/s, loss=0.486, metrics={'acc': 0.7641}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 104.63it/s, loss=0.483, metrics={'acc': 0.7651}]\n",
      "epoch 10: 100%|| 782/782 [00:09<00:00, 80.59it/s, loss=0.486, metrics={'acc': 0.7641}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 100.68it/s, loss=0.484, metrics={'acc': 0.7647}]\n",
      "epoch 11: 100%|| 782/782 [00:09<00:00, 81.84it/s, loss=0.485, metrics={'acc': 0.7645}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 104.89it/s, loss=0.483, metrics={'acc': 0.7649}]\n",
      "epoch 12: 100%|| 782/782 [00:09<00:00, 80.77it/s, loss=0.484, metrics={'acc': 0.7646}]\n",
      "valid: 100%|| 196/196 [00:02<00:00, 96.98it/s, loss=0.484, metrics={'acc': 0.7649}] \n",
      "epoch 13: 100%|| 782/782 [00:09<00:00, 81.14it/s, loss=0.484, metrics={'acc': 0.7649}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 103.10it/s, loss=0.484, metrics={'acc': 0.7649}]\n",
      "epoch 14: 100%|| 782/782 [00:09<00:00, 84.12it/s, loss=0.484, metrics={'acc': 0.7646}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 99.60it/s, loss=0.483, metrics={'acc': 0.764}]  \n",
      "epoch 15: 100%|| 782/782 [00:09<00:00, 79.70it/s, loss=0.483, metrics={'acc': 0.7649}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 100.49it/s, loss=0.484, metrics={'acc': 0.7646}]\n",
      "epoch 16: 100%|| 782/782 [00:09<00:00, 82.80it/s, loss=0.483, metrics={'acc': 0.765}] \n",
      "valid: 100%|| 196/196 [00:01<00:00, 105.81it/s, loss=0.484, metrics={'acc': 0.7648}]\n",
      "epoch 17: 100%|| 782/782 [00:09<00:00, 83.03it/s, loss=0.483, metrics={'acc': 0.7656}]\n",
      "valid: 100%|| 196/196 [00:02<00:00, 96.79it/s, loss=0.484, metrics={'acc': 0.765}]  \n",
      "epoch 18: 100%|| 782/782 [00:10<00:00, 76.30it/s, loss=0.483, metrics={'acc': 0.7655}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 100.26it/s, loss=0.484, metrics={'acc': 0.7645}]\n",
      "epoch 19: 100%|| 782/782 [00:10<00:00, 76.32it/s, loss=0.482, metrics={'acc': 0.7658}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 106.93it/s, loss=0.484, metrics={'acc': 0.7645}]\n",
      "epoch 20: 100%|| 782/782 [00:09<00:00, 79.39it/s, loss=0.482, metrics={'acc': 0.7658}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 101.17it/s, loss=0.484, metrics={'acc': 0.7645}]\n",
      "epoch 21: 100%|| 782/782 [00:09<00:00, 78.97it/s, loss=0.482, metrics={'acc': 0.7656}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 100.46it/s, loss=0.485, metrics={'acc': 0.7648}]\n",
      "epoch 22: 100%|| 782/782 [00:09<00:00, 82.59it/s, loss=0.482, metrics={'acc': 0.7656}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 101.10it/s, loss=0.485, metrics={'acc': 0.7647}]\n",
      "epoch 23: 100%|| 782/782 [00:09<00:00, 83.66it/s, loss=0.482, metrics={'acc': 0.7663}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 100.23it/s, loss=0.484, metrics={'acc': 0.7646}]\n",
      "epoch 24: 100%|| 782/782 [00:09<00:00, 80.37it/s, loss=0.482, metrics={'acc': 0.7659}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 102.07it/s, loss=0.484, metrics={'acc': 0.7643}]\n",
      "epoch 25: 100%|| 782/782 [00:09<00:00, 83.45it/s, loss=0.481, metrics={'acc': 0.7664}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 105.80it/s, loss=0.484, metrics={'acc': 0.7641}]\n",
      "epoch 26: 100%|| 782/782 [00:09<00:00, 80.26it/s, loss=0.481, metrics={'acc': 0.7664}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 102.26it/s, loss=0.484, metrics={'acc': 0.7647}]\n",
      "epoch 27: 100%|| 782/782 [00:09<00:00, 81.11it/s, loss=0.481, metrics={'acc': 0.7662}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 104.00it/s, loss=0.484, metrics={'acc': 0.764}] \n",
      "epoch 28: 100%|| 782/782 [00:09<00:00, 80.91it/s, loss=0.481, metrics={'acc': 0.7665}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 100.73it/s, loss=0.484, metrics={'acc': 0.7643}]\n",
      "epoch 29: 100%|| 782/782 [00:09<00:00, 83.16it/s, loss=0.481, metrics={'acc': 0.7665}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 100.73it/s, loss=0.484, metrics={'acc': 0.7639}]\n",
      "epoch 30: 100%|| 782/782 [00:09<00:00, 81.25it/s, loss=0.481, metrics={'acc': 0.7664}]\n",
      "valid: 100%|| 196/196 [00:01<00:00, 103.09it/s, loss=0.485, metrics={'acc': 0.7645}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    X_wide=X_wide,\n",
    "    X_tab=X_tab,\n",
    "    target=y,\n",
    "    n_epochs=30,\n",
    "    batch_size=1024,\n",
    "    val_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f56b61ab-bcf2-430e-bee5-79d7faacb7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_feather(datapath/test_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca5cde74-1623-40b7-879e-87cb24b397b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# X_test = X_test.to_numpy()\n",
    "X_wide_te = wide_preprocessor.fit_transform(X_test)\n",
    "X_tab_te = tab_preprocessor.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5406295-1bcb-4be3-8515-aa756debd783",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,\n",
       "        33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63,\n",
       "        65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89],\n",
       "       [ 2,  4,  5,  7, 10, 12, 13, 15, 17, 19, 21, 24, 26, 27, 30, 31,\n",
       "        34, 35, 37, 40, 41, 43, 45, 47, 50, 52, 54, 56, 57, 60, 61, 63,\n",
       "        65, 67, 70, 72, 73, 75, 77, 79, 81, 84, 86, 88, 89],\n",
       "       [ 1,  4,  5,  8,  9, 12, 13, 15, 17, 20, 22, 23, 26, 27, 29, 32,\n",
       "        34, 35, 38, 39, 41, 43, 45, 47, 49, 51, 54, 56, 57, 60, 62, 63,\n",
       "        66, 68, 70, 72, 73, 75, 77, 79, 81, 83, 86, 88, 89],\n",
       "       [ 1,  4,  5,  7, 10, 11, 13, 15, 17, 20, 22, 23, 26, 27, 29, 31,\n",
       "        34, 36, 38, 40, 41, 43, 45, 47, 49, 52, 53, 56, 57, 59, 61, 64,\n",
       "        66, 67, 70, 72, 73, 75, 77, 79, 82, 83, 86, 88, 89],\n",
       "       [ 2,  4,  5,  8, 10, 12, 13, 16, 17, 19, 22, 24, 26, 27, 29, 32,\n",
       "        33, 35, 38, 40, 41, 43, 45, 47, 50, 52, 53, 56, 58, 59, 62, 63,\n",
       "        65, 68, 69, 72, 73, 75, 77, 80, 81, 84, 85, 88, 89],\n",
       "       [ 2,  4,  6,  8,  9, 12, 13, 16, 17, 20, 22, 24, 25, 27, 30, 32,\n",
       "        33, 35, 38, 39, 41, 43, 45, 48, 50, 52, 53, 55, 57, 60, 62, 63,\n",
       "        65, 67, 70, 71, 73, 75, 77, 79, 81, 84, 86, 88, 89],\n",
       "       [ 2,  4,  5,  7, 10, 11, 13, 16, 18, 20, 21, 23, 25, 27, 29, 32,\n",
       "        34, 35, 38, 39, 41, 43, 45, 48, 50, 52, 53, 56, 57, 60, 61, 64,\n",
       "        65, 67, 69, 72, 74, 75, 77, 79, 81, 84, 86, 87, 89],\n",
       "       [ 2,  4,  5,  7,  9, 12, 14, 15, 17, 20, 22, 24, 26, 27, 29, 32,\n",
       "        33, 35, 37, 39, 42, 44, 45, 47, 50, 51, 53, 56, 57, 60, 61, 64,\n",
       "        65, 67, 70, 72, 74, 75, 78, 79, 81, 84, 86, 88, 89],\n",
       "       [ 1,  4,  5,  8, 10, 12, 13, 16, 17, 19, 21, 23, 26, 27, 29, 32,\n",
       "        34, 35, 37, 40, 41, 43, 45, 48, 49, 51, 54, 56, 58, 60, 62, 64,\n",
       "        66, 68, 69, 71, 73, 75, 77, 79, 81, 83, 86, 88, 89],\n",
       "       [ 1,  4,  5,  7, 10, 12, 13, 15, 17, 19, 22, 23, 26, 28, 29, 32,\n",
       "        33, 35, 38, 39, 41, 44, 45, 47, 49, 51, 53, 56, 57, 60, 61, 63,\n",
       "        65, 67, 69, 72, 73, 75, 77, 79, 81, 83, 86, 88, 90]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_te[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be80ca97-efab-4bda-889c-e572aeb04999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62606923, -0.18969476, -0.92787077, ..., -1.60943639,\n",
       "         0.22706685, -1.07477182],\n",
       "       [-0.57253077,  0.2098222 , -0.82945475, ..., -0.27047229,\n",
       "        -0.8826086 , -0.40752455],\n",
       "       [-0.95243505, -0.03722716,  1.08705115, ..., -0.24144445,\n",
       "         0.5487114 , -0.00957377],\n",
       "       ...,\n",
       "       [-0.08712007,  1.51269865,  0.38352908, ..., -0.24936747,\n",
       "        -1.37592574, -1.2029806 ],\n",
       "       [-0.1412585 ,  1.40645559,  2.19939253, ..., -1.62665483,\n",
       "         2.64911658,  1.50194527],\n",
       "       [-0.52762128,  2.71380286, -0.24626641, ..., -0.26179802,\n",
       "         0.12574403, -0.6659266 ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab_te[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c904b869-0b6f-443c-895f-b93ca4f80e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 45)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc2df00a-c9d6-4dc7-9a5f-54f496ec2580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 240)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f17cd1d-c053-4fee-b08f-8f6e3ee0a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = pd.read_feather(datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')\n",
    "\n",
    "# low_card_features = [f for f in X_test.columns if X_test[f].nunique() <= 50000]\n",
    "# high_card_features = [f for f in X_test.columns if X_test[f].nunique() > 50000]\n",
    "\n",
    "# wide_cols_pre = [f for f in X_test.columns if X_test[f].nunique() <= max_card_cat and X_test[f].nunique() > 2]\n",
    "# wide_cols_onehot = [f for f in X_test.columns if X_test[f].nunique() == 2]\n",
    "# cont_cols = high_card_features\n",
    "# embed_cols = [f for f in X_test.columns if X_test[f].nunique() <= max_card_embed and X_test[f].nunique() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c06afef5-79e2-4751-8e79-57037f1cf393",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-68a894bef700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# X_test = X_test.to_numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_wide_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwide_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_tab_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtab_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;34mr\"\"\"Returns the processed dataframe\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mdf_wide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_wide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_wide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_crossed_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_crossed_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py\u001b[0m in \u001b[0;36m_prepare_wide\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_cc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# # X_test = X_test.to_numpy()\n",
    "# X_wide_te = wide_preprocessor.transform(X_test)\n",
    "# X_tab_te = tab_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9d22a-cce9-4370-8192-13a432bdd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide_te = wide_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb9552-d4b9-4e83-9cb4-fd22c4d3f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = trainer.predict(X_wide=X_wide_te, X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078f3c6-b2af-412c-8f3d-a5866f13d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8de5b122-a5e4-4ae8-ad5d-bae73d59e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|| 489/489 [00:02<00:00, 176.22it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_proba = trainer.predict_proba(X_wide=X_wide_te, X_tab=X_tab_te, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed2697c9-926b-4802-945b-a768631748e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|| 977/977 [00:04<00:00, 212.77it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_proba_train = trainer.predict_proba(X_wide=X_wide, X_tab=X_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a57780a-421e-4624-972f-b3c654ae4db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8562719341662627"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_score=preds_proba_train[:,1], y_true=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a89c15b-a44f-4a20-95cd-f5a94c062a1a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44021851, 0.55978149],\n",
       "       [0.72445405, 0.27554595],\n",
       "       [0.13637155, 0.86362845],\n",
       "       [0.45089591, 0.54910409],\n",
       "       [0.10132414, 0.89867586],\n",
       "       [0.45762271, 0.54237729],\n",
       "       [0.13202792, 0.86797208],\n",
       "       [0.03556085, 0.96443915],\n",
       "       [0.20678174, 0.79321826],\n",
       "       [0.13734156, 0.86265844],\n",
       "       [0.90057671, 0.09942328],\n",
       "       [0.34125483, 0.65874517],\n",
       "       [0.18667585, 0.81332415],\n",
       "       [0.31571954, 0.68428046],\n",
       "       [0.69393456, 0.30606544],\n",
       "       [0.84645522, 0.15354478],\n",
       "       [0.97542024, 0.02457974],\n",
       "       [0.92568088, 0.07431912],\n",
       "       [0.04913533, 0.95086467],\n",
       "       [0.84242821, 0.15757181]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_proba_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dba64d2b-96a1-4dc9-a837-c27ce7ea5c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82947814, 0.17052183],\n",
       "       [0.15631253, 0.84368747],\n",
       "       [0.670138  , 0.329862  ],\n",
       "       [0.78820115, 0.21179883],\n",
       "       [0.27101159, 0.72898841],\n",
       "       [0.3710376 , 0.6289624 ],\n",
       "       [0.57676411, 0.42323589],\n",
       "       [0.14067805, 0.85932195],\n",
       "       [0.75468326, 0.24531674],\n",
       "       [0.66803461, 0.33196539],\n",
       "       [0.76909131, 0.23090869],\n",
       "       [0.07448471, 0.92551529],\n",
       "       [0.20164472, 0.79835528],\n",
       "       [0.14791131, 0.85208869],\n",
       "       [0.17014694, 0.82985306],\n",
       "       [0.34611297, 0.65388703],\n",
       "       [0.16073269, 0.83926731],\n",
       "       [0.12745643, 0.87254357],\n",
       "       [0.35510087, 0.64489913],\n",
       "       [0.75042468, 0.24957532]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_proba[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a7ea2fe-0f38-4b5b-b9e9-913d8d33e0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/sf/code/kaggle/tabular_playgrounds/oct2021/preds/widedeep_30epochs_bs1024_64x32tabmlp_20211008_test_preds.joblib']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(preds_proba, predpath/'widedeep_30epochs_bs1024_64x32tabmlp_20211008_test_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a3aceea-4794-4f32-bff1-85f5adca222c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90d3c1bc-c093-44f5-bc2e-d9bd05a91176",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'target'] = preds_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0047a2be-2982-461b-aa9a-45d162df091b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0.170522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>0.843687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000002</td>\n",
       "      <td>0.329862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000003</td>\n",
       "      <td>0.211799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000004</td>\n",
       "      <td>0.728988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    target\n",
       "0  1000000  0.170522\n",
       "1  1000001  0.843687\n",
       "2  1000002  0.329862\n",
       "3  1000003  0.211799\n",
       "4  1000004  0.728988"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef4d87-7a55-483a-989f-4acc5af54185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_path = datapath/'submissions'\n",
    "# submission_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8101aa75-ae8f-4fa1-aedf-4f6daaf9ec79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "sample_df.to_csv(subpath/f\"{wandb_config['name']}_widedeep_30epochs_bs1024_64x32tabmlp_20211008_test_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160d6dc-f814-42d4-8bea-89ea5a021937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b1e95-dc11-4648-ad00-f3e81cf9574a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede78877-879f-42a7-827d-4babd3cad534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719be91-1a0b-42c5-8f93-b6a02f677d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37f092-edb9-43a5-b876-073088a54d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621dfbe7-368e-441f-a367-81a090e7d0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4a5f215-b585-4f7b-afac-36e49ee28c8f",
   "metadata": {},
   "source": [
    "## Weights and Biases Run Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a928e1-0a18-4c91-b9bb-a32846e39e5b",
   "metadata": {},
   "source": [
    "Below is the configuration for a Weights and Biases (`wandb`) run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5402d98-7bcd-4e46-b85d-6b0129ca6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "config_run = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['stacking-sklearn', 'attempt'],\n",
    "    'notes': \"Trying a fastai tabular MLP model, for ensembling with the GBMs\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638002ad-9266-44d6-8302-ebce2a6f7b06",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30d6f8-d893-43d3-8a4d-41c5756b6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379b0bf-1c3b-4095-b382-ec5a16e3474d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24391812-dce3-4513-bd38-ee95694730e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_valid, y_train, y_valid, model_config, \n",
    "                                              random_state=42,\n",
    "                                              exmodel_config=exmodel_config, \n",
    "                                              config_run=config_run):#, scaler): # passed in via config dict for now\n",
    "    \"\"\"\n",
    "    Basic training function. Note that some of the options passed via the argument are\n",
    "    in fact hard-coded in, to avoid inconveniences.\n",
    "    :param X_train: the training set features\n",
    "    :param X_valid: the validation set features\n",
    "    :param y_train: the training set targets\n",
    "    :param y_valid: the validation set targets\n",
    "    :param random_staKFold: for reproducibility\n",
    "    :param exmodel_config: dict containing configuration details including the library \n",
    "                            (thus model) used, preprocessing, and cross-validation\n",
    "    :param model_config: dict containing hyperparameter specifications for the model\n",
    "    :param config_run: dict containing wandb run configuration (name, etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    # As of 20210920, best CatBoost config is:\n",
    "    best_20210920_catboost_params = {\n",
    "        'iterations': 3493,\n",
    "        'depth': 5,\n",
    "        'learning_rate': 0.09397459954141321,\n",
    "        'random_strength': 43,\n",
    "        'l2_leaf_reg': 26,\n",
    "        'border_count': 239,\n",
    "        'bagging_temperature': 12.532400413798356,\n",
    "        'od_type': 'Iter'\n",
    "    }\n",
    "    \n",
    "    # catboost 20210921 on colab (only 15 trials though)\n",
    "    best_catboost_params = {\n",
    "        'iterations': 3302,\n",
    "        'depth': 5,\n",
    "        'learning_rate': 0.017183208677599107,\n",
    "        'random_strength': 41,\n",
    "        'l2_leaf_reg': 30,\n",
    "        'border_count': 251,\n",
    "        'bagging_temperature': 9.898390369028036, \n",
    "        'od_type': 'IncToDec'\n",
    "    }\n",
    "    \n",
    "    # optuna 20210921\n",
    "    best_xgboost_params = {\n",
    "        'n_estimators': 1119,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.04123392555159452,\n",
    "        'reg_alpha': 4.511876752318655,\n",
    "        'reg_lambda': 4.074347238862406,\n",
    "        'subsample': 0.8408586950521992\n",
    "    }\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"202109_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=exmodel_config)   \n",
    "        \n",
    "    if exmodel_config['library'] == 'xgboost':\n",
    "        model = XGBClassifier(\n",
    "            tree_method=model_config['tree_method'],\n",
    "            random_state=random_state,\n",
    "            n_jobs=model_config['n_jobs'], \n",
    "            verbosity=model_config['verbosity'], \n",
    "            objective=model_config['objective'],\n",
    "            **best_xgboost_params\n",
    "            # #             eval_metric=model_config['eval_metric'],\n",
    "\n",
    "            # comment out the below for a fairly default model\n",
    "#             booster=model_config['booster'],\n",
    "#             max_depth=model_config['max_depth'],\n",
    "#             learning_rate=model_config['learning_rate'], \n",
    "#             subsample=model_config['subsample'],\n",
    "#             reg_alpha=model_config['reg_alpha'],\n",
    "#             reg_lambda=model_config['reg_lambda'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()],\n",
    "#                                     eval_metric=model_config['eval_metric'],\n",
    "                 )\n",
    "\n",
    "\n",
    "    elif exmodel_config['library'] == 'lightgbm':\n",
    "        model = LGBMClassifier(\n",
    "#             boosting_type=model_config['boosting_type'],\n",
    "#             max_depth=model_config['max_depth']\n",
    "            # TODO\n",
    "            random_state=random_state,\n",
    "            n_jobs=model_config['n_jobs'],\n",
    "            objective=model_config['objective'],\n",
    "#             eval_metric=model_config['eval_metric'],\n",
    "            boosting_type=model_config['boosting_type'],\n",
    "            device_type=model_config['device_type'],\n",
    "            \n",
    "            # comment out the below for a basically default model\n",
    "            n_estimators=model_config['n_estimators'],\n",
    "            learning_rate=model_config['learning_rate'],\n",
    "            max_depth=model_config['max_depth'],\n",
    "            reg_alpha=model_config['reg_alpha'],\n",
    "            reg_lambda=model_config['reg_lambda'],\n",
    "            subsample=model_config['subsample'],\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],\n",
    "#                                     eval_metric=model_config['eval_metric'],\n",
    "                 )\n",
    "        \n",
    "    elif exmodel_config['library'] == 'catboost':\n",
    "        print(\"CatBoost, therefore no WandB callback.\")\n",
    "        model = CatBoostClassifier(\n",
    "#             n_estimators=config['n_estimators'],\n",
    "#             learning_rate=config['learning_rate'],\n",
    "#             max_depth=config['max_depth'],\n",
    "            task_type=model_config['task_type'],\n",
    "    #         n_jobs=config['n_jobs'],\n",
    "    #         verbosity=config['verbosity'],\n",
    "    #         subsample=config['subsample'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "            random_state=random_state,\n",
    "            # objective='Logloss', # default, accepts only one\n",
    "#             custom_metrics=model_config['custom_metrics'],\n",
    "    #         bootstrap_type=config['bootstrap_type'],\n",
    "    #         device:config['device']\n",
    "            **best_catboost_params\n",
    "        ) \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "#     y_train_pred = model.predict(X_train)\n",
    "    y_train_pred = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    train_loss = log_loss(y_train, y_train_pred)\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    wandb.log({'train_loss': train_loss, 'train_auc': train_auc})\n",
    "\n",
    "    if exmodel_config['library'] == 'catboost':\n",
    "        print(model.get_all_params())\n",
    "        wandb.log(model.get_all_params())\n",
    "    else:\n",
    "        wandb.log(model.get_params()) # logging model parameters, trying bare-invocation rather than params: model.get_params()\n",
    "    \n",
    "    # trying with predict_proba\n",
    "    y_pred = model.predict_proba(X_valid)[:,1]\n",
    "#     y_pred = model.predict(X_valid)\n",
    "\n",
    "    valid_loss = log_loss(y_valid, y_pred)\n",
    "    valid_auc = roc_auc_score(y_valid, y_pred)\n",
    "    wandb.log({'valid_loss':valid_loss, 'valid_auc':valid_auc})\n",
    "    print(f\"Valid log-loss is {valid_loss}\\nValid AUC is {valid_auc}\")   \n",
    "#     wandb.finish()   \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfa66428-3fb9-410f-9ea5-50785a4bd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model_config, X=X, y=y, start_fold=0, exmodel_config=exmodel_config, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to handle model training process in the context of cross-validation -- via hold-out or via k-fold.\n",
    "    If exmodel_config['cross_val_strategy'] == None, then any kfolds= input is ignored; otherwise, the number specified is used.\n",
    "    \n",
    "    :param kfolds: int specifying number of k-folds to use in cross-validation\n",
    "    :param exmodel_config: dict containing general config including for cross-validation -- `kfold=1` implies hold-out\n",
    "    \"\"\"\n",
    "    if exmodel_config['kfolds'] == 1:\n",
    "        print(\"Proceeding with holdout\")\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=exmodel_config['test_size'], \n",
    "                                                      random_state=random_state,\n",
    "                                                     )\n",
    "        model = train(X_train, X_valid, y_train, y_valid, exmodel_config=exmodel_config, \n",
    "                                                    model_config=model_config,\n",
    "                                                    config_run=config_run)\n",
    "        wandb.finish()\n",
    "        \n",
    "    else:\n",
    "        X, y = X.to_numpy(), y.to_numpy()\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=random_state)\n",
    "        models = {}\n",
    "        model_path = Path(datapath/f\"models/{config_run['name']}_{exmodel_config['kfolds']}folds/\")\n",
    "        (model_path).mkdir(exist_ok=True)\n",
    "        for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "            if fold < start_fold:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"FOLD {fold}\")\n",
    "                print(\"---------------------------------------------------\")\n",
    "                X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "                model = train(X_train, X_valid, y_train, y_valid, exmodel_config=exmodel_config, \n",
    "                                                    model_config=model_config,\n",
    "                                                    config_run=config_run)\n",
    "                wandb.log({'fold': fold})\n",
    "                models[fold] = model\n",
    "                dump(model, Path(model_path/f\"{exmodel_config['library']}_fold{fold}_model.joblib\"))\n",
    "                wandb.finish()\n",
    "        return models\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a437b-f880-4284-8e02-66a398ba6454",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58234814-68d1-4ee4-bedd-d722c18e4fa6",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4085e60e-13cf-4da7-90b9-30306480b4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library = 'xgboost'\n",
    "# exmodel_config['library'] = library\n",
    "# model_config = model_configurator(library)\n",
    "# xgboost_models = cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b780292-f23c-4a3f-be1e-53038c5ae7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for scaler in [StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler]:\n",
    "#     exmodel_config['scaler'] = scaler\n",
    "#     scaler = scaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "#     exmodel_config['library'] = 'lightgbm'\n",
    "#     model_config = model_configurator('lightgbm')\n",
    "#     cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1106b86c-70c1-4722-a86f-6c53a2e32503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library = 'lightgbm'\n",
    "# exmodel_config['library'] = library\n",
    "# model_config = model_configurator(library)\n",
    "# lightgbm_models = cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542539d-323d-4a4a-b574-d0bf883179b2",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776a94d-01f3-46c0-9c9c-0d162de17e37",
   "metadata": {},
   "source": [
    "## Via `sklearn.ensemble.StackingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca87f5f-e214-4968-a789-74f83dc7a656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e68e3adf-fd9d-4185-87c4-330ecd768679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost_estimators = [(f'xgboost_fold{fold}', xgboost_models[fold]) for fold in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed3aa865-fec7-417b-9369-677d3b840760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaving this default for first try\n",
    "# final_estimator = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ac5214d-a3ab-4219-8047-822c4385e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacker(estimators:dict, library:str, X=X, y=y): #, load_models:bool=False, load_path:Path=None):\n",
    "    \"\"\"\n",
    "    A wrapper that will take a dict of the form {fold:int : model} and a string representing the library (for file-naming), \n",
    "    then run `sklearn.ensemble.StackingClassifier` with it, and save the stacked model afterward\n",
    "    \"\"\"\n",
    "    estimators_list = [(f'{library}_fold{fold}', estimators[fold]) for fold in range(5)]\n",
    "    blender = StackingClassifier(estimators=estimators_list,\n",
    "                                 cv=5,\n",
    "                                 stack_method='predict_proba',\n",
    "                                 n_jobs=2,\n",
    "                                 passthrough=False,\n",
    "                                 verbose=1\n",
    "                                )\n",
    "    print(f\"Starting fitting at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    blender.fit(X,y)\n",
    "    print(f\"Fitting complete at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    dump(blender, filename=datapath/f\"models/{config_run['name']}_{exmodel_config['kfolds']}folds/{library}_stack.joblib\")\n",
    "    print(f\"Blender model saved at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    return blender\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39f08073-4061-4f03-be9c-1c1fa601b624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">catboost_20210922_112905</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/drhfu0df\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/drhfu0df</a><br/>\n",
       "                Run data is saved locally in <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_112905-drhfu0df</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# might encapsulate this in a new version of the above train function later\n",
    "exmodel_config['ensemble'] = 'stacking'\n",
    "\n",
    "wandb.init(\n",
    "        project=\"202109_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=exmodel_config)   \n",
    "\n",
    "random_state = exmodel_config['random_state'] # 42\n",
    "\n",
    "\n",
    "# # optuna 20210921\n",
    "# best_xgboost_params = {\n",
    "#     'n_estimators': 1119,\n",
    "#     'max_depth': 6,\n",
    "#     'learning_rate': 0.04123392555159452,\n",
    "#     'reg_alpha': 4.511876752318655,\n",
    "#     'reg_lambda': 4.074347238862406,\n",
    "#     'subsample': 0.8408586950521992\n",
    "# }\n",
    "\n",
    "# model_config = model_configurator('xgboost')\n",
    "# xgboost_model = XGBClassifier(\n",
    "#             tree_method=model_config['tree_method'],\n",
    "#             random_state=random_state,\n",
    "# #             n_jobs=model_config['n_jobs'], \n",
    "#             verbosity=model_config['verbosity'], \n",
    "#             objective=model_config['objective'],\n",
    "#             **best_xgboost_params\n",
    "#             # #             eval_metric=model_config['eval_metric'],\n",
    "\n",
    "#             # comment out the below for a fairly default model\n",
    "# #             booster=model_config['booster'],\n",
    "# #             max_depth=model_config['max_depth'],\n",
    "# #             learning_rate=model_config['learning_rate'], \n",
    "# #             subsample=model_config['subsample'],\n",
    "# #             reg_alpha=model_config['reg_alpha'],\n",
    "# #             reg_lambda=model_config['reg_lambda'],\n",
    "# #             n_estimators=model_config['n_estimators'],\n",
    "#         )\n",
    "\n",
    "# model_config = model_configurator('lightgbm')\n",
    "# lightgbm_model = LGBMClassifier(\n",
    "#             random_state=random_state,\n",
    "# #             n_jobs=model_config['n_jobs'],\n",
    "#             objective=model_config['objective'],\n",
    "#             boosting_type=model_config['boosting_type'],\n",
    "#             device_type=model_config['device_type'],\n",
    "            \n",
    "#             # comment out the below for a basically default model\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "#             learning_rate=model_config['learning_rate'],\n",
    "#             max_depth=model_config['max_depth'],\n",
    "#             reg_alpha=model_config['reg_alpha'],\n",
    "#             reg_lambda=model_config['reg_lambda'],\n",
    "#             subsample=model_config['subsample'],\n",
    "#         )\n",
    "\n",
    "model_config = model_configurator('catboost', gpu_available=False) # set GPU false to avoid parallel threads blocking GPU\n",
    "\n",
    "# # As of 20210920, best CatBoost config is:\n",
    "# best_20210920_catboost_params = {\n",
    "#     'iterations': 3493,\n",
    "#     'depth': 5,\n",
    "#     'learning_rate': 0.09397459954141321,\n",
    "#     'random_strength': 43,\n",
    "#     'l2_leaf_reg': 26,\n",
    "#     'border_count': 239,\n",
    "#     'bagging_temperature': 12.532400413798356,\n",
    "#     'od_type': 'Iter'\n",
    "# }\n",
    "\n",
    "# catboost 20210921 on colab (only 15 trials though)\n",
    "best_catboost_params = {\n",
    "    'iterations': 3302,\n",
    "    'depth': 5,\n",
    "    'learning_rate': 0.017183208677599107,\n",
    "    'random_strength': 41,\n",
    "    'l2_leaf_reg': 30,\n",
    "    'border_count': 251,\n",
    "    'bagging_temperature': 9.898390369028036, \n",
    "    'od_type': 'IncToDec'\n",
    "}\n",
    "    \n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "            task_type=model_config['task_type'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "            random_state=random_state,\n",
    "            \n",
    "            **best_catboost_params\n",
    "        ) \n",
    "\n",
    "\n",
    "\n",
    "estimators_list = [\n",
    "#     ('xgboost', xgboost_model),\n",
    "#     ('lightgbm', lightgbm_model),\n",
    "    ('catboost', catboost_model)\n",
    "]\n",
    "\n",
    "# wandb.log({'estimators': estimators_list})\n",
    "\n",
    "final_estimator = LogisticRegression(max_iter=1000)\n",
    "exmodel_config['blender_final_estimator'] = str(final_estimator)\n",
    "exmodel_config['blender-passthrough'] = False\n",
    "\n",
    "blender = StackingClassifier(estimators=estimators_list,\n",
    "                             final_estimator=final_estimator,\n",
    "                             cv=5,\n",
    "                             stack_method='predict_proba',\n",
    "                             n_jobs=-1, # 4 is max allowable for CPU\n",
    "                             passthrough=exmodel_config['blender-passthrough'],\n",
    "                             verbose=1\n",
    "                            )\n",
    "\n",
    "\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aca21cef-74f4-4b23-a82e-8c35419dd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'blender-final_estimator': str(blender.final_estimator),\n",
    "#            'blender-final_estimator_params': str(blender.final_estimator.get_params()),\n",
    "           'blender-stack_mdethod': 'predict_proba',\n",
    "           'blender-cv': 5\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e82b11-2324-449a-8b85-58b4cbe9921c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting at 20210922_112912\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting fitting at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "blender.fit(X,y) # unsure of this -- given kwarg cv=5, is it producing the splits? Or do I have to somehow?\n",
    "print(f\"Fitting complete at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1b180-22cf-494b-8987-aec8918d9d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb.log({'xgboost_params':str(blender.estimators[0][1].get_params()),\n",
    "#            'lightgbm_params':str(blender.estimators[1][1].get_params()),\n",
    "# #            'catboost_params':str(blender.estimators[2][1].get_all_params()),\n",
    "#           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8076f3-d1dc-4bd1-80ad-0eb29cc5bd00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = Path(datapath/f\"models/{config_run['name']}/\")\n",
    "(model_path).mkdir(exist_ok=True)\n",
    "dump(blender, filename=model_path/f\"{config_run['name']}_stack.joblib\")\n",
    "print(f\"Blender model saved at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b57e6-e4d7-4cb2-aa5d-7757a8d6d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = blender.predict_proba(X)[:,1]\n",
    "train_loss = log_loss(y_pred=train_preds, y_true=y)\n",
    "train_auc = roc_auc_score(y, train_preds)\n",
    "wandb.log({'train_loss': train_loss, 'train_auc': train_auc})\n",
    "print(f\"train_loss is {train_loss}, train_auc is {train_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec8f4d-ef7e-4382-b5c0-a960099a95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c226e-1ef7-4e03-91a1-06fbb73139f0",
   "metadata": {},
   "source": [
    "# Test set preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ec520-259f-44d2-ad33-7b8c22621132",
   "metadata": {},
   "source": [
    "(Here's where encapsulating the transformations in a pipeline would come in handy. But I'll do it manually for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec74e4-ccb8-43b4-b910-3df1542aaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [x for x in test_df.columns if x != 'claim']\n",
    "# X_test = test_df[features] # this is just for naming consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725cd3e-f883-4d20-837a-9f557b2122a9",
   "metadata": {},
   "source": [
    "Now, let's get the features the model was trained on and subset the test set's features accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66e579-7f01-46e5-a47c-580c8f5d678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation polynomial features\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)\n",
    "# X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1817e3a-7d90-4bc2-8c47-e97806f7dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_poly_names = poly.get_feature_names(X_test.columns)\n",
    "# X_poly_names[100:150]\n",
    "# features = pd.read_csv('X_candidates_20210827.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6bc49-d478-4f59-84d2-5e23e3e236db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks = [feature in X_test_poly_names for feature in features]\n",
    "# checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68187e-271a-4df1-ae02-a2bb5d62c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_final = pd.DataFrame(X_test_poly, columns=X_test_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020ad9b-1b05-49b8-b89b-c90362c256d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_final = X_test_final[features[1:]]\n",
    "# X_test_final = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07699efa-37df-4ed9-aaf2-1b77a73f9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['nan_count'] = X_test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd0c2d-7f9a-4fb6-8b4d-c3c51a9ef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = SimpleImputer(strategy='median', add_indicator=True)\n",
    "# X_test_imputed_np = imputer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ef600-e3cd-44ef-ae66-e1d1663ec4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_imputed = pd.DataFrame(X_test_imputed, columns=[str(x) for x in range(X_test_imputed.shape[1])])\n",
    "# X_test_imputed.to_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da840a-caa6-4d76-a542-c1315a593346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = exmodel_config['scaler']()\n",
    "# X_test_imputed_scaled_np = scaler.fit_transform(X_test_imputed)\n",
    "# X_test_imputed_scaled = pd.DataFrame(X_test_imputed_scaled_np, columns=X_test_imputed.columns)\n",
    "# X_test_imputed_scaled.to_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators_StandardScaled.feather')\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=X_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c062597-20aa-4054-accf-4da20a1400bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path = str(datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')\n",
    "wandb.log({'test_set': test_set_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99cf5b-3477-47f4-9391-73e2ff93c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_imputed_scaled = pd.read_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators_StandardScaled.feather')\n",
    "X_test_imputed_scaled = pd.read_feather(path=datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f347f-872f-4011-9f13-78fad542f36a",
   "metadata": {},
   "source": [
    "## Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773d286-257d-4776-add5-0f724944befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = Path(datapath/\"preds/\")\n",
    "\n",
    "blender_preds = blender.predict_proba(X_test_imputed_scaled)[:,1]\n",
    "dump(blender_preds, preds_path/f\"{config_run['name']}_stack.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719290c6-a10f-4506-8eeb-6a4bbf281b3d",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a464a1c-9ca8-4a07-9cdb-18af399cf95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f4b8-2356-4916-a091-45793db784ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'claim'] = blender_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957ce26-bbf5-4aee-bccd-988f2471db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f6d45-6b2a-4fe8-be77-ecc70cec6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = datapath/'submissions'\n",
    "submission_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ae726f8-0c8e-46ab-bba1-cff095978d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(submission_path/f\"{config_run['name']}_blended.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9672937c-18a0-4eaf-a3f1-54d3a779d672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f227c7b81c0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str(blender.estimators[2][1].get_all_params())\n",
    "# blender.estimators[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4e34083-9f47-4583-b4f3-c10b4f3311d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'leaderboard_auc': ,\n",
    "           'catboost_params': str(best_catboost_params),\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b08b089-292e-43a1-87f1-9d46ac917ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2327090<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_055836-17fcrnlu/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_055836-17fcrnlu/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>blender-final_estimator</td><td>LogisticRegression(m...</td></tr><tr><td>blender-stack_mdethod</td><td>predict_proba</td></tr><tr><td>blender-cv</td><td>5</td></tr><tr><td>_runtime</td><td>16221</td></tr><tr><td>_timestamp</td><td>1632331738</td></tr><tr><td>_step</td><td>4</td></tr><tr><td>xgboost_params</td><td>{'objective': 'binar...</td></tr><tr><td>lightgbm_params</td><td>{'boosting_type': 'g...</td></tr><tr><td>train_loss</td><td>0.48961</td></tr><tr><td>train_auc</td><td>0.84839</td></tr><tr><td>test_set</td><td>/media/sf/easystore/...</td></tr><tr><td>leaderboard_auc</td><td>0.81671</td></tr><tr><td>catboost_params</td><td>{'iterations': 3302,...</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>blender-cv</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>train_auc</td><td></td></tr><tr><td>leaderboard_auc</td><td></td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">stacking_off-shelf_20210922_055836</strong>: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/17fcrnlu\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/17fcrnlu</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afb61b-8022-4f0f-8996-2adfb3ec640c",
   "metadata": {},
   "source": [
    "## Manual Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1524723c-87c3-410f-863b-9387ef5b59e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.425545</td>\n",
       "      <td>-2.357891</td>\n",
       "      <td>-0.637206</td>\n",
       "      <td>-0.866657</td>\n",
       "      <td>-0.111568</td>\n",
       "      <td>-4.829243</td>\n",
       "      <td>-1.171229</td>\n",
       "      <td>-0.603397</td>\n",
       "      <td>-0.596871</td>\n",
       "      <td>-0.516828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247600</td>\n",
       "      <td>-0.323982</td>\n",
       "      <td>1.223569</td>\n",
       "      <td>0.361863</td>\n",
       "      <td>1.071182</td>\n",
       "      <td>-0.361140</td>\n",
       "      <td>0.082051</td>\n",
       "      <td>-0.746590</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.469668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.032371</td>\n",
       "      <td>-2.435680</td>\n",
       "      <td>-0.488960</td>\n",
       "      <td>0.341193</td>\n",
       "      <td>1.069656</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.537069</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>-0.763516</td>\n",
       "      <td>1.056879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.438373</td>\n",
       "      <td>-2.337605</td>\n",
       "      <td>-0.508914</td>\n",
       "      <td>-0.829607</td>\n",
       "      <td>1.485682</td>\n",
       "      <td>3.592008</td>\n",
       "      <td>-1.189087</td>\n",
       "      <td>-0.339152</td>\n",
       "      <td>-0.735281</td>\n",
       "      <td>-0.529158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602333</td>\n",
       "      <td>1.076218</td>\n",
       "      <td>-0.648438</td>\n",
       "      <td>0.463365</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>-0.157989</td>\n",
       "      <td>0.727338</td>\n",
       "      <td>-0.905498</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>-0.511066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "id                                                                         \n",
       "0   0.425545 -2.357891 -0.637206 -0.866657 -0.111568 -4.829243 -1.171229   \n",
       "1   0.247600 -0.323982  1.223569  0.361863  1.071182 -0.361140  0.082051   \n",
       "2   2.032371 -2.435680 -0.488960  0.341193  1.069656  0.118532  0.537069   \n",
       "3   1.438373 -2.337605 -0.508914 -0.829607  1.485682  3.592008 -1.189087   \n",
       "4   0.602333  1.076218 -0.648438  0.463365  0.275053 -0.157989  0.727338   \n",
       "\n",
       "           7         8         9  ...       227       228       229       230  \\\n",
       "id                                ...                                           \n",
       "0  -0.603397 -0.596871 -0.516828  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "1  -0.746590  0.899454  0.469668  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "2  -0.044075 -0.763516  1.056879  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "3  -0.339152 -0.735281 -0.529158  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "4  -0.905498  0.052478 -0.511066  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "\n",
       "         231       232       233      234       235      236  \n",
       "id                                                            \n",
       "0  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "1  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "2  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "3  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "4  -0.127119 -0.127985 -0.128494 -0.12862  7.821398 -0.12703  \n",
       "\n",
       "[5 rows x 237 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fe341d69-270a-445e-9a05-287cd9f12c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0a114476-71cb-4ef2-8f1f-2d8b2c837d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 237)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8dd525a7-3254-4e0c-8295-8871ffadd39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# generate probability predictions for the XGBoost model's folds\n",
    "for fold in xgboost_models.keys():\n",
    "#     X1[f\"xgboost_fold{fold}_pred\"] = xgboost_models[fold].predict(X)\n",
    "    X1[f\"xgboost_fold{fold}_pred\"] = xgboost_models[fold].predict_proba(X)[:,1]\n",
    "#     xgboost_preds[fold] = xgboost_models[fold].predict(X_test_imputed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eff33be8-e342-42e8-843c-75e1acd96120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>xgboost_fold0_pred</th>\n",
       "      <th>xgboost_fold1_pred</th>\n",
       "      <th>xgboost_fold2_pred</th>\n",
       "      <th>xgboost_fold3_pred</th>\n",
       "      <th>xgboost_fold4_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.425545</td>\n",
       "      <td>-2.357891</td>\n",
       "      <td>-0.637206</td>\n",
       "      <td>-0.866657</td>\n",
       "      <td>-0.111568</td>\n",
       "      <td>-4.829243</td>\n",
       "      <td>-1.171229</td>\n",
       "      <td>-0.603397</td>\n",
       "      <td>-0.596871</td>\n",
       "      <td>-0.516828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.582566</td>\n",
       "      <td>0.580950</td>\n",
       "      <td>0.576743</td>\n",
       "      <td>0.569523</td>\n",
       "      <td>0.595877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247600</td>\n",
       "      <td>-0.323982</td>\n",
       "      <td>1.223569</td>\n",
       "      <td>0.361863</td>\n",
       "      <td>1.071182</td>\n",
       "      <td>-0.361140</td>\n",
       "      <td>0.082051</td>\n",
       "      <td>-0.746590</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.469668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.152252</td>\n",
       "      <td>0.150803</td>\n",
       "      <td>0.148316</td>\n",
       "      <td>0.155218</td>\n",
       "      <td>0.147297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.032371</td>\n",
       "      <td>-2.435680</td>\n",
       "      <td>-0.488960</td>\n",
       "      <td>0.341193</td>\n",
       "      <td>1.069656</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.537069</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>-0.763516</td>\n",
       "      <td>1.056879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.794083</td>\n",
       "      <td>0.789945</td>\n",
       "      <td>0.788326</td>\n",
       "      <td>0.787177</td>\n",
       "      <td>0.797979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.438373</td>\n",
       "      <td>-2.337605</td>\n",
       "      <td>-0.508914</td>\n",
       "      <td>-0.829607</td>\n",
       "      <td>1.485682</td>\n",
       "      <td>3.592008</td>\n",
       "      <td>-1.189087</td>\n",
       "      <td>-0.339152</td>\n",
       "      <td>-0.735281</td>\n",
       "      <td>-0.529158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.774001</td>\n",
       "      <td>0.768510</td>\n",
       "      <td>0.774555</td>\n",
       "      <td>0.782187</td>\n",
       "      <td>0.773245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602333</td>\n",
       "      <td>1.076218</td>\n",
       "      <td>-0.648438</td>\n",
       "      <td>0.463365</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>-0.157989</td>\n",
       "      <td>0.727338</td>\n",
       "      <td>-0.905498</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>-0.511066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.759366</td>\n",
       "      <td>0.755764</td>\n",
       "      <td>0.763769</td>\n",
       "      <td>0.758034</td>\n",
       "      <td>0.758038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "id                                                                         \n",
       "0   0.425545 -2.357891 -0.637206 -0.866657 -0.111568 -4.829243 -1.171229   \n",
       "1   0.247600 -0.323982  1.223569  0.361863  1.071182 -0.361140  0.082051   \n",
       "2   2.032371 -2.435680 -0.488960  0.341193  1.069656  0.118532  0.537069   \n",
       "3   1.438373 -2.337605 -0.508914 -0.829607  1.485682  3.592008 -1.189087   \n",
       "4   0.602333  1.076218 -0.648438  0.463365  0.275053 -0.157989  0.727338   \n",
       "\n",
       "           7         8         9  ...       232       233      234       235  \\\n",
       "id                                ...                                          \n",
       "0  -0.603397 -0.596871 -0.516828  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "1  -0.746590  0.899454  0.469668  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "2  -0.044075 -0.763516  1.056879  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "3  -0.339152 -0.735281 -0.529158  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "4  -0.905498  0.052478 -0.511066  ... -0.127985 -0.128494 -0.12862  7.821398   \n",
       "\n",
       "        236  xgboost_fold0_pred  xgboost_fold1_pred  xgboost_fold2_pred  \\\n",
       "id                                                                        \n",
       "0  -0.12703            0.582566            0.580950            0.576743   \n",
       "1  -0.12703            0.152252            0.150803            0.148316   \n",
       "2  -0.12703            0.794083            0.789945            0.788326   \n",
       "3  -0.12703            0.774001            0.768510            0.774555   \n",
       "4  -0.12703            0.759366            0.755764            0.763769   \n",
       "\n",
       "    xgboost_fold3_pred  xgboost_fold4_pred  \n",
       "id                                          \n",
       "0             0.569523            0.595877  \n",
       "1             0.155218            0.147297  \n",
       "2             0.787177            0.797979  \n",
       "3             0.782187            0.773245  \n",
       "4             0.758034            0.758038  \n",
       "\n",
       "[5 rows x 242 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac245be1-3a2d-4c84-b83e-7224e4b13194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
