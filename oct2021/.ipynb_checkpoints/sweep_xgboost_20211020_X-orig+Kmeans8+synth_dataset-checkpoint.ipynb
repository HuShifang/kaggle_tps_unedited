{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {
    "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53"
   },
   "source": [
    "# XGBoost 20210926 GPU sweep\n",
    "Trying after building GPU version locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U_qtimPUchWD",
   "metadata": {
    "id": "U_qtimPUchWD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4",
   "metadata": {
    "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae2ff1e-bd1f-4cc9-8357-5a88d1746ffb",
   "metadata": {
    "id": "dae2ff1e-bd1f-4cc9-8357-5a88d1746ffb"
   },
   "outputs": [],
   "source": [
    "# two manual flags (ex-config)\n",
    "colab = False\n",
    "gpu_available = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16849bd2-428c-497b-ba3b-675002f8d041",
   "metadata": {
    "id": "16849bd2-428c-497b-ba3b-675002f8d041"
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "import multiprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d2654b-3bc6-49b5-ade8-cc82112b60e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12d2654b-3bc6-49b5-ade8-cc82112b60e5",
    "outputId": "6bd53922-c4d7-43ce-c04f-ac1079087966"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"sweep_xgboost_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416d6118-e543-4df4-9219-2d4a63743c3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "416d6118-e543-4df4-9219-2d4a63743c3a",
    "outputId": "5483656e-2943-4d97-b5d4-65cfb9795430",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handle Google Colab-specific library installation/updating\n",
    "if colab:\n",
    "    # much of the below inspired by or cribbed from the May 2021 Kaggle Tabular Playground winner, at \n",
    "    # https://colab.research.google.com/gist/academicsuspect/0aac7bd6e506f5f70295bfc9a3dc2250/tabular-may-baseline.ipynb?authuser=1#scrollTo=LJoVKJb5wN0L\n",
    "    \n",
    "    # Kaggle API for downloading the datasets\n",
    "    !pip install --upgrade -q kaggle\n",
    "\n",
    "    # weights and biases\n",
    "    !pip install -qqqU wandb\n",
    "    \n",
    "    # Optuna for parameter search\n",
    "    !pip install -q optuna\n",
    "\n",
    "    # !pip install --upgrade xgboost\n",
    "\n",
    "    # upgrade sklearn\n",
    "    !pip install --upgrade scikit-learn\n",
    "\n",
    "    # !pip install category_encoders\n",
    "    # !pip install catboost\n",
    "#     !pip install --upgrade -q lightgbm\n",
    "\n",
    "    # lighgbm gpu compatible\n",
    "    # !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "    # ! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;\n",
    "    \n",
    "    # # this part is from https://github.com/rapidsai/gputreeshap/issues/24\n",
    "    # !pip install cmake --upgrade\n",
    "    # # !pip install sklearn --upgrade\n",
    "    # !git clone --recursive https://github.com/dmlc/xgboost\n",
    "    # %cd /content/xgboost\n",
    "    # !mkdir build\n",
    "    # %cd build\n",
    "    # !cmake .. -DUSE_CUDA=ON\n",
    "    # !make -j4\n",
    "    # %cd /content/xgboost/python-package\n",
    "    # !python setup.py install --use-cuda --use-nccl\n",
    "    # !/opt/bin/nvidia-smi\n",
    "    # !pip install shap\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40df194-4474-4bcf-ac5a-98efe24b91fd",
   "metadata": {
    "id": "d40df194-4474-4bcf-ac5a-98efe24b91fd"
   },
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01e85f7-d602-4dde-bef9-611683cd74c4",
   "metadata": {
    "id": "a01e85f7-d602-4dde-bef9-611683cd74c4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "# from wandb.xgboost import wandb_callback\n",
    "# from wandb.lightgbm import wandb_callback\n",
    "# from sklearn.impute import KNNImputer, StandardImputer\n",
    "# import timm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n",
    "\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna\n",
    "# import catboost\n",
    "from sklearn.utils import resample\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6166c2-ca44-4b7c-a4dc-3db47c2624fe",
   "metadata": {
    "id": "5f6166c2-ca44-4b7c-a4dc-3db47c2624fe"
   },
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c18a787-2193-43cb-87ee-51c6ae7b6351",
   "metadata": {
    "id": "3c18a787-2193-43cb-87ee-51c6ae7b6351"
   },
   "outputs": [],
   "source": [
    "# # This is the code for reading the train.csv and converting it to a .feather file\n",
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67530ca9-6317-48be-bf6c-8621158b0020",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a41cd7e-accb-41c4-ad8b-0eaa3e2b0ad5",
    "outputId": "76d62b41-4171-40fa-936c-4481a9fdab36"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "#     datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/sep2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "#     datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')  \n",
    "    root = Path('/home/sf/code/kaggle/tabular_playgrounds/oct2021/')\n",
    "    datapath = root/'datasets'\n",
    "    edapath = root/'EDA'\n",
    "    modelpath = root/'models'\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'optuna_studies'\n",
    "    altdatapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/alt_datasets/')\n",
    "    \n",
    "    for pth in [root, datapath, edapath, modelpath, predpath, subpath, altdatapath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c652e6-5946-46aa-a13e-4c0ebe8a0e4f",
   "metadata": {
    "id": "d1c652e6-5946-46aa-a13e-4c0ebe8a0e4f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# n_trials = int(1000)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbec2e77-2081-4815-ac6d-39f2a2616386",
   "metadata": {
    "id": "fbec2e77-2081-4815-ac6d-39f2a2616386"
   },
   "outputs": [],
   "source": [
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e47b3-43bd-4d35-b463-9d76100c6ed5",
   "metadata": {
    "id": "2f6e47b3-43bd-4d35-b463-9d76100c6ed5"
   },
   "source": [
    "## Ex-Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f08480-1725-4520-8995-92b76b8f2cea",
   "metadata": {
    "id": "fb288275-a858-4806-9dc0-0b316c334536"
   },
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "# in the sweep version, this includes both ex-model parameters and defaults for model parameters\n",
    "exmodel_config = {\n",
    "    # model config\n",
    "    \"library\": 'xgboost',\n",
    "#     \"model\": XGBClassifier,\n",
    "#     \"n_estimators\": 100, \n",
    "#     \"max_depth\": 3,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"test_size\": 0.2,\n",
    "#     \"reg_lambda\": None, \n",
    "#     \"scaler\": \"sklearn.preprocessing.StandardScaler()\", # TODO: experiment with others (but imputation may be slow)\n",
    "#     \"scale_b4_impute\": False,\n",
    "#     \"imputer\": \"sklearn.impute.SimpleImputer(strategy='median', add_indicator=True)\",\n",
    "#     \"knn_imputer_n_neighbors\": None, # None if a different imputer is used\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "    'random_state': SEED,\n",
    "    'optuna': True,\n",
    "#     'optuna_trials': 20,\n",
    "#     'subsample': 1,\n",
    "#     'cross_val_strategy': None, # None for holdout, or the relevant sklearn class\n",
    "#     'kfolds': 1, # if 1, that means just doing holdout\n",
    "#     'test_size': 0.2,\n",
    "    # these are XGBoost default (my choice) params \n",
    "#     \"tree_method\": \"auto\", # set to 'gpu_hist' to try GPU if available\n",
    "#     \"booster\": 'gbtree', # dart may be marginally better, but will opt for this quicker approach as a default\n",
    "#     \"n_estimators\": 200, \n",
    "#     \"max_depth\": 3,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"n_jobs\": -1,\n",
    "#     \"verbosity\": 1,\n",
    "#     \"subsample\": 1,\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}\n",
    "\n",
    "wandb_kwargs = {\n",
    "    # wandb config\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'project': '202110_Kaggle_tabular_playground',\n",
    "    'tags': ['sweep'],\n",
    "    'notes': \"Sweep for XGBoost on GPU with Optuna, based on Boruta-optimized Shapley-value (SHAP)-calculated dataset with 136 features. Hope is that feature selection ==> less overfitting ==> more learning possible\",\n",
    "    'config': exmodel_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d9012-34f1-435a-ba16-4416e0d4a286",
   "metadata": {
    "id": "a52d9012-34f1-435a-ba16-4416e0d4a286"
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252ca14-2718-49d9-89e2-91d55f72d706",
   "metadata": {
    "id": "c912a62f-970a-48b4-b428-d886f2612fc2"
   },
   "source": [
    "**TODO** Write some conditional logic here to automate it -- possibly as part of a sklearn.*pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6919aac1-15d6-4b41-9871-f547602a91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_source = datapath/'train.feather'\n",
    "# df = pd.read_feather(path=datapath/'train.feather')\n",
    "# y = np.array(df.target)\n",
    "# dump(y, filename=datapath/'y.joblib')\n",
    "# del df\n",
    "\n",
    "y = load(datapath/'y.joblib')\n",
    "\n",
    "# df.index.name = 'id'\n",
    "# y_train = df.target\n",
    "# features = [x for x in df.columns if x != 'target']\n",
    "# X_train = df[features]\n",
    "# # X.index.name = 'id'\n",
    "# # y.index.name = 'id'\n",
    "# X = np.array(X_train)\n",
    "# y = np.array(y_train)\n",
    "\n",
    "# del df, X_train, y_train\n",
    "\n",
    "# load the Boruta-filtered green-zone 98 features (based on 200 iterations of the algo)\n",
    "# train_source = '/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/alt_datasets/X_boruta_200iter_filtered_green.joblib'\n",
    "# X = load(train_source)\n",
    "\n",
    "train_source = altdatapath/'train-WITH-KMeans_8cluster_ninit50_maxiter1000_rs42-AND-synthetic.feather' #'X_boruta_shap_200trials.feather'\n",
    "exmodel_config['train_source'] = str(train_source)\n",
    "X = pd.read_feather(path=train_source)\n",
    "X = X.drop(['target'], axis=1)\n",
    "\n",
    "# exmodel_config['feature_count'] = len(X.columns)\n",
    "exmodel_config['feature_count'] = X.shape[1]\n",
    "exmodel_config['instance_count'] = X.shape[0]\n",
    "\n",
    "# exmodel_config['feature_generator'] = None\n",
    "# exmodel_config['feature_generator'] = \"Summary statistics\"\n",
    "\n",
    "# exmodel_config['train_source'] = str(train_source)\n",
    "# test_source = datapath/'test.feather'\n",
    "# exmodel_config['test_source'] = str(test_source)\n",
    "# X_test = pd.read_feather(path=test_source)\n",
    "# X_test = X_test.iloc[:, 1:]\n",
    "# X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd088129-6ae2-4002-bfb5-e58cf671fd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 301), (1000000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6107ef0a-dc4c-4b20-a050-ec8cf1708774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, numpy.ndarray)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d37db-558d-474d-9eca-ce2d38b7636f",
   "metadata": {
    "id": "431d37db-558d-474d-9eca-ce2d38b7636f"
   },
   "source": [
    "# Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69ff4abf-560b-450e-a7a5-040878b66565",
   "metadata": {
    "id": "69ff4abf-560b-450e-a7a5-040878b66565"
   },
   "outputs": [],
   "source": [
    "# wandb_kwargs = {\n",
    "#     # wandb config:\n",
    "#     'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "#     'project': '202109_Kaggle_tabular_playground',\n",
    "#     'tags': ['sweep'],\n",
    "#     'notes': \"Sweep for CatBoost using Optuna\",\n",
    "#     'config': exmodel_config,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702836f-7dfc-41ed-b208-2dfeee8526c0",
   "metadata": {},
   "source": [
    "The best parameters up to present have been:\n",
    "\n",
    "```python\n",
    "best_xgboost_params = {\n",
    "        'n_estimators': 3878,\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.024785857161974977,\n",
    "        'reg_alpha': 26.867682044658245,\n",
    "        'reg_lambda': 10.839759074147148,\n",
    "        'subsample': 0.8208581489835881,\n",
    "        'min_child_weight': 8.829122644339664,\n",
    "        'colsample_bytree': 0.906420714280384,\n",
    "        'gamma': 1.472322916021486\n",
    "    }\n",
    "```\n",
    "\n",
    "These params get the following ROC_AUC scores on a 20% holdout for these dataset versions:\n",
    "\n",
    "| Version | Feature Count | Valid ROC-AUC |\n",
    "| ----- | ----- | ----- |\n",
    "| original | 285 | 0.8572984856383443 |\n",
    "| Boruta green-only | 98 | 0.8553163413048461 |\n",
    "| Boruta green-and-blue | 109 | 0.8558487581638441 |\n",
    "| Boruta with SHAP | 136 | 0.8566790062778752 |\n",
    "| Original plus KMeans 8 plus synth | 301 | 0.8570855909847465 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "367635ba-0911-4ce5-b5c8-fe2768a12e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:49:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "ROC AUC Score of XGBoost = 0.8570855909847465\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=int(SEED), shuffle=True)\n",
    "# # create wrappers for the training and validation partitions\n",
    "# # train_pool = catboost.Pool(X_train, y_train)\n",
    "# # valid_pool = catboost.Pool(X_valid, y_valid)\n",
    "\n",
    "# # experimental parameters -- based off prev. best\n",
    "# params = {\n",
    "#     'n_estimators': 3878,\n",
    "#     'max_depth': 4,\n",
    "#     'learning_rate': 0.024785857161974977,\n",
    "#     'reg_alpha': 26.867682044658245,\n",
    "#     'reg_lambda': 10.839759074147148,\n",
    "#     'subsample': 0.8208581489835881,\n",
    "#     'min_child_weight': 8.829122644339664,\n",
    "#     'colsample_bytree': 0.906420714280384,\n",
    "#     'gamma': 1.472322916021486\n",
    "# }\n",
    "\n",
    "# # instantiate the model, with some parameters locked in, and experimnental ones passed via splat \n",
    "# model = XGBClassifier(\n",
    "#     objective='binary:logistic',\n",
    "#     verbosity=1,\n",
    "#     tree_method='gpu_hist',\n",
    "#     booster='gbtree', # not bothering with dart for time reasons\n",
    "#     random_state=SEED,\n",
    "# #         n_jobs=-1,\n",
    "#     **params\n",
    "# )    \n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "# # generate predictions\n",
    "# preds = model.predict_proba(X_valid)[:,1]\n",
    "# # rounds to the nearest integer, and the nearest even in case of _.5s\n",
    "\n",
    "# # Evaluation\n",
    "# valid_auc = roc_auc_score(y_valid, preds)\n",
    "# print('ROC AUC Score of XGBoost =', valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d93b6f2-2d65-48a9-9862-510bd7d2b75b",
   "metadata": {
    "id": "1d93b6f2-2d65-48a9-9862-510bd7d2b75b"
   },
   "outputs": [],
   "source": [
    "# originally from https://www.kaggle.com/satorushibata/optimize-catboost-hyperparameter-with-optuna-gpu\n",
    "def objective(trial):\n",
    "    # split the (original Kaggle training) data into partitions\n",
    "    # if study.best_trial:\n",
    "    #     print(\"Dumping best params, which are:\")\n",
    "    #     print(str(study.best_trial.params))\n",
    "    #     dump(study.best_trial.params, filename=datapath/'optuna_catboost_best_20210920.joblib')\n",
    "       \n",
    "    # else:\n",
    "    #     print(\"No best study yet\")\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=int(SEED), shuffle=True)\n",
    "    # create wrappers for the training and validation partitions\n",
    "    # train_pool = catboost.Pool(X_train, y_train)\n",
    "    # valid_pool = catboost.Pool(X_valid, y_valid)\n",
    "    \n",
    "    # experimental parameters -- based off prev. best\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 3500, 10000), # was 900-4500 for CPU\n",
    "        'max_depth' : trial.suggest_int('depth', 3, 8),                                       \n",
    "        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.2),               \n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 50),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 30),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "#         'booster': trial.suggest_categorical('boosting_type', ['gbtree', 'dart']),\n",
    "        'min_child_weight': trial.suggest_uniform('min_child_weight', 0.001, 12),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "        'gamma': trial.suggest_uniform('gamma', 0.1, 10)\n",
    "    }  \n",
    "\n",
    "    # instantiate the model, with some parameters locked in, and experimnental ones passed via splat \n",
    "    model = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        verbosity=1,\n",
    "        tree_method='gpu_hist',\n",
    "        booster='gbtree', # not bothering with dart for time reasons\n",
    "        random_state=SEED,\n",
    "#         n_jobs=-1,\n",
    "        **params\n",
    "    )    \n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    # generate predictions\n",
    "    preds = model.predict_proba(X_valid)[:,1]\n",
    "    # rounds to the nearest integer, and the nearest even in case of _.5s\n",
    "\n",
    "    # Evaluation\n",
    "    valid_auc = roc_auc_score(y_valid, preds)\n",
    "    print('ROC AUC Score of XGBoost =', valid_auc)\n",
    "    wandb.log({'valid_auc': valid_auc,\n",
    "              })\n",
    "\n",
    "    return valid_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e85f589-1507-4b75-80d9-8b062970102f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "0e85f589-1507-4b75-80d9-8b062970102f",
    "outputId": "6a01a1a1-8060-429d-9a47-670cbc0435d2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-69ea9289a2cf>:1: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.\n",
      "  wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find sweep_xgboost_20211020.ipynb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/202110_Kaggle_tabular_playground/runs/nap5sp1j\" target=\"_blank\">sweep_xgboost_20211020_210746</a></strong> to <a href=\"https://wandb.ai/hushifang/202110_Kaggle_tabular_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6749b1-dd7d-4789-b0e2-8491d78fe89b",
   "metadata": {
    "id": "ab6749b1-dd7d-4789-b0e2-8491d78fe89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 21:08:23,088]\u001b[0m A new study created in memory with name: xgboost-X_orig+KMeans8+synth_20211020\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction = \"maximize\", \n",
    "                            sampler = TPESampler(seed=int(SEED)), \n",
    "                            study_name=f\"xgboost-X_orig+KMeans8+synth_{datetime.now().strftime('%Y%m%d')}\")\n",
    "\n",
    "# study = load(studypath/f\"optuna_xgboost_study_106trials_20211004.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f02e3b84-ee16-48b9-94db-c738b408a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a454cc8-f135-4d36-8b6c-a964f4b52288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3860cbd2-1d08-4b2e-ac53-92b8a2ce016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost.core.XGBoostError?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c341a341-2f06-4d61-a0cd-3f0751e88861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study.optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe8ad6db-2722-4f04-bd51-4b795bec93c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1cSVFH9gkW_",
    "outputId": "ccc874e6-7dd4-4e24-bec8-35ae48180b40",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 21:16:37,181]\u001b[0m Trial 0 finished with value: 0.8519012987970622 and parameters: {'n_estimators': 5934, 'depth': 8, 'learning_rate': 0.0483437145318464, 'reg_alpha': 0.6502468545951017, 'reg_lambda': 0.004994757081068292, 'subsample': 0.5779972601681014, 'min_child_weight': 0.6979452624062253, 'colsample_bytree': 0.9330880728874675, 'gamma': 6.051038616257767}. Best is trial 0 with value: 0.8519012987970622.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 0 < 1; dropping {'n_estimators': 5934, 'depth': 8, 'learning_rate': 0.0483437145318464, 'reg_alpha': 0.6502468545951017, 'reg_lambda': 0.004994757081068292, 'subsample': 0.5779972601681014, 'min_child_weight': 0.6979452624062253, 'colsample_bytree': 0.9330880728874675, 'gamma': 6.051038616257767, 'value': 0.8519012987970622}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8519012987970622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:16:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 21:26:13,349]\u001b[0m Trial 1 finished with value: 0.8476435903691605 and parameters: {'n_estimators': 8103, 'depth': 3, 'learning_rate': 0.17052641538983093, 'reg_alpha': 8.158738235092015, 'reg_lambda': 0.00892622738184373, 'subsample': 0.5909124836035503, 'min_child_weight': 2.2016707137313523, 'colsample_bytree': 0.6521211214797689, 'gamma': 5.295088673159155}. Best is trial 0 with value: 0.8519012987970622.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 1 < 2; dropping {'n_estimators': 8103, 'depth': 3, 'learning_rate': 0.17052641538983093, 'reg_alpha': 8.158738235092015, 'reg_lambda': 0.00892622738184373, 'subsample': 0.5909124836035503, 'min_child_weight': 2.2016707137313523, 'colsample_bytree': 0.6521211214797689, 'gamma': 5.295088673159155, 'value': 0.8476435903691605}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8476435903691605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 21:35:17,794]\u001b[0m Trial 2 finished with value: 0.8559033709280328 and parameters: {'n_estimators': 6308, 'depth': 4, 'learning_rate': 0.025579488960947364, 'reg_alpha': 0.004523529917658778, 'reg_lambda': 0.02032202659636255, 'subsample': 0.6831809216468459, 'min_child_weight': 5.473383740620215, 'colsample_bytree': 0.8925879806965068, 'gamma': 2.0767704433677614}. Best is trial 2 with value: 0.8559033709280328.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 2 < 3; dropping {'n_estimators': 6308, 'depth': 4, 'learning_rate': 0.025579488960947364, 'reg_alpha': 0.004523529917658778, 'reg_lambda': 0.02032202659636255, 'subsample': 0.6831809216468459, 'min_child_weight': 5.473383740620215, 'colsample_bytree': 0.8925879806965068, 'gamma': 2.0767704433677614, 'value': 0.8559033709280328}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8559033709280328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:35:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 21:50:39,304]\u001b[0m Trial 3 finished with value: 0.8489627272388587 and parameters: {'n_estimators': 6843, 'depth': 6, 'learning_rate': 0.0012790390175145834, 'reg_alpha': 0.7158714383119805, 'reg_lambda': 0.005800389779115683, 'subsample': 0.5325257964926398, 'min_child_weight': 11.386677561502745, 'colsample_bytree': 0.9828160165372797, 'gamma': 8.103133746352965}. Best is trial 2 with value: 0.8559033709280328.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 3 < 4; dropping {'n_estimators': 6843, 'depth': 6, 'learning_rate': 0.0012790390175145834, 'reg_alpha': 0.7158714383119805, 'reg_lambda': 0.005800389779115683, 'subsample': 0.5325257964926398, 'min_child_weight': 11.386677561502745, 'colsample_bytree': 0.9828160165372797, 'gamma': 8.103133746352965, 'value': 0.8489627272388587}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8489627272388587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:50:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 21:57:26,246]\u001b[0m Trial 4 finished with value: 0.8560203767742594 and parameters: {'n_estimators': 5480, 'depth': 3, 'learning_rate': 0.037535371452331254, 'reg_alpha': 0.11702088154220885, 'reg_lambda': 0.0035186816415472676, 'subsample': 0.7475884550556351, 'min_child_weight': 0.41362786486150555, 'colsample_bytree': 0.954660201039391, 'gamma': 2.6619218178401676}. Best is trial 4 with value: 0.8560203767742594.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 4 < 5; dropping {'n_estimators': 5480, 'depth': 3, 'learning_rate': 0.037535371452331254, 'reg_alpha': 0.11702088154220885, 'reg_lambda': 0.0035186816415472676, 'subsample': 0.7475884550556351, 'min_child_weight': 0.41362786486150555, 'colsample_bytree': 0.954660201039391, 'gamma': 2.6619218178401676, 'value': 0.8560203767742594}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8560203767742594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:57:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 22:07:52,902]\u001b[0m Trial 5 finished with value: 0.8558399611633503 and parameters: {'n_estimators': 7807, 'depth': 4, 'learning_rate': 0.015728674194978587, 'reg_alpha': 0.37065955814875856, 'reg_lambda': 0.0067238158696505896, 'subsample': 0.9847923138822793, 'min_child_weight': 9.301818747510014, 'colsample_bytree': 0.9697494707820946, 'gamma': 8.958790769233723}. Best is trial 4 with value: 0.8560203767742594.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 5 < 6; dropping {'n_estimators': 7807, 'depth': 4, 'learning_rate': 0.015728674194978587, 'reg_alpha': 0.37065955814875856, 'reg_lambda': 0.0067238158696505896, 'subsample': 0.9847923138822793, 'min_child_weight': 9.301818747510014, 'colsample_bytree': 0.9697494707820946, 'gamma': 8.958790769233723, 'value': 0.8558399611633503}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8558399611633503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:07:56] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 22:30:20,149]\u001b[0m Trial 6 finished with value: 0.853157451634533 and parameters: {'n_estimators': 7386, 'depth': 8, 'learning_rate': 0.0015981734133214873, 'reg_alpha': 0.008335230071817131, 'reg_lambda': 0.001593999043568401, 'subsample': 0.6626651653816322, 'min_child_weight': 4.664738798984096, 'colsample_bytree': 0.6356745158869479, 'gamma': 8.3045013406041}. Best is trial 4 with value: 0.8560203767742594.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 6 < 7; dropping {'n_estimators': 7386, 'depth': 8, 'learning_rate': 0.0015981734133214873, 'reg_alpha': 0.008335230071817131, 'reg_lambda': 0.001593999043568401, 'subsample': 0.6626651653816322, 'min_child_weight': 4.664738798984096, 'colsample_bytree': 0.6356745158869479, 'gamma': 8.3045013406041, 'value': 0.853157451634533}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.853157451634533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:30:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 22:38:25,694]\u001b[0m Trial 7 finished with value: 0.8563511687063371 and parameters: {'n_estimators': 5819, 'depth': 4, 'learning_rate': 0.017732091590842103, 'reg_alpha': 0.0045940816125026864, 'reg_lambda': 3.9042098517777197, 'subsample': 0.5372753218398854, 'min_child_weight': 11.842656352269607, 'colsample_bytree': 0.8861223846483287, 'gamma': 2.0672852471883068}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 7 < 8; dropping {'n_estimators': 5819, 'depth': 4, 'learning_rate': 0.017732091590842103, 'reg_alpha': 0.0045940816125026864, 'reg_lambda': 3.9042098517777197, 'subsample': 0.5372753218398854, 'min_child_weight': 11.842656352269607, 'colsample_bytree': 0.8861223846483287, 'gamma': 2.0672852471883068, 'value': 0.8563511687063371}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8563511687063371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:38:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 22:43:18,840]\u001b[0m Trial 8 finished with value: 0.8548680168334789 and parameters: {'n_estimators': 3535, 'depth': 7, 'learning_rate': 0.04231554618260076, 'reg_alpha': 2.6642981030636883, 'reg_lambda': 2.838382119353614, 'subsample': 0.5370223258670452, 'min_child_weight': 4.302230276802727, 'colsample_bytree': 0.5579345297625649, 'gamma': 8.644723916168376}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 8 < 9; dropping {'n_estimators': 3535, 'depth': 7, 'learning_rate': 0.04231554618260076, 'reg_alpha': 2.6642981030636883, 'reg_lambda': 2.838382119353614, 'subsample': 0.5370223258670452, 'min_child_weight': 4.302230276802727, 'colsample_bytree': 0.5579345297625649, 'gamma': 8.644723916168376, 'value': 0.8548680168334789}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8548680168334789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 22:56:28,329]\u001b[0m Trial 9 finished with value: 0.8455581525276535 and parameters: {'n_estimators': 7552, 'depth': 4, 'learning_rate': 0.0014003921591980472, 'reg_alpha': 0.028926547478415564, 'reg_lambda': 0.028568350317608886, 'subsample': 0.864803089169032, 'min_child_weight': 7.651052098791203, 'colsample_bytree': 0.9436063712881633, 'gamma': 4.7749277591032975}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 9 < 10; dropping {'n_estimators': 7552, 'depth': 4, 'learning_rate': 0.0014003921591980472, 'reg_alpha': 0.028926547478415564, 'reg_lambda': 0.028568350317608886, 'subsample': 0.864803089169032, 'min_child_weight': 7.651052098791203, 'colsample_bytree': 0.9436063712881633, 'gamma': 4.7749277591032975, 'value': 0.8455581525276535}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8455581525276535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:56:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 23:13:54,119]\u001b[0m Trial 10 finished with value: 0.856092730703807 and parameters: {'n_estimators': 9644, 'depth': 5, 'learning_rate': 0.005343309786264438, 'reg_alpha': 0.0012145134109041748, 'reg_lambda': 24.68158512080416, 'subsample': 0.8451235367845727, 'min_child_weight': 11.676480091474728, 'colsample_bytree': 0.8120417635861829, 'gamma': 0.49401610016244657}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 10 < 11; dropping {'n_estimators': 9644, 'depth': 5, 'learning_rate': 0.005343309786264438, 'reg_alpha': 0.0012145134109041748, 'reg_lambda': 24.68158512080416, 'subsample': 0.8451235367845727, 'min_child_weight': 11.676480091474728, 'colsample_bytree': 0.8120417635861829, 'gamma': 0.49401610016244657, 'value': 0.856092730703807}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.856092730703807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:13:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 23:32:06,738]\u001b[0m Trial 11 finished with value: 0.8559102203518446 and parameters: {'n_estimators': 9973, 'depth': 5, 'learning_rate': 0.004637335904125637, 'reg_alpha': 0.0013009317376811282, 'reg_lambda': 27.26637288668004, 'subsample': 0.8513496163789994, 'min_child_weight': 11.606357124072144, 'colsample_bytree': 0.8081058697314522, 'gamma': 0.14626834995746157}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 11 < 12; dropping {'n_estimators': 9973, 'depth': 5, 'learning_rate': 0.004637335904125637, 'reg_alpha': 0.0013009317376811282, 'reg_lambda': 27.26637288668004, 'subsample': 0.8513496163789994, 'min_child_weight': 11.606357124072144, 'colsample_bytree': 0.8081058697314522, 'gamma': 0.14626834995746157, 'value': 0.8559102203518446}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8559102203518446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:32:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-20 23:49:30,279]\u001b[0m Trial 12 finished with value: 0.8562460244704446 and parameters: {'n_estimators': 9599, 'depth': 5, 'learning_rate': 0.006224211601157298, 'reg_alpha': 0.0010793509750421902, 'reg_lambda': 2.4644469591309033, 'subsample': 0.8544685803302188, 'min_child_weight': 9.263316224548479, 'colsample_bytree': 0.791808882918833, 'gamma': 0.13453675125225145}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 12 < 13; dropping {'n_estimators': 9599, 'depth': 5, 'learning_rate': 0.006224211601157298, 'reg_alpha': 0.0010793509750421902, 'reg_lambda': 2.4644469591309033, 'subsample': 0.8544685803302188, 'min_child_weight': 9.263316224548479, 'colsample_bytree': 0.791808882918833, 'gamma': 0.13453675125225145, 'value': 0.8562460244704446}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8562460244704446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:49:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 00:00:08,644]\u001b[0m Trial 13 finished with value: 0.8547344554658559 and parameters: {'n_estimators': 4692, 'depth': 6, 'learning_rate': 0.006143666518909202, 'reg_alpha': 49.65627329623703, 'reg_lambda': 1.2795913039573996, 'subsample': 0.9418381814885792, 'min_child_weight': 9.43488948293992, 'colsample_bytree': 0.7485426968054262, 'gamma': 2.3173398864563826}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 13 < 14; dropping {'n_estimators': 4692, 'depth': 6, 'learning_rate': 0.006143666518909202, 'reg_alpha': 49.65627329623703, 'reg_lambda': 1.2795913039573996, 'subsample': 0.9418381814885792, 'min_child_weight': 9.43488948293992, 'colsample_bytree': 0.7485426968054262, 'gamma': 2.3173398864563826, 'value': 0.8547344554658559}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8547344554658559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 00:15:08,547]\u001b[0m Trial 14 finished with value: 0.8561693477797931 and parameters: {'n_estimators': 8945, 'depth': 5, 'learning_rate': 0.01125445291895857, 'reg_alpha': 0.029544970845863147, 'reg_lambda': 0.6690231225707517, 'subsample': 0.7655620151757302, 'min_child_weight': 9.435516703632542, 'colsample_bytree': 0.8445779114924311, 'gamma': 3.4835284623307934}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 14 < 15; dropping {'n_estimators': 8945, 'depth': 5, 'learning_rate': 0.01125445291895857, 'reg_alpha': 0.029544970845863147, 'reg_lambda': 0.6690231225707517, 'subsample': 0.7655620151757302, 'min_child_weight': 9.435516703632542, 'colsample_bytree': 0.8445779114924311, 'gamma': 3.4835284623307934, 'value': 0.8561693477797931}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8561693477797931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:15:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 00:23:38,891]\u001b[0m Trial 15 finished with value: 0.848771827986905 and parameters: {'n_estimators': 5121, 'depth': 4, 'learning_rate': 0.0029157657243610675, 'reg_alpha': 0.0053060242718446764, 'reg_lambda': 5.09096092815053, 'subsample': 0.7723220353006143, 'min_child_weight': 7.648317842695962, 'colsample_bytree': 0.7429029142574208, 'gamma': 1.1348833448150124}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 15 < 16; dropping {'n_estimators': 5121, 'depth': 4, 'learning_rate': 0.0029157657243610675, 'reg_alpha': 0.0053060242718446764, 'reg_lambda': 5.09096092815053, 'subsample': 0.7723220353006143, 'min_child_weight': 7.648317842695962, 'colsample_bytree': 0.7429029142574208, 'gamma': 1.1348833448150124, 'value': 0.848771827986905}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.848771827986905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:23:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 00:32:37,861]\u001b[0m Trial 16 finished with value: 0.8518942332104017 and parameters: {'n_estimators': 8702, 'depth': 6, 'learning_rate': 0.11309411033858811, 'reg_alpha': 0.03105775391452539, 'reg_lambda': 0.1433619740060279, 'subsample': 0.9148735608444686, 'min_child_weight': 7.886086506626408, 'colsample_bytree': 0.6943845973931932, 'gamma': 3.7646461354354637}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 16 < 17; dropping {'n_estimators': 8702, 'depth': 6, 'learning_rate': 0.11309411033858811, 'reg_alpha': 0.03105775391452539, 'reg_lambda': 0.1433619740060279, 'subsample': 0.9148735608444686, 'min_child_weight': 7.886086506626408, 'colsample_bytree': 0.6943845973931932, 'gamma': 3.7646461354354637, 'value': 0.8518942332104017}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8518942332104017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:32:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 00:38:04,215]\u001b[0m Trial 17 finished with value: 0.8541146992231202 and parameters: {'n_estimators': 4136, 'depth': 3, 'learning_rate': 0.011245561712803798, 'reg_alpha': 0.003192989680493486, 'reg_lambda': 0.22862668072745812, 'subsample': 0.6673171702375452, 'min_child_weight': 10.364435588287169, 'colsample_bytree': 0.8725677315158955, 'gamma': 1.4012946828583095}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 17 < 18; dropping {'n_estimators': 4136, 'depth': 3, 'learning_rate': 0.011245561712803798, 'reg_alpha': 0.003192989680493486, 'reg_lambda': 0.22862668072745812, 'subsample': 0.6673171702375452, 'min_child_weight': 10.364435588287169, 'colsample_bytree': 0.8725677315158955, 'gamma': 1.4012946828583095, 'value': 0.8541146992231202}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8541146992231202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:38:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 00:52:43,705]\u001b[0m Trial 18 finished with value: 0.8550465977782792 and parameters: {'n_estimators': 6588, 'depth': 7, 'learning_rate': 0.018487441360287675, 'reg_alpha': 0.10800867700378233, 'reg_lambda': 7.975530732387742, 'subsample': 0.7989395122024261, 'min_child_weight': 6.768132413558175, 'colsample_bytree': 0.7791280612329506, 'gamma': 1.4560180524574449}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 18 < 19; dropping {'n_estimators': 6588, 'depth': 7, 'learning_rate': 0.018487441360287675, 'reg_alpha': 0.10800867700378233, 'reg_lambda': 7.975530732387742, 'subsample': 0.7989395122024261, 'min_child_weight': 6.768132413558175, 'colsample_bytree': 0.7791280612329506, 'gamma': 1.4560180524574449, 'value': 0.8550465977782792}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8550465977782792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 01:08:50,769]\u001b[0m Trial 19 finished with value: 0.8541655300914685 and parameters: {'n_estimators': 8684, 'depth': 5, 'learning_rate': 0.002906322015945546, 'reg_alpha': 0.01476092842717108, 'reg_lambda': 0.4708654822585449, 'subsample': 0.7231610042250182, 'min_child_weight': 10.436899334389029, 'colsample_bytree': 0.8960337329732879, 'gamma': 3.5046040931413684}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 19 < 20; dropping {'n_estimators': 8684, 'depth': 5, 'learning_rate': 0.002906322015945546, 'reg_alpha': 0.01476092842717108, 'reg_lambda': 0.4708654822585449, 'subsample': 0.7231610042250182, 'min_child_weight': 10.436899334389029, 'colsample_bytree': 0.8960337329732879, 'gamma': 3.5046040931413684, 'value': 0.8541655300914685}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8541655300914685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:08:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 01:15:19,118]\u001b[0m Trial 20 finished with value: 0.8546043268818104 and parameters: {'n_estimators': 5830, 'depth': 4, 'learning_rate': 0.06794349446297417, 'reg_alpha': 0.0010226667391909664, 'reg_lambda': 2.623622708537351, 'subsample': 0.6200392464368687, 'min_child_weight': 8.812958408005255, 'colsample_bytree': 0.5058104808137766, 'gamma': 7.19551885636909}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 20 < 21; dropping {'n_estimators': 5830, 'depth': 4, 'learning_rate': 0.06794349446297417, 'reg_alpha': 0.0010226667391909664, 'reg_lambda': 2.623622708537351, 'subsample': 0.6200392464368687, 'min_child_weight': 8.812958408005255, 'colsample_bytree': 0.5058104808137766, 'gamma': 7.19551885636909, 'value': 0.8546043268818104}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8546043268818104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:15:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 01:30:58,703]\u001b[0m Trial 21 finished with value: 0.8562339952191178 and parameters: {'n_estimators': 9149, 'depth': 5, 'learning_rate': 0.009567786383359395, 'reg_alpha': 0.03312843842184493, 'reg_lambda': 0.7939849031192693, 'subsample': 0.7942317456572243, 'min_child_weight': 10.26306180333911, 'colsample_bytree': 0.8480166303036103, 'gamma': 3.512732020407266}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 21 < 22; dropping {'n_estimators': 9149, 'depth': 5, 'learning_rate': 0.009567786383359395, 'reg_alpha': 0.03312843842184493, 'reg_lambda': 0.7939849031192693, 'subsample': 0.7942317456572243, 'min_child_weight': 10.26306180333911, 'colsample_bytree': 0.8480166303036103, 'gamma': 3.512732020407266, 'value': 0.8562339952191178}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8562339952191178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:02] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 01:47:31,616]\u001b[0m Trial 22 finished with value: 0.8563412583835294 and parameters: {'n_estimators': 9500, 'depth': 5, 'learning_rate': 0.00804179801369447, 'reg_alpha': 0.002702826772840221, 'reg_lambda': 0.10913766938391886, 'subsample': 0.808845226625444, 'min_child_weight': 10.580976886321348, 'colsample_bytree': 0.8437920609517162, 'gamma': 4.417798489776449}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 22 < 23; dropping {'n_estimators': 9500, 'depth': 5, 'learning_rate': 0.00804179801369447, 'reg_alpha': 0.002702826772840221, 'reg_lambda': 0.10913766938391886, 'subsample': 0.808845226625444, 'min_child_weight': 10.580976886321348, 'colsample_bytree': 0.8437920609517162, 'gamma': 4.417798489776449, 'value': 0.8563412583835294}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8563412583835294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:47:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 02:06:42,390]\u001b[0m Trial 23 finished with value: 0.8561130544927871 and parameters: {'n_estimators': 9471, 'depth': 6, 'learning_rate': 0.007864883178987552, 'reg_alpha': 0.0022482762945815415, 'reg_lambda': 0.06876969086351024, 'subsample': 0.8855999560930906, 'min_child_weight': 11.962001523703094, 'colsample_bytree': 0.8061544685992041, 'gamma': 0.6954003003181719}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 23 < 24; dropping {'n_estimators': 9471, 'depth': 6, 'learning_rate': 0.007864883178987552, 'reg_alpha': 0.0022482762945815415, 'reg_lambda': 0.06876969086351024, 'subsample': 0.8855999560930906, 'min_child_weight': 11.962001523703094, 'colsample_bytree': 0.8061544685992041, 'gamma': 0.6954003003181719, 'value': 0.8561130544927871}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8561130544927871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:06:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 02:20:02,454]\u001b[0m Trial 24 finished with value: 0.8534878457498337 and parameters: {'n_estimators': 8258, 'depth': 4, 'learning_rate': 0.0036663303291863997, 'reg_alpha': 0.010435733386650523, 'reg_lambda': 9.748436086928649, 'subsample': 0.8211338116630436, 'min_child_weight': 10.60058229719336, 'colsample_bytree': 0.6949628191602378, 'gamma': 4.705387291271061}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 24 < 25; dropping {'n_estimators': 8258, 'depth': 4, 'learning_rate': 0.0036663303291863997, 'reg_alpha': 0.010435733386650523, 'reg_lambda': 9.748436086928649, 'subsample': 0.8211338116630436, 'min_child_weight': 10.60058229719336, 'colsample_bytree': 0.6949628191602378, 'gamma': 4.705387291271061, 'value': 0.8534878457498337}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8534878457498337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:20:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 02:30:19,403]\u001b[0m Trial 25 finished with value: 0.8557912804817343 and parameters: {'n_estimators': 7116, 'depth': 5, 'learning_rate': 0.021016665677817334, 'reg_alpha': 0.002602324107987263, 'reg_lambda': 0.246913094077943, 'subsample': 0.7060387435260883, 'min_child_weight': 8.622103797782415, 'colsample_bytree': 0.9158665974452559, 'gamma': 6.447107418862442}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 25 < 26; dropping {'n_estimators': 7116, 'depth': 5, 'learning_rate': 0.021016665677817334, 'reg_alpha': 0.002602324107987263, 'reg_lambda': 0.246913094077943, 'subsample': 0.7060387435260883, 'min_child_weight': 8.622103797782415, 'colsample_bytree': 0.9158665974452559, 'gamma': 6.447107418862442, 'value': 0.8557912804817343}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8557912804817343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:30:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 02:56:40,675]\u001b[0m Trial 26 finished with value: 0.8547582119465855 and parameters: {'n_estimators': 9908, 'depth': 7, 'learning_rate': 0.00214005923207084, 'reg_alpha': 0.012973078572260644, 'reg_lambda': 0.06714518878474646, 'subsample': 0.9137490397787007, 'min_child_weight': 10.879638349781187, 'colsample_bytree': 0.8469995150261526, 'gamma': 2.7559463476881403}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 26 < 27; dropping {'n_estimators': 9908, 'depth': 7, 'learning_rate': 0.00214005923207084, 'reg_alpha': 0.012973078572260644, 'reg_lambda': 0.06714518878474646, 'subsample': 0.9137490397787007, 'min_child_weight': 10.879638349781187, 'colsample_bytree': 0.8469995150261526, 'gamma': 2.7559463476881403, 'value': 0.8547582119465855}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8547582119465855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:56:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 03:08:07,801]\u001b[0m Trial 27 finished with value: 0.8560502257966153 and parameters: {'n_estimators': 9282, 'depth': 3, 'learning_rate': 0.026922867148138713, 'reg_alpha': 0.0023598411772881526, 'reg_lambda': 1.7543247316109092, 'subsample': 0.8200208350807104, 'min_child_weight': 6.780594885804428, 'colsample_bytree': 0.771019740152855, 'gamma': 1.8792488880605727}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 27 < 28; dropping {'n_estimators': 9282, 'depth': 3, 'learning_rate': 0.026922867148138713, 'reg_alpha': 0.0023598411772881526, 'reg_lambda': 1.7543247316109092, 'subsample': 0.8200208350807104, 'min_child_weight': 6.780594885804428, 'colsample_bytree': 0.771019740152855, 'gamma': 1.8792488880605727, 'value': 0.8560502257966153}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8560502257966153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:08:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 03:21:33,948]\u001b[0m Trial 28 finished with value: 0.8556167903591578 and parameters: {'n_estimators': 8293, 'depth': 4, 'learning_rate': 0.007240273976312109, 'reg_alpha': 0.06312757309711023, 'reg_lambda': 5.295737410962649, 'subsample': 0.9875788139367637, 'min_child_weight': 8.658526524656462, 'colsample_bytree': 0.722936407032924, 'gamma': 0.9667855748252316}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 28 < 29; dropping {'n_estimators': 8293, 'depth': 4, 'learning_rate': 0.007240273976312109, 'reg_alpha': 0.06312757309711023, 'reg_lambda': 5.295737410962649, 'subsample': 0.9875788139367637, 'min_child_weight': 8.658526524656462, 'colsample_bytree': 0.722936407032924, 'gamma': 0.9667855748252316, 'value': 0.8556167903591578}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8556167903591578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:21:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 03:30:45,116]\u001b[0m Trial 29 finished with value: 0.8562513005703175 and parameters: {'n_estimators': 6166, 'depth': 5, 'learning_rate': 0.01483175379477883, 'reg_alpha': 0.006138589258963908, 'reg_lambda': 12.996274297148451, 'subsample': 0.619720671481651, 'min_child_weight': 2.5756596716961866, 'colsample_bytree': 0.9199192280908068, 'gamma': 9.679194482941934}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 29 < 30; dropping {'n_estimators': 6166, 'depth': 5, 'learning_rate': 0.01483175379477883, 'reg_alpha': 0.006138589258963908, 'reg_lambda': 12.996274297148451, 'subsample': 0.619720671481651, 'min_child_weight': 2.5756596716961866, 'colsample_bytree': 0.9199192280908068, 'gamma': 9.679194482941934, 'value': 0.8562513005703175}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8562513005703175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:30:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 03:37:16,187]\u001b[0m Trial 30 finished with value: 0.8541686005405648 and parameters: {'n_estimators': 6178, 'depth': 6, 'learning_rate': 0.06212549942435661, 'reg_alpha': 0.009160280508895837, 'reg_lambda': 13.096049597239322, 'subsample': 0.5734030318285789, 'min_child_weight': 1.4781889916830604, 'colsample_bytree': 0.9248093294418563, 'gamma': 9.723230406803662}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 30 < 31; dropping {'n_estimators': 6178, 'depth': 6, 'learning_rate': 0.06212549942435661, 'reg_alpha': 0.009160280508895837, 'reg_lambda': 13.096049597239322, 'subsample': 0.5734030318285789, 'min_child_weight': 1.4781889916830604, 'colsample_bytree': 0.9248093294418563, 'gamma': 9.723230406803662, 'value': 0.8541686005405648}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8541686005405648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:37:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 03:46:43,800]\u001b[0m Trial 31 finished with value: 0.8562009720739798 and parameters: {'n_estimators': 5686, 'depth': 5, 'learning_rate': 0.013664759466225833, 'reg_alpha': 0.005101517097240461, 'reg_lambda': 3.8900152047221135, 'subsample': 0.6048015988570616, 'min_child_weight': 2.726253926865945, 'colsample_bytree': 0.8676021402788938, 'gamma': 5.781505304952806}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 31 < 32; dropping {'n_estimators': 5686, 'depth': 5, 'learning_rate': 0.013664759466225833, 'reg_alpha': 0.005101517097240461, 'reg_lambda': 3.8900152047221135, 'subsample': 0.6048015988570616, 'min_child_weight': 2.726253926865945, 'colsample_bytree': 0.8676021402788938, 'gamma': 5.781505304952806, 'value': 0.8562009720739798}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8562009720739798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:46:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 03:55:08,610]\u001b[0m Trial 32 finished with value: 0.8560791575501966 and parameters: {'n_estimators': 4857, 'depth': 5, 'learning_rate': 0.009017042644190586, 'reg_alpha': 0.0019701649527932136, 'reg_lambda': 17.13437654374741, 'subsample': 0.5612773997234303, 'min_child_weight': 3.504921429767583, 'colsample_bytree': 0.8979739020911907, 'gamma': 6.539114174142863}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 32 < 33; dropping {'n_estimators': 4857, 'depth': 5, 'learning_rate': 0.009017042644190586, 'reg_alpha': 0.0019701649527932136, 'reg_lambda': 17.13437654374741, 'subsample': 0.5612773997234303, 'min_child_weight': 3.504921429767583, 'colsample_bytree': 0.8979739020911907, 'gamma': 6.539114174142863, 'value': 0.8560791575501966}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8560791575501966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:55:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 04:03:50,879]\u001b[0m Trial 33 finished with value: 0.855679821958231 and parameters: {'n_estimators': 6451, 'depth': 4, 'learning_rate': 0.02504072249865618, 'reg_alpha': 0.0050431300947863175, 'reg_lambda': 0.0982987949927833, 'subsample': 0.5111745986368681, 'min_child_weight': 5.439829163606228, 'colsample_bytree': 0.9976180757841523, 'gamma': 4.300194728977004}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 33 < 34; dropping {'n_estimators': 6451, 'depth': 4, 'learning_rate': 0.02504072249865618, 'reg_alpha': 0.0050431300947863175, 'reg_lambda': 0.0982987949927833, 'subsample': 0.5111745986368681, 'min_child_weight': 5.439829163606228, 'colsample_bytree': 0.9976180757841523, 'gamma': 4.300194728977004, 'value': 0.855679821958231}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.855679821958231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:03:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 04:15:18,294]\u001b[0m Trial 34 finished with value: 0.856118121353634 and parameters: {'n_estimators': 6876, 'depth': 5, 'learning_rate': 0.014247545318337938, 'reg_alpha': 0.00425839473235573, 'reg_lambda': 0.02942540456489873, 'subsample': 0.7322104983958965, 'min_child_weight': 11.128177801461423, 'colsample_bytree': 0.8187243120067283, 'gamma': 5.6585461954168625}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 34 < 35; dropping {'n_estimators': 6876, 'depth': 5, 'learning_rate': 0.014247545318337938, 'reg_alpha': 0.00425839473235573, 'reg_lambda': 0.02942540456489873, 'subsample': 0.7322104983958965, 'min_child_weight': 11.128177801461423, 'colsample_bytree': 0.8187243120067283, 'gamma': 5.6585461954168625, 'value': 0.856118121353634}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.856118121353634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:15:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 04:26:35,244]\u001b[0m Trial 35 finished with value: 0.852951697550403 and parameters: {'n_estimators': 6098, 'depth': 6, 'learning_rate': 0.03484815872728634, 'reg_alpha': 1.2947304024819317, 'reg_lambda': 1.395307664052474, 'subsample': 0.6405945324376946, 'min_child_weight': 2.3252786178284373, 'colsample_bytree': 0.8744064072058993, 'gamma': 2.817719677768852}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 35 < 36; dropping {'n_estimators': 6098, 'depth': 6, 'learning_rate': 0.03484815872728634, 'reg_alpha': 1.2947304024819317, 'reg_lambda': 1.395307664052474, 'subsample': 0.6405945324376946, 'min_child_weight': 2.3252786178284373, 'colsample_bytree': 0.8744064072058993, 'gamma': 2.817719677768852, 'value': 0.852951697550403}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.852951697550403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:26:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 04:33:36,798]\u001b[0m Trial 36 finished with value: 0.8501612892796291 and parameters: {'n_estimators': 5377, 'depth': 3, 'learning_rate': 0.004437013550619371, 'reg_alpha': 0.01744339244356072, 'reg_lambda': 6.408100801159461, 'subsample': 0.5489587014073698, 'min_child_weight': 0.05222603038885687, 'colsample_bytree': 0.9400868442774339, 'gamma': 9.991789741587914}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 36 < 37; dropping {'n_estimators': 5377, 'depth': 3, 'learning_rate': 0.004437013550619371, 'reg_alpha': 0.01744339244356072, 'reg_lambda': 6.408100801159461, 'subsample': 0.5489587014073698, 'min_child_weight': 0.05222603038885687, 'colsample_bytree': 0.9400868442774339, 'gamma': 9.991789741587914, 'value': 0.8501612892796291}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8501612892796291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:33:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 04:45:05,252]\u001b[0m Trial 37 finished with value: 0.8560444344504619 and parameters: {'n_estimators': 7895, 'depth': 4, 'learning_rate': 0.006959633499881019, 'reg_alpha': 0.0010466160731983413, 'reg_lambda': 0.39651496063068997, 'subsample': 0.5195612846543524, 'min_child_weight': 1.120295526660651, 'colsample_bytree': 0.9136601143279947, 'gamma': 7.345525421866137}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 37 < 38; dropping {'n_estimators': 7895, 'depth': 4, 'learning_rate': 0.006959633499881019, 'reg_alpha': 0.0010466160731983413, 'reg_lambda': 0.39651496063068997, 'subsample': 0.5195612846543524, 'min_child_weight': 1.120295526660651, 'colsample_bytree': 0.9136601143279947, 'gamma': 7.345525421866137, 'value': 0.8560444344504619}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8560444344504619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:45:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 04:56:32,372]\u001b[0m Trial 38 finished with value: 0.8556998863150616 and parameters: {'n_estimators': 7158, 'depth': 5, 'learning_rate': 0.019763564503231978, 'reg_alpha': 0.22326398744757936, 'reg_lambda': 2.248975108271462, 'subsample': 0.6871755077038504, 'min_child_weight': 9.805698046163974, 'colsample_bytree': 0.7754850801780304, 'gamma': 1.9261397988140807}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 38 < 39; dropping {'n_estimators': 7158, 'depth': 5, 'learning_rate': 0.019763564503231978, 'reg_alpha': 0.22326398744757936, 'reg_lambda': 2.248975108271462, 'subsample': 0.6871755077038504, 'min_child_weight': 9.805698046163974, 'colsample_bytree': 0.7754850801780304, 'gamma': 1.9261397988140807, 'value': 0.8556998863150616}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8556998863150616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:56:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 05:02:33,966]\u001b[0m Trial 39 finished with value: 0.8558461240818702 and parameters: {'n_estimators': 4335, 'depth': 4, 'learning_rate': 0.03078331839904498, 'reg_alpha': 0.007339723005077337, 'reg_lambda': 0.936621256550992, 'subsample': 0.5817358875187779, 'min_child_weight': 4.0833490085555235, 'colsample_bytree': 0.9681165230087982, 'gamma': 4.175078370443178}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 39 < 40; dropping {'n_estimators': 4335, 'depth': 4, 'learning_rate': 0.03078331839904498, 'reg_alpha': 0.007339723005077337, 'reg_lambda': 0.936621256550992, 'subsample': 0.5817358875187779, 'min_child_weight': 4.0833490085555235, 'colsample_bytree': 0.9681165230087982, 'gamma': 4.175078370443178, 'value': 0.8558461240818702}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8558461240818702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:02:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 05:08:50,363]\u001b[0m Trial 40 finished with value: 0.8464624244239088 and parameters: {'n_estimators': 5415, 'depth': 3, 'learning_rate': 0.19731186097976292, 'reg_alpha': 0.06550639077424875, 'reg_lambda': 0.009264344135561767, 'subsample': 0.6368463084102985, 'min_child_weight': 6.245887571850043, 'colsample_bytree': 0.8375688420604991, 'gamma': 5.175325291759752}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 40 < 41; dropping {'n_estimators': 5415, 'depth': 3, 'learning_rate': 0.19731186097976292, 'reg_alpha': 0.06550639077424875, 'reg_lambda': 0.009264344135561767, 'subsample': 0.6368463084102985, 'min_child_weight': 6.245887571850043, 'colsample_bytree': 0.8375688420604991, 'gamma': 5.175325291759752, 'value': 0.8464624244239088}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8464624244239088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:08:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 05:24:17,969]\u001b[0m Trial 41 finished with value: 0.8562308221248082 and parameters: {'n_estimators': 9129, 'depth': 5, 'learning_rate': 0.010359568707878377, 'reg_alpha': 0.022754269132169946, 'reg_lambda': 0.783216332217926, 'subsample': 0.7893905188486425, 'min_child_weight': 10.076910731930553, 'colsample_bytree': 0.8589318327369191, 'gamma': 3.15193243523288}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 41 < 42; dropping {'n_estimators': 9129, 'depth': 5, 'learning_rate': 0.010359568707878377, 'reg_alpha': 0.022754269132169946, 'reg_lambda': 0.783216332217926, 'subsample': 0.7893905188486425, 'min_child_weight': 10.076910731930553, 'colsample_bytree': 0.8589318327369191, 'gamma': 3.15193243523288, 'value': 0.8562308221248082}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8562308221248082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:24:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 05:40:38,316]\u001b[0m Trial 42 finished with value: 0.8556739713096275 and parameters: {'n_estimators': 9676, 'depth': 6, 'learning_rate': 0.0147178791470144, 'reg_alpha': 0.0019355783382956325, 'reg_lambda': 3.060400097600813, 'subsample': 0.8137608036957327, 'min_child_weight': 11.17713790998944, 'colsample_bytree': 0.8222728223672153, 'gamma': 4.102357727421423}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 42 < 43; dropping {'n_estimators': 9676, 'depth': 6, 'learning_rate': 0.0147178791470144, 'reg_alpha': 0.0019355783382956325, 'reg_lambda': 3.060400097600813, 'subsample': 0.8137608036957327, 'min_child_weight': 11.17713790998944, 'colsample_bytree': 0.8222728223672153, 'gamma': 4.102357727421423, 'value': 0.8556739713096275}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8556739713096275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:40:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 05:58:17,926]\u001b[0m Trial 43 finished with value: 0.8466098587081 and parameters: {'n_estimators': 8654, 'depth': 5, 'learning_rate': 0.0010504148927947146, 'reg_alpha': 0.046200764176465854, 'reg_lambda': 0.3987501269227207, 'subsample': 0.8764869080764135, 'min_child_weight': 11.931628818490301, 'colsample_bytree': 0.7945339588435123, 'gamma': 2.2596575276146904}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 43 < 44; dropping {'n_estimators': 8654, 'depth': 5, 'learning_rate': 0.0010504148927947146, 'reg_alpha': 0.046200764176465854, 'reg_lambda': 0.3987501269227207, 'subsample': 0.8764869080764135, 'min_child_weight': 11.931628818490301, 'colsample_bytree': 0.7945339588435123, 'gamma': 2.2596575276146904, 'value': 0.8466098587081}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8466098587081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:58:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 06:15:20,149]\u001b[0m Trial 44 finished with value: 0.8561601766714853 and parameters: {'n_estimators': 9416, 'depth': 5, 'learning_rate': 0.005728130968254168, 'reg_alpha': 0.0035066591254900707, 'reg_lambda': 13.563201004270864, 'subsample': 0.8381520562836805, 'min_child_weight': 10.007434829361436, 'colsample_bytree': 0.8953300866241147, 'gamma': 9.209599796029845}. Best is trial 7 with value: 0.8563511687063371.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 44 < 45; dropping {'n_estimators': 9416, 'depth': 5, 'learning_rate': 0.005728130968254168, 'reg_alpha': 0.0035066591254900707, 'reg_lambda': 13.563201004270864, 'subsample': 0.8381520562836805, 'min_child_weight': 10.007434829361436, 'colsample_bytree': 0.8953300866241147, 'gamma': 9.209599796029845, 'value': 0.8561601766714853}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8561601766714853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:15:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 06:29:00,017]\u001b[0m Trial 45 finished with value: 0.8563816862908156 and parameters: {'n_estimators': 9112, 'depth': 4, 'learning_rate': 0.00989441142001576, 'reg_alpha': 0.0014767529836536453, 'reg_lambda': 1.4197597914792393, 'subsample': 0.7671447687492785, 'min_child_weight': 8.15368231820918, 'colsample_bytree': 0.9581142818850364, 'gamma': 0.3294227048501247}. Best is trial 45 with value: 0.8563816862908156.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 45 < 46; dropping {'n_estimators': 9112, 'depth': 4, 'learning_rate': 0.00989441142001576, 'reg_alpha': 0.0014767529836536453, 'reg_lambda': 1.4197597914792393, 'subsample': 0.7671447687492785, 'min_child_weight': 8.15368231820918, 'colsample_bytree': 0.9581142818850364, 'gamma': 0.3294227048501247, 'value': 0.8563816862908156}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8563816862908156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:29:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 06:43:10,688]\u001b[0m Trial 46 finished with value: 0.8563502557902043 and parameters: {'n_estimators': 9745, 'depth': 4, 'learning_rate': 0.012252411098472188, 'reg_alpha': 0.0019843316756850173, 'reg_lambda': 28.93145761431316, 'subsample': 0.7622490319513108, 'min_child_weight': 7.88719954952392, 'colsample_bytree': 0.9616235185574193, 'gamma': 0.11264955970338111}. Best is trial 45 with value: 0.8563816862908156.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 46 < 47; dropping {'n_estimators': 9745, 'depth': 4, 'learning_rate': 0.012252411098472188, 'reg_alpha': 0.0019843316756850173, 'reg_lambda': 28.93145761431316, 'subsample': 0.7622490319513108, 'min_child_weight': 7.88719954952392, 'colsample_bytree': 0.9616235185574193, 'gamma': 0.11264955970338111, 'value': 0.8563502557902043}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8563502557902043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:43:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 06:57:17,629]\u001b[0m Trial 47 finished with value: 0.8560138109884283 and parameters: {'n_estimators': 9960, 'depth': 4, 'learning_rate': 0.017695876644822105, 'reg_alpha': 0.0067948818171961545, 'reg_lambda': 28.424373601670677, 'subsample': 0.7683392957889553, 'min_child_weight': 7.954314370090295, 'colsample_bytree': 0.9546448210293101, 'gamma': 0.5810859302342282}. Best is trial 45 with value: 0.8563816862908156.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 47 < 48; dropping {'n_estimators': 9960, 'depth': 4, 'learning_rate': 0.017695876644822105, 'reg_alpha': 0.0067948818171961545, 'reg_lambda': 28.424373601670677, 'subsample': 0.7683392957889553, 'min_child_weight': 7.954314370090295, 'colsample_bytree': 0.9546448210293101, 'gamma': 0.5810859302342282, 'value': 0.8560138109884283}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8560138109884283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:57:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 07:09:45,371]\u001b[0m Trial 48 finished with value: 0.8563981996999422 and parameters: {'n_estimators': 8530, 'depth': 4, 'learning_rate': 0.012650399389271821, 'reg_alpha': 0.0016559264310235745, 'reg_lambda': 18.04598339771015, 'subsample': 0.7102073122643161, 'min_child_weight': 5.568430583004577, 'colsample_bytree': 0.9831938272343231, 'gamma': 0.1559503690837449}. Best is trial 48 with value: 0.8563981996999422.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 48 < 49; dropping {'n_estimators': 8530, 'depth': 4, 'learning_rate': 0.012650399389271821, 'reg_alpha': 0.0016559264310235745, 'reg_lambda': 18.04598339771015, 'subsample': 0.7102073122643161, 'min_child_weight': 5.568430583004577, 'colsample_bytree': 0.9831938272343231, 'gamma': 0.1559503690837449, 'value': 0.8563981996999422}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8563981996999422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:09:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 07:20:50,091]\u001b[0m Trial 49 finished with value: 0.855137686305009 and parameters: {'n_estimators': 8946, 'depth': 3, 'learning_rate': 0.050869303267973356, 'reg_alpha': 9.484845312461019, 'reg_lambda': 21.025579527298497, 'subsample': 0.7490774681151725, 'min_child_weight': 5.256192768739342, 'colsample_bytree': 0.9801778391147292, 'gamma': 0.14216592744877352}. Best is trial 48 with value: 0.8563981996999422.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 49 < 50; dropping {'n_estimators': 8946, 'depth': 3, 'learning_rate': 0.050869303267973356, 'reg_alpha': 9.484845312461019, 'reg_lambda': 21.025579527298497, 'subsample': 0.7490774681151725, 'min_child_weight': 5.256192768739342, 'colsample_bytree': 0.9801778391147292, 'gamma': 0.14216592744877352, 'value': 0.855137686305009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.855137686305009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:20:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 07:31:58,194]\u001b[0m Trial 50 finished with value: 0.856391395694204 and parameters: {'n_estimators': 7580, 'depth': 4, 'learning_rate': 0.012019211625034044, 'reg_alpha': 0.0015391489339818653, 'reg_lambda': 8.763604289181531, 'subsample': 0.6987569239277319, 'min_child_weight': 7.192375845497343, 'colsample_bytree': 0.999210325975214, 'gamma': 1.579753225261022}. Best is trial 48 with value: 0.8563981996999422.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 50 < 51; dropping {'n_estimators': 7580, 'depth': 4, 'learning_rate': 0.012019211625034044, 'reg_alpha': 0.0015391489339818653, 'reg_lambda': 8.763604289181531, 'subsample': 0.6987569239277319, 'min_child_weight': 7.192375845497343, 'colsample_bytree': 0.999210325975214, 'gamma': 1.579753225261022, 'value': 0.856391395694204}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.856391395694204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:32:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 07:43:14,537]\u001b[0m Trial 51 finished with value: 0.856334960422917 and parameters: {'n_estimators': 7687, 'depth': 4, 'learning_rate': 0.01206658075267246, 'reg_alpha': 0.0019514068911482508, 'reg_lambda': 9.026429498365856, 'subsample': 0.7094408275399597, 'min_child_weight': 7.060340092913241, 'colsample_bytree': 0.9884226034280617, 'gamma': 1.4755041268419673}. Best is trial 48 with value: 0.8563981996999422.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 51 < 52; dropping {'n_estimators': 7687, 'depth': 4, 'learning_rate': 0.01206658075267246, 'reg_alpha': 0.0019514068911482508, 'reg_lambda': 9.026429498365856, 'subsample': 0.7094408275399597, 'min_child_weight': 7.060340092913241, 'colsample_bytree': 0.9884226034280617, 'gamma': 1.4755041268419673, 'value': 0.856334960422917}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.856334960422917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:43:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 07:55:12,482]\u001b[0m Trial 52 finished with value: 0.8563079666418185 and parameters: {'n_estimators': 8051, 'depth': 4, 'learning_rate': 0.009061144474716294, 'reg_alpha': 0.001435979905866951, 'reg_lambda': 3.955303056800114, 'subsample': 0.6814493140186475, 'min_child_weight': 5.818588035229211, 'colsample_bytree': 0.9613755529390823, 'gamma': 0.951893177649633}. Best is trial 48 with value: 0.8563981996999422.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 52 < 53; dropping {'n_estimators': 8051, 'depth': 4, 'learning_rate': 0.009061144474716294, 'reg_alpha': 0.001435979905866951, 'reg_lambda': 3.955303056800114, 'subsample': 0.6814493140186475, 'min_child_weight': 5.818588035229211, 'colsample_bytree': 0.9613755529390823, 'gamma': 0.951893177649633, 'value': 0.8563079666418185}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8563079666418185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:55:16] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 08:06:58,234]\u001b[0m Trial 53 finished with value: 0.8559314197381037 and parameters: {'n_estimators': 8320, 'depth': 4, 'learning_rate': 0.02231079492217431, 'reg_alpha': 0.002989993357314126, 'reg_lambda': 19.183660884779815, 'subsample': 0.7321503846121328, 'min_child_weight': 7.241425912664457, 'colsample_bytree': 0.9478124623147868, 'gamma': 0.46882474789213413}. Best is trial 48 with value: 0.8563981996999422.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 53 < 54; dropping {'n_estimators': 8320, 'depth': 4, 'learning_rate': 0.02231079492217431, 'reg_alpha': 0.002989993357314126, 'reg_lambda': 19.183660884779815, 'subsample': 0.7321503846121328, 'min_child_weight': 7.241425912664457, 'colsample_bytree': 0.9478124623147868, 'gamma': 0.46882474789213413, 'value': 0.8559314197381037}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8559314197381037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:07:02] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 08:17:54,926]\u001b[0m Trial 54 finished with value: 0.8562633384351004 and parameters: {'n_estimators': 8497, 'depth': 3, 'learning_rate': 0.012071467340419745, 'reg_alpha': 0.0015070465931791832, 'reg_lambda': 7.269099809716813, 'subsample': 0.7592497842413467, 'min_child_weight': 8.263173576789384, 'colsample_bytree': 0.9753391513871984, 'gamma': 1.6420092554781842}. Best is trial 48 with value: 0.8563981996999422.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 54 < 55; dropping {'n_estimators': 8497, 'depth': 3, 'learning_rate': 0.012071467340419745, 'reg_alpha': 0.0015070465931791832, 'reg_lambda': 7.269099809716813, 'subsample': 0.7592497842413467, 'min_child_weight': 8.263173576789384, 'colsample_bytree': 0.9753391513871984, 'gamma': 1.6420092554781842, 'value': 0.8562633384351004}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8562633384351004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:17:58] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 08:31:51,753]\u001b[0m Trial 55 finished with value: 0.8551605631113013 and parameters: {'n_estimators': 8950, 'depth': 4, 'learning_rate': 0.004715626782169541, 'reg_alpha': 0.0036018934890190207, 'reg_lambda': 10.64635101337637, 'subsample': 0.7028283632206299, 'min_child_weight': 4.760158184763182, 'colsample_bytree': 0.997538803763743, 'gamma': 1.1291914110307473}. Best is trial 48 with value: 0.8563981996999422.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 55 < 56; dropping {'n_estimators': 8950, 'depth': 4, 'learning_rate': 0.004715626782169541, 'reg_alpha': 0.0036018934890190207, 'reg_lambda': 10.64635101337637, 'subsample': 0.7028283632206299, 'min_child_weight': 4.760158184763182, 'colsample_bytree': 0.997538803763743, 'gamma': 1.1291914110307473, 'value': 0.8551605631113013}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8551605631113013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:31:55] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fea0f4ee6f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwandbc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoostError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudypath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf\"optuna_xgboost_study_{x}trials_{datetime.now().strftime('%Y%m%d')}.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     dump(study.best_trial.params, filename=datapath/f'optuna_lightgbm_study_best-thru-{x*5}trials_20210927.joblib')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-342bf2617042>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     39\u001b[0m     )    \n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m# generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         )\n\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[0;32m--> 189\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1500\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(1, 500):\n",
    "    study.optimize(objective, n_trials = 1, callbacks = [wandbc], show_progress_bar=False, catch=(xgboost.core.XGBoostError,)) \n",
    "    dump(study, filename=studypath/f\"optuna_xgboost_study_{x}trials_{datetime.now().strftime('%Y%m%d')}.joblib\")\n",
    "#     dump(study.best_trial.params, filename=datapath/f'optuna_lightgbm_study_best-thru-{x*5}trials_20210927.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d6fb3-b4b3-40bd-8ff9-e2919c234f7d",
   "metadata": {
    "id": "27a746ff-c0e1-4218-8809-f102a58d2491"
   },
   "outputs": [],
   "source": [
    "# dump(study, filename=datapath/f\"optuna_xgboost_100trials-complete_{datetime.now().strftime('%Y%m%d')}.joblib\")\n",
    "# dump(study.best_trial.params, filename=datapath/f\"optuna_lightgbm_all-500trials-best_{datetime.now().strftime('%Y%m%d')}.joblib\")\n",
    "# pickle.dump(study.best_trial.params, open('CatBoost_Hyperparameter.pickle', 'wb'))\n",
    "# print('CatBoost Hyperparameter:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd3e56-55e0-4ea3-b1c2-eb3047165063",
   "metadata": {},
   "source": [
    "Best on original dataset was:\n",
    "\n",
    "```python\n",
    "{'n_estimators': 3878,\n",
    " 'depth': 4,\n",
    " 'learning_rate': 0.024785857161974977,\n",
    " 'reg_alpha': 26.867682044658245,\n",
    " 'reg_lambda': 10.839759074147148,\n",
    " 'subsample': 0.8208581489835881,\n",
    " 'min_child_weight': 8.829122644339664,\n",
    " 'colsample_bytree': 0.906420714280384,\n",
    " 'gamma': 1.472322916021486}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e689e-b20c-48e5-a7d9-02467b4f3dbd",
   "metadata": {
    "id": "f02e689e-b20c-48e5-a7d9-02467b4f3dbd"
   },
   "outputs": [],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ybeYZ3omaLWK",
   "metadata": {
    "id": "ybeYZ3omaLWK"
   },
   "outputs": [],
   "source": [
    "wandb.log({'xgboost_params': study.best_trial.params})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b6b43-d07b-49fb-beeb-1482cb5647fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e398cb-f0f4-4400-8fe7-9012b4bc33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sweep_lightgbm_20210922.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
