{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {},
   "source": [
    "Notebook for doing inference only with an ensemble -- experiments are conducted elsewhere. Model is as stated; `MaxAbsScaler` and `SelectKBest(k=80)` seem to be the best options as of 20210824. (Though model hyperparams haven't been fine-tuned with the scaler and the feature selector as of yet.) **And they should be, since performance is down!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01e85f7-d602-4dde-bef9-611683cd74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold#, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "# import timm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7827296-9c74-4b2e-b7e3-fac198991c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'inference_XGBoost_gpu_test_20210826.ipynb'\n",
    "config_run = {\n",
    "    'name': os.environ['WANDB_NOTEBOOK_NAME'][:-6], # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['XGBoost', 'experiment', 'gpu'],\n",
    "    'notes': \"Testing to see if GPU works with best-yet hyperparams\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1c0a51-fa76-46d8-a7eb-277a0b5fb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/202108_august/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c330e216-369e-4b8f-8f60-2885b8253047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')\n",
    "df = pd.read_feather(path='dataset_df.feather')\n",
    "df.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78944561-1fe5-4a29-9967-4bb87227fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1e4c99-64d4-4506-b208-397ce736eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in df.columns if x != 'loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6481808-e7b0-41a8-b4b4-88cbcb91f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e8c97fa-a5b5-4b22-85fb-696383c1d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"library\": \"xgboost\",\n",
    "    \"tree_method\": \"gpu_hist\", # set to 'gpu_hist' to try GPU if available, else 'auto'\n",
    "    \"booster\": 'dart', # dart may be marginally better, but will opt for this quicker approach as a default\n",
    "    \"n_estimators\": 100, # a very low number -- optimal is probably 300ish -- but this will be quicker\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.1522,\n",
    "    \"test_size\": 0.2,\n",
    "    \"scaler\": MaxAbsScaler,\n",
    "    \"feature_selector\": SelectKBest,\n",
    "    \"k_best\": 80,\n",
    "    \"feature_selection_scoring\": f_regression,\n",
    "    'random_state': 42,\n",
    "    'subsample': 1,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b57553a-5f07-49a4-94ac-6ca4f49017b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):#, scaler): # passed in via config dict for now\n",
    "    \"\"\"\n",
    "    Basic training function. Note that some of the options passed via the argument are\n",
    "    in fact hard-coded in, to avoid inconveniences.\n",
    "    :param config: dict with things to be logged in WandB, some to be used in function\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"202108_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=config)   \n",
    "        \n",
    "    # applying hold-out before scaling\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                          test_size=config['test_size'], \n",
    "                                                          random_state=config['random_state']\n",
    "                                                         )\n",
    "    # scaling (i.e. normalizing)\n",
    "    scaler = config['scaler']()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_valid_s = scaler.fit_transform(X_valid)\n",
    "    \n",
    "    # selecting features\n",
    "    selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "                                          k=config['k_best'])\n",
    "    X_train_fs = selector.fit_transform(X_train_s, y_train)\n",
    "    X_valid_fs = X_valid_s[:, selector.get_support()] # ensures same features are used in validation\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        tree_method=config['tree_method'],\n",
    "        booster=config['booster'],\n",
    "        n_estimators=config['n_estimators'], \n",
    "        max_depth=config['max_depth'],\n",
    "        learning_rate=config['learning_rate'], \n",
    "        test_size=config['test_size'],\n",
    "        subsample=config['subsample'],\n",
    "        random_state=config['random_state'],\n",
    "        n_jobs=config['n_jobs'], \n",
    "        verbosity=config['verbosity'], \n",
    "    )\n",
    "#     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "    model.fit(X_train_fs, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "    y_preds = model.predict(X_valid_fs)\n",
    "    mse = mean_squared_error(y_valid, y_preds)\n",
    "    rmse = math.sqrt(abs(mse))\n",
    "    wandb.log({'mse':mse, 'rmse':rmse})\n",
    "    print(f\"MSE is {mse}\\nRMSE is {rmse}\")   \n",
    "    wandb.finish()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c167c03-576a-4837-8ad4-81953ec1c4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">inference_XGBoost_gpu_test_20210826</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hushifang/202108_Kaggle_tabular_playground\" target=\"_blank\">https://wandb.ai/hushifang/202108_Kaggle_tabular_playground</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hushifang/202108_Kaggle_tabular_playground/runs/2qxts275\" target=\"_blank\">https://wandb.ai/hushifang/202108_Kaggle_tabular_playground/runs/2qxts275</a><br/>\n",
       "                Run data is saved locally in <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/aug2021/wandb/run-20210826_112104-2qxts275</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/training.py:20: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[11:21:11] /tmp/build/80754af9/xgboost-split_1619724447847/work/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(+0x85c1f) [0x7fafc48f6c1f]\n  [bt] (1) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x106) [0x7fafc49e48f6]\n  [bt] (2) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x309) [0x7fafc49f7479]\n  [bt] (3) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(+0x186c3d) [0x7fafc49f7c3d]\n  [bt] (4) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(+0x1b0a68) [0x7fafc4a21a68]\n  [bt] (5) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(+0x199a4d) [0x7fafc4a0aa4d]\n  [bt] (6) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fafc48fdc18]\n  [bt] (7) /home/sf/anaconda3/envs/tabular/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69dd) [0x7faff6e329dd]\n  [bt] (8) /home/sf/anaconda3/envs/tabular/lib/python3.8/lib-dynload/../../libffi.so.7(+0x6067) [0x7faff6e32067]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5ea5e78132e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-6d92744e35ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#     wandb.log({'params': model.get_params()}) # logging model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandb_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_fs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[1;32m    598\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \"\"\"\n\u001b[0;32m--> 227\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    228\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1281\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [11:21:11] /tmp/build/80754af9/xgboost-split_1619724447847/work/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(+0x85c1f) [0x7fafc48f6c1f]\n  [bt] (1) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x106) [0x7fafc49e48f6]\n  [bt] (2) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x309) [0x7fafc49f7479]\n  [bt] (3) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(+0x186c3d) [0x7fafc49f7c3d]\n  [bt] (4) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(+0x1b0a68) [0x7fafc4a21a68]\n  [bt] (5) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(+0x199a4d) [0x7fafc4a0aa4d]\n  [bt] (6) /home/sf/anaconda3/envs/tabular/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fafc48fdc18]\n  [bt] (7) /home/sf/anaconda3/envs/tabular/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69dd) [0x7faff6e329dd]\n  [bt] (8) /home/sf/anaconda3/envs/tabular/lib/python3.8/lib-dynload/../../libffi.so.7(+0x6067) [0x7faff6e32067]\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error resolved after 0:51:57.116310, resuming normal operation.\n"
     ]
    }
   ],
   "source": [
    "model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02baf90b-01bc-4945-b64c-5af0c6e309be",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e394bc6-29fe-4033-a850-bf6d55883b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(datapath/'test.csv', index_col='id', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e85f83d-b5a3-4c9e-b654-dfd743f2966e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250000</th>\n",
       "      <td>0.812665</td>\n",
       "      <td>15</td>\n",
       "      <td>-1.239120</td>\n",
       "      <td>-0.893251</td>\n",
       "      <td>295.5770</td>\n",
       "      <td>15.87120</td>\n",
       "      <td>23.04360</td>\n",
       "      <td>0.942256</td>\n",
       "      <td>29.898000</td>\n",
       "      <td>1.11394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446389</td>\n",
       "      <td>-422.332</td>\n",
       "      <td>-1.44630</td>\n",
       "      <td>1.69075</td>\n",
       "      <td>1.059300</td>\n",
       "      <td>-3.010570</td>\n",
       "      <td>1.94664</td>\n",
       "      <td>0.529470</td>\n",
       "      <td>1.386950</td>\n",
       "      <td>8.78767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250001</th>\n",
       "      <td>0.190344</td>\n",
       "      <td>131</td>\n",
       "      <td>-0.501361</td>\n",
       "      <td>0.801921</td>\n",
       "      <td>64.8866</td>\n",
       "      <td>3.09703</td>\n",
       "      <td>344.80500</td>\n",
       "      <td>0.807194</td>\n",
       "      <td>38.421900</td>\n",
       "      <td>1.09695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377179</td>\n",
       "      <td>10352.200</td>\n",
       "      <td>21.06270</td>\n",
       "      <td>1.84351</td>\n",
       "      <td>0.251895</td>\n",
       "      <td>4.440570</td>\n",
       "      <td>1.90309</td>\n",
       "      <td>0.248534</td>\n",
       "      <td>0.863881</td>\n",
       "      <td>11.79390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250002</th>\n",
       "      <td>0.919671</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.057382</td>\n",
       "      <td>0.901419</td>\n",
       "      <td>11961.2000</td>\n",
       "      <td>16.39650</td>\n",
       "      <td>273.24000</td>\n",
       "      <td>-0.003300</td>\n",
       "      <td>37.940000</td>\n",
       "      <td>1.15222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990140</td>\n",
       "      <td>3224.020</td>\n",
       "      <td>-2.25287</td>\n",
       "      <td>1.55100</td>\n",
       "      <td>-0.559157</td>\n",
       "      <td>17.838600</td>\n",
       "      <td>1.83385</td>\n",
       "      <td>0.931796</td>\n",
       "      <td>2.336870</td>\n",
       "      <td>9.05400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250003</th>\n",
       "      <td>0.860985</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.549509</td>\n",
       "      <td>0.471799</td>\n",
       "      <td>7501.6000</td>\n",
       "      <td>2.80698</td>\n",
       "      <td>71.08170</td>\n",
       "      <td>0.792136</td>\n",
       "      <td>0.395235</td>\n",
       "      <td>1.20157</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396880</td>\n",
       "      <td>9689.760</td>\n",
       "      <td>14.77150</td>\n",
       "      <td>1.41390</td>\n",
       "      <td>0.329272</td>\n",
       "      <td>0.802437</td>\n",
       "      <td>2.23251</td>\n",
       "      <td>0.893348</td>\n",
       "      <td>1.359470</td>\n",
       "      <td>4.84833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250004</th>\n",
       "      <td>0.313229</td>\n",
       "      <td>89</td>\n",
       "      <td>0.588509</td>\n",
       "      <td>0.167705</td>\n",
       "      <td>2931.2600</td>\n",
       "      <td>4.34986</td>\n",
       "      <td>1.57187</td>\n",
       "      <td>1.118300</td>\n",
       "      <td>7.754630</td>\n",
       "      <td>1.16807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862502</td>\n",
       "      <td>2693.350</td>\n",
       "      <td>44.18050</td>\n",
       "      <td>1.58020</td>\n",
       "      <td>-0.191021</td>\n",
       "      <td>26.253000</td>\n",
       "      <td>2.68238</td>\n",
       "      <td>0.361923</td>\n",
       "      <td>1.532800</td>\n",
       "      <td>3.70660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              f0   f1        f2        f3          f4        f5         f6  \\\n",
       "id                                                                           \n",
       "250000  0.812665   15 -1.239120 -0.893251    295.5770  15.87120   23.04360   \n",
       "250001  0.190344  131 -0.501361  0.801921     64.8866   3.09703  344.80500   \n",
       "250002  0.919671   19 -0.057382  0.901419  11961.2000  16.39650  273.24000   \n",
       "250003  0.860985   19 -0.549509  0.471799   7501.6000   2.80698   71.08170   \n",
       "250004  0.313229   89  0.588509  0.167705   2931.2600   4.34986    1.57187   \n",
       "\n",
       "              f7         f8       f9  ...       f90        f91       f92  \\\n",
       "id                                    ...                                  \n",
       "250000  0.942256  29.898000  1.11394  ...  0.446389   -422.332  -1.44630   \n",
       "250001  0.807194  38.421900  1.09695  ...  0.377179  10352.200  21.06270   \n",
       "250002 -0.003300  37.940000  1.15222  ...  0.990140   3224.020  -2.25287   \n",
       "250003  0.792136   0.395235  1.20157  ...  1.396880   9689.760  14.77150   \n",
       "250004  1.118300   7.754630  1.16807  ...  0.862502   2693.350  44.18050   \n",
       "\n",
       "            f93       f94        f95      f96       f97       f98       f99  \n",
       "id                                                                           \n",
       "250000  1.69075  1.059300  -3.010570  1.94664  0.529470  1.386950   8.78767  \n",
       "250001  1.84351  0.251895   4.440570  1.90309  0.248534  0.863881  11.79390  \n",
       "250002  1.55100 -0.559157  17.838600  1.83385  0.931796  2.336870   9.05400  \n",
       "250003  1.41390  0.329272   0.802437  2.23251  0.893348  1.359470   4.84833  \n",
       "250004  1.58020 -0.191021  26.253000  2.68238  0.361923  1.532800   3.70660  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1ec74e4-ccb8-43b4-b910-3df1542aaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df[features] # this is just for naming consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725cd3e-f883-4d20-837a-9f557b2122a9",
   "metadata": {},
   "source": [
    "Now, let's get the features the model was trained on and subset the test set's features accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8929a3a1-56ca-4f20-a44f-e3877c6dabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:48:45] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"test_size\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1522, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=400, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             test_size=0.2, tree_method='auto', validate_parameters=1,\n",
       "             verbosity=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying hold-out before scaling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=config['test_size'], \n",
    "                                                      random_state=config['random_state']\n",
    "                                                     )\n",
    "# scaling (i.e. normalizing)\n",
    "scaler = config['scaler']()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.fit_transform(X_test)\n",
    "\n",
    "# selecting features\n",
    "selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "                                      k=config['k_best'])\n",
    "X_train_fs = selector.fit_transform(X_train_s, y_train)\n",
    "X_test_fs = X_test_s[:, selector.get_support()]\n",
    "\n",
    "model = XGBRegressor(\n",
    "    tree_method=config['tree_method'],\n",
    "    booster=config['booster'],\n",
    "    n_estimators=config['n_estimators'], \n",
    "    max_depth=config['max_depth'],\n",
    "    learning_rate=config['learning_rate'], \n",
    "    test_size=config['test_size'],\n",
    "    subsample=config['subsample'],\n",
    "    random_state=config['random_state'],\n",
    "    n_jobs=config['n_jobs'], \n",
    "    verbosity=config['verbosity'], \n",
    ")\n",
    "#     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "model.fit(X_train_fs, y_train)#, callbacks=[wandb.xgboost.wandb_callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd935b52-5b2c-4db4-8704-34d61a0e2013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_test_preds = model.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a464a1c-9ca8-4a07-9cdb-18af399cf95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9d2f4b8-2356-4916-a091-45793db784ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'loss'] = y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c957ce26-bbf5-4aee-bccd-988f2471db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250000</td>\n",
       "      <td>8.235917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250001</td>\n",
       "      <td>4.625789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250002</td>\n",
       "      <td>7.081776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250003</td>\n",
       "      <td>6.641549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250004</td>\n",
       "      <td>7.322997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      loss\n",
       "0  250000  8.235917\n",
       "1  250001  4.625789\n",
       "2  250002  7.081776\n",
       "3  250003  6.641549\n",
       "4  250004  7.322997"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4cc6d50-92bc-4295-9acc-5d345eb96755",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('202108241140_XGBoost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06093b1-d26d-4388-bac8-343363168c7c",
   "metadata": {},
   "source": [
    "This got 7.90537 on the LB -- worse than before feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67df4d3-54f8-43fe-9f14-d3751986a58d",
   "metadata": {},
   "source": [
    "# Experiment - fitting model on full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8331118-e7a3-4578-8e43-cf93067c6b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:15] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"test_size\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1522, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=400, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             test_size=0.2, tree_method='auto', validate_parameters=1,\n",
       "             verbosity=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying hold-out before scaling\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                       test_size=config['test_size'], \n",
    "#                                                       random_state=config['random_state']\n",
    "#                                                      )\n",
    "# scaling (i.e. normalizing)\n",
    "scaler = config['scaler']()\n",
    "X_s = scaler.fit_transform(X)\n",
    "X_test_s = scaler.fit_transform(X_test)\n",
    "\n",
    "# selecting features\n",
    "selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "                                      k=config['k_best'])\n",
    "X_fs = selector.fit_transform(X_s, y)\n",
    "X_test_fs = X_test_s[:, selector.get_support()]\n",
    "\n",
    "model = XGBRegressor(\n",
    "    tree_method=config['tree_method'],\n",
    "    booster=config['booster'],\n",
    "    n_estimators=config['n_estimators'], \n",
    "    max_depth=config['max_depth'],\n",
    "    learning_rate=config['learning_rate'], \n",
    "    test_size=config['test_size'],\n",
    "    subsample=config['subsample'],\n",
    "    random_state=config['random_state'],\n",
    "    n_jobs=config['n_jobs'], \n",
    "    verbosity=config['verbosity'], \n",
    ")\n",
    "#     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "model.fit(X_fs, y)#, callbacks=[wandb.xgboost.wandb_callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d90ba24b-75cd-4c2d-a2cf-fe7bf16b3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_test_preds = model.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7db93b42-8460-4793-bd0f-60ddb1d7e84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bfc7e54-043d-4abb-818e-503846c0f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'loss'] = y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50b58c49-59e9-4751-8373-98536c9a121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250000</td>\n",
       "      <td>8.027956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250001</td>\n",
       "      <td>4.305676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250002</td>\n",
       "      <td>7.300106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250003</td>\n",
       "      <td>6.988875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250004</td>\n",
       "      <td>7.316631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      loss\n",
       "0  250000  8.027956\n",
       "1  250001  4.305676\n",
       "2  250002  7.300106\n",
       "3  250003  6.988875\n",
       "4  250004  7.316631"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41d0f44a-2ca7-486f-9197-48534a35043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('202108241211_XGBoost_fullset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
