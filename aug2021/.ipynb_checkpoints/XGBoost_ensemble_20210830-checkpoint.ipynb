{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {},
   "source": [
    "Notebook for doing inference only with an ensemble -- experiments are conducted elsewhere. Model is as stated; `MaxAbsScaler` and `SelectKBest(k=80)` seem to be the best options as of 20210824. (Though model hyperparams haven't been fine-tuned with the scaler and the feature selector as of yet.) **And they should be, since performance is down!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01e85f7-d602-4dde-bef9-611683cd74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "# import timm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7827296-9c74-4b2e-b7e3-fac198991c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'XGBoost_ensemble_20210830.ipynb'\n",
    "config = {\n",
    "    # model config\n",
    "    \"model\":XGBRegressor,\n",
    "    \"tree_method\": \"auto\", # set to 'gpu_hist' to try GPU if available\n",
    "    \"booster\": 'dart', # dart may be marginally better, but will opt for this quicker approach as a default\n",
    "    \"n_estimators\": 1000, \n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.1522,\n",
    "    \"test_size\": 0.2,\n",
    "    \"scaler\": MaxAbsScaler,\n",
    "#     \"task_type\": \"GPU\", # for CatBoost only\n",
    "#     \"reg_alpha\": 2.8,\n",
    "#     \"reg_lambda\": 3.987,\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "    'random_state': 42,\n",
    "    'subsample': 1,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "    'k_folds': 5,\n",
    "    'features_created': True,\n",
    "    'feature_creator': PolynomialFeatures,\n",
    "}\n",
    "\n",
    "config_run = {\n",
    "    # wandb config:\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['XGBoost', 'kfold', 'scaling', 'feature-creation', 'feature-selection'],\n",
    "    'notes': \"Running most of the parameters from the run j7tlo010 from the sweep b2zv3fsy, the most successful yet; just increasing n_estimators from 400 to 1000 and adding k-fold, feature generation/selection, etc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1c0a51-fa76-46d8-a7eb-277a0b5fb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/202108_august/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c330e216-369e-4b8f-8f60-2885b8253047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')\n",
    "\n",
    "# load unaltered dataset\n",
    "df = pd.read_feather(path='dataset_df.feather')\n",
    "df.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78944561-1fe5-4a29-9967-4bb87227fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77e37c-8f76-44b2-9e04-7fa02430ce96",
   "metadata": {},
   "source": [
    "# Feature Creation and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb77f573-e284-49a7-96a8-bb33b5b44147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the polynomialfeatures generated with `PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)`\n",
    "# X_np = np.load(datapath/'X_poly_unscaled.npy')\n",
    "# X = pd.DataFrame(X_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e905f6-f5e4-4848-98e3-ddc846ab4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1e4c99-64d4-4506-b208-397ce736eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep features from unaltered dataset\n",
    "features = [x for x in df.columns if x != 'loss']\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2728178c-5214-4c6c-ab86-5d63aeb2c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbdaa47-cab5-441b-885d-d21b33c604cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_names = poly.get_feature_names(X.columns)\n",
    "# X_poly_names[100:150]\n",
    "features = pd.read_csv('X_candidates_20210827.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6bd0c-1121-4430-966d-7a318d925d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = [feature in X_poly_names for feature in features]\n",
    "# checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c528d2-2148-409f-9489-254358a4e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X_poly, columns=X_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0682b3-1fef-4c54-9a33-99f659a2c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[features[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c845cbf-334e-432e-9f85-a8b68284580b",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "Now, going to scale using `MaxAbsScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c839fb-0ffc-4f4f-bc90-a73a13afb16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = config['scaler']()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=X_poly_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0333a-b1cb-45bd-adc2-e6563ae31770",
   "metadata": {},
   "source": [
    "# K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17679c-152f-4aff-a1dd-d09ab727ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUALLY probably better to save those as pickles or .npy files; I'll generate them later, regardless\n",
    "# results = {} # for storing k-fold models' predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd996d02-2530-4a40-84ec-01f8cde635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=config['k_folds'], shuffle=True, random_state=config['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57553a-5f07-49a4-94ac-6ca4f49017b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_valid, y_train, y_valid, config):#, scaler): # passed in via config dict for now\n",
    "    \"\"\"\n",
    "    Basic training function. Note that some of the options passed via the argument are\n",
    "    in fact hard-coded in, to avoid inconveniences.\n",
    "    :param config: dict with things to be logged in WandB, some to be used in function\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"202108_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=config)   \n",
    "        \n",
    "    # applying hold-out before scaling\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                           test_size=config['test_size'], \n",
    "#                                                           random_state=config['random_state']\n",
    "#                                                          )\n",
    "    \n",
    "    # strictly speaking should do the below, but doing beforehand faster and fine in this context\n",
    "    # scaling (i.e. normalizing)\n",
    "#     scaler = config['scaler']()\n",
    "#     X_train_s = scaler.fit_transform(X_train)\n",
    "#     X_valid_s = scaler.fit_transform(X_valid)\n",
    "    \n",
    "    # selecting features\n",
    "#     selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "#                                           k=config['k_best'])\n",
    "#     X_train_fs = selector.fit_transform(X_train_s, y_train)\n",
    "#     X_valid_fs = X_valid_s[:, selector.get_support()] # ensures same features are used in validation\n",
    "\n",
    "#     # split the dataset\n",
    "#     model = CatBoostRegressor(\n",
    "#         n_estimators=config['n_estimators'],\n",
    "#         learning_rate=config['learning_rate'],\n",
    "#         max_depth=config['max_depth'],\n",
    "#         task_type=config['task_type'],\n",
    "# #         n_jobs=config['n_jobs'],\n",
    "# #         verbosity=config['verbosity'],\n",
    "# #         subsample=config['subsample'],\n",
    "#         random_state=config['random_state'],\n",
    "# #         bootstrap_type=config['bootstrap_type'],\n",
    "# #         device:config['device']\n",
    "#     ) \n",
    "\n",
    "    model = XGBRegressor(\n",
    "        tree_method=config['tree_method'],\n",
    "        booster=config['booster'],\n",
    "        n_estimators=config['n_estimators'], \n",
    "        max_depth=config['max_depth'],\n",
    "        learning_rate=config['learning_rate'], \n",
    "#         test_size=config['test_size'],\n",
    "        subsample=config['subsample'],\n",
    "        reg_alpha=config['reg_alpha'],\n",
    "        reg_lambda=config['reg_lambda'],\n",
    "        random_state=config['random_state'],\n",
    "        n_jobs=config['n_jobs'], \n",
    "        verbosity=config['verbosity'], \n",
    "    )\n",
    "#     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "    model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "    y_preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_preds)\n",
    "    rmse = math.sqrt(abs(mse))\n",
    "    wandb.log({'mse':mse, 'rmse':rmse})\n",
    "    print(f\"MSE is {mse}\\nRMSE is {rmse}\")   \n",
    "#     wandb.finish()   \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75f832-d012-4021-9628-984209569b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529afca9-85db-4499-a909-089fb2f10899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(f\"./models/{config_run['name']}_{config['k_folds']}folds/\")\n",
    "(model_path).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b675b9-3eef-4546-a638-0418ce991653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "#     if fold == 0:\n",
    "#         continue\n",
    "#     else:\n",
    "    print(f\"FOLD {fold}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    X_train, X_valid = X_scaled[train_ids], X_scaled[valid_ids] # requires X to be a numpy.ndarray\n",
    "    y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "    model = train(X_train, X_valid, y_train, y_valid, config)\n",
    "    wandb.log({'fold': fold})\n",
    "    models[fold] = model\n",
    "    dump(model, Path(model_path/f\"catboost_fold{fold}_model.joblib\"))\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e972a4-67b9-4fd8-96b1-6525e805f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     dump(preds, f\"./preds/{config_rn['name']}/xgboost_fold{fold}_preds.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02baf90b-01bc-4945-b64c-5af0c6e309be",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e394bc6-29fe-4033-a850-bf6d55883b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(datapath/'test.csv', index_col='id', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85f83d-b5a3-4c9e-b654-dfd743f2966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ec520-259f-44d2-ad33-7b8c22621132",
   "metadata": {},
   "source": [
    "(Here's where encapsulating the transformations in a pipeline would come in handy. But I'll do it manually for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec74e4-ccb8-43b4-b910-3df1542aaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in test_df.columns if x != 'loss']\n",
    "X_test = test_df[features] # this is just for naming consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725cd3e-f883-4d20-837a-9f557b2122a9",
   "metadata": {},
   "source": [
    "Now, let's get the features the model was trained on and subset the test set's features accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66e579-7f01-46e5-a47c-580c8f5d678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)\n",
    "X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1817e3a-7d90-4bc2-8c47-e97806f7dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_poly_names = poly.get_feature_names(X_test.columns)\n",
    "# X_poly_names[100:150]\n",
    "features = pd.read_csv('X_candidates_20210827.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6bc49-d478-4f59-84d2-5e23e3e236db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks = [feature in X_test_poly_names for feature in features]\n",
    "# checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68187e-271a-4df1-ae02-a2bb5d62c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = pd.DataFrame(X_test_poly, columns=X_test_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020ad9b-1b05-49b8-b89b-c90362c256d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = X_test_final[features[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c226e-1ef7-4e03-91a1-06fbb73139f0",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "Now, going to scale using `MaxAbsScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da840a-caa6-4d76-a542-c1315a593346",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = config['scaler']()\n",
    "X_test_scaled = scaler.fit_transform(X_test_final)\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=X_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3cbac-0995-4015-9940-fe3e8ec94724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929a3a1-56ca-4f20-a44f-e3877c6dabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying hold-out before scaling\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                       test_size=config['test_size'], \n",
    "#                                                       random_state=config['random_state']\n",
    "#                                                      )\n",
    "# # scaling (i.e. normalizing)\n",
    "# scaler = config['scaler']()\n",
    "# X_train_s = scaler.fit_transform(X_train)\n",
    "# X_test_s = scaler.fit_transform(X_test)\n",
    "\n",
    "# # selecting features\n",
    "# selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "#                                       k=config['k_best'])\n",
    "# X_train_fs = selector.fit_transform(X_train_s, y_train)\n",
    "# X_test_fs = X_test_s[:, selector.get_support()]\n",
    "\n",
    "# model = XGBRegressor(\n",
    "#     tree_method=config['tree_method'],\n",
    "#     booster=config['booster'],\n",
    "#     n_estimators=config['n_estimators'], \n",
    "#     max_depth=config['max_depth'],\n",
    "#     learning_rate=config['learning_rate'], \n",
    "#     test_size=config['test_size'],\n",
    "#     subsample=config['subsample'],\n",
    "#     random_state=config['random_state'],\n",
    "#     n_jobs=config['n_jobs'], \n",
    "#     verbosity=config['verbosity'], \n",
    "# )\n",
    "# #     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "# model.fit(X_train_fs, y_train)#, callbacks=[wandb.xgboost.wandb_callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151abea-d9b0-48eb-969f-5c342fc13474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348db7c-c494-4616-a08f-55016fac5351",
   "metadata": {},
   "source": [
    "Now, iterate over the dict containing the models trained on the 5 folds, and store the predictions in a new dict `preds`\n",
    "**OR**\n",
    "load from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608ff29-4ceb-43cb-a0bb-e1450c0b8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_models = {}\n",
    "# saved_models_path = Path('/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/aug2021/models/inference_ensemble_20210828_204126_5folds/')\n",
    "# for fold in range(5):\n",
    "#     loaded_models[fold] = load(filename=Path(saved_models_path/f'xgboost_fold{fold}_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285899b2-0b2e-4db3-9b1e-a0d8328b84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa072384-5154-4d54-8ddf-5452e9323882",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for fold in models.keys():\n",
    "    preds[fold] = models[fold].predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a464a1c-9ca8-4a07-9cdb-18af399cf95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e6414-5f72-4a48-b5a1-a8d24486bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc067fb9-ae8e-4b33-9d75-e5405043aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = (preds[0] + preds[1] + preds[2] + preds[3] + preds[4]) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca32f6-2267-4c98-9aa9-c4a31d69bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f4b8-2356-4916-a091-45793db784ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'loss'] = final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957ce26-bbf5-4aee-bccd-988f2471db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4cc6d50-92bc-4295-9acc-5d345eb96755",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('202108302123_XGBoost_n-est1000per.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67df4d3-54f8-43fe-9f14-d3751986a58d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment - fitting model on full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8331118-e7a3-4578-8e43-cf93067c6b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:15] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"test_size\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1522, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=400, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             test_size=0.2, tree_method='auto', validate_parameters=1,\n",
       "             verbosity=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying hold-out before scaling\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                       test_size=config['test_size'], \n",
    "#                                                       random_state=config['random_state']\n",
    "#                                                      )\n",
    "# scaling (i.e. normalizing)\n",
    "scaler = config['scaler']()\n",
    "X_s = scaler.fit_transform(X)\n",
    "X_test_s = scaler.fit_transform(X_test)\n",
    "\n",
    "# selecting features\n",
    "selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "                                      k=config['k_best'])\n",
    "X_fs = selector.fit_transform(X_s, y)\n",
    "X_test_fs = X_test_s[:, selector.get_support()]\n",
    "\n",
    "model = XGBRegressor(\n",
    "    tree_method=config['tree_method'],\n",
    "    booster=config['booster'],\n",
    "    n_estimators=config['n_estimators'], \n",
    "    max_depth=config['max_depth'],\n",
    "    learning_rate=config['learning_rate'], \n",
    "    test_size=config['test_size'],\n",
    "    subsample=config['subsample'],\n",
    "    random_state=config['random_state'],\n",
    "    n_jobs=config['n_jobs'], \n",
    "    verbosity=config['verbosity'], \n",
    ")\n",
    "#     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "model.fit(X_fs, y)#, callbacks=[wandb.xgboost.wandb_callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d90ba24b-75cd-4c2d-a2cf-fe7bf16b3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_test_preds = model.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7db93b42-8460-4793-bd0f-60ddb1d7e84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bfc7e54-043d-4abb-818e-503846c0f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'loss'] = y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50b58c49-59e9-4751-8373-98536c9a121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250000</td>\n",
       "      <td>8.027956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250001</td>\n",
       "      <td>4.305676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250002</td>\n",
       "      <td>7.300106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250003</td>\n",
       "      <td>6.988875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250004</td>\n",
       "      <td>7.316631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      loss\n",
       "0  250000  8.027956\n",
       "1  250001  4.305676\n",
       "2  250002  7.300106\n",
       "3  250003  6.988875\n",
       "4  250004  7.316631"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41d0f44a-2ca7-486f-9197-48534a35043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('202108241211_XGBoost_fullset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
