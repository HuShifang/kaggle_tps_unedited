{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {},
   "source": [
    "Notebook for doing inference only with an ensemble -- experiments are conducted elsewhere. Model is as stated; `MaxAbsScaler` and `SelectKBest(k=80)` seem to be the best options as of 20210824. (Though model hyperparams haven't been fine-tuned with the scaler and the feature selector as of yet.) **And they should be, since performance is down!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01e85f7-d602-4dde-bef9-611683cd74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "# import timm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7827296-9c74-4b2e-b7e3-fac198991c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'inference_ensemble_20210827.ipynb'\n",
    "config = {\n",
    "    # model config\n",
    "    \"model\":XGBRegressor,\n",
    "    \"tree_method\": \"auto\", # set to 'gpu_hist' to try GPU if available\n",
    "    \"booster\": 'gbtree', # dart may be marginally better, but will opt for this quicker approach as a default\n",
    "    \"n_estimators\": 1000, \n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.1522,\n",
    "    \"test_size\": 0.2,\n",
    "    \"scaler\": MaxAbsScaler,\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "    'random_state': 42,\n",
    "    'subsample': 1,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "    'k_folds': 5,\n",
    "    'features_created': True,\n",
    "    'feature_creator': PolynomialFeatures,\n",
    "}\n",
    "\n",
    "config_run = {\n",
    "    # wandb config:\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['XGBoost', 'kfold', 'scaling', 'feature-creation'],\n",
    "    'notes': \"\"\"K-fold cross-val test using all the degree-2 generated polynomial features, for comparison\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1c0a51-fa76-46d8-a7eb-277a0b5fb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/202108_august/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c330e216-369e-4b8f-8f60-2885b8253047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')\n",
    "\n",
    "# load unaltered dataset\n",
    "df = pd.read_feather(path='dataset_df.feather')\n",
    "df.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78944561-1fe5-4a29-9967-4bb87227fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31139a51-750d-40e8-aa5e-7e76e4aaac3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb77f573-e284-49a7-96a8-bb33b5b44147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the polynomialfeatures generated with `PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)`\n",
    "# X_np = np.load(datapath/'X_poly_unscaled.npy')\n",
    "# X = pd.DataFrame(X_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e905f6-f5e4-4848-98e3-ddc846ab4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1e4c99-64d4-4506-b208-397ce736eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep features from unaltered dataset\n",
    "features = [x for x in df.columns if x != 'loss']\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2728178c-5214-4c6c-ab86-5d63aeb2c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cbdaa47-cab5-441b-885d-d21b33c604cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_names = poly.get_feature_names(X.columns)\n",
    "# X_poly_names[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c528d2-2148-409f-9489-254358a4e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X_poly, columns=X_poly_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c845cbf-334e-432e-9f85-a8b68284580b",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "Now, going to scale using `MaxAbsScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c839fb-0ffc-4f4f-bc90-a73a13afb16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = config['scaler']()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0333a-b1cb-45bd-adc2-e6563ae31770",
   "metadata": {},
   "source": [
    "# K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a17679c-152f-4aff-a1dd-d09ab727ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUALLY probably better to save those as pickles or .npy files; I'll generate them later, regardless\n",
    "# results = {} # for storing k-fold models' predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd996d02-2530-4a40-84ec-01f8cde635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=config['k_folds'], shuffle=True, random_state=config['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b57553a-5f07-49a4-94ac-6ca4f49017b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_valid, y_train, y_valid, config):#, scaler): # passed in via config dict for now\n",
    "    \"\"\"\n",
    "    Basic training function. Note that some of the options passed via the argument are\n",
    "    in fact hard-coded in, to avoid inconveniences.\n",
    "    :param config: dict with things to be logged in WandB, some to be used in function\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"202108_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=config)   \n",
    "        \n",
    "    # applying hold-out before scaling\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                           test_size=config['test_size'], \n",
    "#                                                           random_state=config['random_state']\n",
    "#                                                          )\n",
    "    \n",
    "    # strictly speaking should do the below, but doing beforehand faster and fine in this context\n",
    "    # scaling (i.e. normalizing)\n",
    "#     scaler = config['scaler']()\n",
    "#     X_train_s = scaler.fit_transform(X_train)\n",
    "#     X_valid_s = scaler.fit_transform(X_valid)\n",
    "    \n",
    "    # selecting features\n",
    "#     selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "#                                           k=config['k_best'])\n",
    "#     X_train_fs = selector.fit_transform(X_train_s, y_train)\n",
    "#     X_valid_fs = X_valid_s[:, selector.get_support()] # ensures same features are used in validation\n",
    "\n",
    "    # split the dataset\n",
    "    model = XGBRegressor(\n",
    "        tree_method=config['tree_method'],\n",
    "        booster=config['booster'],\n",
    "        n_estimators=config['n_estimators'], \n",
    "        max_depth=config['max_depth'],\n",
    "        learning_rate=config['learning_rate'], \n",
    "#         test_size=config['test_size'],\n",
    "        subsample=config['subsample'],\n",
    "        random_state=config['random_state'],\n",
    "        n_jobs=config['n_jobs'], \n",
    "        verbosity=config['verbosity'], \n",
    "    )\n",
    "#     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "    model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "    y_preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_preds)\n",
    "    rmse = math.sqrt(abs(mse))\n",
    "    wandb.log({'mse':mse, 'rmse':rmse})\n",
    "    print(f\"MSE is {mse}\\nRMSE is {rmse}\")   \n",
    "    wandb.finish()   \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf615cf-be6a-4d94-8ee0-2aa5c47ab5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "529afca9-85db-4499-a909-089fb2f10899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(f\"./models/{config_run['name']}_{config['k_folds']}folds/\")\n",
    "(model_path).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b675b9-3eef-4546-a638-0418ce991653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">inference_ensemble_20210827_094714</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hushifang/202108_Kaggle_tabular_playground\" target=\"_blank\">https://wandb.ai/hushifang/202108_Kaggle_tabular_playground</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hushifang/202108_Kaggle_tabular_playground/runs/q3jjslku\" target=\"_blank\">https://wandb.ai/hushifang/202108_Kaggle_tabular_playground/runs/q3jjslku</a><br/>\n",
       "                Run data is saved locally in <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/aug2021/wandb/run-20210827_094750-q3jjslku</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-24fc97c28778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fold'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-9bbe9e32e97c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, X_valid, y_train, y_valid, config)\u001b[0m\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#     wandb.log({'params': model.get_params()}) # logging model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandb_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[0;32m--> 189\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1497\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "#     if fold == 0:\n",
    "#         continue\n",
    "#     else:\n",
    "    print(f\"FOLD {fold}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "    y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "    model = train(X_train, X_valid, y_train, y_valid, config)\n",
    "    wandb.log({'fold': fold})\n",
    "    models[fold] = model\n",
    "    dump(model, Path(model_path/f\"xgboost_fold{fold}_model.joblib\"))\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3e972a4-67b9-4fd8-96b1-6525e805f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     dump(preds, f\"./preds/{config_rn['name']}/xgboost_fold{fold}_preds.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02baf90b-01bc-4945-b64c-5af0c6e309be",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e394bc6-29fe-4033-a850-bf6d55883b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(datapath/'test.csv', index_col='id', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e85f83d-b5a3-4c9e-b654-dfd743f2966e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250000</th>\n",
       "      <td>0.812665</td>\n",
       "      <td>15</td>\n",
       "      <td>-1.239120</td>\n",
       "      <td>-0.893251</td>\n",
       "      <td>295.5770</td>\n",
       "      <td>15.87120</td>\n",
       "      <td>23.04360</td>\n",
       "      <td>0.942256</td>\n",
       "      <td>29.898000</td>\n",
       "      <td>1.11394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446389</td>\n",
       "      <td>-422.332</td>\n",
       "      <td>-1.44630</td>\n",
       "      <td>1.69075</td>\n",
       "      <td>1.059300</td>\n",
       "      <td>-3.010570</td>\n",
       "      <td>1.94664</td>\n",
       "      <td>0.529470</td>\n",
       "      <td>1.386950</td>\n",
       "      <td>8.78767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250001</th>\n",
       "      <td>0.190344</td>\n",
       "      <td>131</td>\n",
       "      <td>-0.501361</td>\n",
       "      <td>0.801921</td>\n",
       "      <td>64.8866</td>\n",
       "      <td>3.09703</td>\n",
       "      <td>344.80500</td>\n",
       "      <td>0.807194</td>\n",
       "      <td>38.421900</td>\n",
       "      <td>1.09695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377179</td>\n",
       "      <td>10352.200</td>\n",
       "      <td>21.06270</td>\n",
       "      <td>1.84351</td>\n",
       "      <td>0.251895</td>\n",
       "      <td>4.440570</td>\n",
       "      <td>1.90309</td>\n",
       "      <td>0.248534</td>\n",
       "      <td>0.863881</td>\n",
       "      <td>11.79390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250002</th>\n",
       "      <td>0.919671</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.057382</td>\n",
       "      <td>0.901419</td>\n",
       "      <td>11961.2000</td>\n",
       "      <td>16.39650</td>\n",
       "      <td>273.24000</td>\n",
       "      <td>-0.003300</td>\n",
       "      <td>37.940000</td>\n",
       "      <td>1.15222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990140</td>\n",
       "      <td>3224.020</td>\n",
       "      <td>-2.25287</td>\n",
       "      <td>1.55100</td>\n",
       "      <td>-0.559157</td>\n",
       "      <td>17.838600</td>\n",
       "      <td>1.83385</td>\n",
       "      <td>0.931796</td>\n",
       "      <td>2.336870</td>\n",
       "      <td>9.05400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250003</th>\n",
       "      <td>0.860985</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.549509</td>\n",
       "      <td>0.471799</td>\n",
       "      <td>7501.6000</td>\n",
       "      <td>2.80698</td>\n",
       "      <td>71.08170</td>\n",
       "      <td>0.792136</td>\n",
       "      <td>0.395235</td>\n",
       "      <td>1.20157</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396880</td>\n",
       "      <td>9689.760</td>\n",
       "      <td>14.77150</td>\n",
       "      <td>1.41390</td>\n",
       "      <td>0.329272</td>\n",
       "      <td>0.802437</td>\n",
       "      <td>2.23251</td>\n",
       "      <td>0.893348</td>\n",
       "      <td>1.359470</td>\n",
       "      <td>4.84833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250004</th>\n",
       "      <td>0.313229</td>\n",
       "      <td>89</td>\n",
       "      <td>0.588509</td>\n",
       "      <td>0.167705</td>\n",
       "      <td>2931.2600</td>\n",
       "      <td>4.34986</td>\n",
       "      <td>1.57187</td>\n",
       "      <td>1.118300</td>\n",
       "      <td>7.754630</td>\n",
       "      <td>1.16807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862502</td>\n",
       "      <td>2693.350</td>\n",
       "      <td>44.18050</td>\n",
       "      <td>1.58020</td>\n",
       "      <td>-0.191021</td>\n",
       "      <td>26.253000</td>\n",
       "      <td>2.68238</td>\n",
       "      <td>0.361923</td>\n",
       "      <td>1.532800</td>\n",
       "      <td>3.70660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              f0   f1        f2        f3          f4        f5         f6  \\\n",
       "id                                                                           \n",
       "250000  0.812665   15 -1.239120 -0.893251    295.5770  15.87120   23.04360   \n",
       "250001  0.190344  131 -0.501361  0.801921     64.8866   3.09703  344.80500   \n",
       "250002  0.919671   19 -0.057382  0.901419  11961.2000  16.39650  273.24000   \n",
       "250003  0.860985   19 -0.549509  0.471799   7501.6000   2.80698   71.08170   \n",
       "250004  0.313229   89  0.588509  0.167705   2931.2600   4.34986    1.57187   \n",
       "\n",
       "              f7         f8       f9  ...       f90        f91       f92  \\\n",
       "id                                    ...                                  \n",
       "250000  0.942256  29.898000  1.11394  ...  0.446389   -422.332  -1.44630   \n",
       "250001  0.807194  38.421900  1.09695  ...  0.377179  10352.200  21.06270   \n",
       "250002 -0.003300  37.940000  1.15222  ...  0.990140   3224.020  -2.25287   \n",
       "250003  0.792136   0.395235  1.20157  ...  1.396880   9689.760  14.77150   \n",
       "250004  1.118300   7.754630  1.16807  ...  0.862502   2693.350  44.18050   \n",
       "\n",
       "            f93       f94        f95      f96       f97       f98       f99  \n",
       "id                                                                           \n",
       "250000  1.69075  1.059300  -3.010570  1.94664  0.529470  1.386950   8.78767  \n",
       "250001  1.84351  0.251895   4.440570  1.90309  0.248534  0.863881  11.79390  \n",
       "250002  1.55100 -0.559157  17.838600  1.83385  0.931796  2.336870   9.05400  \n",
       "250003  1.41390  0.329272   0.802437  2.23251  0.893348  1.359470   4.84833  \n",
       "250004  1.58020 -0.191021  26.253000  2.68238  0.361923  1.532800   3.70660  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1ec74e4-ccb8-43b4-b910-3df1542aaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df[features] # this is just for naming consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725cd3e-f883-4d20-837a-9f557b2122a9",
   "metadata": {},
   "source": [
    "Now, let's get the features the model was trained on and subset the test set's features accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8929a3a1-56ca-4f20-a44f-e3877c6dabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:48:45] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"test_size\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1522, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=400, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             test_size=0.2, tree_method='auto', validate_parameters=1,\n",
       "             verbosity=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying hold-out before scaling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=config['test_size'], \n",
    "                                                      random_state=config['random_state']\n",
    "                                                     )\n",
    "# scaling (i.e. normalizing)\n",
    "scaler = config['scaler']()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.fit_transform(X_test)\n",
    "\n",
    "# selecting features\n",
    "selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "                                      k=config['k_best'])\n",
    "X_train_fs = selector.fit_transform(X_train_s, y_train)\n",
    "X_test_fs = X_test_s[:, selector.get_support()]\n",
    "\n",
    "model = XGBRegressor(\n",
    "    tree_method=config['tree_method'],\n",
    "    booster=config['booster'],\n",
    "    n_estimators=config['n_estimators'], \n",
    "    max_depth=config['max_depth'],\n",
    "    learning_rate=config['learning_rate'], \n",
    "    test_size=config['test_size'],\n",
    "    subsample=config['subsample'],\n",
    "    random_state=config['random_state'],\n",
    "    n_jobs=config['n_jobs'], \n",
    "    verbosity=config['verbosity'], \n",
    ")\n",
    "#     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "model.fit(X_train_fs, y_train)#, callbacks=[wandb.xgboost.wandb_callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd935b52-5b2c-4db4-8704-34d61a0e2013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_test_preds = model.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a464a1c-9ca8-4a07-9cdb-18af399cf95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9d2f4b8-2356-4916-a091-45793db784ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'loss'] = y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c957ce26-bbf5-4aee-bccd-988f2471db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250000</td>\n",
       "      <td>8.235917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250001</td>\n",
       "      <td>4.625789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250002</td>\n",
       "      <td>7.081776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250003</td>\n",
       "      <td>6.641549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250004</td>\n",
       "      <td>7.322997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      loss\n",
       "0  250000  8.235917\n",
       "1  250001  4.625789\n",
       "2  250002  7.081776\n",
       "3  250003  6.641549\n",
       "4  250004  7.322997"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4cc6d50-92bc-4295-9acc-5d345eb96755",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('202108241140_XGBoost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06093b1-d26d-4388-bac8-343363168c7c",
   "metadata": {},
   "source": [
    "This got 7.90537 on the LB -- worse than before feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67df4d3-54f8-43fe-9f14-d3751986a58d",
   "metadata": {},
   "source": [
    "# Experiment - fitting model on full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8331118-e7a3-4578-8e43-cf93067c6b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:15] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"test_size\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1522, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=400, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             test_size=0.2, tree_method='auto', validate_parameters=1,\n",
       "             verbosity=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying hold-out before scaling\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                       test_size=config['test_size'], \n",
    "#                                                       random_state=config['random_state']\n",
    "#                                                      )\n",
    "# scaling (i.e. normalizing)\n",
    "scaler = config['scaler']()\n",
    "X_s = scaler.fit_transform(X)\n",
    "X_test_s = scaler.fit_transform(X_test)\n",
    "\n",
    "# selecting features\n",
    "selector = config['feature_selector'](score_func=config[\"feature_selection_scoring\"], \n",
    "                                      k=config['k_best'])\n",
    "X_fs = selector.fit_transform(X_s, y)\n",
    "X_test_fs = X_test_s[:, selector.get_support()]\n",
    "\n",
    "model = XGBRegressor(\n",
    "    tree_method=config['tree_method'],\n",
    "    booster=config['booster'],\n",
    "    n_estimators=config['n_estimators'], \n",
    "    max_depth=config['max_depth'],\n",
    "    learning_rate=config['learning_rate'], \n",
    "    test_size=config['test_size'],\n",
    "    subsample=config['subsample'],\n",
    "    random_state=config['random_state'],\n",
    "    n_jobs=config['n_jobs'], \n",
    "    verbosity=config['verbosity'], \n",
    ")\n",
    "#     wandb.log({'params': model.get_params()}) # logging model parameters\n",
    "model.fit(X_fs, y)#, callbacks=[wandb.xgboost.wandb_callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d90ba24b-75cd-4c2d-a2cf-fe7bf16b3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_test_preds = model.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7db93b42-8460-4793-bd0f-60ddb1d7e84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bfc7e54-043d-4abb-818e-503846c0f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'loss'] = y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50b58c49-59e9-4751-8373-98536c9a121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250000</td>\n",
       "      <td>8.027956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250001</td>\n",
       "      <td>4.305676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250002</td>\n",
       "      <td>7.300106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250003</td>\n",
       "      <td>6.988875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250004</td>\n",
       "      <td>7.316631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      loss\n",
       "0  250000  8.027956\n",
       "1  250001  4.305676\n",
       "2  250002  7.300106\n",
       "3  250003  6.988875\n",
       "4  250004  7.316631"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41d0f44a-2ca7-486f-9197-48534a35043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('202108241211_XGBoost_fullset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
