{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "sweep_lightgbm_20210922.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53"
      },
      "source": [
        "# LightGBM Hyperparameter Sweep 20210922\n",
        "Integrating some enhancements introduced in the XGBoost version, and implementing the frequent-serialization approach."
      ],
      "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_qtimPUchWD"
      },
      "source": [
        ""
      ],
      "id": "U_qtimPUchWD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4"
      },
      "source": [
        "# Setup"
      ],
      "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dae2ff1e-bd1f-4cc9-8357-5a88d1746ffb"
      },
      "source": [
        "# two manual flags (ex-config)\n",
        "colab = True\n",
        "gpu_available = False"
      ],
      "id": "dae2ff1e-bd1f-4cc9-8357-5a88d1746ffb",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16849bd2-428c-497b-ba3b-675002f8d041"
      },
      "source": [
        "# basic imports\n",
        "from pathlib import Path\n",
        "import os\n",
        "import math\n",
        "from datetime import datetime\n",
        "import random\n",
        "import multiprocessing\n",
        "import pickle"
      ],
      "id": "16849bd2-428c-497b-ba3b-675002f8d041",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12d2654b-3bc6-49b5-ade8-cc82112b60e5",
        "outputId": "6bd53922-c4d7-43ce-c04f-ac1079087966"
      },
      "source": [
        "%matplotlib inline\n",
        "%config Completer.use_jedi = False\n",
        "os.environ['WANDB_NOTEBOOK_NAME'] = f\"sweep_lightgbm_{datetime.now().strftime('%Y%m%d')}.ipynb\""
      ],
      "id": "12d2654b-3bc6-49b5-ade8-cc82112b60e5",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "416d6118-e543-4df4-9219-2d4a63743c3a",
        "outputId": "5483656e-2943-4d97-b5d4-65cfb9795430"
      },
      "source": [
        "# handle Google Colab-specific library installation/updating\n",
        "if colab:\n",
        "    # much of the below inspired by or cribbed from the May 2021 Kaggle Tabular Playground winner, at \n",
        "    # https://colab.research.google.com/gist/academicsuspect/0aac7bd6e506f5f70295bfc9a3dc2250/tabular-may-baseline.ipynb?authuser=1#scrollTo=LJoVKJb5wN0L\n",
        "    \n",
        "    # Kaggle API for downloading the datasets\n",
        "    !pip install --upgrade -q kaggle\n",
        "\n",
        "    # weights and biases\n",
        "    !pip install -qqqU wandb\n",
        "    \n",
        "    # Optuna for parameter search\n",
        "    !pip install -q optuna\n",
        "\n",
        "    # !pip install --upgrade xgboost\n",
        "\n",
        "    # upgrade sklearn\n",
        "    !pip install --upgrade scikit-learn\n",
        "\n",
        "    # !pip install category_encoders\n",
        "    # !pip install catboost\n",
        "    !pip install --upgrade -q lightgbm\n",
        "\n",
        "    # lighgbm gpu compatible\n",
        "    # !git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "    # ! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;\n",
        "    \n",
        "    # # this part is from https://github.com/rapidsai/gputreeshap/issues/24\n",
        "    # !pip install cmake --upgrade\n",
        "    # # !pip install sklearn --upgrade\n",
        "    # !git clone --recursive https://github.com/dmlc/xgboost\n",
        "    # %cd /content/xgboost\n",
        "    # !mkdir build\n",
        "    # %cd build\n",
        "    # !cmake .. -DUSE_CUDA=ON\n",
        "    # !make -j4\n",
        "    # %cd /content/xgboost/python-package\n",
        "    # !python setup.py install --use-cuda --use-nccl\n",
        "    # !/opt/bin/nvidia-smi\n",
        "    # !pip install shap\n",
        "    "
      ],
      "id": "416d6118-e543-4df4-9219-2d4a63743c3a",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 28.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 139 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 58.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 302 kB 31.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 208 kB 59.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 57.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.2.0\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 33.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d40df194-4474-4bcf-ac5a-98efe24b91fd"
      },
      "source": [
        "Now, non-stdlib imports"
      ],
      "id": "d40df194-4474-4bcf-ac5a-98efe24b91fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a01e85f7-d602-4dde-bef9-611683cd74c4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# general ML tooling\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "import wandb\n",
        "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
        "# from wandb.xgboost import wandb_callback\n",
        "# from wandb.lightgbm import wandb_callback\n",
        "# from sklearn.impute import KNNImputer, StandardImputer\n",
        "# import timm\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# from catboost import CatBoostClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
        "# from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from joblib import dump, load\n",
        "# feature engineering tools\n",
        "# from sklearn.feature_selection import mutual_info_regression\n",
        "# import featuretools as ft\n",
        "\n",
        "\n",
        "from optuna.samplers import TPESampler\n",
        "import optuna\n",
        "# import catboost\n",
        "from sklearn.utils import resample\n",
        "import sklearn.metrics"
      ],
      "id": "a01e85f7-d602-4dde-bef9-611683cd74c4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f6166c2-ca44-4b7c-a4dc-3db47c2624fe"
      },
      "source": [
        "Now, datapath setup"
      ],
      "id": "5f6166c2-ca44-4b7c-a4dc-3db47c2624fe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c18a787-2193-43cb-87ee-51c6ae7b6351"
      },
      "source": [
        "# # This is the code for reading the train.csv and converting it to a .feather file\n",
        "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
        "# df.index.name = None\n",
        "# df.to_feather(path='./dataset_df.feather')"
      ],
      "id": "3c18a787-2193-43cb-87ee-51c6ae7b6351",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a41cd7e-accb-41c4-ad8b-0eaa3e2b0ad5",
        "outputId": "76d62b41-4171-40fa-936c-4481a9fdab36"
      },
      "source": [
        "if colab:\n",
        "    # mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # handling datapath\n",
        "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/sep2021/')\n",
        "    \n",
        "else:\n",
        "    # if on local machine\n",
        "    datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')    \n",
        "    \n"
      ],
      "id": "0a41cd7e-accb-41c4-ad8b-0eaa3e2b0ad5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1c652e6-5946-46aa-a13e-4c0ebe8a0e4f"
      },
      "source": [
        "\n",
        "# n_trials = int(1000)\n",
        "SEED = 42"
      ],
      "id": "d1c652e6-5946-46aa-a13e-4c0ebe8a0e4f",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbec2e77-2081-4815-ac6d-39f2a2616386"
      },
      "source": [
        "# Function to seed everything\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "seed_everything(SEED)"
      ],
      "id": "fbec2e77-2081-4815-ac6d-39f2a2616386",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f6e47b3-43bd-4d35-b463-9d76100c6ed5"
      },
      "source": [
        "## Ex-Model Config"
      ],
      "id": "2f6e47b3-43bd-4d35-b463-9d76100c6ed5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb288275-a858-4806-9dc0-0b316c334536"
      },
      "source": [
        "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
        "# in the sweep version, this includes both ex-model parameters and defaults for model parameters\n",
        "exmodel_config = {\n",
        "    # model config\n",
        "    \"library\": 'catboost',\n",
        "#     \"model\": XGBClassifier,\n",
        "#     \"n_estimators\": 100, \n",
        "#     \"max_depth\": 3,\n",
        "#     \"learning_rate\": 0.1,\n",
        "#     \"test_size\": 0.2,\n",
        "#     \"reg_lambda\": None, \n",
        "    \"scaler\": \"sklearn.preprocessing.StandardScaler()\", # TODO: experiment with others (but imputation may be slow)\n",
        "    \"scale_b4_impute\": False,\n",
        "    \"imputer\": \"sklearn.impute.SimpleImputer(strategy='median', add_indicator=True)\",\n",
        "    \"knn_imputer_n_neighbors\": None, # None if a different imputer is used\n",
        "#     \"feature_selector\": SelectKBest,\n",
        "#     \"k_best\": 80,\n",
        "#     \"feature_selection_scoring\": f_regression,\n",
        "    'random_state': SEED,\n",
        "    'optuna': True,\n",
        "    'optuna_trials': 100,\n",
        "#     'subsample': 1,\n",
        "#     'cross_val_strategy': None, # None for holdout, or the relevant sklearn class\n",
        "#     'kfolds': 1, # if 1, that means just doing holdout\n",
        "#     'test_size': 0.2,\n",
        "    # these are XGBoost default (my choice) params \n",
        "#     \"tree_method\": \"auto\", # set to 'gpu_hist' to try GPU if available\n",
        "#     \"booster\": 'gbtree', # dart may be marginally better, but will opt for this quicker approach as a default\n",
        "#     \"n_estimators\": 200, \n",
        "#     \"max_depth\": 3,\n",
        "#     \"learning_rate\": 0.1,\n",
        "#     \"n_jobs\": -1,\n",
        "#     \"verbosity\": 1,\n",
        "#     \"subsample\": 1,\n",
        "#     'features_created': False,\n",
        "#     'feature_creator': None,\n",
        "}\n",
        "\n",
        "wandb_kwargs = {\n",
        "    # wandb config\n",
        "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
        "    'project': '202109_Kaggle_tabular_playground',\n",
        "    'tags': ['sweep'],\n",
        "    'notes': \"Integrating some enhancements introduced in the XGBoost version, and implementing the frequent-serialization approach.\",\n",
        "    'config': exmodel_config,\n",
        "}"
      ],
      "id": "fb288275-a858-4806-9dc0-0b316c334536",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a52d9012-34f1-435a-ba16-4416e0d4a286"
      },
      "source": [
        "## Data Setup"
      ],
      "id": "a52d9012-34f1-435a-ba16-4416e0d4a286"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c912a62f-970a-48b4-b428-d886f2612fc2"
      },
      "source": [
        "**TODO** Write some conditional logic here to automate it -- possibly as part of a sklearn.*pipeline"
      ],
      "id": "c912a62f-970a-48b4-b428-d886f2612fc2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f6625d6-7561-48a2-acb0-6efb3c371005"
      },
      "source": [
        "X_source = 'X_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather'\n",
        "X_train = pd.read_feather(datapath/X_source) \n",
        "y_train = load(datapath/'y.joblib')    \n",
        "# X.index.name = 'id'\n",
        "# y.index.name = 'id'\n",
        "X = np.array(X_train)\n",
        "y = np.array(y_train)\n",
        "\n",
        "del X_train, y_train"
      ],
      "id": "7f6625d6-7561-48a2-acb0-6efb3c371005",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8330d84d-3c72-4cca-b95a-739712014dd3"
      },
      "source": [
        "exmodel_config['feature_count'] = X.shape[1]\n",
        "exmodel_config['feature_generator'] = \"Summary statistics\"\n",
        "exmodel_config['X_source'] = X_source"
      ],
      "id": "8330d84d-3c72-4cca-b95a-739712014dd3",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "431d37db-558d-474d-9eca-ce2d38b7636f"
      },
      "source": [
        "# Experiment setup"
      ],
      "id": "431d37db-558d-474d-9eca-ce2d38b7636f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ff4abf-560b-450e-a7a5-040878b66565"
      },
      "source": [
        "# wandb_kwargs = {\n",
        "#     # wandb config:\n",
        "#     'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
        "#     'project': '202109_Kaggle_tabular_playground',\n",
        "#     'tags': ['sweep'],\n",
        "#     'notes': \"Sweep for CatBoost using Optuna\",\n",
        "#     'config': exmodel_config,\n",
        "# }"
      ],
      "id": "69ff4abf-560b-450e-a7a5-040878b66565",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d93b6f2-2d65-48a9-9862-510bd7d2b75b"
      },
      "source": [
        "# originally from https://www.kaggle.com/satorushibata/optimize-catboost-hyperparameter-with-optuna-gpu\n",
        "def objective(trial):\n",
        "    # split the (original Kaggle training) data into partitions\n",
        "    # if study.best_trial:\n",
        "    #     print(\"Dumping best params, which are:\")\n",
        "    #     print(str(study.best_trial.params))\n",
        "    #     dump(study.best_trial.params, filename=datapath/'optuna_catboost_best_20210920.joblib')\n",
        "       \n",
        "    # else:\n",
        "    #     print(\"No best study yet\")\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=int(SEED), shuffle=True)\n",
        "    # create wrappers for the training and validation partitions\n",
        "    # train_pool = catboost.Pool(X_train, y_train)\n",
        "    # valid_pool = catboost.Pool(X_valid, y_valid)\n",
        "    \n",
        "    # experimental parameters\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 1200),\n",
        "        'max_depth' : trial.suggest_int('depth', 2, 7),                                       \n",
        "        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.4),               \n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 5),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 7),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
        "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart'])\n",
        "    }  \n",
        "\n",
        "    # instantiate the model, with some parameters locked in, and experimnental ones passed via splat \n",
        "    model = LGBMClassifier(\n",
        "        objective='binary',\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='auc',\n",
        "        device_type='cpu',\n",
        "        max_bin=63,\n",
        "        **params\n",
        "    )       \n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    # generate predictions\n",
        "    preds = model.predict_proba(X_valid)[:,1]\n",
        "    # rounds to the nearest integer, and the nearest even in case of _.5s\n",
        "\n",
        "    # Evaluation\n",
        "    valid_auc = roc_auc_score(y_valid, preds)\n",
        "    print('ROC AUC Score of XGBoost =', valid_auc)\n",
        "    wandb.log({'valid_auc': valid_auc,\n",
        "              })\n",
        "\n",
        "    return valid_auc"
      ],
      "id": "1d93b6f2-2d65-48a9-9862-510bd7d2b75b",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "0e85f589-1507-4b75-80d9-8b062970102f",
        "outputId": "6a01a1a1-8060-429d-9a47-670cbc0435d2"
      },
      "source": [
        "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)"
      ],
      "id": "0e85f589-1507-4b75-80d9-8b062970102f",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: ExperimentalWarning:\n",
            "\n",
            "WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find sweep_lightgbm_20210922.ipynb\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">sweep_lightgbm_20210922_190033</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/w5tgqbsf\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/w5tgqbsf</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210922_190107-w5tgqbsf</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ab6749b1-dd7d-4789-b0e2-8491d78fe89b"
      },
      "source": [
        "# study = optuna.create_study(direction = \"maximize\", \n",
        "#                             sampler = TPESampler(seed=int(SEED)), \n",
        "#                             study_name='lightgbm_20210922')\n",
        "\n",
        "study = load(datapath/f'optuna_lightgbm_study_5trials_20210922.joblib')\n"
      ],
      "id": "ab6749b1-dd7d-4789-b0e2-8491d78fe89b",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1cSVFH9gkW_",
        "outputId": "ccc874e6-7dd4-4e24-bec8-35ae48180b40"
      },
      "source": [
        "\n",
        "for x in range(2,20):\n",
        "    study.optimize(objective, n_trials = 5, n_jobs=-1, callbacks = [wandbc]) #n_jobs = multiprocessing.cpu_count())\n",
        "    print(f\"{x*5} trials complete\")\n",
        "    dump(study, filename=datapath/f'optuna_lightgbm_study_{x*5}trials_20210922.joblib')\n",
        "    dump(study.best_trial.params, filename=datapath/f'optuna_lightgbm_study_best-thru-{x*5}trials_20210922.joblib')"
      ],
      "id": "F1cSVFH9gkW_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:397: FutureWarning:\n",
            "\n",
            "`n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27a746ff-c0e1-4218-8809-f102a58d2491"
      },
      "source": [
        "dump(study, filename=datapath/'optuna_lightgbm_100trials-complete_20210922.joblib')\n",
        "dump(study.best_trial.params, filename=datapath/'optuna_lightgbm_all-100trials-best_20210922.joblib')\n",
        "# pickle.dump(study.best_trial.params, open('CatBoost_Hyperparameter.pickle', 'wb'))\n",
        "# print('CatBoost Hyperparameter:', study.best_trial.params)"
      ],
      "id": "27a746ff-c0e1-4218-8809-f102a58d2491",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f02e689e-b20c-48e5-a7d9-02467b4f3dbd"
      },
      "source": [
        "study.best_trial.params"
      ],
      "id": "f02e689e-b20c-48e5-a7d9-02467b4f3dbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybeYZ3omaLWK"
      },
      "source": [
        ""
      ],
      "id": "ybeYZ3omaLWK",
      "execution_count": null,
      "outputs": []
    }
  ]
}