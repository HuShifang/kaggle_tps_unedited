{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52b26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "# if '/sf/' in pwd:\n",
    "#     COLAB, SAGE = False, False\n",
    "# elif 'google.colab' in str(get_ipython()):\n",
    "#     COLAB, SAGE = True, False # do colab-specific installs later\n",
    "# else:\n",
    "#     COLAB, SAGE = False, True\n",
    "    \n",
    "CONTEXT = 'local' # or 'colab', 'sage', 'kaggle'\n",
    "USE_GPU = True \n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db58d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import requests # for telegram notifications\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0cd2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series\n",
    "import tsfresh\n",
    "\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "# from darts.models import ExponentialSmoothing, AutoARIMA, ARIMA, Prophet, RandomForest, RegressionEnsembleModel, RegressionModel, TFTModel, TCNModel, TransformerModel, NBEATSModel\n",
    "from darts.metrics import smape\n",
    "\n",
    "import holidays\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"nb_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a6aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import (\n",
    "    NaiveSeasonal,\n",
    "    NaiveDrift,\n",
    "#     Prophet, # on 20220108 postponing this due to df vs ts object wrinkles\n",
    "    ExponentialSmoothing,\n",
    "    ARIMA,\n",
    "    AutoARIMA,\n",
    "    RegressionEnsembleModel,\n",
    "    RegressionModel,\n",
    "    Theta,\n",
    "    FFT\n",
    ")\n",
    "\n",
    "from prophet import Prophet # for now, just imporing the native API\n",
    "from neuralprophet import NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f623c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "# import torch\n",
    "# from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "\n",
    "# widedeep\n",
    "# from pytorch_widedeep import Trainer\n",
    "# from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "# from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT#, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "# from pytorch_widedeep.metrics import Accuracy\n",
    "# from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efbd5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTEXT == 'colab':\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    # datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/dec2021/')\n",
    "    root = Path('') # TODO\n",
    "\n",
    "elif CONTEXT == 'sage':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "elif CONTEXT == 'kaggle':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "else: # if on local machine\n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'studies'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath, studypath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b4fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything but the models\n",
    "def seed_everything(seed, pytorch=True, reproducible=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if pytorch:\n",
    "        torch.manual_seed(seed) # set torch CPU seed\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed) # set torch GPU(s) seed(s)\n",
    "        if reproducible and torch.backends.cudnn.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f7fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Function to reduce memory usage by downcasting datatypes in a Pandas DataFrame when possible.\n",
    "    \n",
    "    h/t to Bryan Arnold (https://www.kaggle.com/puremath86/label-correction-experiments-tps-nov-21)\n",
    "    \"\"\"\n",
    "    \n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c50666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_api_token = 'your_api_token' # for Galileo (jupyter_watcher_bot) on Telegram\n",
    "tg_chat_id = 'your_chat_id'\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_tg_message(text='Cell execution completed.'):  \n",
    "    \"\"\"\n",
    "    h/t Ivan Dembicki Jr. for the base version \n",
    "    (https://medium.com/@ivan.dembicki.jr/notifications-in-jupyter-notebook-with-telegram-f2e892c55173)\n",
    "    \"\"\"\n",
    "    requests.post('https://api.telegram.org/' +  'bot{}/sendMessage'.format(tg_api_token),\n",
    "                  params=dict(chat_id=tg_chat_id, text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a9c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(y_true, y_pred):\n",
    "    '''\n",
    "    h/t Jean-FranÃ§ois Puget (@CPMP) -- see https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\n",
    "    '''\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b0e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'train.csv'),\n",
    "#     'target_source': str(datapath/'train.csv'),\n",
    "    'test_source': str(datapath/'test.csv'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "df_train = pd.read_csv(datapath/'train.csv')\n",
    "df_test = pd.read_csv(datapath/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b0dcfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "\n",
    "countries = ['Sweden', 'Finland', 'Norway']\n",
    "stores = ['KaggleMart', 'KaggleRama']\n",
    "products = ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e78f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if date is a holiday    \n",
    "def isHoliday(country, date):\n",
    "    # h/t @sumeetbohra for following (https://www.kaggle.com/sumeetbohra/eda-dataviz-fe-lightgbm)\n",
    "    country_holidays = holidays.CountryHoliday(country, years = date.year)\n",
    "    return int(date in country_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8761f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['isHoliday'] = df_train.apply(lambda x: isHoliday(x['country'], x['date'].date()), axis = 1)\n",
    "# df_test['isHoliday'] = df_test.apply(lambda x: isHoliday(x['country'], x['date'].date()), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e2aa703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norway_2018 = holidays.CountryHoliday('Norway', years=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "598c6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norway_2018.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3d1344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in native Prophet API, holidays must be a DataFrame with \"ds\" and \"holiday\" columns.\n",
    "# holidays_train = pd.DataFrame({\n",
    "#     'ds': df_train['date'],\n",
    "#     'holiday': df_train['isHoliday']\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1e219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id       date  country       store         product  num_sold\n",
      "26278   26278 2018-12-30   Sweden  KaggleRama      Kaggle Hat      2138\n",
      "26279   26279 2018-12-30   Sweden  KaggleRama  Kaggle Sticker       587\n",
      "26280   26280 2018-12-31  Finland  KaggleMart      Kaggle Mug       469\n",
      "26281   26281 2018-12-31  Finland  KaggleMart      Kaggle Hat       822\n",
      "26282   26282 2018-12-31  Finland  KaggleMart  Kaggle Sticker       238\n",
      "26283   26283 2018-12-31  Finland  KaggleRama      Kaggle Mug       831\n",
      "26284   26284 2018-12-31  Finland  KaggleRama      Kaggle Hat      1231\n",
      "26285   26285 2018-12-31  Finland  KaggleRama  Kaggle Sticker       360\n",
      "26286   26286 2018-12-31   Norway  KaggleMart      Kaggle Mug       728\n",
      "26287   26287 2018-12-31   Norway  KaggleMart      Kaggle Hat      1124\n",
      "26288   26288 2018-12-31   Norway  KaggleMart  Kaggle Sticker       351\n",
      "26289   26289 2018-12-31   Norway  KaggleRama      Kaggle Mug      1383\n",
      "26290   26290 2018-12-31   Norway  KaggleRama      Kaggle Hat      2128\n",
      "26291   26291 2018-12-31   Norway  KaggleRama  Kaggle Sticker       561\n",
      "26292   26292 2018-12-31   Sweden  KaggleMart      Kaggle Mug       570\n",
      "26293   26293 2018-12-31   Sweden  KaggleMart      Kaggle Hat       823\n",
      "26294   26294 2018-12-31   Sweden  KaggleMart  Kaggle Sticker       250\n",
      "26295   26295 2018-12-31   Sweden  KaggleRama      Kaggle Mug      1004\n",
      "26296   26296 2018-12-31   Sweden  KaggleRama      Kaggle Hat      1441\n",
      "26297   26297 2018-12-31   Sweden  KaggleRama  Kaggle Sticker       388"
     ]
    }
   ],
   "source": [
    "df_train.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31715e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'general_random_state': SEED,\n",
    "    'model_goal': 'forecasting', # or 'residual', in boosted hybrids\n",
    "#     'cross_validation_type': 'holdout',\n",
    "#     'validation_set_size': 0.25,\n",
    "}\n",
    "\n",
    "# following are only applicable for residual models in time series context\n",
    "# folds = 5\n",
    "# training_params['cross_val_strategy'] = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "656ccc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will add kwargs later\n",
    "model_params = {\n",
    "    'architecture': 'Prophet',\n",
    "    'library': 'prophet',\n",
    "    'hyperparams': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5c09076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['baseline', 'forecasting', 'prophet'],\n",
    "    'notes': \"Baseline run for Facebook Prophet via native API as forecaster, following @gunesevitan's notebook.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "647293a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [\n",
    "    ('2015-01-01', '2018-01-01'),\n",
    "    ('2018-01-01', '2019-01-01'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "446839e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "#     'holidays':holidays_train, # will add this in-function\n",
    "    'n_changepoints':10,\n",
    "    'changepoint_range':0.4,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_prior_scale':25,\n",
    "    'holidays_prior_scale':100,\n",
    "    'changepoint_prior_scale':0.01,\n",
    "    'interval_width':0.5,\n",
    "    'uncertainty_samples':False\n",
    "}\n",
    "\n",
    "model_params['hyperparams'] = str(prophet_kwargs)\n",
    "model_params['holiday_source'] = 'Prophet builtin for each country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4788d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline -- alter as needed later\n",
    "exmodel_config = {\n",
    "    **dataset_params,\n",
    "    **training_params,\n",
    "    **model_params \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32b5599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_trainer(prophet_kwargs=prophet_kwargs, countries=countries, stores=stores, products=products, folds=folds, \n",
    "                    df_train=df_train, df_test=df_test, wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "\n",
    "                    model = Prophet(**prophet_kwargs)\n",
    "#                                         growth='linear',\n",
    "#                     #                     holidays=holidays_train,\n",
    "#                                         n_changepoints=10,\n",
    "#                                         changepoint_range=0.4,\n",
    "#                                         yearly_seasonality=True,\n",
    "#                                         weekly_seasonality=True,\n",
    "#                                         daily_seasonality=False,\n",
    "#                                         seasonality_mode='additive',\n",
    "#                                         seasonality_prior_scale=25,\n",
    "#                                         holidays_prior_scale=100,\n",
    "#                                         changepoint_prior_scale=0.01,\n",
    "#                                         interval_width=0.5,\n",
    "#                                         uncertainty_samples=False\n",
    "#                                     )\n",
    "                    model.add_country_holidays(country_name=country) # uses FacebookProphet API to add holidays\n",
    "                    model.fit(train)\n",
    "        \n",
    "                    train_predictions = model.predict(train[['ds']])['yhat']\n",
    "                    val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    df_train.loc[train_idx, 'prophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log(f\"{(country,store,product)}_valid_smape\": val_score)\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test_predictions = model.predict(test[['ds']])['yhat']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'prophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train, df_test, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c5b3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_trainer(prophet_kwargs=prophet_kwargs, countries=countries, stores=stores, products=products, folds=folds, \n",
    "                    df_train=df_train, df_test=df_test, wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "\n",
    "                    model = Prophet(**prophet_kwargs)\n",
    "#                                         growth='linear',\n",
    "#                     #                     holidays=holidays_train,\n",
    "#                                         n_changepoints=10,\n",
    "#                                         changepoint_range=0.4,\n",
    "#                                         yearly_seasonality=True,\n",
    "#                                         weekly_seasonality=True,\n",
    "#                                         daily_seasonality=False,\n",
    "#                                         seasonality_mode='additive',\n",
    "#                                         seasonality_prior_scale=25,\n",
    "#                                         holidays_prior_scale=100,\n",
    "#                                         changepoint_prior_scale=0.01,\n",
    "#                                         interval_width=0.5,\n",
    "#                                         uncertainty_samples=False\n",
    "#                                     )\n",
    "                    model.add_country_holidays(country_name=country) # uses FacebookProphet API to add holidays\n",
    "                    model.fit(train)\n",
    "        \n",
    "                    train_predictions = model.predict(train[['ds']])['yhat']\n",
    "                    val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    df_train.loc[train_idx, 'prophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score}\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test_predictions = model.predict(test[['ds']])['yhat']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'prophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train, df_test, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5ac7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_trainer(prophet_kwargs=prophet_kwargs, countries=countries, stores=stores, products=products, folds=folds, \n",
    "                    df_train=df_train, df_test=df_test, wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n",
    "\n",
    "                    model = Prophet(**prophet_kwargs)\n",
    "#                                         growth='linear',\n",
    "#                     #                     holidays=holidays_train,\n",
    "#                                         n_changepoints=10,\n",
    "#                                         changepoint_range=0.4,\n",
    "#                                         yearly_seasonality=True,\n",
    "#                                         weekly_seasonality=True,\n",
    "#                                         daily_seasonality=False,\n",
    "#                                         seasonality_mode='additive',\n",
    "#                                         seasonality_prior_scale=25,\n",
    "#                                         holidays_prior_scale=100,\n",
    "#                                         changepoint_prior_scale=0.01,\n",
    "#                                         interval_width=0.5,\n",
    "#                                         uncertainty_samples=False\n",
    "#                                     )\n",
    "                    model.add_country_holidays(country_name=country) # uses FacebookProphet API to add holidays\n",
    "                    model.fit(train)\n",
    "        \n",
    "                    train_predictions = model.predict(train[['ds']])['yhat']\n",
    "                    val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    df_train.loc[train_idx, 'prophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test_predictions = model.predict(test[['ds']])['yhat']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'prophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train, df_test, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a630dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_prophet, df_test_prophet = exdarts_trainer(model)\n",
    "df_train_prophet, df_test_prophet, train_smape_prophet, val_smape_prophet = prophet_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebcddc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/202201_Kaggle_tabular_playground/runs/3nbabao5\" target=\"_blank\">nb_20220108_102629</a></strong> to <a href=\"https://wandb.ai/hushifang/202201_Kaggle_tabular_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_train_prophet, df_test_prophet = exdarts_trainer(model)\n",
    "df_train_prophet, df_test_prophet, train_smape_prophet, val_smape_prophet = prophet_trainer(wandb_tracked=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
