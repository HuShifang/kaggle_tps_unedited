{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# GBM Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7c0bc-ccc7-4454-816a-c1f35bfd6f07",
   "metadata": {},
   "source": [
    "20211202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9149871d-3242-4bb9-81b1-61c0a76a656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "COLAB = False # will trigger manual installation of packages\n",
    "SAGE = True # if notebook will be used on Amazon SageMaker\n",
    "USE_GPU = False \n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797f54a-c512-46e3-b1f3-d7cea8f35c9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b527dd9-23a5-46d5-949c-f6f2d771390b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.7 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 39.8 MB 46.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 93.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting numpy>=1.14.6\n",
      "  Downloading numpy-1.21.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 59.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=fc83c343fa12fdaa37143ac161916973d1050e897fec8252cbd66ca5f9a0b1c1\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 numpy-1.21.4 scikit-learn-1.0.1 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.0.0\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Collecting psutil>=5.0.0\n",
      "  Downloading psutil-5.8.0-cp39-cp39-manylinux2010_x86_64.whl (293 kB)\n",
      "\u001b[K     |████████████████████████████████| 293 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 30.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[K     |████████████████████████████████| 661 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from wandb) (2.8.2)\n",
      "Collecting requests<3,>=2.0.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting Click!=8.0.0,>=7.0\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 41.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting configparser>=3.8.1\n",
      "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
      "\u001b[K     |████████████████████████████████| 180 kB 39.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.9-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 44.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 40.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor<2.0.0,>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: promise, subprocess32, termcolor, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=3ccb4daca5b63bbc2f296dd5dbcf62b979e11f7ecb2bb07d2accd52451665c2e\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=9928a92e63191fdd4ff7fe6151a06206d48e0f52e6c1829551365576c20e306a\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/5d/f1/ce/3658488c09ec9d6b3037da989642c87575411f113798c90e14\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=6df6a86ad71cb141d37565750cb1b545e42e4ecfedd63b3a149d1ddcb4c9ba59\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=b1b555231453da96d164259331697260ff02319302ef6203e98f1fa02749bd6e\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built promise subprocess32 termcolor pathtools\n",
      "Installing collected packages: smmap, urllib3, typing-extensions, termcolor, idna, gitdb, charset-normalizer, certifi, yaspin, subprocess32, shortuuid, sentry-sdk, requests, PyYAML, psutil, protobuf, promise, pathtools, GitPython, docker-pycreds, configparser, Click, wandb\n",
      "Successfully installed Click-8.0.3 GitPython-3.1.24 PyYAML-6.0 certifi-2021.10.8 charset-normalizer-2.0.9 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 idna-3.3 pathtools-0.1.2 promise-2.3 protobuf-3.19.1 psutil-5.8.0 requests-2.26.0 sentry-sdk-1.5.0 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 termcolor-1.1.0 typing-extensions-4.0.1 urllib3-1.26.7 wandb-0.12.7 yaspin-2.1.0\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.0.3-cp39-none-manylinux1_x86_64.whl (76.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 76.3 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (1.21.4)\n",
      "Requirement already satisfied: scipy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (1.7.3)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2 MB 71.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plotly\n",
      "  Downloading plotly-5.4.0-py2.py3-none-any.whl (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 73.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas>=0.24.0\n",
      "  Downloading pandas-1.3.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 76.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (1.16.0)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.19-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 75.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (21.3)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.3-py3-none-any.whl (884 kB)\n",
      "\u001b[K     |████████████████████████████████| 884 kB 64.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools-scm>=4\n",
      "  Downloading setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (3.0.6)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 65.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 66.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tomli>=1.0.0\n",
      "  Downloading tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib->catboost) (59.2.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tomli, tenacity, setuptools-scm, pytz, pillow, kiwisolver, fonttools, cycler, plotly, pandas, matplotlib, graphviz, catboost\n",
      "Successfully installed catboost-1.0.3 cycler-0.11.0 fonttools-4.28.3 graphviz-0.19 kiwisolver-1.3.2 matplotlib-3.5.0 pandas-1.3.4 pillow-8.4.0 plotly-5.4.0 pytz-2021.3 setuptools-scm-6.3.2 tenacity-8.0.1 tomli-1.2.2\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from lightgbm) (0.37.0)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from lightgbm) (1.21.4)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from lightgbm) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.1\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.1-py3-none-manylinux2014_x86_64.whl (173.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 173.5 MB 33.8 MB/s eta 0:00:011     |███████████████████████████▌    | 149.2 MB 33.8 MB/s eta 0:00:01�█▉ | 167.4 MB 33.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from xgboost) (1.21.4)\n",
      "Requirement already satisfied: scipy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from xgboost) (1.7.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.1\n"
     ]
    }
   ],
   "source": [
    "if SAGE:\n",
    "    !pip install --upgrade sklearn\n",
    "    !pip install --upgrade wandb\n",
    "    !pip install --upgrade catboost\n",
    "    !pip install --upgrade lightgbm\n",
    "    !pip install --upgrade xgboost\n",
    "    !pip install optuna\n",
    "    !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ab5185-9999-40e3-aba9-b4e6a965b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import requests # for telegram notifications\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b614bc-76c5-43a9-a23f-71ca21e6f0d8",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8bbc347-4b2b-428f-8628-07bc0fcf85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model selection\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, log_loss, f1_score, fbeta_score\n",
    "\n",
    "# eda\n",
    "# import missingno\n",
    "# import doubtlab \n",
    "\n",
    "# data cleaning\n",
    "# from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "# import cleanlab\n",
    "\n",
    "# normalization\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\n",
    "# from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "# feature generation\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# import category_encoders as ce\n",
    "\n",
    "# models\n",
    "from catboost import CatBoostClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "\n",
    "# feature reduction\n",
    "# from sklearn.decomposition import PCA\n",
    "# from umap import UMAP\n",
    "\n",
    "# clustering\n",
    "# from sklearn.cluster import DBSCAN, KMeans\n",
    "# import hdbscan\n",
    "\n",
    "# feature selection\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "# import featuretools as ft\n",
    "# from BorutaShap import BorutaShap\n",
    "# from boruta import BorutaPy\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"gbms_{datetime.now().strftime('%Y%m%d')}.ipynb\"\n",
    "\n",
    "# hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "from optuna.samplers import TPESampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0f2234-70b6-48b8-986c-d608f033952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deep learning\n",
    "# import torch\n",
    "# from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "\n",
    "# # widedeep\n",
    "# from pytorch_widedeep import Trainer\n",
    "# from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "# from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT#, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "# from pytorch_widedeep.metrics import Accuracy\n",
    "# from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97eee37-af6d-4e13-bad5-3109805e7780",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bb6a1-5b53-4caf-8143-6a99509dc0e8",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e426c44-807e-4a93-8a80-01b8471af7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # handling datapath\n",
    "    # datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/dec2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "    if SAGE:\n",
    "        root = Path('/home/studio-lab-user/sagemaker-studiolab-notebooks')\n",
    "    else:\n",
    "        root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/dec2021/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'studies'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath, studypath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd240c-0e91-42da-be87-5bde74e957d1",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6744ad23-307b-4470-ad71-8825b35a22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything but the models\n",
    "def seed_everything(seed, pytorch=True, reproducible=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # if pytorch:\n",
    "    #     torch.manual_seed(seed) # set torch CPU seed\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         torch.cuda.manual_seed_all(seed) # set torch GPU(s) seed(s)\n",
    "    #     if reproducible and torch.backends.cudnn.is_available():\n",
    "    #         torch.backends.cudnn.deterministic = True\n",
    "    #         torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef91eec7-92a7-433c-9d12-18f398ff452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Function to reduce memory usage by downcasting datatypes in a Pandas DataFrame when possible.\n",
    "    \n",
    "    h/t to Bryan Arnold (https://www.kaggle.com/puremath86/label-correction-experiments-tps-nov-21)\n",
    "    \"\"\"\n",
    "    \n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d282d62c-db15-44c4-8af7-02a0756ab634",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_api_token = 'your_api_token' # for Galileo (jupyter_watcher_bot) on Telegram\n",
    "tg_chat_id = 'your_chat_id'\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_tg_message(text='Cell execution completed.'):  \n",
    "    \"\"\"\n",
    "    h/t Ivan Dembicki Jr. for the base version \n",
    "    (https://medium.com/@ivan.dembicki.jr/notifications-in-jupyter-notebook-with-telegram-f2e892c55173)\n",
    "    \"\"\"\n",
    "    requests.post('https://api.telegram.org/' +  'bot{}/sendMessage'.format(tg_api_token),\n",
    "                  params=dict(chat_id=tg_chat_id, text=text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7119a9-d04d-43cb-9f17-f60933a4fda3",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd703085-3fb2-4243-a0f7-42376a3ecd2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'X_orig.feather'),\n",
    "    'target_source': str(datapath/'y_orig.joblib'),\n",
    "    'test_source': str(datapath/'X_test_orig.feather'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "# X = load(dataset_params['train_source'])\n",
    "# if SAGE:\n",
    "#     X = load(datapath/'X_orig.joblib')\n",
    "# else:\n",
    "X = pd.read_feather(dataset_params['train_source'])\n",
    "y = load(dataset_params['target_source'])\n",
    "# X_test = load(dataset_params['test_source'])\n",
    "# X_test = pd.read_feather(dataset_params['test_source'])\n",
    "\n",
    "# reduce memory usage\n",
    "# X = reduce_memory_usage(X)\n",
    "# X_test = reduce_memory_usage(X)\n",
    "\n",
    "# metadata logging\n",
    "dataset_params['feature_count'] = X.shape[1]\n",
    "dataset_params['instance_count'] = X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0d397-1a03-4d54-9178-513837cbe00d",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f5f07-b20b-4565-8a24-5147c06aca84",
   "metadata": {},
   "source": [
    "First, going to try some of the basic tweaks suggested [here](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293612)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0443c853-c96f-46a3-9a59-31247dfd8176",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Soil_Type7' 'Soil_Type15'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_96/1218028844.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# remove unuseful features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'Soil_Type7'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Soil_Type15'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# X_test = X_test.drop(['Soil_Type7', 'Soil_Type15'], axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4905\u001b[0m         \"\"\"\n\u001b[0;32m-> 4906\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4150\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4185\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6017\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Soil_Type7' 'Soil_Type15'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# remove unuseful features\n",
    "# X = X.drop([ 'Soil_Type7', 'Soil_Type15'], axis=1)\n",
    "# X_test = X_test.drop(['Soil_Type7', 'Soil_Type15'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "954896c4-9474-4ab1-9e23-fe903bb82903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000000, 52)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8611034a-0720-4ca0-a454-a2164d9d427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra feature engineering\n",
    "def r(x):\n",
    "    if x+180>360:\n",
    "        return x-180\n",
    "    else:\n",
    "        return x+180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac5276dc-1c26-4d3e-95d4-4928ac165da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe(df):\n",
    "    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n",
    "    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n",
    "    df['Aspect2'] = df.Aspect.map(r)\n",
    "    ### source: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373\n",
    "    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n",
    "    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n",
    "    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n",
    "    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n",
    "    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n",
    "    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n",
    "    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n",
    "    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n",
    "    ########\n",
    "    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n",
    "    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n",
    "    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n",
    "    df['Euclidean_Distance_to_Hydrology'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "    df['Manhattan_Distance_to_Hydrology'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n",
    "    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n",
    "    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f2d6241-7a87-4600-a898-3304196bb17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/1105890022.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n",
      "/tmp/ipykernel_96/1105890022.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_96/3379659380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X = fe(X)\n",
    "# X_test = fe(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc2048d9-2a68-481d-9e8e-b6f0b9df1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summed features pointed out by @craigmthomas (https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/292823)\n",
    "soil_features = [x for x in X.columns if x.startswith(\"Soil_Type\")]\n",
    "wilderness_features = [x for x in X.columns if x.startswith(\"Wilderness_Area\")]\n",
    "\n",
    "X[\"soil_type_count\"] = X[soil_features].sum(axis=1)\n",
    "# X_test[\"soil_type_count\"] = X_test[soil_features].sum(axis=1)\n",
    "\n",
    "X[\"wilderness_area_count\"] = X[wilderness_features].sum(axis=1)\n",
    "# X_test[\"wilderness_area_count\"] = X_test[wilderness_features].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ade18003-d8ba-486a-af54-b463bcb6b72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
       "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
       "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
       "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n",
       "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
       "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
       "       'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n",
       "       'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n",
       "       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
       "       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
       "       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
       "       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
       "       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
       "       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'EHiElv',\n",
       "       'EViElv', 'Aspect2', 'Highwater', 'EVDtH', 'EHDtH',\n",
       "       'Euclidean_Distance_to_Hydrology', 'Manhattan_Distance_to_Hydrology',\n",
       "       'Hydro_Fire_1', 'Hydro_Fire_2', 'Hydro_Road_1', 'Hydro_Road_2',\n",
       "       'Fire_Road_1', 'Fire_Road_2', 'Hillshade_3pm_is_zero',\n",
       "       'soil_type_count', 'wilderness_area_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d88cb-84cc-4336-b3f0-954ba4bdbacf",
   "metadata": {},
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6742e7c-022d-4e43-bf6d-fd8a3195d2d7",
   "metadata": {},
   "source": [
    "Initialized above, but now records of feature engineering efforts included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15b5d80b-8c2b-4cea-8029-29a06f8958fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params['feature_count'] = X.shape[1]\n",
    "dataset_params['instance_count'] = X.shape[0]\n",
    "\n",
    "# might eventually shift from dict to tuple\n",
    "\n",
    "# simplest approach: k-v where key is new feature, v is string with the operation to get it\n",
    "# sacrifices sortability, but could recover that through regexes, and it's much quicker to input\n",
    "dataset_params['feature_combinations'] = {\n",
    "    'EHiElv': \"df['Horizontal_Distance_To_Roadways'] * df['Elevation']\",\n",
    "    'EViElv': \"df['Vertical_Distance_To_Hydrology'] * df['Elevation']\",\n",
    "    'EVDtH': \"df.Elevation - df.Vertical_Distance_To_Hydrology\",\n",
    "    'EHDtH': \"df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\",\n",
    "    'Euclidean_Distance_to_Hydrology': \"(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\",\n",
    "    'Manhattan_Distance_to_Hydrology': \"df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\",\n",
    "    'Hydro_Fire_1': \"df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\",\n",
    "    'Hydro_Fire_2': \"abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\",\n",
    "    'Hydro_Road_1': \"abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\",\n",
    "    'Hydro_Road_2': \"abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\",\n",
    "    'Fire_Road_1': \"abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\",\n",
    "    'Fire_Road_2': \"abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\"\n",
    "}\n",
    "\n",
    "dataset_params['feature_clipping'] = [\n",
    "    {\n",
    "        'features': ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm'],\n",
    "        'range': range(0,256)\n",
    "    },\n",
    "    {\n",
    "        'features': ['Aspect'],\n",
    "        'range': range(0,360)\n",
    "    }\n",
    "]\n",
    "\n",
    "# the features that are just getting the one-hots counted\n",
    "dataset_params['feature_counts'] = ['Soil_Type*', 'Wilderness_Area*']\n",
    "dataset_params['feature_complements'] = [\n",
    "    {\n",
    "        'old': 'Aspect', \n",
    "        'new': 'Aspect2',\n",
    "        'operation': 'If x < 180 return x-180, else return x + 180'\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset_params['feature_indicators'] = {\n",
    "    'Hillshade_3pm_is_zero': \"(df.Hillshade_3pm == 0).astype(int)\",\n",
    "}\n",
    "\n",
    "dataset_params['feature_typecasting'] = {\n",
    "    'Highwater': \"(df.Vertical_Distance_To_Hydrology < 0).astype(int)\"\n",
    "}\n",
    "# dataset_params['feature_combinations'] = [\n",
    "#     {\n",
    "#         'old': ['Horizontal_Distance_To_Roadways', 'Elevation'], \n",
    "#         'new': 'EHiElv',\n",
    "#         'operation': '*'\n",
    "#     },\n",
    "#     {\n",
    "#         'old': ('Vertical_Distance_To_Hydrology', 'Elevation'), \n",
    "#         'new': 'EViElv',\n",
    "#         'operation': '*'\n",
    "#     },\n",
    "#     {\n",
    "#         'old': ['Elevation', 'Vertical_Distance_To_Hydrology'],\n",
    "#         'new': 'EVDtH',\n",
    "#         'operation': '-'\n",
    "#     },\n",
    "#     {\n",
    "#         'old': ['Elevation', 'Horizontal_Distance_To_Hydrology'],\n",
    "#         'new': 'EHDtH',\n",
    "#         'operation': 'df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2'\n",
    "#     },\n",
    "    # {\n",
    "    #     'old': ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology'],\n",
    "    #     'new': 'Euclidean_Distance_to_Hydrology',\n",
    "    #     'operation': \"(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\"\n",
    "    # },\n",
    "    # {\n",
    "    #     'old': ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology'],\n",
    "    #     'new': 'Manhattan_Distance_to_Hydrology',\n",
    "    #     'operation': '+'\n",
    "    # },\n",
    "    # {\n",
    "    #     'old': ['Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points'],\n",
    "    #     'new': \n",
    "    \n",
    "# dataset_params['feature_crosses'] = [\n",
    "#     {\n",
    "#         'old': ['Horizontal_Distance_To_Roadways', 'Elevation'], \n",
    "#         'new': 'EHiElv',\n",
    "#         'operation': '*'\n",
    "#     },\n",
    "#     {\n",
    "#         'old': ('Vertical_Distance_To_Hydrology', 'Elevation'), \n",
    "#         'new': 'EViElv',\n",
    "#         'operation': '*'\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# dataset_params['feature_additions'] = [\n",
    "#     {\n",
    "#         'old': ['Elevation', 'Vertical_Distance_To_Hydrology'],\n",
    "#         'new': 'EVDtH',\n",
    "#         'operation': '-'\n",
    "#     }\n",
    "# ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e9478-2380-4c4d-8641-5daa72049b6c",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc156663-c689-4dfe-a80b-5efaaae1afab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # optuna 20211124, with corrected dataset and RobustScaler\n",
    "# best_xgboost_params = {\n",
    "#     'n_estimators': 9872,\n",
    "#     'max_depth': 3,\n",
    "#     'learning_rate': 0.12943882615104757,\n",
    "#     'reg_alpha': 4.793236314677738,\n",
    "#     'reg_lambda': 0.03427038053813167,\n",
    "#     'subsample': 0.5026684329097286,\n",
    "#     'min_child_weight': 3.2374430610042664,\n",
    "#     'colsample_bytree': 0.9875504456465564,\n",
    "#     'gamma': 4.691772640321729\n",
    "# }\n",
    "\n",
    "# # best as of 20211125, with corrected dataset and RobustScaler\n",
    "# best_lightgbm_params = {\n",
    "#     'n_estimators': 6986,\n",
    "#     'max_depth': 3,\n",
    "#     'learning_rate': 0.09080435106650955,\n",
    "#     'reg_alpha': 19.060739534647425,\n",
    "#     'reg_lambda': 0.12865332700612375,\n",
    "#     'subsample': 0.5612404690403716,\n",
    "#     'boosting_type': 'goss',\n",
    "#     'min_child_samples': 17,\n",
    "#     'num_leaves': 59,\n",
    "#     'colsample_bytree': 0.5125554530181221\n",
    "# }\n",
    "\n",
    "# # best as of 20211126, with corrected dataset and RobustScaler\n",
    "# best_catboost_params = {\n",
    "#     'iterations': 17997,\n",
    "#     'depth': 4,\n",
    "#     'learning_rate': 0.05807421036756052,\n",
    "#     'random_strength': 27,\n",
    "#     'od_wait': 1664,\n",
    "#     'reg_lambda': 57.67864249277457,\n",
    "#     'border_count': 275,\n",
    "#     'min_child_samples': 10,\n",
    "#     'leaf_estimation_iterations': 2\n",
    "# }\n",
    "\n",
    "# # # 20211021 lv2 on the K-Means 8-cluster, synth dataset\n",
    "# # lv2_xgboost_params = {\n",
    "# #     'n_estimators': 1534,\n",
    "# #     'max_depth': 4,\n",
    "# #     'learning_rate': 0.0062941159127744535,\n",
    "# #     'reg_alpha': 21.3946930650266,\n",
    "# #     'reg_lambda': 0.021003786013817635,\n",
    "# #     'subsample': 0.5726680367393964,\n",
    "# #     'min_child_weight': 0.07566661785187714,\n",
    "# #     'colsample_bytree': 0.7850419523745037,\n",
    "# #     'gamma': 4.26660233356059\n",
    "# # }\n",
    "\n",
    "# # # 20211021 lv2 on the K-Means 8-cluster, synth dataset\n",
    "# # lv2_lightgbm_params = {\n",
    "# #     'n_estimators': 5776,\n",
    "# #     'max_depth': 4,\n",
    "# #     'learning_rate': 0.0010172282832994653,\n",
    "# #     'reg_alpha': 0.013879765609402173,\n",
    "# #     'reg_lambda': 0.002787031048344079,\n",
    "# #     'subsample': 0.800000753298926,\n",
    "# #     'boosting_type': 'gbdt',\n",
    "# #     'min_child_samples': 11,\n",
    "# #     'num_leaves': 190,\n",
    "# #     'colsample_bytree': 0.9976443570341007\n",
    "# # }\n",
    "\n",
    "# # # 20211021 lv2 on the K-Means 8-cluster, synth dataset\n",
    "# # lv2_catboost_params = {\n",
    "# #     'iterations': 2000,\n",
    "# #     'depth': 6,\n",
    "# #     'learning_rate': 0.002984126581340097,\n",
    "# #     'random_strength': 0,\n",
    "# #     'od_wait': 334,\n",
    "# #     'reg_lambda': 33.469738674488084,\n",
    "# #     'border_count': 158,\n",
    "# #     'min_child_samples': 8,\n",
    "# #     'leaf_estimation_iterations': 4\n",
    "# # }\n",
    "\n",
    "# # # initial, non-default guess -- need to get optuna working (20211010)\n",
    "# # # basic_widedeep_tabmlp_params = {\n",
    "    \n",
    "# # # }\n",
    "\n",
    "# # # basic_widedeep_trainer_params = {\n",
    "# # #     optimizers=AdamW()\n",
    "# # # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2aec7210-135f-402a-af31-330eb0f37276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.basic import LightGBMError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930047e-a9fd-419b-a1a4-0856e3b3ce37",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb78111b-532c-41ce-b18a-aa2a7d868a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'general_random_state': SEED,\n",
    "}\n",
    "\n",
    "folds = 5\n",
    "training_params['cross_val_strategy'] = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52835a6b-bfe2-4152-a2ee-c6d796b7c614",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d69e9346-ec9b-4549-b7d9-9837bcea2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline -- alter as needed later\n",
    "exmodel_config = {\n",
    "#     'general_random_state': SEED,\n",
    "# #     'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "#     'cross_val_strategy': KFold, \n",
    "#     'kfolds': 5, # if 1, that means just doing holdout\n",
    "#     'test_size': 0.2,\n",
    "    **training_params,\n",
    "    **dataset_params\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b463481-20bb-451f-bda8-051992e75e4c",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b4eaf-277f-40a4-a16b-9da08f218556",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Weights and Biases Run Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca3847-48f7-4d3f-8f86-5e6706d3cd64",
   "metadata": {},
   "source": [
    "Below is the configuration for a Weights and Biases (`wandb`) run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5402d98-7bcd-4e46-b85d-6b0129ca6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['study'],\n",
    "    'notes': \"Optuna study of feature-engineered notebook, using new (more model-, infrastructure-agnostic) code\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827289c5-e0e7-4382-9cbd-d6174a5cd68b",
   "metadata": {},
   "source": [
    "# Cross-Validator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb083fa8-eb24-4fb0-b812-1cec9214e6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=5, random_state=42, shuffle=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params['cross_val_strategy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70da4cd9-98d8-4f43-afa5-2467800d8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(arch:str, X=X, y=y, X_test=None, model_params:dict={}, training_params=training_params, dataset_params=dataset_params,\n",
    "                         folds=list(range(folds)), exmodel_config=exmodel_config, wandb_config=wandb_config,  telegram=True, random_state=42, \n",
    "                         wandb_tracked=True, encode_cats=False):\n",
    "    \"\"\"\n",
    "    Function to handle model training process in the context of cross-validation -- via hold-out or via k-fold.\n",
    "    If exmodel_config['cross_val_strategy'] == None, then any kfolds= input is ignored; otherwise, the number specified is used.\n",
    "    \n",
    "    :param kfolds: int specifying number of k-folds to use in cross-validation\n",
    "    :param exmodel_config: dict containing general config including for cross-validation -- `kfold=1` implies hold-out\n",
    "    \"\"\"\n",
    "    # if exmodel_config['kfolds'] == 1: # holdout case\n",
    "    #     print(\"Proceeding with holdout\")\n",
    "    #     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, \n",
    "    #                                                           random_state=SEED)                 \n",
    "    # else: # k-fold cross validation case\n",
    "    #     # prepare for k-fold cross-validation; random-state here is notebook-wide, not per-model\n",
    "    #     # shuffle on the initial sets, but not subsequently -- performing the same operation twice means a very different dataset\n",
    "    #     if shuffle_kfolds:\n",
    "    #         kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=SEED)\n",
    "    #     else:\n",
    "    #         kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=False)\n",
    "    \n",
    "    kfold = training_params['cross_val_strategy']\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202112_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )   \n",
    "    \n",
    "    # initialize lists for out-of-fold preds and ground truth\n",
    "    oof_preds, oof_y = [], []\n",
    "    \n",
    "    # initialize a numpy.ndarray containing the fold-model's preds for test set\n",
    "    \n",
    "    # test_preds = np.zeros((X_test.shape[0]))\n",
    "    # test_probs = np.zeros((X_test.shape[0]))\n",
    "    # preprocessing\n",
    "    # if using a GBM, simply use the RobustScaler\n",
    "        # scaler = RobustScaler()\n",
    "        # X = scaler.fit_transform(X)\n",
    "        # X_test = scaler.transform(X_test)\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "        if fold not in folds: # skip folds that are already trained, i.e. that haven't been specified\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"FOLD {fold}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "            else:\n",
    "                X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:] # bc need pandas.DataFrames for ce\n",
    "                \n",
    "                # scaling\n",
    "                # category_encoding\n",
    "                # if encode_cats:\n",
    "                #     encoder = ce.WOEEncoder(cols=categoricals)\n",
    "                #     encoder.fit(X_train,y_train)\n",
    "                #     X_train = encoder.transform(X_train)\n",
    "                #     X_valid = encoder.transform(X_valid)\n",
    "                # # exmodel_config['feature_count'] = len(X.columns)\n",
    "                #     wandb.log({\n",
    "                #         'feature_count': X_train.shape[1],\n",
    "                #         'instance_count': X_train.shape[0],\n",
    "                #         'encoder': str(encoder)\n",
    "                #     })\n",
    "        \n",
    "        # define models\n",
    "        if arch == 'xgboost':\n",
    "            if USE_GPU:\n",
    "                model = XGBClassifier(\n",
    "                    booster='gbtree',\n",
    "                    tree_method='gpu_hist',\n",
    "                    random_state=random_state,\n",
    "                    n_jobs=-1, \n",
    "                    verbosity=1, \n",
    "                    # objective='binary:logistic',\n",
    "                    objective='multi:softmax',\n",
    "                    **model_params)\n",
    "            else:\n",
    "                model = XGBClassifier(\n",
    "                    booster='gbtree',\n",
    "                    tree_method='hist',\n",
    "                    random_state=random_state,\n",
    "                    n_jobs=-1,\n",
    "                    verbosity=1,\n",
    "                    objective='multi:softmax',\n",
    "                    **model_params)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            \n",
    "            y_valid_preds = model.predict(X_valid)\n",
    "            # y_valid_probs = model.predict_proba(X_valid)\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            # oof_probs.extend(y_valid_probs)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            # test_preds += model.predict(X_test)\n",
    "            # test_probs += model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "        elif arch == 'lightgbm':\n",
    "            if USE_GPU:\n",
    "                model = LGBMClassifier(\n",
    "                    objective='binary',\n",
    "                    random_state=random_state,\n",
    "                    device_type='gpu',\n",
    "                    max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "                    gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "                    **model_params)\n",
    "            else:\n",
    "                model = LGBMClassifier(\n",
    "                    objective='binary',\n",
    "                    random_state=random_state,\n",
    "                    device_type='cpu',\n",
    "                    n_jobs=-1,\n",
    "                    **model_params)\n",
    "\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "#             except LightGBMError:\n",
    "#                 model = LGBMClassifier(\n",
    "#                     objective='binary',\n",
    "#                     random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "#     #                 eval_metric='auc',\n",
    "#     #                 device_type='gpu',\n",
    "#     #                 max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     #                 gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "#                     **params)\n",
    "                \n",
    "#                 if wandb_tracked:\n",
    "#                     model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "#                 else:\n",
    "#                     model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict(X_valid)\n",
    "            # y_valid_probs = model.predict_proba(X_valid)\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            # oof_probs.extend(y_valid_probs)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            # test_preds += model.predict(X_test)\n",
    "            # test_probs += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "            \n",
    "        elif arch == 'catboost':\n",
    "            if USE_GPU:\n",
    "                model = CatBoostClassifier(\n",
    "                    task_type='GPU',\n",
    "                    silent=True,\n",
    "                    random_state=random_state,\n",
    "                    **model_params) \n",
    "            else:\n",
    "                model = CatBoostClassifier(\n",
    "                    task_type='CPU',\n",
    "                    silent=True,\n",
    "                    random_state=random_state,\n",
    "                    **model_params)\n",
    "        \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_valid_preds = model.predict(X_valid)\n",
    "            # y_valid_probs = model.predict_proba(X_valid)[:,1] # this would only take one of 7 cols\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            # oof_probs.extend(y_valid_probs)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            # test_preds += model.predict(X_test).flatten()\n",
    "            # test_probs += model.predict_proba(X_test)[:,1]\n",
    "            \n",
    "#         valid_loss = log_loss(y_valid, y_pred)\n",
    "        # give the valid AUC score, for edification\n",
    "\n",
    "        fold_accuracy = accuracy_score(y_true=y_valid, y_pred=y_valid_preds) # or should be preds?\n",
    "        # fold_confusion = confusion_matrix(y_true=y_valid, y_pred=y_valid_preds)# , labels=list(range(7)))\n",
    "        # fold_log_loss = log_loss(y_pred=y_valid_preds, y_true=y_valid,) #labels=list(range(7)))\n",
    "        # fold_roc_auc = roc_auc_score(y_true=y_valid, y_score=y_valid_probs)\n",
    "        # fold_f1_score = f1_score(\n",
    "        # fold_fbeta_score = fbeta_score(\n",
    "        \n",
    "        if wandb_tracked:\n",
    "            wandb.log({f'fold{fold}_accuracy': fold_accuracy,\n",
    "                       # f'fold{fold}_confusion': fold_confusion,\n",
    "                       # f'fold{fold}_log_loss': fold_log_loss,\n",
    "                       # f'fold{fold}_roc_auc': fold_roc_auc,\n",
    "                      })\n",
    "        fold_human_results = f\"{os.environ['WANDB_NOTEBOOK_NAME']}\\nMetrics for fold {fold} are: \\nAccuracy: {fold_accuracy}\"\n",
    "        print(fold_human_results)\n",
    "        if telegram:\n",
    "            send_tg_message(text=f\"{arch} model's fold {fold} complete.\\n\"+fold_human_results)\n",
    "        # dump(model, Path(runpath/f\"{arch}_fold{fold}_rs{random_state}_model.joblib\"))\n",
    "\n",
    "    model_accuracy = accuracy_score(y_true=oof_y, y_pred=oof_preds) \n",
    "    # model_confusion = confusion_matrix(y_true=oof_y, y_pred=oof_preds, labels=list(range(7)))\n",
    "    # model_log_loss = log_loss(y_pred=oof_preds, y_true=oof_y, labels=list(range(7)))\n",
    "    # model_valid_auc = roc_auc_score(oof_y, oof_preds)\n",
    "    model_human_results = f\"{os.environ['WANDB_NOTEBOOK_NAME']}\\nMetrics for model {arch} are: \\nAccuracy: {model_accuracy}\"\n",
    "    print(model_human_results)\n",
    "    if telegram:\n",
    "        send_tg_message(text=f\"{arch} model run complete.\\n\"+model_human_results)\n",
    "    if wandb_tracked:\n",
    "        wandb.log({f'model_accuracy': fold_accuracy,\n",
    "                   # f'model_confusion': fold_confusion,\n",
    "                   # f'model_log_loss': fold_log_loss,\n",
    "                   # f'model_roc_auc': fold_roc_auc,\n",
    "                   'model_params': str(model.get_params()),\n",
    "                   'model_seed': random_state,\n",
    "                  })\n",
    "        wandb.finish()\n",
    "    \n",
    "    # finalize test preds\n",
    "    # test_probs /= exmodel_config['kfolds']\n",
    "    # test_preds /= exmodel_config['kfolds']\n",
    "    \n",
    "    \n",
    "    # save OOF preds and test-set preds\n",
    "#     if 'widedeep' in arch:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "#     else:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "    # if not (datapath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\").is_file():\n",
    "    #     dump(oof_y, predpath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\")\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         if 'widedeep' in arch:\n",
    "#         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "#                    'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()), \n",
    "#         #                    'model_params': str(model.get_params()),\n",
    "#         })\n",
    "# #         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "# # #                    'model_params': str(model.get_params()),\n",
    "# #                   })\n",
    "#         wandb.finish()\n",
    "    # return oof_preds, test_preds#, model_confusion\n",
    "    return model_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2808c8e-2240-4e76-8e6f-a5115d73ed46",
   "metadata": {},
   "source": [
    "## Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53d18864-92d6-4346-a346-9542b3fd9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'catboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51fe80e5-a547-495e-9b70-d294dc1b9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally from https://www.kaggle.com/satorushibata/optimize-catboost-hyperparameter-with-optuna-gpu\n",
    "def objective(trial, arch=arch):\n",
    "    \"\"\"\n",
    "    Wrapper around cross_validation_trainer to test different model hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if arch == 'catboost':\n",
    "        model_params = {\n",
    "            'iterations' : trial.suggest_int('iterations', 2000, 30000),                         \n",
    "            'depth' : trial.suggest_int('depth', 3, 10),                                       \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.5),               \n",
    "            'random_strength': trial.suggest_int('random_strength', 0, 100), \n",
    "    #         'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n",
    "    #         'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['MVC', 'Bernoulli']),#, 'Poisson']),\n",
    "            'od_wait': trial.suggest_int('od_wait', 20, 2000),\n",
    "            'reg_lambda': trial.suggest_uniform('reg_lambda', 2, 70), # aka l2_leaf_reg\n",
    "            'border_count': trial.suggest_int('border_count', 50, 275),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 20), # aka min_data_in_leaf\n",
    "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 5),\n",
    "            # 'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
    "    #         'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "    #         'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "            # 'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "    #         'max_leaves': trial.suggest_int('max_leaves', 32, 128)\n",
    "        }\n",
    "        \n",
    "    elif arch == 'xgboost':\n",
    "        model_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 10000), # was 900-4500 for CPU\n",
    "            'max_depth' : trial.suggest_int('depth', 3, 10),                                       \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.3),               \n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 50),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 30),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "    #         'booster': trial.suggest_categorical('boosting_type', ['gbtree', 'dart']),\n",
    "            'min_child_weight': trial.suggest_uniform('min_child_weight', 0.001, 12),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "            'gamma': trial.suggest_uniform('gamma', 0.1, 10)\n",
    "        } \n",
    "    \n",
    "    return cross_validate_model(arch, model_params=model_params, wandb_tracked=False, telegram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a4e2b9b-6b24-40bd-83e5-9e3f90cb9e1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "0e85f589-1507-4b75-80d9-8b062970102f",
    "outputId": "6a01a1a1-8060-429d-9a47-670cbc0435d2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/4127186664.py:1: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.\n",
      "  wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_config)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find gbms_20211210.ipynb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/studio-lab-user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/uncategorized/runs/3cj37pte\" target=\"_blank\">gbms_20211210_182953</a></strong> to <a href=\"https://wandb.ai/hushifang/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69acd63c-e202-4d17-ae83-e4f8c030b490",
   "metadata": {
    "id": "ab6749b1-dd7d-4789-b0e2-8491d78fe89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-10 18:32:18,401]\u001b[0m A new study created in memory with name: catboost_study-20211210183218\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "study = optuna.create_study(direction = \"maximize\", \n",
    "                            sampler = TPESampler(seed=int(SEED)), \n",
    "                            study_name=f\"{arch}_study-{start_time}\")\n",
    "\n",
    "# study = load(studypath/f\"optuna_xgboost_study_106trials_20211004.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a585ab8d-37f8-423e-9188-a582eecc8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a1f97-0beb-4598-93c2-b1e8037138e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a734b126-0001-4eb4-809a-beae021637b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost.core.XGBoostError?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aad439f-3f4e-4bcc-b031-36be0562a9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study.optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba38886-c229-4094-9d06-c5e430eef36d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1cSVFH9gkW_",
    "outputId": "ccc874e6-7dd4-4e24-bec8-35ae48180b40",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for x in range(1, 500):\n",
    "    study.optimize(objective, n_trials = 1, callbacks = [wandbc], show_progress_bar=False)#, catch=(xgboost.core.XGBoostError,)) \n",
    "    dump(study, filename=studypath/f\"optuna_{arch}_study-{start_time}.joblib\")\n",
    "#     dump(study.best_trial.params, filename=datapath/f'optuna_lightgbm_study_best-thru-{x*5}trials_20210927.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738f809-eef0-4354-bf6f-a16f1a793c04",
   "metadata": {
    "id": "ybeYZ3omaLWK"
   },
   "outputs": [],
   "source": [
    "wandb.log({'best_params': str(study.best_trial.params)})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac281d00-4ada-45c2-a413-9e4b22514c3e",
   "metadata": {
    "id": "f02e689e-b20c-48e5-a7d9-02467b4f3dbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 9872,\n",
       " 'depth': 3,\n",
       " 'learning_rate': 0.12943882615104757,\n",
       " 'reg_alpha': 4.793236314677738,\n",
       " 'reg_lambda': 0.03427038053813167,\n",
       " 'subsample': 0.5026684329097286,\n",
       " 'min_child_weight': 3.2374430610042664,\n",
       " 'colsample_bytree': 0.9875504456465564,\n",
       " 'gamma': 4.691772640321729}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f053d-97c4-4954-97fd-002e10aa2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
