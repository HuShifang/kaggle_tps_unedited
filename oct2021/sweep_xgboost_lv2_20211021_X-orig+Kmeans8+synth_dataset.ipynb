{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {
    "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53"
   },
   "source": [
    "# XGBoost Hyperparameter Sweep 20211031\n",
    "For optimizing lv2 predictions off of a lv1 with 5 random seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4",
   "metadata": {
    "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae2ff1e-bd1f-4cc9-8357-5a88d1746ffb",
   "metadata": {
    "id": "dae2ff1e-bd1f-4cc9-8357-5a88d1746ffb"
   },
   "outputs": [],
   "source": [
    "# two manual flags (ex-config)\n",
    "colab = False\n",
    "gpu_available = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16849bd2-428c-497b-ba3b-675002f8d041",
   "metadata": {
    "id": "16849bd2-428c-497b-ba3b-675002f8d041"
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "import multiprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d2654b-3bc6-49b5-ade8-cc82112b60e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12d2654b-3bc6-49b5-ade8-cc82112b60e5",
    "outputId": "6bd53922-c4d7-43ce-c04f-ac1079087966"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"sweep_xgboost_lv2_{datetime.now().strftime('%Y%m%d')}_X-orig+Kmeans8+synth_dataset.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416d6118-e543-4df4-9219-2d4a63743c3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "416d6118-e543-4df4-9219-2d4a63743c3a",
    "outputId": "5483656e-2943-4d97-b5d4-65cfb9795430",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handle Google Colab-specific library installation/updating\n",
    "if colab:\n",
    "    # much of the below inspired by or cribbed from the May 2021 Kaggle Tabular Playground winner, at \n",
    "    # https://colab.research.google.com/gist/academicsuspect/0aac7bd6e506f5f70295bfc9a3dc2250/tabular-may-baseline.ipynb?authuser=1#scrollTo=LJoVKJb5wN0L\n",
    "    \n",
    "    # Kaggle API for downloading the datasets\n",
    "    !pip install --upgrade -q kaggle\n",
    "\n",
    "    # weights and biases\n",
    "    !pip install -qqqU wandb\n",
    "    \n",
    "    # Optuna for parameter search\n",
    "    !pip install -q optuna\n",
    "\n",
    "    # !pip install --upgrade xgboost\n",
    "\n",
    "    # upgrade sklearn\n",
    "    !pip install --upgrade scikit-learn\n",
    "\n",
    "    # !pip install category_encoders\n",
    "    # !pip install catboost\n",
    "#     !pip install --upgrade -q lightgbm\n",
    "\n",
    "    # lighgbm gpu compatible\n",
    "    # !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "    # ! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;\n",
    "    \n",
    "    # # this part is from https://github.com/rapidsai/gputreeshap/issues/24\n",
    "    # !pip install cmake --upgrade\n",
    "    # # !pip install sklearn --upgrade\n",
    "    # !git clone --recursive https://github.com/dmlc/xgboost\n",
    "    # %cd /content/xgboost\n",
    "    # !mkdir build\n",
    "    # %cd build\n",
    "    # !cmake .. -DUSE_CUDA=ON\n",
    "    # !make -j4\n",
    "    # %cd /content/xgboost/python-package\n",
    "    # !python setup.py install --use-cuda --use-nccl\n",
    "    # !/opt/bin/nvidia-smi\n",
    "    # !pip install shap\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40df194-4474-4bcf-ac5a-98efe24b91fd",
   "metadata": {
    "id": "d40df194-4474-4bcf-ac5a-98efe24b91fd"
   },
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01e85f7-d602-4dde-bef9-611683cd74c4",
   "metadata": {
    "id": "a01e85f7-d602-4dde-bef9-611683cd74c4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "# from wandb.xgboost import wandb_callback\n",
    "# from wandb.lightgbm import wandb_callback\n",
    "# from sklearn.impute import KNNImputer, StandardImputer\n",
    "# import timm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n",
    "\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna\n",
    "# import catboost\n",
    "from sklearn.utils import resample\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6166c2-ca44-4b7c-a4dc-3db47c2624fe",
   "metadata": {
    "id": "5f6166c2-ca44-4b7c-a4dc-3db47c2624fe"
   },
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c18a787-2193-43cb-87ee-51c6ae7b6351",
   "metadata": {
    "id": "3c18a787-2193-43cb-87ee-51c6ae7b6351"
   },
   "outputs": [],
   "source": [
    "# # This is the code for reading the train.csv and converting it to a .feather file\n",
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67530ca9-6317-48be-bf6c-8621158b0020",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a41cd7e-accb-41c4-ad8b-0eaa3e2b0ad5",
    "outputId": "76d62b41-4171-40fa-936c-4481a9fdab36"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "#     datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/sep2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "#     datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')  \n",
    "    root = Path('/home/sf/code/kaggle/tabular_playgrounds/oct2021/')\n",
    "    datapath = root/'datasets'\n",
    "    edapath = root/'EDA'\n",
    "    modelpath = root/'models'\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'optuna_studies'\n",
    "    altdatapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/alt_datasets/')\n",
    "    \n",
    "    for pth in [root, datapath, edapath, modelpath, predpath, subpath, altdatapath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c652e6-5946-46aa-a13e-4c0ebe8a0e4f",
   "metadata": {
    "id": "d1c652e6-5946-46aa-a13e-4c0ebe8a0e4f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# n_trials = int(1000)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbec2e77-2081-4815-ac6d-39f2a2616386",
   "metadata": {
    "id": "fbec2e77-2081-4815-ac6d-39f2a2616386"
   },
   "outputs": [],
   "source": [
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e47b3-43bd-4d35-b463-9d76100c6ed5",
   "metadata": {
    "id": "2f6e47b3-43bd-4d35-b463-9d76100c6ed5"
   },
   "source": [
    "## Ex-Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f08480-1725-4520-8995-92b76b8f2cea",
   "metadata": {
    "id": "fb288275-a858-4806-9dc0-0b316c334536"
   },
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "# in the sweep version, this includes both ex-model parameters and defaults for model parameters\n",
    "exmodel_config = {\n",
    "    # model config\n",
    "    \"library\": 'xgboost',\n",
    "#     \"model\": XGBClassifier,\n",
    "#     \"n_estimators\": 100, \n",
    "#     \"max_depth\": 3,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"test_size\": 0.2,\n",
    "#     \"reg_lambda\": None, \n",
    "#     \"scaler\": \"sklearn.preprocessing.StandardScaler()\", # TODO: experiment with others (but imputation may be slow)\n",
    "#     \"scale_b4_impute\": False,\n",
    "#     \"imputer\": \"sklearn.impute.SimpleImputer(strategy='median', add_indicator=True)\",\n",
    "#     \"knn_imputer_n_neighbors\": None, # None if a different imputer is used\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "    'random_state': SEED,\n",
    "    'optuna': True,\n",
    "#     'optuna_trials': 20,\n",
    "#     'subsample': 1,\n",
    "#     'cross_val_strategy': None, # None for holdout, or the relevant sklearn class\n",
    "#     'kfolds': 1, # if 1, that means just doing holdout\n",
    "#     'test_size': 0.2,\n",
    "    # these are XGBoost default (my choice) params \n",
    "#     \"tree_method\": \"auto\", # set to 'gpu_hist' to try GPU if available\n",
    "#     \"booster\": 'gbtree', # dart may be marginally better, but will opt for this quicker approach as a default\n",
    "#     \"n_estimators\": 200, \n",
    "#     \"max_depth\": 3,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"n_jobs\": -1,\n",
    "#     \"verbosity\": 1,\n",
    "#     \"subsample\": 1,\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}\n",
    "\n",
    "wandb_kwargs = {\n",
    "    # wandb config\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'project': '202110_Kaggle_tabular_playground',\n",
    "    'tags': ['sweep'],\n",
    "    'notes': \"Sweep for XGBoost on GPU with Optuna, For lv2 model based on 5rs / architecture preds from original dataset\",\n",
    "    'config': exmodel_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d9012-34f1-435a-ba16-4416e0d4a286",
   "metadata": {
    "id": "a52d9012-34f1-435a-ba16-4416e0d4a286"
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252ca14-2718-49d9-89e2-91d55f72d706",
   "metadata": {
    "id": "c912a62f-970a-48b4-b428-d886f2612fc2"
   },
   "source": [
    "**TODO** Write some conditional logic here to automate it -- possibly as part of a sklearn.*pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6919aac1-15d6-4b41-9871-f547602a91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_source = datapath/'train.feather'\n",
    "# df = pd.read_feather(path=datapath/'train.feather')\n",
    "# y = np.array(df.target)\n",
    "# dump(y, filename=datapath/'y.joblib')\n",
    "# del df\n",
    "\n",
    "# y = load(datapath/'y.joblib')\n",
    "y = load(predpath/'5folds_rs42_oof_y.joblib')\n",
    "\n",
    "# df.index.name = 'id'\n",
    "# y_train = df.target\n",
    "# features = [x for x in df.columns if x != 'target']\n",
    "# X_train = df[features]\n",
    "# # X.index.name = 'id'\n",
    "# # y.index.name = 'id'\n",
    "# X = np.array(X_train)\n",
    "# y = np.array(y_train)\n",
    "\n",
    "# del df, X_train, y_train\n",
    "\n",
    "# load the Boruta-filtered green-zone 98 features (based on 200 iterations of the algo)\n",
    "# train_source = '/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/alt_datasets/X_boruta_200iter_filtered_green.joblib'\n",
    "# X = load(train_source)\n",
    "\n",
    "# train_source = altdatapath/'train-WITH-KMeans_8cluster_ninit50_maxiter1000_rs42-AND-synthetic.feather' #'X_boruta_shap_200trials.feather'\n",
    "# train_source = predpath/'stacking_manual_20211020_104938_X_orig+KMeans8+synth_oof_lv1.feather'\n",
    "train_source = altdatapath/'oof_lv1_5rs.feather'\n",
    "exmodel_config['train_source'] = str(train_source)\n",
    "X = pd.read_feather(path=train_source)\n",
    "if 'target' in X.columns:\n",
    "    X = X.drop(['target'], axis=1)\n",
    "\n",
    "# exmodel_config['feature_count'] = len(X.columns)\n",
    "exmodel_config['feature_count'] = X.shape[1]\n",
    "exmodel_config['instance_count'] = X.shape[0]\n",
    "\n",
    "# exmodel_config['feature_generator'] = None\n",
    "# exmodel_config['feature_generator'] = \"Summary statistics\"\n",
    "\n",
    "# exmodel_config['train_source'] = str(train_source)\n",
    "# test_source = datapath/'test.feather'\n",
    "# exmodel_config['test_source'] = str(test_source)\n",
    "# X_test = pd.read_feather(path=test_source)\n",
    "# X_test = X_test.iloc[:, 1:]\n",
    "# X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f294219-fcd3-42c7-a659-9a56ad79f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd088129-6ae2-4002-bfb5-e58cf671fd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 20), (1000000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6107ef0a-dc4c-4b20-a050-ec8cf1708774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d37db-558d-474d-9eca-ce2d38b7636f",
   "metadata": {
    "id": "431d37db-558d-474d-9eca-ce2d38b7636f"
   },
   "source": [
    "# Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ff4abf-560b-450e-a7a5-040878b66565",
   "metadata": {
    "id": "69ff4abf-560b-450e-a7a5-040878b66565"
   },
   "outputs": [],
   "source": [
    "# wandb_kwargs = {\n",
    "#     # wandb config:\n",
    "#     'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "#     'project': '202109_Kaggle_tabular_playground',\n",
    "#     'tags': ['sweep'],\n",
    "#     'notes': \"Sweep for CatBoost using Optuna\",\n",
    "#     'config': exmodel_config,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702836f-7dfc-41ed-b208-2dfeee8526c0",
   "metadata": {},
   "source": [
    "The best parameters up to present have been:\n",
    "\n",
    "```python\n",
    "best_xgboost_params = {\n",
    "        'n_estimators': 3878,\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.024785857161974977,\n",
    "        'reg_alpha': 26.867682044658245,\n",
    "        'reg_lambda': 10.839759074147148,\n",
    "        'subsample': 0.8208581489835881,\n",
    "        'min_child_weight': 8.829122644339664,\n",
    "        'colsample_bytree': 0.906420714280384,\n",
    "        'gamma': 1.472322916021486\n",
    "    }\n",
    "```\n",
    "\n",
    "These params get the following ROC_AUC scores on a 20% holdout for these dataset versions:\n",
    "\n",
    "| Version | Feature Count | Valid ROC-AUC |\n",
    "| ----- | ----- | ----- |\n",
    "| original | 285 | 0.8572984856383443 |\n",
    "| Boruta green-only | 98 | 0.8553163413048461 |\n",
    "| Boruta green-and-blue | 109 | 0.8558487581638441 |\n",
    "| Boruta with SHAP | 136 | 0.8566790062778752 |\n",
    "| Original plus KMeans 8 plus synth | 301 | 0.8570855909847465 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367635ba-0911-4ce5-b5c8-fe2768a12e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=int(SEED), shuffle=True)\n",
    "# # create wrappers for the training and validation partitions\n",
    "# # train_pool = catboost.Pool(X_train, y_train)\n",
    "# # valid_pool = catboost.Pool(X_valid, y_valid)\n",
    "\n",
    "# # experimental parameters -- based off prev. best\n",
    "# params = {\n",
    "#     'n_estimators': 3878,\n",
    "#     'max_depth': 4,\n",
    "#     'learning_rate': 0.024785857161974977,\n",
    "#     'reg_alpha': 26.867682044658245,\n",
    "#     'reg_lambda': 10.839759074147148,\n",
    "#     'subsample': 0.8208581489835881,\n",
    "#     'min_child_weight': 8.829122644339664,\n",
    "#     'colsample_bytree': 0.906420714280384,\n",
    "#     'gamma': 1.472322916021486\n",
    "# }\n",
    "\n",
    "# # instantiate the model, with some parameters locked in, and experimnental ones passed via splat \n",
    "# model = XGBClassifier(\n",
    "#     objective='binary:logistic',\n",
    "#     verbosity=1,\n",
    "#     tree_method='gpu_hist',\n",
    "#     booster='gbtree', # not bothering with dart for time reasons\n",
    "#     random_state=SEED,\n",
    "# #         n_jobs=-1,\n",
    "#     **params\n",
    "# )    \n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "# # generate predictions\n",
    "# preds = model.predict_proba(X_valid)[:,1]\n",
    "# # rounds to the nearest integer, and the nearest even in case of _.5s\n",
    "\n",
    "# # Evaluation\n",
    "# valid_auc = roc_auc_score(y_valid, preds)\n",
    "# print('ROC AUC Score of XGBoost =', valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d93b6f2-2d65-48a9-9862-510bd7d2b75b",
   "metadata": {
    "id": "1d93b6f2-2d65-48a9-9862-510bd7d2b75b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# originally from https://www.kaggle.com/satorushibata/optimize-catboost-hyperparameter-with-optuna-gpu\n",
    "def objective(trial):\n",
    "    # split the (original Kaggle training) data into partitions\n",
    "    # if study.best_trial:\n",
    "    #     print(\"Dumping best params, which are:\")\n",
    "    #     print(str(study.best_trial.params))\n",
    "    #     dump(study.best_trial.params, filename=datapath/'optuna_catboost_best_20210920.joblib')\n",
    "       \n",
    "    # else:\n",
    "    #     print(\"No best study yet\")\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=int(SEED), shuffle=True)\n",
    "    # create wrappers for the training and validation partitions\n",
    "    # train_pool = catboost.Pool(X_train, y_train)\n",
    "    # valid_pool = catboost.Pool(X_valid, y_valid)\n",
    "    \n",
    "    # experimental parameters -- based off prev. best\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 350, 10000), # was 900-4500 for CPU\n",
    "        'max_depth' : trial.suggest_int('depth', 3, 8),                                       \n",
    "        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.3),               \n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 50),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 30),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "#         'booster': trial.suggest_categorical('boosting_type', ['gbtree', 'dart']),\n",
    "        'min_child_weight': trial.suggest_uniform('min_child_weight', 0.001, 12),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "        'gamma': trial.suggest_uniform('gamma', 0.1, 10)\n",
    "    }  \n",
    "\n",
    "    # instantiate the model, with some parameters locked in, and experimnental ones passed via splat \n",
    "    model = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        verbosity=1,\n",
    "        tree_method='gpu_hist',\n",
    "        booster='gbtree', # not bothering with dart for time reasons\n",
    "        random_state=SEED,\n",
    "#         n_jobs=-1,\n",
    "        **params\n",
    "    )    \n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    # generate predictions\n",
    "    preds = model.predict_proba(X_valid)[:,1]\n",
    "    # rounds to the nearest integer, and the nearest even in case of _.5s\n",
    "\n",
    "    # Evaluation\n",
    "    valid_auc = roc_auc_score(y_valid, preds)\n",
    "    print('ROC AUC Score of XGBoost =', valid_auc)\n",
    "    wandb.log({'valid_auc': valid_auc,\n",
    "              })\n",
    "\n",
    "    return valid_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e85f589-1507-4b75-80d9-8b062970102f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "0e85f589-1507-4b75-80d9-8b062970102f",
    "outputId": "6a01a1a1-8060-429d-9a47-670cbc0435d2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-69ea9289a2cf>:1: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.\n",
      "  wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find sweep_xgboost_lv2_20211031_X-orig+Kmeans8+synth_dataset.ipynb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/202110_Kaggle_tabular_playground/runs/juld9j4c\" target=\"_blank\">sweep_xgboost_lv2_20211031_X-orig+Kmeans8+synth_dataset_162015</a></strong> to <a href=\"https://wandb.ai/hushifang/202110_Kaggle_tabular_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab6749b1-dd7d-4789-b0e2-8491d78fe89b",
   "metadata": {
    "id": "ab6749b1-dd7d-4789-b0e2-8491d78fe89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:20:22,667]\u001b[0m A new study created in memory with name: xgboost-lv2-X_orig_5rs_20211031\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction = \"maximize\", \n",
    "                            sampler = TPESampler(seed=int(SEED)), \n",
    "                            study_name=f\"xgboost-lv2-X_orig_5rs_{datetime.now().strftime('%Y%m%d')}\")\n",
    "\n",
    "# study = load(studypath/f\"optuna_xgboost_study_106trials_20211004.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f02e3b84-ee16-48b9-94db-c738b408a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a454cc8-f135-4d36-8b6c-a964f4b52288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3860cbd2-1d08-4b2e-ac53-92b8a2ce016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost.core.XGBoostError?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c341a341-2f06-4d61-a0cd-3f0751e88861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study.optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe8ad6db-2722-4f04-bd51-4b795bec93c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1cSVFH9gkW_",
    "outputId": "ccc874e6-7dd4-4e24-bec8-35ae48180b40",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:20:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:20:39,820]\u001b[0m Trial 0 finished with value: 0.8567745571085623 and parameters: {'n_estimators': 3964, 'depth': 8, 'learning_rate': 0.06504856968981275, 'reg_alpha': 0.6502468545951017, 'reg_lambda': 0.004994757081068292, 'subsample': 0.5779972601681014, 'min_child_weight': 0.6979452624062253, 'colsample_bytree': 0.9330880728874675, 'gamma': 6.051038616257767}. Best is trial 0 with value: 0.8567745571085623.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 0 < 1; dropping {'n_estimators': 3964, 'depth': 8, 'learning_rate': 0.06504856968981275, 'reg_alpha': 0.6502468545951017, 'reg_lambda': 0.004994757081068292, 'subsample': 0.5779972601681014, 'min_child_weight': 0.6979452624062253, 'colsample_bytree': 0.9330880728874675, 'gamma': 6.051038616257767, 'value': 0.8567745571085623}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8567745571085623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:20:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:21:03,489]\u001b[0m Trial 1 finished with value: 0.8566107431017014 and parameters: {'n_estimators': 7183, 'depth': 3, 'learning_rate': 0.2526878207508456, 'reg_alpha': 8.158738235092015, 'reg_lambda': 0.00892622738184373, 'subsample': 0.5909124836035503, 'min_child_weight': 2.2016707137313523, 'colsample_bytree': 0.6521211214797689, 'gamma': 5.295088673159155}. Best is trial 0 with value: 0.8567745571085623.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 1 < 2; dropping {'n_estimators': 7183, 'depth': 3, 'learning_rate': 0.2526878207508456, 'reg_alpha': 8.158738235092015, 'reg_lambda': 0.00892622738184373, 'subsample': 0.5909124836035503, 'min_child_weight': 2.2016707137313523, 'colsample_bytree': 0.6521211214797689, 'gamma': 5.295088673159155, 'value': 0.8566107431017014}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8566107431017014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:21:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:21:27,519]\u001b[0m Trial 2 finished with value: 0.8565019690483414 and parameters: {'n_estimators': 4518, 'depth': 4, 'learning_rate': 0.032781876533976156, 'reg_alpha': 0.004523529917658778, 'reg_lambda': 0.02032202659636255, 'subsample': 0.6831809216468459, 'min_child_weight': 5.473383740620215, 'colsample_bytree': 0.8925879806965068, 'gamma': 2.0767704433677614}. Best is trial 0 with value: 0.8567745571085623.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 2 < 3; dropping {'n_estimators': 4518, 'depth': 4, 'learning_rate': 0.032781876533976156, 'reg_alpha': 0.004523529917658778, 'reg_lambda': 0.02032202659636255, 'subsample': 0.6831809216468459, 'min_child_weight': 5.473383740620215, 'colsample_bytree': 0.8925879806965068, 'gamma': 2.0767704433677614, 'value': 0.8565019690483414}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8565019690483414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:21:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:22:11,086]\u001b[0m Trial 3 finished with value: 0.856907606229623 and parameters: {'n_estimators': 5312, 'depth': 6, 'learning_rate': 0.0013033567475147442, 'reg_alpha': 0.7158714383119805, 'reg_lambda': 0.005800389779115683, 'subsample': 0.5325257964926398, 'min_child_weight': 11.386677561502745, 'colsample_bytree': 0.9828160165372797, 'gamma': 8.103133746352965}. Best is trial 3 with value: 0.856907606229623.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 3 < 4; dropping {'n_estimators': 5312, 'depth': 6, 'learning_rate': 0.0013033567475147442, 'reg_alpha': 0.7158714383119805, 'reg_lambda': 0.005800389779115683, 'subsample': 0.5325257964926398, 'min_child_weight': 11.386677561502745, 'colsample_bytree': 0.9828160165372797, 'gamma': 8.103133746352965, 'value': 0.856907606229623}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.856907606229623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:22:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:22:25,135]\u001b[0m Trial 4 finished with value: 0.8566910863257707 and parameters: {'n_estimators': 3289, 'depth': 3, 'learning_rate': 0.04953682563497157, 'reg_alpha': 0.11702088154220885, 'reg_lambda': 0.0035186816415472676, 'subsample': 0.7475884550556351, 'min_child_weight': 0.41362786486150555, 'colsample_bytree': 0.954660201039391, 'gamma': 2.6619218178401676}. Best is trial 3 with value: 0.856907606229623.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 4 < 5; dropping {'n_estimators': 3289, 'depth': 3, 'learning_rate': 0.04953682563497157, 'reg_alpha': 0.11702088154220885, 'reg_lambda': 0.0035186816415472676, 'subsample': 0.7475884550556351, 'min_child_weight': 0.41362786486150555, 'colsample_bytree': 0.954660201039391, 'gamma': 2.6619218178401676, 'value': 0.8566910863257707}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8566910863257707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:22:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:22:46,896]\u001b[0m Trial 5 finished with value: 0.8568889733276577 and parameters: {'n_estimators': 6744, 'depth': 4, 'learning_rate': 0.01942099825171803, 'reg_alpha': 0.37065955814875856, 'reg_lambda': 0.0067238158696505896, 'subsample': 0.9847923138822793, 'min_child_weight': 9.301818747510014, 'colsample_bytree': 0.9697494707820946, 'gamma': 8.958790769233723}. Best is trial 3 with value: 0.856907606229623.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 5 < 6; dropping {'n_estimators': 6744, 'depth': 4, 'learning_rate': 0.01942099825171803, 'reg_alpha': 0.37065955814875856, 'reg_lambda': 0.0067238158696505896, 'subsample': 0.9847923138822793, 'min_child_weight': 9.301818747510014, 'colsample_bytree': 0.9697494707820946, 'gamma': 8.958790769233723, 'value': 0.8568889733276577}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568889733276577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:22:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:23:31,769]\u001b[0m Trial 6 finished with value: 0.8569070366280344 and parameters: {'n_estimators': 6120, 'depth': 8, 'learning_rate': 0.0016565580440884786, 'reg_alpha': 0.008335230071817131, 'reg_lambda': 0.001593999043568401, 'subsample': 0.6626651653816322, 'min_child_weight': 4.664738798984096, 'colsample_bytree': 0.6356745158869479, 'gamma': 8.3045013406041}. Best is trial 3 with value: 0.856907606229623.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 6 < 7; dropping {'n_estimators': 6120, 'depth': 8, 'learning_rate': 0.0016565580440884786, 'reg_alpha': 0.008335230071817131, 'reg_lambda': 0.001593999043568401, 'subsample': 0.6626651653816322, 'min_child_weight': 4.664738798984096, 'colsample_bytree': 0.6356745158869479, 'gamma': 8.3045013406041, 'value': 0.8569070366280344}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569070366280344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:23:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:23:53,430]\u001b[0m Trial 7 finished with value: 0.8566717296217867 and parameters: {'n_estimators': 3793, 'depth': 4, 'learning_rate': 0.022096526145513846, 'reg_alpha': 0.0045940816125026864, 'reg_lambda': 3.9042098517777197, 'subsample': 0.5372753218398854, 'min_child_weight': 11.842656352269607, 'colsample_bytree': 0.8861223846483287, 'gamma': 2.0672852471883068}. Best is trial 3 with value: 0.856907606229623.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 7 < 8; dropping {'n_estimators': 3793, 'depth': 4, 'learning_rate': 0.022096526145513846, 'reg_alpha': 0.0045940816125026864, 'reg_lambda': 3.9042098517777197, 'subsample': 0.5372753218398854, 'min_child_weight': 11.842656352269607, 'colsample_bytree': 0.8861223846483287, 'gamma': 2.0672852471883068, 'value': 0.8566717296217867}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8566717296217867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:23:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:23:56,069]\u001b[0m Trial 8 finished with value: 0.8569089264833052 and parameters: {'n_estimators': 403, 'depth': 7, 'learning_rate': 0.0563600475052774, 'reg_alpha': 2.6642981030636883, 'reg_lambda': 2.838382119353614, 'subsample': 0.5370223258670452, 'min_child_weight': 4.302230276802727, 'colsample_bytree': 0.5579345297625649, 'gamma': 8.644723916168376}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 8 < 9; dropping {'n_estimators': 403, 'depth': 7, 'learning_rate': 0.0563600475052774, 'reg_alpha': 2.6642981030636883, 'reg_lambda': 2.838382119353614, 'subsample': 0.5370223258670452, 'min_child_weight': 4.302230276802727, 'colsample_bytree': 0.5579345297625649, 'gamma': 8.644723916168376, 'value': 0.8569089264833052}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569089264833052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:23:56] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:24:34,130]\u001b[0m Trial 9 finished with value: 0.8568953331953946 and parameters: {'n_estimators': 6365, 'depth': 4, 'learning_rate': 0.0014369502768990666, 'reg_alpha': 0.028926547478415564, 'reg_lambda': 0.028568350317608886, 'subsample': 0.864803089169032, 'min_child_weight': 7.651052098791203, 'colsample_bytree': 0.9436063712881633, 'gamma': 4.7749277591032975}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 9 < 10; dropping {'n_estimators': 6365, 'depth': 4, 'learning_rate': 0.0014369502768990666, 'reg_alpha': 0.028926547478415564, 'reg_lambda': 0.028568350317608886, 'subsample': 0.864803089169032, 'min_child_weight': 7.651052098791203, 'colsample_bytree': 0.9436063712881633, 'gamma': 4.7749277591032975, 'value': 0.8568953331953946}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568953331953946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:24:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:24:40,400]\u001b[0m Trial 10 finished with value: 0.8568799710525512 and parameters: {'n_estimators': 548, 'depth': 6, 'learning_rate': 0.0057835683328601424, 'reg_alpha': 25.193261592370117, 'reg_lambda': 24.215992044481425, 'subsample': 0.8451235367845727, 'min_child_weight': 3.095865504265305, 'colsample_bytree': 0.5055571924113557, 'gamma': 9.696199420265538}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 10 < 11; dropping {'n_estimators': 548, 'depth': 6, 'learning_rate': 0.0057835683328601424, 'reg_alpha': 25.193261592370117, 'reg_lambda': 24.215992044481425, 'subsample': 0.8451235367845727, 'min_child_weight': 3.095865504265305, 'colsample_bytree': 0.5055571924113557, 'gamma': 9.696199420265538, 'value': 0.8568799710525512}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568799710525512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:24:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:25:15,875]\u001b[0m Trial 11 finished with value: 0.8568998150078941 and parameters: {'n_estimators': 9712, 'depth': 6, 'learning_rate': 0.006265045297405034, 'reg_alpha': 2.0678704761423647, 'reg_lambda': 0.35538772799202606, 'subsample': 0.5154800030768701, 'min_child_weight': 11.28268770042188, 'colsample_bytree': 0.7866454888256145, 'gamma': 7.433049102858982}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 11 < 12; dropping {'n_estimators': 9712, 'depth': 6, 'learning_rate': 0.006265045297405034, 'reg_alpha': 2.0678704761423647, 'reg_lambda': 0.35538772799202606, 'subsample': 0.5154800030768701, 'min_child_weight': 11.28268770042188, 'colsample_bytree': 0.7866454888256145, 'gamma': 7.433049102858982, 'value': 0.8568998150078941}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568998150078941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:25:16] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:25:18,500]\u001b[0m Trial 12 finished with value: 0.8568848430661389 and parameters: {'n_estimators': 503, 'depth': 7, 'learning_rate': 0.16017451645115668, 'reg_alpha': 2.627574281927009, 'reg_lambda': 0.5332090840296356, 'subsample': 0.6372424866412405, 'min_child_weight': 7.532087700126141, 'colsample_bytree': 0.5167929424672362, 'gamma': 6.9631014192624}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 12 < 13; dropping {'n_estimators': 503, 'depth': 7, 'learning_rate': 0.16017451645115668, 'reg_alpha': 2.627574281927009, 'reg_lambda': 0.5332090840296356, 'subsample': 0.6372424866412405, 'min_child_weight': 7.532087700126141, 'colsample_bytree': 0.5167929424672362, 'gamma': 6.9631014192624, 'value': 0.8568848430661389}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568848430661389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:25:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:25:29,310]\u001b[0m Trial 13 finished with value: 0.8569022324646361 and parameters: {'n_estimators': 1858, 'depth': 7, 'learning_rate': 0.006095386499338796, 'reg_alpha': 49.65627329623703, 'reg_lambda': 1.7655362188643002, 'subsample': 0.5127747474052715, 'min_child_weight': 3.9676254361559256, 'colsample_bytree': 0.7648198488403328, 'gamma': 9.983182168887833}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 13 < 14; dropping {'n_estimators': 1858, 'depth': 7, 'learning_rate': 0.006095386499338796, 'reg_alpha': 49.65627329623703, 'reg_lambda': 1.7655362188643002, 'subsample': 0.5127747474052715, 'min_child_weight': 3.9676254361559256, 'colsample_bytree': 0.7648198488403328, 'gamma': 9.983182168887833, 'value': 0.8569022324646361}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569022324646361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:25:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:25:58,241]\u001b[0m Trial 14 finished with value: 0.8562452920824952 and parameters: {'n_estimators': 8667, 'depth': 7, 'learning_rate': 0.10038701694957171, 'reg_alpha': 0.09014037986173169, 'reg_lambda': 0.0709422654078536, 'subsample': 0.7655620151757302, 'min_child_weight': 6.706024197936731, 'colsample_bytree': 0.6471208002214497, 'gamma': 3.7652044058182774}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 14 < 15; dropping {'n_estimators': 8667, 'depth': 7, 'learning_rate': 0.10038701694957171, 'reg_alpha': 0.09014037986173169, 'reg_lambda': 0.0709422654078536, 'subsample': 0.7655620151757302, 'min_child_weight': 6.706024197936731, 'colsample_bytree': 0.6471208002214497, 'gamma': 3.7652044058182774, 'value': 0.8562452920824952}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8562452920824952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:25:58] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:26:13,947]\u001b[0m Trial 15 finished with value: 0.8569032692675276 and parameters: {'n_estimators': 1874, 'depth': 5, 'learning_rate': 0.002734259025483008, 'reg_alpha': 1.2976213221595907, 'reg_lambda': 16.281489111694658, 'subsample': 0.6067816387994542, 'min_child_weight': 9.054538400000514, 'colsample_bytree': 0.8352725713024433, 'gamma': 7.880321003921752}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 15 < 16; dropping {'n_estimators': 1874, 'depth': 5, 'learning_rate': 0.002734259025483008, 'reg_alpha': 1.2976213221595907, 'reg_lambda': 16.281489111694658, 'subsample': 0.6067816387994542, 'min_child_weight': 9.054538400000514, 'colsample_bytree': 0.8352725713024433, 'gamma': 7.880321003921752, 'value': 0.8569032692675276}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569032692675276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:26:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:27:01,993]\u001b[0m Trial 16 finished with value: 0.8564367975165845 and parameters: {'n_estimators': 5258, 'depth': 6, 'learning_rate': 0.011218855608069756, 'reg_alpha': 8.485571126247757, 'reg_lambda': 0.13781096594280337, 'subsample': 0.7580143193422603, 'min_child_weight': 9.932330562095588, 'colsample_bytree': 0.6943845973931932, 'gamma': 0.23047422237114557}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 16 < 17; dropping {'n_estimators': 5258, 'depth': 6, 'learning_rate': 0.011218855608069756, 'reg_alpha': 8.485571126247757, 'reg_lambda': 0.13781096594280337, 'subsample': 0.7580143193422603, 'min_child_weight': 9.932330562095588, 'colsample_bytree': 0.6943845973931932, 'gamma': 0.23047422237114557, 'value': 0.8564367975165845}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8564367975165845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:27:02] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:27:12,719]\u001b[0m Trial 17 finished with value: 0.8568969481998989 and parameters: {'n_estimators': 2408, 'depth': 5, 'learning_rate': 0.011648821855852205, 'reg_alpha': 6.361572953808567, 'reg_lambda': 4.412721138165254, 'subsample': 0.6900913631930554, 'min_child_weight': 2.2648833769944483, 'colsample_bytree': 0.7107053414739195, 'gamma': 6.394821350110929}. Best is trial 8 with value: 0.8569089264833052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 17 < 18; dropping {'n_estimators': 2408, 'depth': 5, 'learning_rate': 0.011648821855852205, 'reg_alpha': 6.361572953808567, 'reg_lambda': 4.412721138165254, 'subsample': 0.6900913631930554, 'min_child_weight': 2.2648833769944483, 'colsample_bytree': 0.7107053414739195, 'gamma': 6.394821350110929, 'value': 0.8568969481998989}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568969481998989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:27:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:27:50,441]\u001b[0m Trial 18 finished with value: 0.8569145201489052 and parameters: {'n_estimators': 7943, 'depth': 7, 'learning_rate': 0.0028449169511802976, 'reg_alpha': 0.001079627815782365, 'reg_lambda': 1.3244077888823096, 'subsample': 0.5490851573245075, 'min_child_weight': 5.108638302486011, 'colsample_bytree': 0.5897670850320078, 'gamma': 8.738156944298614}. Best is trial 18 with value: 0.8569145201489052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 18 < 19; dropping {'n_estimators': 7943, 'depth': 7, 'learning_rate': 0.0028449169511802976, 'reg_alpha': 0.001079627815782365, 'reg_lambda': 1.3244077888823096, 'subsample': 0.5490851573245075, 'min_child_weight': 5.108638302486011, 'colsample_bytree': 0.5897670850320078, 'gamma': 8.738156944298614, 'value': 0.8569145201489052}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569145201489052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:27:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:28:27,327]\u001b[0m Trial 19 finished with value: 0.8569141821479627 and parameters: {'n_estimators': 8046, 'depth': 8, 'learning_rate': 0.0031535673119604496, 'reg_alpha': 0.001595877295784664, 'reg_lambda': 0.9276823351587395, 'subsample': 0.5697705541247967, 'min_child_weight': 5.752758775505524, 'colsample_bytree': 0.5774774629590134, 'gamma': 8.982494180659982}. Best is trial 18 with value: 0.8569145201489052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 19 < 20; dropping {'n_estimators': 8046, 'depth': 8, 'learning_rate': 0.0031535673119604496, 'reg_alpha': 0.001595877295784664, 'reg_lambda': 0.9276823351587395, 'subsample': 0.5697705541247967, 'min_child_weight': 5.752758775505524, 'colsample_bytree': 0.5774774629590134, 'gamma': 8.982494180659982, 'value': 0.8569141821479627}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569141821479627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:28:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:29:03,771]\u001b[0m Trial 20 finished with value: 0.8569142704482089 and parameters: {'n_estimators': 8017, 'depth': 8, 'learning_rate': 0.0032629462621328923, 'reg_alpha': 0.0010426657777225991, 'reg_lambda': 0.927881845085354, 'subsample': 0.6200392464368687, 'min_child_weight': 5.610469756138408, 'colsample_bytree': 0.5754574090669539, 'gamma': 8.917894734741536}. Best is trial 18 with value: 0.8569145201489052.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 20 < 21; dropping {'n_estimators': 8017, 'depth': 8, 'learning_rate': 0.0032629462621328923, 'reg_alpha': 0.0010426657777225991, 'reg_lambda': 0.927881845085354, 'subsample': 0.6200392464368687, 'min_child_weight': 5.610469756138408, 'colsample_bytree': 0.5754574090669539, 'gamma': 8.917894734741536, 'value': 0.8569142704482089}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569142704482089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:29:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:29:40,869]\u001b[0m Trial 21 finished with value: 0.8569157614523671 and parameters: {'n_estimators': 7896, 'depth': 8, 'learning_rate': 0.00300451738623939, 'reg_alpha': 0.001148390168156581, 'reg_lambda': 0.8632943448292806, 'subsample': 0.5880221408055133, 'min_child_weight': 5.837682214042644, 'colsample_bytree': 0.5854822546561775, 'gamma': 8.917716054956825}. Best is trial 21 with value: 0.8569157614523671.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 21 < 22; dropping {'n_estimators': 7896, 'depth': 8, 'learning_rate': 0.00300451738623939, 'reg_alpha': 0.001148390168156581, 'reg_lambda': 0.8632943448292806, 'subsample': 0.5880221408055133, 'min_child_weight': 5.837682214042644, 'colsample_bytree': 0.5854822546561775, 'gamma': 8.917716054956825, 'value': 0.8569157614523671}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569157614523671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:29:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:30:19,843]\u001b[0m Trial 22 finished with value: 0.8569133300955863 and parameters: {'n_estimators': 8050, 'depth': 8, 'learning_rate': 0.0026825690403786797, 'reg_alpha': 0.0016063783860783352, 'reg_lambda': 0.20052981253424565, 'subsample': 0.6247169050806954, 'min_child_weight': 6.501997107454347, 'colsample_bytree': 0.5826099599707831, 'gamma': 9.386749050829426}. Best is trial 21 with value: 0.8569157614523671.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 22 < 23; dropping {'n_estimators': 8050, 'depth': 8, 'learning_rate': 0.0026825690403786797, 'reg_alpha': 0.0016063783860783352, 'reg_lambda': 0.20052981253424565, 'subsample': 0.6247169050806954, 'min_child_weight': 6.501997107454347, 'colsample_bytree': 0.5826099599707831, 'gamma': 9.386749050829426, 'value': 0.8569133300955863}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569133300955863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:30:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:31:01,235]\u001b[0m Trial 23 finished with value: 0.8569127196938839 and parameters: {'n_estimators': 9746, 'depth': 8, 'learning_rate': 0.003688054380058554, 'reg_alpha': 0.0012191256521460938, 'reg_lambda': 8.709318243025091, 'subsample': 0.6435544847082245, 'min_child_weight': 4.975686487382656, 'colsample_bytree': 0.6011092721851449, 'gamma': 7.094756361823698}. Best is trial 21 with value: 0.8569157614523671.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 23 < 24; dropping {'n_estimators': 9746, 'depth': 8, 'learning_rate': 0.003688054380058554, 'reg_alpha': 0.0012191256521460938, 'reg_lambda': 8.709318243025091, 'subsample': 0.6435544847082245, 'min_child_weight': 4.975686487382656, 'colsample_bytree': 0.6011092721851449, 'gamma': 7.094756361823698, 'value': 0.8569127196938839}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569127196938839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:31:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:32:12,142]\u001b[0m Trial 24 finished with value: 0.8569058414247012 and parameters: {'n_estimators': 8802, 'depth': 7, 'learning_rate': 0.0010791285619214564, 'reg_alpha': 0.01653999121424828, 'reg_lambda': 1.1246285644950107, 'subsample': 0.7144563043864602, 'min_child_weight': 7.411141779520299, 'colsample_bytree': 0.6975115409747275, 'gamma': 6.149119270638638}. Best is trial 21 with value: 0.8569157614523671.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 24 < 25; dropping {'n_estimators': 8802, 'depth': 7, 'learning_rate': 0.0010791285619214564, 'reg_alpha': 0.01653999121424828, 'reg_lambda': 1.1246285644950107, 'subsample': 0.7144563043864602, 'min_child_weight': 7.411141779520299, 'colsample_bytree': 0.6975115409747275, 'gamma': 6.149119270638638, 'value': 0.8569058414247012}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569058414247012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:32:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:32:54,348]\u001b[0m Trial 25 finished with value: 0.8569164899543988 and parameters: {'n_estimators': 7194, 'depth': 8, 'learning_rate': 0.002025764200289733, 'reg_alpha': 0.003374199381134067, 'reg_lambda': 0.5424935537649992, 'subsample': 0.5683162795967847, 'min_child_weight': 3.51327029569875, 'colsample_bytree': 0.5414403310742756, 'gamma': 7.870852281537746}. Best is trial 25 with value: 0.8569164899543988.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 25 < 26; dropping {'n_estimators': 7194, 'depth': 8, 'learning_rate': 0.002025764200289733, 'reg_alpha': 0.003374199381134067, 'reg_lambda': 0.5424935537649992, 'subsample': 0.5683162795967847, 'min_child_weight': 3.51327029569875, 'colsample_bytree': 0.5414403310742756, 'gamma': 7.870852281537746, 'value': 0.8569164899543988}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569164899543988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:32:55] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:33:20,732]\u001b[0m Trial 26 finished with value: 0.856907126478285 and parameters: {'n_estimators': 7064, 'depth': 7, 'learning_rate': 0.00912792192333895, 'reg_alpha': 0.0036460165830851324, 'reg_lambda': 0.3615479561671184, 'subsample': 0.5639871677301805, 'min_child_weight': 3.35986117127564, 'colsample_bytree': 0.540240281713268, 'gamma': 7.33310929636887}. Best is trial 25 with value: 0.8569164899543988.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 26 < 27; dropping {'n_estimators': 7064, 'depth': 7, 'learning_rate': 0.00912792192333895, 'reg_alpha': 0.0036460165830851324, 'reg_lambda': 0.3615479561671184, 'subsample': 0.5639871677301805, 'min_child_weight': 3.35986117127564, 'colsample_bytree': 0.540240281713268, 'gamma': 7.33310929636887, 'value': 0.856907126478285}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.856907126478285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:33:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:34:01,108]\u001b[0m Trial 27 finished with value: 0.8569016166629186 and parameters: {'n_estimators': 5742, 'depth': 8, 'learning_rate': 0.002100080328741079, 'reg_alpha': 0.01975816090645588, 'reg_lambda': 0.11283899008178311, 'subsample': 0.8214553849136108, 'min_child_weight': 1.7787066751575766, 'colsample_bytree': 0.621755689219156, 'gamma': 7.955214485933839}. Best is trial 25 with value: 0.8569164899543988.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 27 < 28; dropping {'n_estimators': 5742, 'depth': 8, 'learning_rate': 0.002100080328741079, 'reg_alpha': 0.01975816090645588, 'reg_lambda': 0.11283899008178311, 'subsample': 0.8214553849136108, 'min_child_weight': 1.7787066751575766, 'colsample_bytree': 0.621755689219156, 'gamma': 7.955214485933839, 'value': 0.8569016166629186}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569016166629186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:35:11,981]\u001b[0m Trial 28 finished with value: 0.8568975106514674 and parameters: {'n_estimators': 7662, 'depth': 7, 'learning_rate': 0.0010083871852237618, 'reg_alpha': 0.002483369897957641, 'reg_lambda': 7.517381111385791, 'subsample': 0.9217051453210541, 'min_child_weight': 3.185725469204564, 'colsample_bytree': 0.531155817343147, 'gamma': 5.594478605700383}. Best is trial 25 with value: 0.8569164899543988.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 28 < 29; dropping {'n_estimators': 7662, 'depth': 7, 'learning_rate': 0.0010083871852237618, 'reg_alpha': 0.002483369897957641, 'reg_lambda': 7.517381111385791, 'subsample': 0.9217051453210541, 'min_child_weight': 3.185725469204564, 'colsample_bytree': 0.531155817343147, 'gamma': 5.594478605700383, 'value': 0.8568975106514674}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568975106514674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:36:07,869]\u001b[0m Trial 29 finished with value: 0.8568197122844956 and parameters: {'n_estimators': 8838, 'depth': 8, 'learning_rate': 0.00474541703870553, 'reg_alpha': 0.00827532438053137, 'reg_lambda': 0.045595015201187104, 'subsample': 0.5007180429400027, 'min_child_weight': 1.3144162658774237, 'colsample_bytree': 0.6796102022110047, 'gamma': 4.59127813724702}. Best is trial 25 with value: 0.8569164899543988.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 29 < 30; dropping {'n_estimators': 8838, 'depth': 8, 'learning_rate': 0.00474541703870553, 'reg_alpha': 0.00827532438053137, 'reg_lambda': 0.045595015201187104, 'subsample': 0.5007180429400027, 'min_child_weight': 1.3144162658774237, 'colsample_bytree': 0.6796102022110047, 'gamma': 4.59127813724702, 'value': 0.8568197122844956}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568197122844956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:36:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:36:54,595]\u001b[0m Trial 30 finished with value: 0.856911049589226 and parameters: {'n_estimators': 8913, 'depth': 8, 'learning_rate': 0.0019417644688080472, 'reg_alpha': 0.03756232256284382, 'reg_lambda': 1.9626224850638216, 'subsample': 0.5788964062629987, 'min_child_weight': 3.7559614477217957, 'colsample_bytree': 0.7334567004168044, 'gamma': 9.460384603620344}. Best is trial 25 with value: 0.8569164899543988.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 30 < 31; dropping {'n_estimators': 8913, 'depth': 8, 'learning_rate': 0.0019417644688080472, 'reg_alpha': 0.03756232256284382, 'reg_lambda': 1.9626224850638216, 'subsample': 0.5788964062629987, 'min_child_weight': 3.7559614477217957, 'colsample_bytree': 0.7334567004168044, 'gamma': 9.460384603620344, 'value': 0.856911049589226}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.856911049589226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:36:55] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:37:29,029]\u001b[0m Trial 31 finished with value: 0.8569165048544404 and parameters: {'n_estimators': 7651, 'depth': 8, 'learning_rate': 0.003947544472693994, 'reg_alpha': 0.0011070124064676246, 'reg_lambda': 0.8108289711001738, 'subsample': 0.6080110794163263, 'min_child_weight': 5.147083508731836, 'colsample_bytree': 0.5984193538905249, 'gamma': 8.676722721686408}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 31 < 32; dropping {'n_estimators': 7651, 'depth': 8, 'learning_rate': 0.003947544472693994, 'reg_alpha': 0.0011070124064676246, 'reg_lambda': 0.8108289711001738, 'subsample': 0.6080110794163263, 'min_child_weight': 5.147083508731836, 'colsample_bytree': 0.5984193538905249, 'gamma': 8.676722721686408, 'value': 0.8569165048544404}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569165048544404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:37:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:38:06,358]\u001b[0m Trial 32 finished with value: 0.8569074509791901 and parameters: {'n_estimators': 7589, 'depth': 8, 'learning_rate': 0.00410627336404385, 'reg_alpha': 0.00823801891592462, 'reg_lambda': 0.4885150715023978, 'subsample': 0.5582552237059151, 'min_child_weight': 6.510438868090795, 'colsample_bytree': 0.6045551900068308, 'gamma': 6.462106096712658}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 32 < 33; dropping {'n_estimators': 7589, 'depth': 8, 'learning_rate': 0.00410627336404385, 'reg_alpha': 0.00823801891592462, 'reg_lambda': 0.4885150715023978, 'subsample': 0.5582552237059151, 'min_child_weight': 6.510438868090795, 'colsample_bytree': 0.6045551900068308, 'gamma': 6.462106096712658, 'value': 0.8569074509791901}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569074509791901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:38:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:38:32,259]\u001b[0m Trial 33 finished with value: 0.8569145533489979 and parameters: {'n_estimators': 6759, 'depth': 7, 'learning_rate': 0.008530993382421507, 'reg_alpha': 0.0029535255061163695, 'reg_lambda': 0.22153391584715273, 'subsample': 0.5936539702037942, 'min_child_weight': 4.8519584955124175, 'colsample_bytree': 0.5389434085392616, 'gamma': 8.342400490362532}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 33 < 34; dropping {'n_estimators': 6759, 'depth': 7, 'learning_rate': 0.008530993382421507, 'reg_alpha': 0.0029535255061163695, 'reg_lambda': 0.22153391584715273, 'subsample': 0.5936539702037942, 'min_child_weight': 4.8519584955124175, 'colsample_bytree': 0.5389434085392616, 'gamma': 8.342400490362532, 'value': 0.8569145533489979}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569145533489979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:38:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:38:59,481]\u001b[0m Trial 34 finished with value: 0.85691558325187 and parameters: {'n_estimators': 6894, 'depth': 8, 'learning_rate': 0.00774594674275648, 'reg_alpha': 0.002925190472502194, 'reg_lambda': 0.2538322121045323, 'subsample': 0.5932075668051952, 'min_child_weight': 2.3078218403464676, 'colsample_bytree': 0.5460190002574195, 'gamma': 7.91507327562635}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 34 < 35; dropping {'n_estimators': 6894, 'depth': 8, 'learning_rate': 0.00774594674275648, 'reg_alpha': 0.002925190472502194, 'reg_lambda': 0.2538322121045323, 'subsample': 0.5932075668051952, 'min_child_weight': 2.3078218403464676, 'colsample_bytree': 0.5460190002574195, 'gamma': 7.91507327562635, 'value': 0.85691558325187}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.85691558325187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:39:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:39:20,489]\u001b[0m Trial 35 finished with value: 0.856907964080621 and parameters: {'n_estimators': 4692, 'depth': 8, 'learning_rate': 0.007936309933334183, 'reg_alpha': 0.005978971335883916, 'reg_lambda': 0.6595339698212679, 'subsample': 0.6678245216424936, 'min_child_weight': 2.553809024397921, 'colsample_bytree': 0.6654927785389436, 'gamma': 7.683649959400888}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 35 < 36; dropping {'n_estimators': 4692, 'depth': 8, 'learning_rate': 0.007936309933334183, 'reg_alpha': 0.005978971335883916, 'reg_lambda': 0.6595339698212679, 'subsample': 0.6678245216424936, 'min_child_weight': 2.553809024397921, 'colsample_bytree': 0.6654927785389436, 'gamma': 7.683649959400888, 'value': 0.856907964080621}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.856907964080621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:39:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:39:45,456]\u001b[0m Trial 36 finished with value: 0.8568908413328673 and parameters: {'n_estimators': 7358, 'depth': 8, 'learning_rate': 0.030084852759592336, 'reg_alpha': 0.002317777878560706, 'reg_lambda': 0.08134827925405787, 'subsample': 0.7139603031627981, 'min_child_weight': 0.05222603038885687, 'colsample_bytree': 0.5545027526689944, 'gamma': 6.879472747450057}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 36 < 37; dropping {'n_estimators': 7358, 'depth': 8, 'learning_rate': 0.030084852759592336, 'reg_alpha': 0.002317777878560706, 'reg_lambda': 0.08134827925405787, 'subsample': 0.7139603031627981, 'min_child_weight': 0.05222603038885687, 'colsample_bytree': 0.5545027526689944, 'gamma': 6.879472747450057, 'value': 0.8568908413328673}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568908413328673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:39:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:40:09,878]\u001b[0m Trial 37 finished with value: 0.8568864060704979 and parameters: {'n_estimators': 5831, 'depth': 8, 'learning_rate': 0.013878428298327835, 'reg_alpha': 0.05272988320889208, 'reg_lambda': 0.013378598152516429, 'subsample': 0.5986571074522543, 'min_child_weight': 0.9497733558064256, 'colsample_bytree': 0.6230341406761042, 'gamma': 5.631900439288703}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 37 < 38; dropping {'n_estimators': 5831, 'depth': 8, 'learning_rate': 0.013878428298327835, 'reg_alpha': 0.05272988320889208, 'reg_lambda': 0.013378598152516429, 'subsample': 0.5986571074522543, 'min_child_weight': 0.9497733558064256, 'colsample_bytree': 0.6230341406761042, 'gamma': 5.631900439288703, 'value': 0.8568864060704979}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568864060704979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:40:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:40:38,278]\u001b[0m Trial 38 finished with value: 0.8569086363324958 and parameters: {'n_estimators': 6588, 'depth': 3, 'learning_rate': 0.0021441204323839652, 'reg_alpha': 0.01446673767634864, 'reg_lambda': 0.20497350718019117, 'subsample': 0.6556529863513626, 'min_child_weight': 2.6636099902218446, 'colsample_bytree': 0.5211646027411042, 'gamma': 8.253623844455348}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 38 < 39; dropping {'n_estimators': 6588, 'depth': 3, 'learning_rate': 0.0021441204323839652, 'reg_alpha': 0.01446673767634864, 'reg_lambda': 0.20497350718019117, 'subsample': 0.6556529863513626, 'min_child_weight': 2.6636099902218446, 'colsample_bytree': 0.5211646027411042, 'gamma': 8.253623844455348, 'value': 0.8569086363324958}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569086363324958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:40:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:41:10,826]\u001b[0m Trial 39 finished with value: 0.8569110576392487 and parameters: {'n_estimators': 7108, 'depth': 8, 'learning_rate': 0.004421869370624678, 'reg_alpha': 0.15960883831209602, 'reg_lambda': 0.36238723312150584, 'subsample': 0.7015192765029804, 'min_child_weight': 4.218490430648032, 'colsample_bytree': 0.5037657506628094, 'gamma': 9.288221846194357}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 39 < 40; dropping {'n_estimators': 7108, 'depth': 8, 'learning_rate': 0.004421869370624678, 'reg_alpha': 0.15960883831209602, 'reg_lambda': 0.36238723312150584, 'subsample': 0.7015192765029804, 'min_child_weight': 4.218490430648032, 'colsample_bytree': 0.5037657506628094, 'gamma': 9.288221846194357, 'value': 0.8569110576392487}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569110576392487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:41:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:41:38,561]\u001b[0m Trial 40 finished with value: 0.8567318874895611 and parameters: {'n_estimators': 9272, 'depth': 6, 'learning_rate': 0.2956614597706401, 'reg_alpha': 0.004941760815546605, 'reg_lambda': 2.645876677055612, 'subsample': 0.5887663591658137, 'min_child_weight': 6.002229784098234, 'colsample_bytree': 0.5619507815615987, 'gamma': 8.628662244315043}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 40 < 41; dropping {'n_estimators': 9272, 'depth': 6, 'learning_rate': 0.2956614597706401, 'reg_alpha': 0.004941760815546605, 'reg_lambda': 2.645876677055612, 'subsample': 0.5887663591658137, 'min_child_weight': 6.002229784098234, 'colsample_bytree': 0.5619507815615987, 'gamma': 8.628662244315043, 'value': 0.8567318874895611}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8567318874895611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:41:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:42:05,640]\u001b[0m Trial 41 finished with value: 0.8569154894516086 and parameters: {'n_estimators': 6983, 'depth': 7, 'learning_rate': 0.00689666976542476, 'reg_alpha': 0.0027249521748128, 'reg_lambda': 0.24769351426219993, 'subsample': 0.5974905433921965, 'min_child_weight': 4.686795335693375, 'colsample_bytree': 0.5342456518862059, 'gamma': 8.041880345471583}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 41 < 42; dropping {'n_estimators': 6983, 'depth': 7, 'learning_rate': 0.00689666976542476, 'reg_alpha': 0.0027249521748128, 'reg_lambda': 0.24769351426219993, 'subsample': 0.5974905433921965, 'min_child_weight': 4.686795335693375, 'colsample_bytree': 0.5342456518862059, 'gamma': 8.041880345471583, 'value': 0.8569154894516086}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569154894516086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:42:26,311]\u001b[0m Trial 42 finished with value: 0.8569106157880164 and parameters: {'n_estimators': 6121, 'depth': 8, 'learning_rate': 0.017259718643401183, 'reg_alpha': 0.001989885057975289, 'reg_lambda': 0.26735854981695023, 'subsample': 0.6203300184367349, 'min_child_weight': 4.487912672728222, 'colsample_bytree': 0.5564040818648989, 'gamma': 9.957463989346724}. Best is trial 31 with value: 0.8569165048544404.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 42 < 43; dropping {'n_estimators': 6121, 'depth': 8, 'learning_rate': 0.017259718643401183, 'reg_alpha': 0.001989885057975289, 'reg_lambda': 0.26735854981695023, 'subsample': 0.6203300184367349, 'min_child_weight': 4.487912672728222, 'colsample_bytree': 0.5564040818648989, 'gamma': 9.957463989346724, 'value': 0.8569106157880164}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569106157880164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:43:00,279]\u001b[0m Trial 43 finished with value: 0.8569198162136754 and parameters: {'n_estimators': 8398, 'depth': 7, 'learning_rate': 0.005152495267590912, 'reg_alpha': 0.003501877866082655, 'reg_lambda': 0.5892650659577978, 'subsample': 0.5321195352481216, 'min_child_weight': 1.7950489828988663, 'colsample_bytree': 0.6100580666401978, 'gamma': 7.836182280294878}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 43 < 44; dropping {'n_estimators': 8398, 'depth': 7, 'learning_rate': 0.005152495267590912, 'reg_alpha': 0.003501877866082655, 'reg_lambda': 0.5892650659577978, 'subsample': 0.5321195352481216, 'min_child_weight': 1.7950489828988663, 'colsample_bytree': 0.6100580666401978, 'gamma': 7.836182280294878, 'value': 0.8569198162136754}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569198162136754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:43:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:43:36,745]\u001b[0m Trial 44 finished with value: 0.8569086894826441 and parameters: {'n_estimators': 8367, 'depth': 8, 'learning_rate': 0.004945324856536367, 'reg_alpha': 0.011721886473175399, 'reg_lambda': 0.6494801521781453, 'subsample': 0.5318179757314384, 'min_child_weight': 2.083166757591901, 'colsample_bytree': 0.6153316026166756, 'gamma': 6.69244318537931}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 44 < 45; dropping {'n_estimators': 8367, 'depth': 8, 'learning_rate': 0.004945324856536367, 'reg_alpha': 0.011721886473175399, 'reg_lambda': 0.6494801521781453, 'subsample': 0.5318179757314384, 'min_child_weight': 2.083166757591901, 'colsample_bytree': 0.6153316026166756, 'gamma': 6.69244318537931, 'value': 0.8569086894826441}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569086894826441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:43:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:44:18,400]\u001b[0m Trial 45 finished with value: 0.8569099848862569 and parameters: {'n_estimators': 7519, 'depth': 7, 'learning_rate': 0.0021278292763556067, 'reg_alpha': 0.3158434640666459, 'reg_lambda': 0.7248642227664047, 'subsample': 0.5372944691108633, 'min_child_weight': 1.5279715319775722, 'colsample_bytree': 0.6631361354560831, 'gamma': 7.7119808784993245}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 45 < 46; dropping {'n_estimators': 7519, 'depth': 7, 'learning_rate': 0.0021278292763556067, 'reg_alpha': 0.3158434640666459, 'reg_lambda': 0.7248642227664047, 'subsample': 0.5372944691108633, 'min_child_weight': 1.5279715319775722, 'colsample_bytree': 0.6631361354560831, 'gamma': 7.7119808784993245, 'value': 0.8569099848862569}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569099848862569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:44:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:45:15,488]\u001b[0m Trial 46 finished with value: 0.8569123289427942 and parameters: {'n_estimators': 9403, 'depth': 8, 'learning_rate': 0.001488748210746428, 'reg_alpha': 0.005761903175873231, 'reg_lambda': 1.4680709514285828, 'subsample': 0.5209686860699321, 'min_child_weight': 3.588282758285901, 'colsample_bytree': 0.6386443413238928, 'gamma': 7.378203052278044}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 46 < 47; dropping {'n_estimators': 9403, 'depth': 8, 'learning_rate': 0.001488748210746428, 'reg_alpha': 0.005761903175873231, 'reg_lambda': 1.4680709514285828, 'subsample': 0.5209686860699321, 'min_child_weight': 3.588282758285901, 'colsample_bytree': 0.6386443413238928, 'gamma': 7.378203052278044, 'value': 0.8569123289427942}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569123289427942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:45:16] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:45:36,849]\u001b[0m Trial 47 finished with value: 0.8569142562981693 and parameters: {'n_estimators': 4630, 'depth': 7, 'learning_rate': 0.005557997996561788, 'reg_alpha': 0.0032820955889409685, 'reg_lambda': 0.0030341415509801983, 'subsample': 0.5719096847724411, 'min_child_weight': 2.7644662777419864, 'colsample_bytree': 0.917951518973829, 'gamma': 9.247322280543981}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 47 < 48; dropping {'n_estimators': 4630, 'depth': 7, 'learning_rate': 0.005557997996561788, 'reg_alpha': 0.0032820955889409685, 'reg_lambda': 0.0030341415509801983, 'subsample': 0.5719096847724411, 'min_child_weight': 2.7644662777419864, 'colsample_bytree': 0.917951518973829, 'gamma': 9.247322280543981, 'value': 0.8569142562981693}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569142562981693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:45:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:46:26,272]\u001b[0m Trial 48 finished with value: 0.8569095747851132 and parameters: {'n_estimators': 8401, 'depth': 8, 'learning_rate': 0.0016467360336050545, 'reg_alpha': 0.0016559264310235745, 'reg_lambda': 4.3639854708378385, 'subsample': 0.6722603601285471, 'min_child_weight': 0.6780242006077817, 'colsample_bytree': 0.5949677386835913, 'gamma': 8.418302961894888}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 48 < 49; dropping {'n_estimators': 8401, 'depth': 8, 'learning_rate': 0.0016467360336050545, 'reg_alpha': 0.0016559264310235745, 'reg_lambda': 4.3639854708378385, 'subsample': 0.6722603601285471, 'min_child_weight': 0.6780242006077817, 'colsample_bytree': 0.5949677386835913, 'gamma': 8.418302961894888, 'value': 0.8569095747851132}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569095747851132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:46:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:47:07,265]\u001b[0m Trial 49 finished with value: 0.8568835153124359 and parameters: {'n_estimators': 6462, 'depth': 6, 'learning_rate': 0.003945468625362624, 'reg_alpha': 0.007760199394902381, 'reg_lambda': 0.11266106636485763, 'subsample': 0.5492515496099784, 'min_child_weight': 1.2645880998749695, 'colsample_bytree': 0.5749884330185173, 'gamma': 3.9719046677849033}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 49 < 50; dropping {'n_estimators': 6462, 'depth': 6, 'learning_rate': 0.003945468625362624, 'reg_alpha': 0.007760199394902381, 'reg_lambda': 0.11266106636485763, 'subsample': 0.5492515496099784, 'min_child_weight': 1.2645880998749695, 'colsample_bytree': 0.5749884330185173, 'gamma': 3.9719046677849033, 'value': 0.8568835153124359}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568835153124359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:47:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:48:05,619]\u001b[0m Trial 50 finished with value: 0.8568472584613192 and parameters: {'n_estimators': 5722, 'depth': 7, 'learning_rate': 0.0024523963341764603, 'reg_alpha': 0.00424473536057114, 'reg_lambda': 2.4945090919345496, 'subsample': 0.6426202064947102, 'min_child_weight': 8.41095864999208, 'colsample_bytree': 0.8562762917677651, 'gamma': 1.1587226167876357}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 50 < 51; dropping {'n_estimators': 5722, 'depth': 7, 'learning_rate': 0.0024523963341764603, 'reg_alpha': 0.00424473536057114, 'reg_lambda': 2.4945090919345496, 'subsample': 0.6426202064947102, 'min_child_weight': 8.41095864999208, 'colsample_bytree': 0.8562762917677651, 'gamma': 1.1587226167876357, 'value': 0.8568472584613192}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568472584613192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:48:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:48:31,365]\u001b[0m Trial 51 finished with value: 0.8569116085907852 and parameters: {'n_estimators': 6950, 'depth': 5, 'learning_rate': 0.006575340535293173, 'reg_alpha': 0.0024865930733404483, 'reg_lambda': 0.41617078002579694, 'subsample': 0.6000234180832185, 'min_child_weight': 5.176876915355959, 'colsample_bytree': 0.5429840155834513, 'gamma': 8.00234192865881}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 51 < 52; dropping {'n_estimators': 6950, 'depth': 5, 'learning_rate': 0.006575340535293173, 'reg_alpha': 0.0024865930733404483, 'reg_lambda': 0.41617078002579694, 'subsample': 0.6000234180832185, 'min_child_weight': 5.176876915355959, 'colsample_bytree': 0.5429840155834513, 'gamma': 8.00234192865881, 'value': 0.8569116085907852}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569116085907852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:48:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:49:01,292]\u001b[0m Trial 52 finished with value: 0.8569121621923292 and parameters: {'n_estimators': 8352, 'depth': 7, 'learning_rate': 0.0070776199527980855, 'reg_alpha': 0.001105290048491268, 'reg_lambda': 0.05053237983138507, 'subsample': 0.58347509552859, 'min_child_weight': 3.8822693160041344, 'colsample_bytree': 0.5130001651122527, 'gamma': 8.861456312391875}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 52 < 53; dropping {'n_estimators': 8352, 'depth': 7, 'learning_rate': 0.0070776199527980855, 'reg_alpha': 0.001105290048491268, 'reg_lambda': 0.05053237983138507, 'subsample': 0.58347509552859, 'min_child_weight': 3.8822693160041344, 'colsample_bytree': 0.5130001651122527, 'gamma': 8.861456312391875, 'value': 0.8569121621923292}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569121621923292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:02] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:49:27,763]\u001b[0m Trial 53 finished with value: 0.85691554735177 and parameters: {'n_estimators': 7271, 'depth': 7, 'learning_rate': 0.010324002978485205, 'reg_alpha': 0.0018369958263996355, 'reg_lambda': 0.2531769281825454, 'subsample': 0.5069565925827056, 'min_child_weight': 6.064351640277105, 'colsample_bytree': 0.5659537579223148, 'gamma': 7.547338343500332}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 53 < 54; dropping {'n_estimators': 7271, 'depth': 7, 'learning_rate': 0.010324002978485205, 'reg_alpha': 0.0018369958263996355, 'reg_lambda': 0.2531769281825454, 'subsample': 0.5069565925827056, 'min_child_weight': 6.064351640277105, 'colsample_bytree': 0.5659537579223148, 'gamma': 7.547338343500332, 'value': 0.85691554735177}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.85691554735177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:49:55,636]\u001b[0m Trial 54 finished with value: 0.8569041222699066 and parameters: {'n_estimators': 7725, 'depth': 8, 'learning_rate': 0.010477326633654799, 'reg_alpha': 0.001574176722812077, 'reg_lambda': 0.13391349377815584, 'subsample': 0.5101721048709404, 'min_child_weight': 7.050723094153504, 'colsample_bytree': 0.6372957626914384, 'gamma': 7.60355304823981}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 54 < 55; dropping {'n_estimators': 7725, 'depth': 8, 'learning_rate': 0.010477326633654799, 'reg_alpha': 0.001574176722812077, 'reg_lambda': 0.13391349377815584, 'subsample': 0.5101721048709404, 'min_child_weight': 7.050723094153504, 'colsample_bytree': 0.6372957626914384, 'gamma': 7.60355304823981, 'value': 0.8569041222699066}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569041222699066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:56] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:50:19,730]\u001b[0m Trial 55 finished with value: 0.8568920953363646 and parameters: {'n_estimators': 7351, 'depth': 6, 'learning_rate': 0.024370482961329962, 'reg_alpha': 0.004197795368362081, 'reg_lambda': 1.0149230721857982, 'subsample': 0.530055205044762, 'min_child_weight': 5.77255454551217, 'colsample_bytree': 0.6087382626169092, 'gamma': 7.164704810272058}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 55 < 56; dropping {'n_estimators': 7351, 'depth': 6, 'learning_rate': 0.024370482961329962, 'reg_alpha': 0.004197795368362081, 'reg_lambda': 1.0149230721857982, 'subsample': 0.530055205044762, 'min_child_weight': 5.77255454551217, 'colsample_bytree': 0.6087382626169092, 'gamma': 7.164704810272058, 'value': 0.8568920953363646}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568920953363646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:50:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:50:40,753]\u001b[0m Trial 56 finished with value: 0.8569168926055217 and parameters: {'n_estimators': 6131, 'depth': 8, 'learning_rate': 0.015702650853240047, 'reg_alpha': 0.01104309966265359, 'reg_lambda': 0.6954270024091173, 'subsample': 0.5470242359968996, 'min_child_weight': 6.045592195444086, 'colsample_bytree': 0.569351780921409, 'gamma': 8.568472766305646}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 56 < 57; dropping {'n_estimators': 6131, 'depth': 8, 'learning_rate': 0.015702650853240047, 'reg_alpha': 0.01104309966265359, 'reg_lambda': 0.6954270024091173, 'subsample': 0.5470242359968996, 'min_child_weight': 6.045592195444086, 'colsample_bytree': 0.569351780921409, 'gamma': 8.568472766305646, 'value': 0.8569168926055217}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569168926055217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:50:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:51:00,946]\u001b[0m Trial 57 finished with value: 0.8569083443816816 and parameters: {'n_estimators': 6464, 'depth': 8, 'learning_rate': 0.04418804318525734, 'reg_alpha': 0.006101047300691368, 'reg_lambda': 0.5806999501147557, 'subsample': 0.5526016790817966, 'min_child_weight': 8.24413182368351, 'colsample_bytree': 0.5890069847681919, 'gamma': 9.766177827262702}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 57 < 58; dropping {'n_estimators': 6464, 'depth': 8, 'learning_rate': 0.04418804318525734, 'reg_alpha': 0.006101047300691368, 'reg_lambda': 0.5806999501147557, 'subsample': 0.5526016790817966, 'min_child_weight': 8.24413182368351, 'colsample_bytree': 0.5890069847681919, 'gamma': 9.766177827262702, 'value': 0.8569083443816816}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569083443816816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:51:18,935]\u001b[0m Trial 58 finished with value: 0.8569062130257376 and parameters: {'n_estimators': 4975, 'depth': 8, 'learning_rate': 0.013821795311471681, 'reg_alpha': 0.024021482170833566, 'reg_lambda': 1.4443745881884387, 'subsample': 0.6226462290711287, 'min_child_weight': 5.304115509889047, 'colsample_bytree': 0.5023928449827151, 'gamma': 8.560288006262232}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 58 < 59; dropping {'n_estimators': 4975, 'depth': 8, 'learning_rate': 0.013821795311471681, 'reg_alpha': 0.024021482170833566, 'reg_lambda': 1.4443745881884387, 'subsample': 0.6226462290711287, 'min_child_weight': 5.304115509889047, 'colsample_bytree': 0.5023928449827151, 'gamma': 8.560288006262232, 'value': 0.8569062130257376}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569062130257376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:51:50,598]\u001b[0m Trial 59 finished with value: 0.8569004184595771 and parameters: {'n_estimators': 6159, 'depth': 8, 'learning_rate': 0.0032925027892580644, 'reg_alpha': 0.010786698327584684, 'reg_lambda': 0.8804690528545144, 'subsample': 0.7938929612497877, 'min_child_weight': 6.945964553178131, 'colsample_bytree': 0.6564888123674865, 'gamma': 9.022010578681842}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 59 < 60; dropping {'n_estimators': 6159, 'depth': 8, 'learning_rate': 0.0032925027892580644, 'reg_alpha': 0.010786698327584684, 'reg_lambda': 0.8804690528545144, 'subsample': 0.7938929612497877, 'min_child_weight': 6.945964553178131, 'colsample_bytree': 0.6564888123674865, 'gamma': 9.022010578681842, 'value': 0.8569004184595771}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569004184595771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:53:21,838]\u001b[0m Trial 60 finished with value: 0.8568525504760781 and parameters: {'n_estimators': 7902, 'depth': 8, 'learning_rate': 0.0012514477303947895, 'reg_alpha': 0.06376677905874807, 'reg_lambda': 6.648021077057055, 'subsample': 0.5736987296052254, 'min_child_weight': 1.956912360578048, 'colsample_bytree': 0.7999072482814998, 'gamma': 2.673010945363621}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 60 < 61; dropping {'n_estimators': 7902, 'depth': 8, 'learning_rate': 0.0012514477303947895, 'reg_alpha': 0.06376677905874807, 'reg_lambda': 6.648021077057055, 'subsample': 0.5736987296052254, 'min_child_weight': 1.956912360578048, 'colsample_bytree': 0.7999072482814998, 'gamma': 2.673010945363621, 'value': 0.8568525504760781}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8568525504760781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:53:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:53:43,737]\u001b[0m Trial 61 finished with value: 0.8569126404936631 and parameters: {'n_estimators': 6741, 'depth': 7, 'learning_rate': 0.01681468066890726, 'reg_alpha': 0.0015201649814034636, 'reg_lambda': 0.4116542165445966, 'subsample': 0.539884243375963, 'min_child_weight': 5.863466711163901, 'colsample_bytree': 0.5605268415451673, 'gamma': 8.152167490697154}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 61 < 62; dropping {'n_estimators': 6741, 'depth': 7, 'learning_rate': 0.01681468066890726, 'reg_alpha': 0.0015201649814034636, 'reg_lambda': 0.4116542165445966, 'subsample': 0.539884243375963, 'min_child_weight': 5.863466711163901, 'colsample_bytree': 0.5605268415451673, 'gamma': 8.152167490697154, 'value': 0.8569126404936631}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569126404936631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:53:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-31 16:54:07,388]\u001b[0m Trial 62 finished with value: 0.8569073103287979 and parameters: {'n_estimators': 5575, 'depth': 8, 'learning_rate': 0.00533803873214827, 'reg_alpha': 0.0010438635300909136, 'reg_lambda': 0.15744353596298255, 'subsample': 0.5004542401341248, 'min_child_weight': 6.043925362080036, 'colsample_bytree': 0.5804971152677676, 'gamma': 8.616686127079127}. Best is trial 43 with value: 0.8569198162136754.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 62 < 63; dropping {'n_estimators': 5575, 'depth': 8, 'learning_rate': 0.00533803873214827, 'reg_alpha': 0.0010438635300909136, 'reg_lambda': 0.15744353596298255, 'subsample': 0.5004542401341248, 'min_child_weight': 6.043925362080036, 'colsample_bytree': 0.5804971152677676, 'gamma': 8.616686127079127, 'value': 0.8569073103287979}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of XGBoost = 0.8569073103287979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-704bbf3bc677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwandbc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoostError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudypath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf\"optuna_xgboost_lv2_study_{x}trials_{datetime.now().strftime('%Y%m%d')}.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     dump(study.best_trial.params, filename=datapath/f'optuna_lightgbm_study_best-thru-{x*5}trials_20210927.joblib')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-df05bdc3ca1a>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     39\u001b[0m     )    \n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m# generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         )\n\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[0;32m--> 189\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1500\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(1, 500):\n",
    "    study.optimize(objective, n_trials = 1, callbacks = [wandbc], show_progress_bar=False, catch=(xgboost.core.XGBoostError,)) \n",
    "    dump(study, filename=studypath/f\"optuna_xgboost_lv2_study_{x}trials_{datetime.now().strftime('%Y%m%d')}.joblib\")\n",
    "#     dump(study.best_trial.params, filename=datapath/f'optuna_lightgbm_study_best-thru-{x*5}trials_20210927.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d6fb3-b4b3-40bd-8ff9-e2919c234f7d",
   "metadata": {
    "id": "27a746ff-c0e1-4218-8809-f102a58d2491"
   },
   "outputs": [],
   "source": [
    "# dump(study, filename=datapath/f\"optuna_xgboost_100trials-complete_{datetime.now().strftime('%Y%m%d')}.joblib\")\n",
    "# dump(study.best_trial.params, filename=datapath/f\"optuna_lightgbm_all-500trials-best_{datetime.now().strftime('%Y%m%d')}.joblib\")\n",
    "# pickle.dump(study.best_trial.params, open('CatBoost_Hyperparameter.pickle', 'wb'))\n",
    "# print('CatBoost Hyperparameter:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd3e56-55e0-4ea3-b1c2-eb3047165063",
   "metadata": {},
   "source": [
    "Best on original dataset was:\n",
    "\n",
    "```python\n",
    "{'n_estimators': 3878,\n",
    " 'depth': 4,\n",
    " 'learning_rate': 0.024785857161974977,\n",
    " 'reg_alpha': 26.867682044658245,\n",
    " 'reg_lambda': 10.839759074147148,\n",
    " 'subsample': 0.8208581489835881,\n",
    " 'min_child_weight': 8.829122644339664,\n",
    " 'colsample_bytree': 0.906420714280384,\n",
    " 'gamma': 1.472322916021486}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f02e689e-b20c-48e5-a7d9-02467b4f3dbd",
   "metadata": {
    "id": "f02e689e-b20c-48e5-a7d9-02467b4f3dbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 8398,\n",
       " 'depth': 7,\n",
       " 'learning_rate': 0.005152495267590912,\n",
       " 'reg_alpha': 0.003501877866082655,\n",
       " 'reg_lambda': 0.5892650659577978,\n",
       " 'subsample': 0.5321195352481216,\n",
       " 'min_child_weight': 1.7950489828988663,\n",
       " 'colsample_bytree': 0.6100580666401978,\n",
       " 'gamma': 7.836182280294878}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ybeYZ3omaLWK",
   "metadata": {
    "id": "ybeYZ3omaLWK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1181903... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>valid_auc</td><td></td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>valid_auc</td><td>0.85691</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sweep_xgboost_lv2_20211031_X-orig+Kmeans8+synth_dataset_162015</strong>: <a href=\"https://wandb.ai/hushifang/202110_Kaggle_tabular_playground/runs/juld9j4c\" target=\"_blank\">https://wandb.ai/hushifang/202110_Kaggle_tabular_playground/runs/juld9j4c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211031_162016-juld9j4c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'xgboost_params': study.best_trial.params})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b6b43-d07b-49fb-beeb-1482cb5647fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e398cb-f0f4-4400-8fe7-9012b4bc33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sweep_lightgbm_20210922.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
