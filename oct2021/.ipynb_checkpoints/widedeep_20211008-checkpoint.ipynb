{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4e7f70-25a3-4d58-b98a-3a695e55ee53",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "Setting up a more robust baseline notebook, suitable for use with all of the \"Big Three\" (XGBoost, CatBoost, LightGBM) libraries and on either Google Colab or the local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e124c3d-0e1f-4053-8e72-52569a4fe3e4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae2ff1e-bd1f-4cc9-8357-5a88d1746ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two manual flags (ex-config)\n",
    "colab = False\n",
    "gpu_available = True\n",
    "# libraries = ['xgboost', 'lightgbm', 'catboost']\n",
    "libraries = ['fastai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16849bd2-428c-497b-ba3b-675002f8d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d2654b-3bc6-49b5-ade8-cc82112b60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"widedeep_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416d6118-e543-4df4-9219-2d4a63743c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle Google Colab-specific library installation/updating\n",
    "if colab:\n",
    "    # much of the below inspired by or cribbed from the May 2021 Kaggle Tabular Playground winner, at \n",
    "    # https://colab.research.google.com/gist/academicsuspect/0aac7bd6e506f5f70295bfc9a3dc2250/tabular-may-baseline.ipynb?authuser=1#scrollTo=LJoVKJb5wN0L\n",
    "    \n",
    "    # Kaggle API for downloading the datasets\n",
    "    !pip install --upgrade -q kaggle\n",
    "\n",
    "    # weights and biases\n",
    "    !pip install -qqqU wandb\n",
    "    \n",
    "    # Optuna for parameter search\n",
    "    !pip install -q optuna\n",
    "\n",
    "    # upgrade sklearn\n",
    "    !pip install --upgrade scikit-learn\n",
    "\n",
    "    !pip install category_encoders\n",
    "    \n",
    "    if 'catboost' in libraries:\n",
    "        !pip install catboost\n",
    "    \n",
    "    if 'xgboost' in libraries:\n",
    "        if gpu_available: \n",
    "            # this part is from https://github.com/rapidsai/gputreeshap/issues/24\n",
    "            !pip install cmake --upgrade\n",
    "            # !pip install sklearn --upgrade\n",
    "            !git clone --recursive https://github.com/dmlc/xgboost\n",
    "            %cd /content/xgboost\n",
    "            !mkdir build\n",
    "            %cd build\n",
    "            !cmake .. -DUSE_CUDA=ON\n",
    "            !make -j4\n",
    "            %cd /content/xgboost/python-package\n",
    "            !python setup.py install --use-cuda --use-nccl\n",
    "            !/opt/bin/nvidia-smi\n",
    "            !pip install shap\n",
    "        else:\n",
    "            !pip install --upgrade xgboost\n",
    "    if 'lightgbm' in libraries:\n",
    "        if gpu_available:\n",
    "            # lighgbm gpu compatible\n",
    "            !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "            ! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;\n",
    "        else:\n",
    "            !pip install --upgrade lightgbm\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40df194-4474-4bcf-ac5a-98efe24b91fd",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01e85f7-d602-4dde-bef9-611683cd74c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /tmp/pip-req-build-1_ic8ial/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "# from wandb.xgboost import wandb_callback\n",
    "# from wandb.lightgbm import wandb_callback\n",
    "from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "# import timm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler #, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce765a83-270a-4762-915b-17ff3319fbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6166c2-ca44-4b7c-a4dc-3db47c2624fe",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c18a787-2193-43cb-87ee-51c6ae7b6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is the code for reading the train.csv and converting it to a .feather file\n",
    "# df = pd.read_csv(datapath/'train.csv', index_col='id', low_memory=False)\n",
    "# df.index.name = None\n",
    "# df.to_feather(path='./dataset_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a41cd7e-accb-41c4-ad8b-0eaa3e2b0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/sep2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "    datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e47b3-43bd-4d35-b463-9d76100c6ed5",
   "metadata": {},
   "source": [
    "## Ex-Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb288275-a858-4806-9dc0-0b316c334536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "exmodel_config = {\n",
    "    # model config\n",
    "#     \"model\": XGBClassifier,\n",
    "#     \"n_estimators\": 100, \n",
    "#     \"max_depth\": 3,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"test_size\": 0.2,\n",
    "#     \"reg_lambda\": None, \n",
    "    \"library\": 'widedeep',\n",
    "    \"scaler\": StandardScaler, # TODO: experiment with others (but imputation may be slow)\n",
    "    \"scale_b4_impute\": False,\n",
    "    \"imputer\": SimpleImputer(strategy='median', add_indicator=True),\n",
    "#     \"knn_imputer_n_neighbors\": None, # None if a different imputer is used\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "    'random_state': 42,\n",
    "    'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "    'features_categorized': True,\n",
    "#     'subsample': 1,\n",
    "#     'cross_val_strategy': KFold, # None for holdout, or the relevant sklearn class\n",
    "#     'kfolds': 5, # if 1, that means just doing holdout\n",
    "#     'test_size': 0.2,\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d9012-34f1-435a-ba16-4416e0d4a286",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912a62f-970a-48b4-b428-d886f2612fc2",
   "metadata": {},
   "source": [
    "Due to the importance of identifying categorical variables for deep learning on tabular data (namely, the generation of embeddings containing meaningful information about them), I'm going to try using `fastai`'s `cont_cat_split` on the original dataset (post-imputation and generation of features based on summary statistics) and then proceeding with the other transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a3ac2c-37e8-4372-b64e-225f6dc3c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here's how to load the original, unaltered dataset and separate features from targets\n",
    "# df = pd.read_feather(path=datapath/'dataset_df.feather') # this is the unaltered original dataset\n",
    "# features = [x for x in df.columns if x != 'claim']\n",
    "# X = df[features]\n",
    "# y = df.claim\n",
    "\n",
    "\n",
    "\n",
    "# load the version of the dataset with imputations; X and y were stored separately, as feather and joblib respectively\n",
    "X = pd.read_feather(datapath/'X_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather') \n",
    "y = load(datapath/'y.joblib')    \n",
    "X.index.name = 'id'\n",
    "y.index.name = 'id'\n",
    "\n",
    "exmodel_config['feature_count'] = len(X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf791b86-a049-439b-a92a-fbbad524682e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f73f73e-393d-4ea0-ac2b-f5805a5b5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "113d4b09-4800-406b-b79f-cd36fce94c47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      147965\n",
       "1      117427\n",
       "2      274624\n",
       "3      265618\n",
       "4      125414\n",
       "5      286473\n",
       "6      201975\n",
       "7      209765\n",
       "8      368026\n",
       "9      370266\n",
       "10     157265\n",
       "11     325095\n",
       "12     117373\n",
       "13     231413\n",
       "14     308572\n",
       "15     313089\n",
       "16      77994\n",
       "17     172559\n",
       "18     160073\n",
       "19     318317\n",
       "20     313905\n",
       "21      83438\n",
       "22     148460\n",
       "23     176988\n",
       "24     208329\n",
       "25     333767\n",
       "26     290999\n",
       "27     269916\n",
       "28     233936\n",
       "29     135072\n",
       "30     263378\n",
       "31     297891\n",
       "32     281103\n",
       "33     245721\n",
       "34     357008\n",
       "35     316292\n",
       "36     210948\n",
       "37      38267\n",
       "38     229893\n",
       "39     223437\n",
       "40     307516\n",
       "41     264543\n",
       "42      81540\n",
       "43     213949\n",
       "44     245352\n",
       "45      84044\n",
       "46     212671\n",
       "47      87459\n",
       "48     245196\n",
       "49     127050\n",
       "50     305728\n",
       "51     212595\n",
       "52     342517\n",
       "53       7459\n",
       "54     117101\n",
       "55     240290\n",
       "56     246959\n",
       "57     163484\n",
       "58      76378\n",
       "59     158582\n",
       "60     245796\n",
       "61     266661\n",
       "62     194153\n",
       "63     267355\n",
       "64     142378\n",
       "65     149027\n",
       "66     213419\n",
       "67     209319\n",
       "68      14049\n",
       "69     210462\n",
       "70      30980\n",
       "71     203071\n",
       "72     357058\n",
       "73     346323\n",
       "74     227088\n",
       "75      78697\n",
       "76     292411\n",
       "77     183952\n",
       "78     292606\n",
       "79     212278\n",
       "80      33486\n",
       "81     214663\n",
       "82     306521\n",
       "83     339635\n",
       "84     218333\n",
       "85     285557\n",
       "86     213305\n",
       "87     193832\n",
       "88     205000\n",
       "89     107019\n",
       "90     117596\n",
       "91     362662\n",
       "92     199921\n",
       "93     203994\n",
       "94     222502\n",
       "95     306483\n",
       "96        429\n",
       "97     335221\n",
       "98      41905\n",
       "99     190383\n",
       "100    223050\n",
       "101    261000\n",
       "102    312859\n",
       "103    318262\n",
       "104    155296\n",
       "105    223838\n",
       "106    252276\n",
       "107    305686\n",
       "108    207140\n",
       "109    206037\n",
       "110     44838\n",
       "111    328337\n",
       "112    291842\n",
       "113    316128\n",
       "114     12970\n",
       "115    336136\n",
       "116    200080\n",
       "117    164263\n",
       "118        15\n",
       "119       135\n",
       "120    957919\n",
       "121    183098\n",
       "122    957919\n",
       "123    957919\n",
       "124    957919\n",
       "125    957919\n",
       "126    218219\n",
       "127    191646\n",
       "128         2\n",
       "129         2\n",
       "130         2\n",
       "131         2\n",
       "132         2\n",
       "133         2\n",
       "134         2\n",
       "135         2\n",
       "136         2\n",
       "137         2\n",
       "138         2\n",
       "139         2\n",
       "140         2\n",
       "141         2\n",
       "142         2\n",
       "143         2\n",
       "144         2\n",
       "145         2\n",
       "146         2\n",
       "147         2\n",
       "148         2\n",
       "149         2\n",
       "150         2\n",
       "151         2\n",
       "152         2\n",
       "153         2\n",
       "154         2\n",
       "155         2\n",
       "156         2\n",
       "157         2\n",
       "158         2\n",
       "159         2\n",
       "160         2\n",
       "161         2\n",
       "162         2\n",
       "163         2\n",
       "164         2\n",
       "165         2\n",
       "166         2\n",
       "167         2\n",
       "168         2\n",
       "169         2\n",
       "170         2\n",
       "171         2\n",
       "172         2\n",
       "173         2\n",
       "174         2\n",
       "175         2\n",
       "176         2\n",
       "177         2\n",
       "178         2\n",
       "179         2\n",
       "180         2\n",
       "181         2\n",
       "182         2\n",
       "183         2\n",
       "184         2\n",
       "185         2\n",
       "186         2\n",
       "187         2\n",
       "188         2\n",
       "189         2\n",
       "190         2\n",
       "191         2\n",
       "192         2\n",
       "193         2\n",
       "194         2\n",
       "195         2\n",
       "196         2\n",
       "197         2\n",
       "198         2\n",
       "199         2\n",
       "200         2\n",
       "201         2\n",
       "202         2\n",
       "203         2\n",
       "204         2\n",
       "205         2\n",
       "206         2\n",
       "207         2\n",
       "208         2\n",
       "209         2\n",
       "210         2\n",
       "211         2\n",
       "212         2\n",
       "213         2\n",
       "214         2\n",
       "215         2\n",
       "216         2\n",
       "217         2\n",
       "218         2\n",
       "219         2\n",
       "220         2\n",
       "221         2\n",
       "222         2\n",
       "223         2\n",
       "224         2\n",
       "225         2\n",
       "226         2\n",
       "227         2\n",
       "228         2\n",
       "229         2\n",
       "230         2\n",
       "231         2\n",
       "232         2\n",
       "233         2\n",
       "234         2\n",
       "235         2\n",
       "236         2\n",
       "237         2\n",
       "238         2\n",
       "239         2\n",
       "240         2\n",
       "241         2\n",
       "242         2\n",
       "243         2\n",
       "244         2\n",
       "245         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eae57f-6a1e-4d5f-8355-62ff4c2a2bcf",
   "metadata": {},
   "source": [
    "I'll follow JH's advice here and set 10k as the ceiling for the `max_card=`; this might be something to experiment with later using Optuna. (Esp. as JH says that numbers in the 5000s and higher make him very nervous.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b2ffcd-49c8-4cf2-885e-a9dd0e640869",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_card_embed = 10000\n",
    "max_card_cat = 100000\n",
    "exmodel_config['max_card_for_embedding'] = max_card_embed\n",
    "exmodel_config['max_card_for_categorical'] = max_card_cat\n",
    "\n",
    "\n",
    "# X_orig = X.iloc[:, :118] # excluding summary, meta-statistics\n",
    "# X_meta = X.iloc[:, 118:] # including summary, meta-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53f9d86-dce7-46db-83c1-929dc1ec7ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exmodel_config['max_card_for_categorical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86b704d8-ec62-4d98-8410-1e636e492178",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_card_features = [f for f in X.columns if X[f].nunique() <= 50000]\n",
    "high_card_features = [f for f in X.columns if X[f].nunique() > 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0b2e8b-50da-4656-b14b-2af467387223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(low_card_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9d9f790-8b15-4011-ab42-2b342a455e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_card_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbd183-5501-4dcd-870e-b5f2eae70749",
   "metadata": {},
   "source": [
    "# WideDeep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5a9cb-9524-4d16-9a98-d00ea8f99f03",
   "metadata": {},
   "source": [
    "## (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f712681f-f201-4c6b-9983-bd95f02ecb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(datapath/\"adult.csv.zip\")\n",
    "# df[\"income_label\"] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "# df.drop(\"income\", axis=1, inplace=True)\n",
    "# df_train, df_test = train_test_split(df, test_size=0.2, stratify=df.income_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb4e450b-08f7-4e02-9a6d-233b7225466a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for f in df.columns:\n",
    "#     print(f\"{f}: {df[f].nunique()}\")\n",
    "#     print(f\"NaNs: {df[f].isna().sum()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c3ae8b-e0a2-4948-9dee-1dc6a62765c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3256738c-6bf9-4e63-8e69-82645bbe11f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 957919 entries, 0 to 957918\n",
      "Columns: 246 entries, 0 to 245\n",
      "dtypes: float64(246)\n",
      "memory usage: 1.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7f230f28-cf7a-4f1f-ab9c-3e0075cbebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide_cols_pre = [f for f in X.columns if X[f].nunique() <= max_card_cat and X[f].nunique() > 2]\n",
    "# wide_cols_onehot = [f for f in X.columns if X[f].nunique() == 2]\n",
    "wide_cols = [f for f in X.columns if X[f].nunique() <= max_card_cat]\n",
    "cont_cols = high_card_features\n",
    "embed_cols = [f for f in X.columns if X[f].nunique() <= max_card_embed and X[f].nunique() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d37360c-a8c0-4393-b616-f787ef46f7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 117, 4)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wide_cols), len(cont_cols), len(embed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3be34d81-a953-4183-9fca-2d14e5cc50c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 21,\n",
       " 37,\n",
       " 42,\n",
       " 45,\n",
       " 47,\n",
       " 53,\n",
       " 58,\n",
       " 68,\n",
       " 70,\n",
       " 75,\n",
       " 80,\n",
       " 96,\n",
       " 98,\n",
       " 110,\n",
       " 114,\n",
       " 118,\n",
       " 119,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0622ca79-c708-4b94-b98d-613f16ff42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_np = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e400b37c-656a-41eb-b7be-9125484be23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06ebc1ab-0e32-4ae7-ab18-62fb345481cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "X_wide_pre = wide_preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e4f4953-f2bc-4d58-86ec-f7228b7e00cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 136)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9700db34-7b28-4e8c-82d2-63d009295fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,  77995, 161433, ..., 794314, 794316, 794318],\n",
       "       [     2,  77996, 161434, ..., 794314, 794316, 794318],\n",
       "       [     3,  77997, 161435, ..., 794314, 794316, 794318],\n",
       "       ...,\n",
       "       [     8,  78002, 161440, ..., 794314, 794316, 794318],\n",
       "       [     9,  78003, 161441, ..., 794314, 794316, 794318],\n",
       "       [    10,  78004, 161442, ..., 794314, 794316, 794318]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_pre[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b40fbb61-9229-40b8-bc29-a4045521a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide_pre_df = pd.DataFrame(X_wide_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf8db2e8-2be9-4615-a2ce-1899d1ea965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>77995</td>\n",
       "      <td>161433</td>\n",
       "      <td>199700</td>\n",
       "      <td>281240</td>\n",
       "      <td>365284</td>\n",
       "      <td>452743</td>\n",
       "      <td>460202</td>\n",
       "      <td>536580</td>\n",
       "      <td>550629</td>\n",
       "      <td>581609</td>\n",
       "      <td>660306</td>\n",
       "      <td>693792</td>\n",
       "      <td>694221</td>\n",
       "      <td>736126</td>\n",
       "      <td>780964</td>\n",
       "      <td>793934</td>\n",
       "      <td>793949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>77996</td>\n",
       "      <td>161434</td>\n",
       "      <td>199701</td>\n",
       "      <td>281241</td>\n",
       "      <td>365285</td>\n",
       "      <td>452744</td>\n",
       "      <td>460203</td>\n",
       "      <td>536581</td>\n",
       "      <td>550630</td>\n",
       "      <td>581610</td>\n",
       "      <td>660307</td>\n",
       "      <td>693793</td>\n",
       "      <td>694222</td>\n",
       "      <td>736127</td>\n",
       "      <td>780965</td>\n",
       "      <td>793935</td>\n",
       "      <td>793950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>77997</td>\n",
       "      <td>161435</td>\n",
       "      <td>199702</td>\n",
       "      <td>281242</td>\n",
       "      <td>365286</td>\n",
       "      <td>452745</td>\n",
       "      <td>460204</td>\n",
       "      <td>536582</td>\n",
       "      <td>550631</td>\n",
       "      <td>581611</td>\n",
       "      <td>660308</td>\n",
       "      <td>693794</td>\n",
       "      <td>694223</td>\n",
       "      <td>736128</td>\n",
       "      <td>780966</td>\n",
       "      <td>793936</td>\n",
       "      <td>793951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>77998</td>\n",
       "      <td>161436</td>\n",
       "      <td>199703</td>\n",
       "      <td>281243</td>\n",
       "      <td>365287</td>\n",
       "      <td>452746</td>\n",
       "      <td>460205</td>\n",
       "      <td>536583</td>\n",
       "      <td>550632</td>\n",
       "      <td>581612</td>\n",
       "      <td>660309</td>\n",
       "      <td>693795</td>\n",
       "      <td>694224</td>\n",
       "      <td>736129</td>\n",
       "      <td>780967</td>\n",
       "      <td>793937</td>\n",
       "      <td>793952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>77999</td>\n",
       "      <td>161437</td>\n",
       "      <td>199704</td>\n",
       "      <td>281244</td>\n",
       "      <td>365288</td>\n",
       "      <td>452747</td>\n",
       "      <td>460206</td>\n",
       "      <td>536584</td>\n",
       "      <td>550633</td>\n",
       "      <td>581613</td>\n",
       "      <td>660310</td>\n",
       "      <td>693794</td>\n",
       "      <td>694225</td>\n",
       "      <td>736130</td>\n",
       "      <td>780968</td>\n",
       "      <td>793938</td>\n",
       "      <td>793953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1       2       3       4       5       6       7       8       9   \\\n",
       "0   1  77995  161433  199700  281240  365284  452743  460202  536580  550629   \n",
       "1   2  77996  161434  199701  281241  365285  452744  460203  536581  550630   \n",
       "2   3  77997  161435  199702  281242  365286  452745  460204  536582  550631   \n",
       "3   4  77998  161436  199703  281243  365287  452746  460205  536583  550632   \n",
       "4   5  77999  161437  199704  281244  365288  452747  460206  536584  550633   \n",
       "\n",
       "       10      11      12      13      14      15      16      17  \n",
       "0  581609  660306  693792  694221  736126  780964  793934  793949  \n",
       "1  581610  660307  693793  694222  736127  780965  793935  793950  \n",
       "2  581611  660308  693794  694223  736128  780966  793936  793951  \n",
       "3  581612  660309  693795  694224  736129  780967  793937  793952  \n",
       "4  581613  660310  693794  694225  736130  780968  793938  793953  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_wide_pre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "619e112c-4969-45eb-bc93-50fb2c079bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>7.814957</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>7.795214</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>7.846019</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>7.784252</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>7.80212</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>7.858285</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>-0.128284</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>-0.128137</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>7.799560</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.128090</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>7.814957</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128187</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.127732</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.127178</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127846</td>\n",
       "      <td>-0.128334</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127711</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.127076</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>-0.128095</td>\n",
       "      <td>-0.127072</td>\n",
       "      <td>7.795214</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127939</td>\n",
       "      <td>-0.127508</td>\n",
       "      <td>7.804170</td>\n",
       "      <td>7.809044</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>-0.127639</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-0.127648</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.127888</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>-0.128992</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.127546</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12769</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>7.806991</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.128804</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.128019</td>\n",
       "      <td>-0.12774</td>\n",
       "      <td>-0.128506</td>\n",
       "      <td>-0.127960</td>\n",
       "      <td>-0.127466</td>\n",
       "      <td>-0.128086</td>\n",
       "      <td>-0.12793</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>-0.128107</td>\n",
       "      <td>-0.128553</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.127825</td>\n",
       "      <td>-0.128498</td>\n",
       "      <td>-0.127956</td>\n",
       "      <td>-0.128322</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>7.801096</td>\n",
       "      <td>-0.128745</td>\n",
       "      <td>-0.128464</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-0.128401</td>\n",
       "      <td>-0.12854</td>\n",
       "      <td>-0.128061</td>\n",
       "      <td>-0.128536</td>\n",
       "      <td>-0.127195</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>7.828890</td>\n",
       "      <td>-0.127487</td>\n",
       "      <td>-0.127597</td>\n",
       "      <td>-0.128183</td>\n",
       "      <td>-0.128779</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>-0.128031</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>-0.12747</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>-0.128015</td>\n",
       "      <td>-0.12817</td>\n",
       "      <td>-0.128275</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.128065</td>\n",
       "      <td>-0.127884</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.127339</td>\n",
       "      <td>7.858285</td>\n",
       "      <td>-0.127352</td>\n",
       "      <td>-0.127968</td>\n",
       "      <td>-0.128355</td>\n",
       "      <td>-0.12761</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>-0.128666</td>\n",
       "      <td>-0.12697</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.127572</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         128       129       130       131       132       133       134  \\\n",
       "id                                                                         \n",
       "0  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "1  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "2  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "3  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "4  -0.127178 -0.126936 -0.128208 -0.128498 -0.127846 -0.128334 -0.128263   \n",
       "\n",
       "         135       136       137      138       139       140       141  \\\n",
       "id                                                                        \n",
       "0  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "1  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "2  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "3  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "4  -0.127711 -0.127186 -0.127076 -0.12793 -0.128637 -0.128095 -0.127072   \n",
       "\n",
       "         142      143       144       145       146       147       148  \\\n",
       "id                                                                        \n",
       "0  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "1  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "2   7.795214 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "3  -0.128284 -0.12801 -0.127939 -0.127508 -0.128137 -0.128057 -0.128052   \n",
       "4   7.795214 -0.12801 -0.127939 -0.127508  7.804170  7.809044 -0.128052   \n",
       "\n",
       "         149       150       151       152       153      154       155  \\\n",
       "id                                                                        \n",
       "0  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "1  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "2  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "3  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "4  -0.127309 -0.127639 -0.128792 -0.128271 -0.127648 -0.12801 -0.127254   \n",
       "\n",
       "         156       157       158       159       160       161       162  \\\n",
       "id                                                                         \n",
       "0  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "1  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "2  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "3  -0.127888 -0.127791 -0.128992 -0.128368  7.799560 -0.127182 -0.127546   \n",
       "4  -0.127888 -0.127791 -0.128992 -0.128368 -0.128212 -0.127182 -0.127546   \n",
       "\n",
       "         163       164       165       166      167       168       169  \\\n",
       "id                                                                        \n",
       "0  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "1  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "2  -0.127669  7.846019 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "3  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "4  -0.127669 -0.127453 -0.127968 -0.128494 -0.12769 -0.127808 -0.127892   \n",
       "\n",
       "         170       171       172       173       174       175      176  \\\n",
       "id                                                                        \n",
       "0  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "1  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "2  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "3  -0.128057 -0.128090 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "4  -0.128057  7.806991 -0.128174 -0.128804 -0.128343 -0.128019 -0.12774   \n",
       "\n",
       "         177       178       179       180      181       182       183  \\\n",
       "id                                                                        \n",
       "0  -0.128506  7.814957 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "1  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "2  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "3  -0.128506  7.814957 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "4  -0.128506 -0.127960 -0.127466 -0.128086 -0.12793 -0.127918 -0.128107   \n",
       "\n",
       "         184       185       186       187       188       189       190  \\\n",
       "id                                                                         \n",
       "0  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "1  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "2  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "3  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "4  -0.128553 -0.128057 -0.127825 -0.128498 -0.127956 -0.128322 -0.127867   \n",
       "\n",
       "         191       192       193       194       195       196       197  \\\n",
       "id                                                                         \n",
       "0  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "1  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "2  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745  7.784252 -0.127242   \n",
       "3  -0.128574 -0.127884 -0.127884 -0.128187 -0.128745 -0.128464 -0.127242   \n",
       "4  -0.128574 -0.127884 -0.127884  7.801096 -0.128745 -0.128464 -0.127242   \n",
       "\n",
       "        198       199       200      201       202       203       204  \\\n",
       "id                                                                       \n",
       "0  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "1  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "2  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "3  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "4  -0.12817 -0.127059 -0.128401 -0.12854 -0.128061 -0.128536 -0.127195   \n",
       "\n",
       "         205       206       207       208       209       210       211  \\\n",
       "id                                                                         \n",
       "0  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "1  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "2  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "3  -0.128031 -0.127732 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "4  -0.128031  7.828890 -0.127487 -0.127597 -0.128183 -0.128779 -0.127762   \n",
       "\n",
       "         212       213      214       215       216      217       218  \\\n",
       "id                                                                       \n",
       "0  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "1  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "2  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015  7.80212 -0.128275   \n",
       "3  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "4  -0.128031 -0.128343 -0.12747 -0.128448 -0.128015 -0.12817 -0.128275   \n",
       "\n",
       "         219       220       221       222       223       224       225  \\\n",
       "id                                                                         \n",
       "0  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "1  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "2  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339  7.858285 -0.127352   \n",
       "3  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339 -0.127254 -0.127352   \n",
       "4  -0.128212 -0.128065 -0.127884 -0.128662 -0.127339  7.858285 -0.127352   \n",
       "\n",
       "         226       227      228       229       230      231       232  \\\n",
       "id                                                                       \n",
       "0  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "1  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "2  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "3  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "4  -0.127968 -0.128355 -0.12761 -0.126843 -0.128666 -0.12697 -0.127766   \n",
       "\n",
       "         233       234       235       236       237       238       239  \\\n",
       "id                                                                         \n",
       "0  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "1  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "2  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "3  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "4  -0.128431 -0.127757 -0.127572 -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "\n",
       "         240       241       242      243       244      245  \n",
       "id                                                            \n",
       "0  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "1  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "2  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "3  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "4  -0.127119 -0.127985 -0.128494 -0.12862  7.821398 -0.12703  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.loc[:, wide_cols_onehot].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4ad554a-9bff-402c-a202-b9059e9d7c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# X_wide = X_wide_pre_df.join(X.loc[:,wide_cols_onehot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23289b-824b-4384-84ad-783671d680d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da2a03e5-fa67-4bcd-bd82-257d07fbf13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wide = X_wide_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4e3a43e-4052-4461-bd36-ba8400145eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2eaac21f-fbb0-4d37-a70f-16dcb116192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_preprocessor = TabPreprocessor(embed_cols=embed_cols, continuous_cols=cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7a2446c-4182-45be-a5c2-06d450f578c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 957919 entries, 0 to 957918\n",
      "Columns: 246 entries, 0 to 245\n",
      "dtypes: float64(246)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "# X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64cf66a6-7382-4358-9617-1f10cda0af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tab = tab_preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9825519e-8aa8-498b-aca6-1ba1a98592c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(957919, 121)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14d819db-4b99-4f60-98be-4dc6a323c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptabular = TabMlp(\n",
    "    mlp_hidden_dims=[64,32],\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    embed_input=tab_preprocessor.embeddings_input,\n",
    "    continuous_cols=cont_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7306cc9-5e5e-49a1-b6cc-2e8d789f9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3a253cf-51d3-4b59-97ae-a6b7e060265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide = np.array(X_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de8d01a0-3f3b-4e8e-861b-c9da6a8db682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 136)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7800573b-d576-4687-9917-cf4f6a2d19f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 121)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dade972-be94-44b6-8b48-1b53c57c6b19",
   "metadata": {},
   "source": [
    "<!-- 39774, 758737, 552968 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af89e657-f704-43fd-b76f-1055ccb229ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fe7e338-d8c6-44ed-8e51-32f5c35e622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2abd8b0-994f-4cc1-aa4e-6ea2e20049ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|| 749/749 [02:36<00:00,  4.79it/s, loss=0.899, metrics={'acc': 0.6769}]\n",
      "valid: 100%|| 188/188 [00:02<00:00, 92.52it/s, loss=0.836, metrics={'acc': 0.6942}] \n",
      "epoch 2: 100%|| 749/749 [02:32<00:00,  4.90it/s, loss=0.787, metrics={'acc': 0.7015}]\n",
      "valid: 100%|| 188/188 [00:01<00:00, 125.08it/s, loss=0.798, metrics={'acc': 0.6938}]\n",
      "epoch 3: 100%|| 749/749 [02:32<00:00,  4.90it/s, loss=0.722, metrics={'acc': 0.7144}]\n",
      "valid: 100%|| 188/188 [00:01<00:00, 128.40it/s, loss=0.776, metrics={'acc': 0.6945}]\n",
      "epoch 4: 100%|| 749/749 [02:34<00:00,  4.86it/s, loss=0.671, metrics={'acc': 0.7268}]\n",
      "valid: 100%|| 188/188 [00:01<00:00, 124.43it/s, loss=0.762, metrics={'acc': 0.6958}]\n",
      "epoch 5: 100%|| 749/749 [02:34<00:00,  4.86it/s, loss=0.627, metrics={'acc': 0.7383}]\n",
      "valid: 100%|| 188/188 [00:01<00:00, 127.39it/s, loss=0.752, metrics={'acc': 0.6929}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    X_wide=X_wide,\n",
    "    X_tab=X_tab,\n",
    "    target=y,\n",
    "    n_epochs=5,\n",
    "    batch_size=1024,\n",
    "    val_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea409dfb-98a2-4b05-8a14-6b055eafdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_feather(datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10eec40b-95d9-4b46-9798-0600f29aeea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.751704</td>\n",
       "      <td>0.968278</td>\n",
       "      <td>-0.427066</td>\n",
       "      <td>-0.840621</td>\n",
       "      <td>0.095301</td>\n",
       "      <td>0.464038</td>\n",
       "      <td>-0.824375</td>\n",
       "      <td>-1.085380</td>\n",
       "      <td>-0.767857</td>\n",
       "      <td>0.095726</td>\n",
       "      <td>0.294270</td>\n",
       "      <td>1.587074</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>-0.537820</td>\n",
       "      <td>0.253845</td>\n",
       "      <td>0.896228</td>\n",
       "      <td>1.888856</td>\n",
       "      <td>1.569833</td>\n",
       "      <td>-0.196590</td>\n",
       "      <td>-0.724887</td>\n",
       "      <td>-0.658471</td>\n",
       "      <td>-0.094833</td>\n",
       "      <td>0.237521</td>\n",
       "      <td>0.487661</td>\n",
       "      <td>-1.018911</td>\n",
       "      <td>-0.085047</td>\n",
       "      <td>-0.699558</td>\n",
       "      <td>1.819097</td>\n",
       "      <td>-0.745413</td>\n",
       "      <td>-0.209014</td>\n",
       "      <td>-0.536655</td>\n",
       "      <td>-0.551061</td>\n",
       "      <td>-0.626290</td>\n",
       "      <td>-0.145177</td>\n",
       "      <td>0.954797</td>\n",
       "      <td>0.030325</td>\n",
       "      <td>-0.333978</td>\n",
       "      <td>-0.716172</td>\n",
       "      <td>1.831318</td>\n",
       "      <td>-0.815223</td>\n",
       "      <td>-0.680160</td>\n",
       "      <td>-0.808847</td>\n",
       "      <td>0.148292</td>\n",
       "      <td>-0.167851</td>\n",
       "      <td>-1.015892</td>\n",
       "      <td>0.543848</td>\n",
       "      <td>-1.664666</td>\n",
       "      <td>0.323791</td>\n",
       "      <td>-0.874336</td>\n",
       "      <td>-2.071137</td>\n",
       "      <td>-0.619346</td>\n",
       "      <td>-0.885334</td>\n",
       "      <td>-0.576842</td>\n",
       "      <td>1.463713</td>\n",
       "      <td>-0.026447</td>\n",
       "      <td>0.659552</td>\n",
       "      <td>0.069464</td>\n",
       "      <td>-0.238719</td>\n",
       "      <td>-0.666550</td>\n",
       "      <td>-1.811843</td>\n",
       "      <td>-0.989331</td>\n",
       "      <td>0.020936</td>\n",
       "      <td>0.333912</td>\n",
       "      <td>-0.957043</td>\n",
       "      <td>0.848880</td>\n",
       "      <td>0.785600</td>\n",
       "      <td>0.792306</td>\n",
       "      <td>-0.637652</td>\n",
       "      <td>-0.425662</td>\n",
       "      <td>-0.833097</td>\n",
       "      <td>0.628813</td>\n",
       "      <td>1.638262</td>\n",
       "      <td>-0.412419</td>\n",
       "      <td>-0.525297</td>\n",
       "      <td>-0.646473</td>\n",
       "      <td>-1.325236</td>\n",
       "      <td>-0.591492</td>\n",
       "      <td>-0.062932</td>\n",
       "      <td>-0.081760</td>\n",
       "      <td>1.964202</td>\n",
       "      <td>0.407308</td>\n",
       "      <td>1.453569</td>\n",
       "      <td>-0.362350</td>\n",
       "      <td>-0.064352</td>\n",
       "      <td>1.046524</td>\n",
       "      <td>-1.046313</td>\n",
       "      <td>-0.836077</td>\n",
       "      <td>-0.154928</td>\n",
       "      <td>-0.846819</td>\n",
       "      <td>-0.477627</td>\n",
       "      <td>-2.453846</td>\n",
       "      <td>-0.139056</td>\n",
       "      <td>-0.180401</td>\n",
       "      <td>-1.779057</td>\n",
       "      <td>-0.148828</td>\n",
       "      <td>-0.005655</td>\n",
       "      <td>-0.464128</td>\n",
       "      <td>-0.859371</td>\n",
       "      <td>-0.412277</td>\n",
       "      <td>-1.517857</td>\n",
       "      <td>1.980152</td>\n",
       "      <td>-0.750306</td>\n",
       "      <td>-0.413239</td>\n",
       "      <td>-0.386818</td>\n",
       "      <td>-0.115089</td>\n",
       "      <td>-0.745005</td>\n",
       "      <td>0.362949</td>\n",
       "      <td>-0.463772</td>\n",
       "      <td>-0.794857</td>\n",
       "      <td>-0.132273</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>-0.509415</td>\n",
       "      <td>-1.246808</td>\n",
       "      <td>0.210003</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.629358</td>\n",
       "      <td>-0.618295</td>\n",
       "      <td>-0.231429</td>\n",
       "      <td>-0.442610</td>\n",
       "      <td>-0.034637</td>\n",
       "      <td>-0.155002</td>\n",
       "      <td>1.330175</td>\n",
       "      <td>-0.027994</td>\n",
       "      <td>-0.151745</td>\n",
       "      <td>-0.281398</td>\n",
       "      <td>0.550393</td>\n",
       "      <td>0.013257</td>\n",
       "      <td>-0.884058</td>\n",
       "      <td>-0.126828</td>\n",
       "      <td>-0.127478</td>\n",
       "      <td>-0.126687</td>\n",
       "      <td>-0.126175</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.127658</td>\n",
       "      <td>-0.127822</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.126877</td>\n",
       "      <td>-0.127519</td>\n",
       "      <td>-0.127724</td>\n",
       "      <td>-0.129077</td>\n",
       "      <td>-0.127806</td>\n",
       "      <td>-0.126315</td>\n",
       "      <td>-0.128695</td>\n",
       "      <td>-0.127879</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.12841</td>\n",
       "      <td>-0.128272</td>\n",
       "      <td>-0.126894</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.127494</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.126431</td>\n",
       "      <td>-0.127379</td>\n",
       "      <td>-0.129345</td>\n",
       "      <td>-0.128067</td>\n",
       "      <td>-0.127978</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.127009</td>\n",
       "      <td>-0.127231</td>\n",
       "      <td>-0.126803</td>\n",
       "      <td>-0.127691</td>\n",
       "      <td>-0.127576</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.127281</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.128744</td>\n",
       "      <td>-0.127511</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.12783</td>\n",
       "      <td>-0.126704</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128459</td>\n",
       "      <td>-0.127847</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.128728</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.128541</td>\n",
       "      <td>-0.126547</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128736</td>\n",
       "      <td>-0.1271</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128801</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.129515</td>\n",
       "      <td>-0.127568</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.12902</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.12756</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128076</td>\n",
       "      <td>-0.129134</td>\n",
       "      <td>-0.127486</td>\n",
       "      <td>-0.12863</td>\n",
       "      <td>-0.128084</td>\n",
       "      <td>7.792981</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.126621</td>\n",
       "      <td>-0.127593</td>\n",
       "      <td>-0.128141</td>\n",
       "      <td>-0.127797</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.127937</td>\n",
       "      <td>-0.127264</td>\n",
       "      <td>-0.128027</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.126819</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127609</td>\n",
       "      <td>-0.127108</td>\n",
       "      <td>-0.126605</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.12949</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.127773</td>\n",
       "      <td>-0.12663</td>\n",
       "      <td>-0.128704</td>\n",
       "      <td>-0.128598</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127904</td>\n",
       "      <td>-0.128059</td>\n",
       "      <td>-0.128476</td>\n",
       "      <td>-0.126506</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.126365</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.129045</td>\n",
       "      <td>-0.126423</td>\n",
       "      <td>-0.127428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.913581</td>\n",
       "      <td>0.184047</td>\n",
       "      <td>-0.353273</td>\n",
       "      <td>2.503619</td>\n",
       "      <td>0.188649</td>\n",
       "      <td>-0.562836</td>\n",
       "      <td>-0.584997</td>\n",
       "      <td>0.522611</td>\n",
       "      <td>-0.512844</td>\n",
       "      <td>-0.432564</td>\n",
       "      <td>0.782779</td>\n",
       "      <td>1.459134</td>\n",
       "      <td>0.362927</td>\n",
       "      <td>-0.732927</td>\n",
       "      <td>-0.531157</td>\n",
       "      <td>-0.643969</td>\n",
       "      <td>-0.175145</td>\n",
       "      <td>2.140006</td>\n",
       "      <td>0.499012</td>\n",
       "      <td>-0.705995</td>\n",
       "      <td>-0.318749</td>\n",
       "      <td>0.955799</td>\n",
       "      <td>0.596087</td>\n",
       "      <td>-0.741604</td>\n",
       "      <td>1.844986</td>\n",
       "      <td>-0.592684</td>\n",
       "      <td>0.231928</td>\n",
       "      <td>-0.557752</td>\n",
       "      <td>1.615232</td>\n",
       "      <td>-0.832240</td>\n",
       "      <td>0.529083</td>\n",
       "      <td>1.395408</td>\n",
       "      <td>0.063284</td>\n",
       "      <td>1.477203</td>\n",
       "      <td>-0.607693</td>\n",
       "      <td>0.066323</td>\n",
       "      <td>-1.191912</td>\n",
       "      <td>-0.462801</td>\n",
       "      <td>0.050357</td>\n",
       "      <td>-0.813357</td>\n",
       "      <td>-0.662537</td>\n",
       "      <td>-0.648801</td>\n",
       "      <td>-0.224800</td>\n",
       "      <td>-0.963386</td>\n",
       "      <td>2.254599</td>\n",
       "      <td>0.158790</td>\n",
       "      <td>-1.297993</td>\n",
       "      <td>-0.919179</td>\n",
       "      <td>-0.790977</td>\n",
       "      <td>-2.045965</td>\n",
       "      <td>0.498839</td>\n",
       "      <td>2.187002</td>\n",
       "      <td>0.483492</td>\n",
       "      <td>1.009265</td>\n",
       "      <td>1.649467</td>\n",
       "      <td>1.032962</td>\n",
       "      <td>-0.038555</td>\n",
       "      <td>0.753564</td>\n",
       "      <td>-0.811298</td>\n",
       "      <td>-0.311674</td>\n",
       "      <td>-0.560421</td>\n",
       "      <td>-0.695927</td>\n",
       "      <td>0.053301</td>\n",
       "      <td>0.827917</td>\n",
       "      <td>0.727760</td>\n",
       "      <td>0.262516</td>\n",
       "      <td>-1.189286</td>\n",
       "      <td>0.069433</td>\n",
       "      <td>0.148949</td>\n",
       "      <td>1.200276</td>\n",
       "      <td>1.606983</td>\n",
       "      <td>0.797347</td>\n",
       "      <td>-0.563326</td>\n",
       "      <td>-0.351463</td>\n",
       "      <td>-0.841337</td>\n",
       "      <td>0.074523</td>\n",
       "      <td>-0.540233</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.767644</td>\n",
       "      <td>-0.784410</td>\n",
       "      <td>1.101888</td>\n",
       "      <td>1.437974</td>\n",
       "      <td>2.901721</td>\n",
       "      <td>-0.596012</td>\n",
       "      <td>2.645387</td>\n",
       "      <td>-1.054988</td>\n",
       "      <td>-0.641274</td>\n",
       "      <td>1.965607</td>\n",
       "      <td>-1.008591</td>\n",
       "      <td>-0.348907</td>\n",
       "      <td>0.046766</td>\n",
       "      <td>-0.524472</td>\n",
       "      <td>-0.142755</td>\n",
       "      <td>0.628105</td>\n",
       "      <td>-0.970454</td>\n",
       "      <td>-0.405823</td>\n",
       "      <td>0.481852</td>\n",
       "      <td>0.402860</td>\n",
       "      <td>-0.551052</td>\n",
       "      <td>-0.627485</td>\n",
       "      <td>-0.519872</td>\n",
       "      <td>-0.932559</td>\n",
       "      <td>-0.346737</td>\n",
       "      <td>-0.478548</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>-1.076239</td>\n",
       "      <td>0.268719</td>\n",
       "      <td>-0.830292</td>\n",
       "      <td>1.413474</td>\n",
       "      <td>0.986151</td>\n",
       "      <td>-0.556124</td>\n",
       "      <td>-0.542603</td>\n",
       "      <td>1.006487</td>\n",
       "      <td>0.302786</td>\n",
       "      <td>-0.506379</td>\n",
       "      <td>0.022709</td>\n",
       "      <td>0.854729</td>\n",
       "      <td>-0.681230</td>\n",
       "      <td>-0.935935</td>\n",
       "      <td>-1.163156</td>\n",
       "      <td>-0.287454</td>\n",
       "      <td>1.005741</td>\n",
       "      <td>-0.274192</td>\n",
       "      <td>-0.292826</td>\n",
       "      <td>-0.385354</td>\n",
       "      <td>0.429414</td>\n",
       "      <td>-0.243408</td>\n",
       "      <td>-1.006832</td>\n",
       "      <td>-0.126828</td>\n",
       "      <td>-0.127478</td>\n",
       "      <td>-0.126687</td>\n",
       "      <td>-0.126175</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.127658</td>\n",
       "      <td>-0.127822</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.126877</td>\n",
       "      <td>-0.127519</td>\n",
       "      <td>-0.127724</td>\n",
       "      <td>-0.129077</td>\n",
       "      <td>-0.127806</td>\n",
       "      <td>-0.126315</td>\n",
       "      <td>-0.128695</td>\n",
       "      <td>-0.127879</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.12841</td>\n",
       "      <td>-0.128272</td>\n",
       "      <td>-0.126894</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.127494</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.126431</td>\n",
       "      <td>-0.127379</td>\n",
       "      <td>-0.129345</td>\n",
       "      <td>-0.128067</td>\n",
       "      <td>-0.127978</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.127009</td>\n",
       "      <td>-0.127231</td>\n",
       "      <td>-0.126803</td>\n",
       "      <td>-0.127691</td>\n",
       "      <td>-0.127576</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.127281</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.128744</td>\n",
       "      <td>-0.127511</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.12783</td>\n",
       "      <td>-0.126704</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128459</td>\n",
       "      <td>-0.127847</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.128728</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.128541</td>\n",
       "      <td>-0.126547</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128736</td>\n",
       "      <td>-0.1271</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128801</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.129515</td>\n",
       "      <td>-0.127568</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.12902</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.12756</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128076</td>\n",
       "      <td>-0.129134</td>\n",
       "      <td>-0.127486</td>\n",
       "      <td>-0.12863</td>\n",
       "      <td>-0.128084</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.126621</td>\n",
       "      <td>-0.127593</td>\n",
       "      <td>-0.128141</td>\n",
       "      <td>-0.127797</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.127937</td>\n",
       "      <td>-0.127264</td>\n",
       "      <td>-0.128027</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.126819</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127609</td>\n",
       "      <td>-0.127108</td>\n",
       "      <td>-0.126605</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.12949</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.127773</td>\n",
       "      <td>-0.12663</td>\n",
       "      <td>-0.128704</td>\n",
       "      <td>-0.128598</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127904</td>\n",
       "      <td>-0.128059</td>\n",
       "      <td>-0.128476</td>\n",
       "      <td>-0.126506</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.126365</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.129045</td>\n",
       "      <td>-0.126423</td>\n",
       "      <td>-0.127428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.694558</td>\n",
       "      <td>0.679362</td>\n",
       "      <td>-0.515168</td>\n",
       "      <td>-0.731396</td>\n",
       "      <td>-0.063744</td>\n",
       "      <td>0.186457</td>\n",
       "      <td>0.827822</td>\n",
       "      <td>-0.170170</td>\n",
       "      <td>-0.597921</td>\n",
       "      <td>-0.512770</td>\n",
       "      <td>-0.820196</td>\n",
       "      <td>-0.864226</td>\n",
       "      <td>1.229800</td>\n",
       "      <td>-0.997208</td>\n",
       "      <td>-0.041742</td>\n",
       "      <td>2.474075</td>\n",
       "      <td>-0.516905</td>\n",
       "      <td>-0.905959</td>\n",
       "      <td>-0.719207</td>\n",
       "      <td>2.453757</td>\n",
       "      <td>-0.666196</td>\n",
       "      <td>1.416064</td>\n",
       "      <td>0.561363</td>\n",
       "      <td>-1.233139</td>\n",
       "      <td>0.363539</td>\n",
       "      <td>-0.153538</td>\n",
       "      <td>-0.753065</td>\n",
       "      <td>-0.977576</td>\n",
       "      <td>1.582552</td>\n",
       "      <td>-0.891516</td>\n",
       "      <td>0.854381</td>\n",
       "      <td>-1.686447</td>\n",
       "      <td>-0.630413</td>\n",
       "      <td>0.917298</td>\n",
       "      <td>-0.800475</td>\n",
       "      <td>-0.383654</td>\n",
       "      <td>-0.152517</td>\n",
       "      <td>-0.013698</td>\n",
       "      <td>1.421560</td>\n",
       "      <td>-0.800002</td>\n",
       "      <td>0.678951</td>\n",
       "      <td>1.349640</td>\n",
       "      <td>1.354400</td>\n",
       "      <td>1.019803</td>\n",
       "      <td>-0.421299</td>\n",
       "      <td>1.223441</td>\n",
       "      <td>-0.980069</td>\n",
       "      <td>0.097238</td>\n",
       "      <td>-0.966543</td>\n",
       "      <td>0.593753</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>-1.014916</td>\n",
       "      <td>0.029021</td>\n",
       "      <td>0.630872</td>\n",
       "      <td>1.499162</td>\n",
       "      <td>0.339007</td>\n",
       "      <td>0.561246</td>\n",
       "      <td>0.740199</td>\n",
       "      <td>1.369262</td>\n",
       "      <td>0.835835</td>\n",
       "      <td>-0.563563</td>\n",
       "      <td>1.118850</td>\n",
       "      <td>0.413232</td>\n",
       "      <td>0.749485</td>\n",
       "      <td>0.887943</td>\n",
       "      <td>-0.530724</td>\n",
       "      <td>0.330726</td>\n",
       "      <td>-0.540148</td>\n",
       "      <td>-0.360259</td>\n",
       "      <td>-0.841787</td>\n",
       "      <td>-1.087088</td>\n",
       "      <td>-0.131217</td>\n",
       "      <td>-0.666113</td>\n",
       "      <td>0.167155</td>\n",
       "      <td>-0.853630</td>\n",
       "      <td>-0.749515</td>\n",
       "      <td>-0.645966</td>\n",
       "      <td>-0.910024</td>\n",
       "      <td>-0.337017</td>\n",
       "      <td>-0.803533</td>\n",
       "      <td>-0.105199</td>\n",
       "      <td>0.644774</td>\n",
       "      <td>0.270020</td>\n",
       "      <td>0.210557</td>\n",
       "      <td>-0.613863</td>\n",
       "      <td>0.267352</td>\n",
       "      <td>-1.001924</td>\n",
       "      <td>1.188931</td>\n",
       "      <td>1.446396</td>\n",
       "      <td>0.662903</td>\n",
       "      <td>0.260671</td>\n",
       "      <td>-0.527644</td>\n",
       "      <td>1.587620</td>\n",
       "      <td>0.708334</td>\n",
       "      <td>1.410587</td>\n",
       "      <td>-0.155128</td>\n",
       "      <td>-0.470698</td>\n",
       "      <td>-0.573225</td>\n",
       "      <td>-0.251247</td>\n",
       "      <td>0.240875</td>\n",
       "      <td>0.866243</td>\n",
       "      <td>-0.555523</td>\n",
       "      <td>2.232848</td>\n",
       "      <td>0.133247</td>\n",
       "      <td>0.993754</td>\n",
       "      <td>-0.477394</td>\n",
       "      <td>0.271009</td>\n",
       "      <td>-0.855420</td>\n",
       "      <td>1.423724</td>\n",
       "      <td>-0.708549</td>\n",
       "      <td>0.076225</td>\n",
       "      <td>-0.561605</td>\n",
       "      <td>-0.502072</td>\n",
       "      <td>-0.268384</td>\n",
       "      <td>-0.470481</td>\n",
       "      <td>-0.629750</td>\n",
       "      <td>0.601385</td>\n",
       "      <td>-1.034715</td>\n",
       "      <td>-0.442610</td>\n",
       "      <td>-0.034637</td>\n",
       "      <td>-0.948329</td>\n",
       "      <td>0.416796</td>\n",
       "      <td>-0.933648</td>\n",
       "      <td>-0.950110</td>\n",
       "      <td>-0.504031</td>\n",
       "      <td>-0.357964</td>\n",
       "      <td>-0.916449</td>\n",
       "      <td>0.855258</td>\n",
       "      <td>-0.126828</td>\n",
       "      <td>-0.127478</td>\n",
       "      <td>-0.126687</td>\n",
       "      <td>-0.126175</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.127658</td>\n",
       "      <td>-0.127822</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.126877</td>\n",
       "      <td>-0.127519</td>\n",
       "      <td>-0.127724</td>\n",
       "      <td>-0.129077</td>\n",
       "      <td>-0.127806</td>\n",
       "      <td>-0.126315</td>\n",
       "      <td>-0.128695</td>\n",
       "      <td>-0.127879</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.12841</td>\n",
       "      <td>-0.128272</td>\n",
       "      <td>-0.126894</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.127494</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.126431</td>\n",
       "      <td>-0.127379</td>\n",
       "      <td>-0.129345</td>\n",
       "      <td>-0.128067</td>\n",
       "      <td>-0.127978</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.127009</td>\n",
       "      <td>-0.127231</td>\n",
       "      <td>-0.126803</td>\n",
       "      <td>-0.127691</td>\n",
       "      <td>-0.127576</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>7.797444</td>\n",
       "      <td>-0.127281</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.128744</td>\n",
       "      <td>-0.127511</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.12783</td>\n",
       "      <td>-0.126704</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128459</td>\n",
       "      <td>-0.127847</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.128728</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.128541</td>\n",
       "      <td>-0.126547</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128736</td>\n",
       "      <td>-0.1271</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128801</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.129515</td>\n",
       "      <td>-0.127568</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.12902</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.12756</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128076</td>\n",
       "      <td>-0.129134</td>\n",
       "      <td>-0.127486</td>\n",
       "      <td>-0.12863</td>\n",
       "      <td>-0.128084</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.126621</td>\n",
       "      <td>-0.127593</td>\n",
       "      <td>-0.128141</td>\n",
       "      <td>-0.127797</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.127937</td>\n",
       "      <td>-0.127264</td>\n",
       "      <td>-0.128027</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.126819</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127609</td>\n",
       "      <td>-0.127108</td>\n",
       "      <td>-0.126605</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.12949</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.127773</td>\n",
       "      <td>-0.12663</td>\n",
       "      <td>-0.128704</td>\n",
       "      <td>-0.128598</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127904</td>\n",
       "      <td>-0.128059</td>\n",
       "      <td>-0.128476</td>\n",
       "      <td>-0.126506</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.126365</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.129045</td>\n",
       "      <td>-0.126423</td>\n",
       "      <td>-0.127428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.837726</td>\n",
       "      <td>0.339277</td>\n",
       "      <td>-0.474190</td>\n",
       "      <td>3.122010</td>\n",
       "      <td>0.426420</td>\n",
       "      <td>-0.044651</td>\n",
       "      <td>-0.972258</td>\n",
       "      <td>-0.145231</td>\n",
       "      <td>-0.783784</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>-1.927408</td>\n",
       "      <td>1.350249</td>\n",
       "      <td>0.182419</td>\n",
       "      <td>-0.891518</td>\n",
       "      <td>-1.018552</td>\n",
       "      <td>2.467541</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>1.722104</td>\n",
       "      <td>0.055257</td>\n",
       "      <td>-0.243399</td>\n",
       "      <td>-0.685267</td>\n",
       "      <td>0.519907</td>\n",
       "      <td>-0.141960</td>\n",
       "      <td>2.132586</td>\n",
       "      <td>-0.351455</td>\n",
       "      <td>-0.595874</td>\n",
       "      <td>-0.763673</td>\n",
       "      <td>0.023498</td>\n",
       "      <td>-0.521678</td>\n",
       "      <td>-0.499183</td>\n",
       "      <td>0.335070</td>\n",
       "      <td>-0.417375</td>\n",
       "      <td>-0.339350</td>\n",
       "      <td>-0.403920</td>\n",
       "      <td>0.358163</td>\n",
       "      <td>1.752987</td>\n",
       "      <td>0.312649</td>\n",
       "      <td>0.256485</td>\n",
       "      <td>0.433688</td>\n",
       "      <td>-0.802652</td>\n",
       "      <td>-0.265331</td>\n",
       "      <td>1.563002</td>\n",
       "      <td>0.288298</td>\n",
       "      <td>0.139869</td>\n",
       "      <td>0.858205</td>\n",
       "      <td>-0.076926</td>\n",
       "      <td>0.098262</td>\n",
       "      <td>-0.740700</td>\n",
       "      <td>0.578541</td>\n",
       "      <td>0.316091</td>\n",
       "      <td>-0.630003</td>\n",
       "      <td>-0.509272</td>\n",
       "      <td>-0.495719</td>\n",
       "      <td>0.430361</td>\n",
       "      <td>-0.948177</td>\n",
       "      <td>-0.457535</td>\n",
       "      <td>-0.386012</td>\n",
       "      <td>0.636068</td>\n",
       "      <td>1.036644</td>\n",
       "      <td>-0.724280</td>\n",
       "      <td>-0.579233</td>\n",
       "      <td>-0.882536</td>\n",
       "      <td>-0.528785</td>\n",
       "      <td>-0.742628</td>\n",
       "      <td>1.073733</td>\n",
       "      <td>-2.322800</td>\n",
       "      <td>1.605690</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.975049</td>\n",
       "      <td>1.199666</td>\n",
       "      <td>-0.136038</td>\n",
       "      <td>0.006952</td>\n",
       "      <td>2.837742</td>\n",
       "      <td>-0.520679</td>\n",
       "      <td>-0.816043</td>\n",
       "      <td>0.233348</td>\n",
       "      <td>0.207093</td>\n",
       "      <td>-0.830033</td>\n",
       "      <td>-0.437317</td>\n",
       "      <td>-0.404215</td>\n",
       "      <td>-0.900053</td>\n",
       "      <td>1.660584</td>\n",
       "      <td>-0.374089</td>\n",
       "      <td>-0.778953</td>\n",
       "      <td>0.363649</td>\n",
       "      <td>-0.663762</td>\n",
       "      <td>-0.900234</td>\n",
       "      <td>0.233728</td>\n",
       "      <td>-0.741002</td>\n",
       "      <td>-0.708002</td>\n",
       "      <td>0.066075</td>\n",
       "      <td>-0.579166</td>\n",
       "      <td>-1.102966</td>\n",
       "      <td>0.207542</td>\n",
       "      <td>0.839596</td>\n",
       "      <td>-0.299733</td>\n",
       "      <td>-0.490406</td>\n",
       "      <td>0.098282</td>\n",
       "      <td>-0.895793</td>\n",
       "      <td>0.435478</td>\n",
       "      <td>0.385685</td>\n",
       "      <td>1.514675</td>\n",
       "      <td>0.917220</td>\n",
       "      <td>-0.521825</td>\n",
       "      <td>-1.231826</td>\n",
       "      <td>3.613937</td>\n",
       "      <td>-0.952575</td>\n",
       "      <td>0.179350</td>\n",
       "      <td>1.583678</td>\n",
       "      <td>0.943601</td>\n",
       "      <td>-0.616461</td>\n",
       "      <td>-0.519192</td>\n",
       "      <td>-0.036459</td>\n",
       "      <td>-0.387144</td>\n",
       "      <td>-0.521264</td>\n",
       "      <td>2.560401</td>\n",
       "      <td>0.706337</td>\n",
       "      <td>-0.563179</td>\n",
       "      <td>-0.935935</td>\n",
       "      <td>-1.163156</td>\n",
       "      <td>2.407592</td>\n",
       "      <td>-1.777520</td>\n",
       "      <td>2.323779</td>\n",
       "      <td>2.397136</td>\n",
       "      <td>2.346159</td>\n",
       "      <td>0.367916</td>\n",
       "      <td>2.387944</td>\n",
       "      <td>0.196935</td>\n",
       "      <td>-0.126828</td>\n",
       "      <td>-0.127478</td>\n",
       "      <td>-0.126687</td>\n",
       "      <td>-0.126175</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.127658</td>\n",
       "      <td>-0.127822</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.126877</td>\n",
       "      <td>-0.127519</td>\n",
       "      <td>-0.127724</td>\n",
       "      <td>-0.129077</td>\n",
       "      <td>-0.127806</td>\n",
       "      <td>-0.126315</td>\n",
       "      <td>-0.128695</td>\n",
       "      <td>-0.127879</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.12841</td>\n",
       "      <td>-0.128272</td>\n",
       "      <td>-0.126894</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.127494</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.126431</td>\n",
       "      <td>-0.127379</td>\n",
       "      <td>-0.129345</td>\n",
       "      <td>-0.128067</td>\n",
       "      <td>-0.127978</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.127009</td>\n",
       "      <td>-0.127231</td>\n",
       "      <td>-0.126803</td>\n",
       "      <td>-0.127691</td>\n",
       "      <td>-0.127576</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.127281</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.128744</td>\n",
       "      <td>-0.127511</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.12783</td>\n",
       "      <td>-0.126704</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128459</td>\n",
       "      <td>-0.127847</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.128728</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.128541</td>\n",
       "      <td>-0.126547</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128736</td>\n",
       "      <td>-0.1271</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128801</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.129515</td>\n",
       "      <td>-0.127568</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.12902</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.12756</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128076</td>\n",
       "      <td>-0.129134</td>\n",
       "      <td>-0.127486</td>\n",
       "      <td>-0.12863</td>\n",
       "      <td>-0.128084</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.126621</td>\n",
       "      <td>-0.127593</td>\n",
       "      <td>-0.128141</td>\n",
       "      <td>-0.127797</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.127937</td>\n",
       "      <td>-0.127264</td>\n",
       "      <td>-0.128027</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.126819</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127609</td>\n",
       "      <td>-0.127108</td>\n",
       "      <td>-0.126605</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.12949</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.127773</td>\n",
       "      <td>-0.12663</td>\n",
       "      <td>-0.128704</td>\n",
       "      <td>-0.128598</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127904</td>\n",
       "      <td>-0.128059</td>\n",
       "      <td>-0.128476</td>\n",
       "      <td>-0.126506</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.126365</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.129045</td>\n",
       "      <td>-0.126423</td>\n",
       "      <td>-0.127428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.237172</td>\n",
       "      <td>-2.442706</td>\n",
       "      <td>1.036035</td>\n",
       "      <td>0.701092</td>\n",
       "      <td>1.003695</td>\n",
       "      <td>-0.837415</td>\n",
       "      <td>1.118867</td>\n",
       "      <td>0.154011</td>\n",
       "      <td>-0.768622</td>\n",
       "      <td>0.073582</td>\n",
       "      <td>0.766561</td>\n",
       "      <td>0.014387</td>\n",
       "      <td>0.740279</td>\n",
       "      <td>-0.721846</td>\n",
       "      <td>-1.252427</td>\n",
       "      <td>0.353497</td>\n",
       "      <td>-0.203435</td>\n",
       "      <td>0.613535</td>\n",
       "      <td>-1.043149</td>\n",
       "      <td>0.954753</td>\n",
       "      <td>-0.560617</td>\n",
       "      <td>-1.661382</td>\n",
       "      <td>0.652344</td>\n",
       "      <td>0.644744</td>\n",
       "      <td>0.049008</td>\n",
       "      <td>0.142589</td>\n",
       "      <td>-0.758819</td>\n",
       "      <td>-0.977637</td>\n",
       "      <td>1.629602</td>\n",
       "      <td>1.400235</td>\n",
       "      <td>0.766681</td>\n",
       "      <td>1.184030</td>\n",
       "      <td>-0.747111</td>\n",
       "      <td>-0.428913</td>\n",
       "      <td>-0.797404</td>\n",
       "      <td>-0.449215</td>\n",
       "      <td>1.688384</td>\n",
       "      <td>1.673614</td>\n",
       "      <td>-0.761875</td>\n",
       "      <td>-0.778883</td>\n",
       "      <td>-0.707102</td>\n",
       "      <td>-0.853185</td>\n",
       "      <td>-3.116618</td>\n",
       "      <td>-1.025830</td>\n",
       "      <td>-0.827048</td>\n",
       "      <td>-0.201275</td>\n",
       "      <td>1.215369</td>\n",
       "      <td>-0.679285</td>\n",
       "      <td>-0.333675</td>\n",
       "      <td>0.940520</td>\n",
       "      <td>-0.227671</td>\n",
       "      <td>-0.681174</td>\n",
       "      <td>-0.538202</td>\n",
       "      <td>-1.802282</td>\n",
       "      <td>0.036005</td>\n",
       "      <td>-0.424086</td>\n",
       "      <td>-1.061590</td>\n",
       "      <td>-2.386683</td>\n",
       "      <td>-0.275487</td>\n",
       "      <td>1.328893</td>\n",
       "      <td>-0.097465</td>\n",
       "      <td>-0.415497</td>\n",
       "      <td>-1.031416</td>\n",
       "      <td>-0.120169</td>\n",
       "      <td>-1.135318</td>\n",
       "      <td>0.127382</td>\n",
       "      <td>0.090168</td>\n",
       "      <td>-0.049644</td>\n",
       "      <td>-0.994043</td>\n",
       "      <td>-0.848861</td>\n",
       "      <td>-0.608259</td>\n",
       "      <td>-0.725843</td>\n",
       "      <td>-0.343367</td>\n",
       "      <td>2.615374</td>\n",
       "      <td>-0.849832</td>\n",
       "      <td>0.081675</td>\n",
       "      <td>2.361217</td>\n",
       "      <td>-0.655917</td>\n",
       "      <td>-0.530237</td>\n",
       "      <td>1.913836</td>\n",
       "      <td>1.870015</td>\n",
       "      <td>0.443875</td>\n",
       "      <td>-0.623531</td>\n",
       "      <td>-0.698811</td>\n",
       "      <td>0.383363</td>\n",
       "      <td>-0.207545</td>\n",
       "      <td>-0.494442</td>\n",
       "      <td>-1.225568</td>\n",
       "      <td>-0.890671</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>-0.161638</td>\n",
       "      <td>-0.882829</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>-0.957368</td>\n",
       "      <td>-0.604126</td>\n",
       "      <td>-0.181648</td>\n",
       "      <td>0.368971</td>\n",
       "      <td>-0.640781</td>\n",
       "      <td>1.024079</td>\n",
       "      <td>-0.304932</td>\n",
       "      <td>-0.969297</td>\n",
       "      <td>-0.616898</td>\n",
       "      <td>1.022784</td>\n",
       "      <td>-0.099087</td>\n",
       "      <td>0.475426</td>\n",
       "      <td>0.392832</td>\n",
       "      <td>-0.896007</td>\n",
       "      <td>-0.492508</td>\n",
       "      <td>0.061379</td>\n",
       "      <td>1.876532</td>\n",
       "      <td>-0.502208</td>\n",
       "      <td>-0.667787</td>\n",
       "      <td>-0.131009</td>\n",
       "      <td>0.352563</td>\n",
       "      <td>2.579061</td>\n",
       "      <td>-0.544035</td>\n",
       "      <td>-0.541321</td>\n",
       "      <td>-0.935935</td>\n",
       "      <td>-1.163156</td>\n",
       "      <td>1.856834</td>\n",
       "      <td>-1.175219</td>\n",
       "      <td>2.292012</td>\n",
       "      <td>1.888400</td>\n",
       "      <td>2.290951</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>2.407292</td>\n",
       "      <td>0.399006</td>\n",
       "      <td>-0.126828</td>\n",
       "      <td>-0.127478</td>\n",
       "      <td>-0.126687</td>\n",
       "      <td>-0.126175</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.127658</td>\n",
       "      <td>-0.127822</td>\n",
       "      <td>-0.127182</td>\n",
       "      <td>-0.126877</td>\n",
       "      <td>-0.127519</td>\n",
       "      <td>-0.127724</td>\n",
       "      <td>-0.129077</td>\n",
       "      <td>-0.127806</td>\n",
       "      <td>-0.126315</td>\n",
       "      <td>-0.128695</td>\n",
       "      <td>-0.127879</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.12841</td>\n",
       "      <td>-0.128272</td>\n",
       "      <td>-0.126894</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.127494</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.126431</td>\n",
       "      <td>-0.127379</td>\n",
       "      <td>-0.129345</td>\n",
       "      <td>-0.128067</td>\n",
       "      <td>-0.127978</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.127009</td>\n",
       "      <td>-0.127231</td>\n",
       "      <td>-0.126803</td>\n",
       "      <td>-0.127691</td>\n",
       "      <td>-0.127576</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.128247</td>\n",
       "      <td>-0.127281</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.128744</td>\n",
       "      <td>-0.127511</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.12783</td>\n",
       "      <td>-0.126704</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128459</td>\n",
       "      <td>-0.127847</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.128728</td>\n",
       "      <td>-0.128378</td>\n",
       "      <td>-0.128541</td>\n",
       "      <td>-0.126547</td>\n",
       "      <td>-0.12801</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128647</td>\n",
       "      <td>-0.128736</td>\n",
       "      <td>-0.1271</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128801</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.127757</td>\n",
       "      <td>-0.129515</td>\n",
       "      <td>-0.127568</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128002</td>\n",
       "      <td>-0.12902</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.12756</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.128524</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>-0.128076</td>\n",
       "      <td>-0.129134</td>\n",
       "      <td>-0.127486</td>\n",
       "      <td>-0.12863</td>\n",
       "      <td>-0.128084</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.126621</td>\n",
       "      <td>-0.127593</td>\n",
       "      <td>-0.128141</td>\n",
       "      <td>-0.127797</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.127937</td>\n",
       "      <td>-0.127264</td>\n",
       "      <td>-0.128027</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.127363</td>\n",
       "      <td>-0.126819</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.127609</td>\n",
       "      <td>-0.127108</td>\n",
       "      <td>-0.126605</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.12949</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.127773</td>\n",
       "      <td>-0.12663</td>\n",
       "      <td>-0.128704</td>\n",
       "      <td>-0.128598</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128263</td>\n",
       "      <td>-0.127904</td>\n",
       "      <td>-0.128059</td>\n",
       "      <td>-0.128476</td>\n",
       "      <td>-0.126506</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>-0.126365</td>\n",
       "      <td>-0.127896</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>-0.129045</td>\n",
       "      <td>-0.126423</td>\n",
       "      <td>-0.127428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.751704  0.968278 -0.427066 -0.840621  0.095301  0.464038 -0.824375   \n",
       "1  0.913581  0.184047 -0.353273  2.503619  0.188649 -0.562836 -0.584997   \n",
       "2  0.694558  0.679362 -0.515168 -0.731396 -0.063744  0.186457  0.827822   \n",
       "3 -0.837726  0.339277 -0.474190  3.122010  0.426420 -0.044651 -0.972258   \n",
       "4 -0.237172 -2.442706  1.036035  0.701092  1.003695 -0.837415  1.118867   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -1.085380 -0.767857  0.095726  0.294270  1.587074  0.099936 -0.537820   \n",
       "1  0.522611 -0.512844 -0.432564  0.782779  1.459134  0.362927 -0.732927   \n",
       "2 -0.170170 -0.597921 -0.512770 -0.820196 -0.864226  1.229800 -0.997208   \n",
       "3 -0.145231 -0.783784 -0.528587 -1.927408  1.350249  0.182419 -0.891518   \n",
       "4  0.154011 -0.768622  0.073582  0.766561  0.014387  0.740279 -0.721846   \n",
       "\n",
       "         14        15        16        17        18        19        20  \\\n",
       "0  0.253845  0.896228  1.888856  1.569833 -0.196590 -0.724887 -0.658471   \n",
       "1 -0.531157 -0.643969 -0.175145  2.140006  0.499012 -0.705995 -0.318749   \n",
       "2 -0.041742  2.474075 -0.516905 -0.905959 -0.719207  2.453757 -0.666196   \n",
       "3 -1.018552  2.467541  0.007737  1.722104  0.055257 -0.243399 -0.685267   \n",
       "4 -1.252427  0.353497 -0.203435  0.613535 -1.043149  0.954753 -0.560617   \n",
       "\n",
       "         21        22        23        24        25        26        27  \\\n",
       "0 -0.094833  0.237521  0.487661 -1.018911 -0.085047 -0.699558  1.819097   \n",
       "1  0.955799  0.596087 -0.741604  1.844986 -0.592684  0.231928 -0.557752   \n",
       "2  1.416064  0.561363 -1.233139  0.363539 -0.153538 -0.753065 -0.977576   \n",
       "3  0.519907 -0.141960  2.132586 -0.351455 -0.595874 -0.763673  0.023498   \n",
       "4 -1.661382  0.652344  0.644744  0.049008  0.142589 -0.758819 -0.977637   \n",
       "\n",
       "         28        29        30        31        32        33        34  \\\n",
       "0 -0.745413 -0.209014 -0.536655 -0.551061 -0.626290 -0.145177  0.954797   \n",
       "1  1.615232 -0.832240  0.529083  1.395408  0.063284  1.477203 -0.607693   \n",
       "2  1.582552 -0.891516  0.854381 -1.686447 -0.630413  0.917298 -0.800475   \n",
       "3 -0.521678 -0.499183  0.335070 -0.417375 -0.339350 -0.403920  0.358163   \n",
       "4  1.629602  1.400235  0.766681  1.184030 -0.747111 -0.428913 -0.797404   \n",
       "\n",
       "         35        36        37        38        39        40        41  \\\n",
       "0  0.030325 -0.333978 -0.716172  1.831318 -0.815223 -0.680160 -0.808847   \n",
       "1  0.066323 -1.191912 -0.462801  0.050357 -0.813357 -0.662537 -0.648801   \n",
       "2 -0.383654 -0.152517 -0.013698  1.421560 -0.800002  0.678951  1.349640   \n",
       "3  1.752987  0.312649  0.256485  0.433688 -0.802652 -0.265331  1.563002   \n",
       "4 -0.449215  1.688384  1.673614 -0.761875 -0.778883 -0.707102 -0.853185   \n",
       "\n",
       "         42        43        44        45        46        47        48  \\\n",
       "0  0.148292 -0.167851 -1.015892  0.543848 -1.664666  0.323791 -0.874336   \n",
       "1 -0.224800 -0.963386  2.254599  0.158790 -1.297993 -0.919179 -0.790977   \n",
       "2  1.354400  1.019803 -0.421299  1.223441 -0.980069  0.097238 -0.966543   \n",
       "3  0.288298  0.139869  0.858205 -0.076926  0.098262 -0.740700  0.578541   \n",
       "4 -3.116618 -1.025830 -0.827048 -0.201275  1.215369 -0.679285 -0.333675   \n",
       "\n",
       "         49        50        51        52        53        54        55  \\\n",
       "0 -2.071137 -0.619346 -0.885334 -0.576842  1.463713 -0.026447  0.659552   \n",
       "1 -2.045965  0.498839  2.187002  0.483492  1.009265  1.649467  1.032962   \n",
       "2  0.593753  0.006015 -1.014916  0.029021  0.630872  1.499162  0.339007   \n",
       "3  0.316091 -0.630003 -0.509272 -0.495719  0.430361 -0.948177 -0.457535   \n",
       "4  0.940520 -0.227671 -0.681174 -0.538202 -1.802282  0.036005 -0.424086   \n",
       "\n",
       "         56        57        58        59        60        61        62  \\\n",
       "0  0.069464 -0.238719 -0.666550 -1.811843 -0.989331  0.020936  0.333912   \n",
       "1 -0.038555  0.753564 -0.811298 -0.311674 -0.560421 -0.695927  0.053301   \n",
       "2  0.561246  0.740199  1.369262  0.835835 -0.563563  1.118850  0.413232   \n",
       "3 -0.386012  0.636068  1.036644 -0.724280 -0.579233 -0.882536 -0.528785   \n",
       "4 -1.061590 -2.386683 -0.275487  1.328893 -0.097465 -0.415497 -1.031416   \n",
       "\n",
       "         63        64        65        66        67        68        69  \\\n",
       "0 -0.957043  0.848880  0.785600  0.792306 -0.637652 -0.425662 -0.833097   \n",
       "1  0.827917  0.727760  0.262516 -1.189286  0.069433  0.148949  1.200276   \n",
       "2  0.749485  0.887943 -0.530724  0.330726 -0.540148 -0.360259 -0.841787   \n",
       "3 -0.742628  1.073733 -2.322800  1.605690  0.006446  0.975049  1.199666   \n",
       "4 -0.120169 -1.135318  0.127382  0.090168 -0.049644 -0.994043 -0.848861   \n",
       "\n",
       "         70        71        72        73        74        75        76  \\\n",
       "0  0.628813  1.638262 -0.412419 -0.525297 -0.646473 -1.325236 -0.591492   \n",
       "1  1.606983  0.797347 -0.563326 -0.351463 -0.841337  0.074523 -0.540233   \n",
       "2 -1.087088 -0.131217 -0.666113  0.167155 -0.853630 -0.749515 -0.645966   \n",
       "3 -0.136038  0.006952  2.837742 -0.520679 -0.816043  0.233348  0.207093   \n",
       "4 -0.608259 -0.725843 -0.343367  2.615374 -0.849832  0.081675  2.361217   \n",
       "\n",
       "         77        78        79        80        81        82        83  \\\n",
       "0 -0.062932 -0.081760  1.964202  0.407308  1.453569 -0.362350 -0.064352   \n",
       "1 -0.000196 -0.767644 -0.784410  1.101888  1.437974  2.901721 -0.596012   \n",
       "2 -0.910024 -0.337017 -0.803533 -0.105199  0.644774  0.270020  0.210557   \n",
       "3 -0.830033 -0.437317 -0.404215 -0.900053  1.660584 -0.374089 -0.778953   \n",
       "4 -0.655917 -0.530237  1.913836  1.870015  0.443875 -0.623531 -0.698811   \n",
       "\n",
       "         84        85        86        87        88        89        90  \\\n",
       "0  1.046524 -1.046313 -0.836077 -0.154928 -0.846819 -0.477627 -2.453846   \n",
       "1  2.645387 -1.054988 -0.641274  1.965607 -1.008591 -0.348907  0.046766   \n",
       "2 -0.613863  0.267352 -1.001924  1.188931  1.446396  0.662903  0.260671   \n",
       "3  0.363649 -0.663762 -0.900234  0.233728 -0.741002 -0.708002  0.066075   \n",
       "4  0.383363 -0.207545 -0.494442 -1.225568 -0.890671  0.035754  0.064838   \n",
       "\n",
       "         91        92        93        94        95        96        97  \\\n",
       "0 -0.139056 -0.180401 -1.779057 -0.148828 -0.005655 -0.464128 -0.859371   \n",
       "1 -0.524472 -0.142755  0.628105 -0.970454 -0.405823  0.481852  0.402860   \n",
       "2 -0.527644  1.587620  0.708334  1.410587 -0.155128 -0.470698 -0.573225   \n",
       "3 -0.579166 -1.102966  0.207542  0.839596 -0.299733 -0.490406  0.098282   \n",
       "4 -0.161638 -0.882829  0.164080 -0.957368 -0.604126 -0.181648  0.368971   \n",
       "\n",
       "         98        99       100       101       102       103       104  \\\n",
       "0 -0.412277 -1.517857  1.980152 -0.750306 -0.413239 -0.386818 -0.115089   \n",
       "1 -0.551052 -0.627485 -0.519872 -0.932559 -0.346737 -0.478548  0.623596   \n",
       "2 -0.251247  0.240875  0.866243 -0.555523  2.232848  0.133247  0.993754   \n",
       "3 -0.895793  0.435478  0.385685  1.514675  0.917220 -0.521825 -1.231826   \n",
       "4 -0.640781  1.024079 -0.304932 -0.969297 -0.616898  1.022784 -0.099087   \n",
       "\n",
       "        105       106       107       108       109       110       111  \\\n",
       "0 -0.745005  0.362949 -0.463772 -0.794857 -0.132273 -0.003437 -0.509415   \n",
       "1 -1.076239  0.268719 -0.830292  1.413474  0.986151 -0.556124 -0.542603   \n",
       "2 -0.477394  0.271009 -0.855420  1.423724 -0.708549  0.076225 -0.561605   \n",
       "3  3.613937 -0.952575  0.179350  1.583678  0.943601 -0.616461 -0.519192   \n",
       "4  0.475426  0.392832 -0.896007 -0.492508  0.061379  1.876532 -0.502208   \n",
       "\n",
       "        112       113       114       115       116       117       118  \\\n",
       "0 -1.246808  0.210003 -0.127254 -0.629358 -0.618295 -0.231429 -0.442610   \n",
       "1  1.006487  0.302786 -0.506379  0.022709  0.854729 -0.681230 -0.935935   \n",
       "2 -0.502072 -0.268384 -0.470481 -0.629750  0.601385 -1.034715 -0.442610   \n",
       "3 -0.036459 -0.387144 -0.521264  2.560401  0.706337 -0.563179 -0.935935   \n",
       "4 -0.667787 -0.131009  0.352563  2.579061 -0.544035 -0.541321 -0.935935   \n",
       "\n",
       "        119       120       121       122       123       124       125  \\\n",
       "0 -0.034637 -0.155002  1.330175 -0.027994 -0.151745 -0.281398  0.550393   \n",
       "1 -1.163156 -0.287454  1.005741 -0.274192 -0.292826 -0.385354  0.429414   \n",
       "2 -0.034637 -0.948329  0.416796 -0.933648 -0.950110 -0.504031 -0.357964   \n",
       "3 -1.163156  2.407592 -1.777520  2.323779  2.397136  2.346159  0.367916   \n",
       "4 -1.163156  1.856834 -1.175219  2.292012  1.888400  2.290951  0.569465   \n",
       "\n",
       "        126       127       128       129       130       131       132  \\\n",
       "0  0.013257 -0.884058 -0.126828 -0.127478 -0.126687 -0.126175 -0.127363   \n",
       "1 -0.243408 -1.006832 -0.126828 -0.127478 -0.126687 -0.126175 -0.127363   \n",
       "2 -0.916449  0.855258 -0.126828 -0.127478 -0.126687 -0.126175 -0.127363   \n",
       "3  2.387944  0.196935 -0.126828 -0.127478 -0.126687 -0.126175 -0.127363   \n",
       "4  2.407292  0.399006 -0.126828 -0.127478 -0.126687 -0.126175 -0.127363   \n",
       "\n",
       "        133       134       135       136       137       138       139  \\\n",
       "0 -0.127658 -0.127822 -0.127182 -0.126877 -0.127519 -0.127724 -0.129077   \n",
       "1 -0.127658 -0.127822 -0.127182 -0.126877 -0.127519 -0.127724 -0.129077   \n",
       "2 -0.127658 -0.127822 -0.127182 -0.126877 -0.127519 -0.127724 -0.129077   \n",
       "3 -0.127658 -0.127822 -0.127182 -0.126877 -0.127519 -0.127724 -0.129077   \n",
       "4 -0.127658 -0.127822 -0.127182 -0.126877 -0.127519 -0.127724 -0.129077   \n",
       "\n",
       "        140       141       142       143       144      145       146  \\\n",
       "0 -0.127806 -0.126315 -0.128695 -0.127879 -0.128247 -0.12841 -0.128272   \n",
       "1 -0.127806 -0.126315 -0.128695 -0.127879 -0.128247 -0.12841 -0.128272   \n",
       "2 -0.127806 -0.126315 -0.128695 -0.127879 -0.128247 -0.12841 -0.128272   \n",
       "3 -0.127806 -0.126315 -0.128695 -0.127879 -0.128247 -0.12841 -0.128272   \n",
       "4 -0.127806 -0.126315 -0.128695 -0.127879 -0.128247 -0.12841 -0.128272   \n",
       "\n",
       "        147       148       149       150       151       152       153  \\\n",
       "0 -0.126894 -0.128378 -0.127494 -0.127617 -0.126431 -0.127379 -0.129345   \n",
       "1 -0.126894 -0.128378 -0.127494 -0.127617 -0.126431 -0.127379 -0.129345   \n",
       "2 -0.126894 -0.128378 -0.127494 -0.127617 -0.126431 -0.127379 -0.129345   \n",
       "3 -0.126894 -0.128378 -0.127494 -0.127617 -0.126431 -0.127379 -0.129345   \n",
       "4 -0.126894 -0.128378 -0.127494 -0.127617 -0.126431 -0.127379 -0.129345   \n",
       "\n",
       "        154       155       156       157       158       159       160  \\\n",
       "0 -0.128067 -0.127978 -0.127912 -0.127617 -0.127009 -0.127231 -0.126803   \n",
       "1 -0.128067 -0.127978 -0.127912 -0.127617 -0.127009 -0.127231 -0.126803   \n",
       "2 -0.128067 -0.127978 -0.127912 -0.127617 -0.127009 -0.127231 -0.126803   \n",
       "3 -0.128067 -0.127978 -0.127912 -0.127617 -0.127009 -0.127231 -0.126803   \n",
       "4 -0.128067 -0.127978 -0.127912 -0.127617 -0.127009 -0.127231 -0.126803   \n",
       "\n",
       "        161       162       163       164       165       166       167  \\\n",
       "0 -0.127691 -0.127576 -0.128231 -0.128247 -0.127281 -0.128182 -0.128744   \n",
       "1 -0.127691 -0.127576 -0.128231 -0.128247 -0.127281 -0.128182 -0.128744   \n",
       "2 -0.127691 -0.127576 -0.128231  7.797444 -0.127281 -0.128182 -0.128744   \n",
       "3 -0.127691 -0.127576 -0.128231 -0.128247 -0.127281 -0.128182 -0.128744   \n",
       "4 -0.127691 -0.127576 -0.128231 -0.128247 -0.127281 -0.128182 -0.128744   \n",
       "\n",
       "        168       169      170       171       172       173       174  \\\n",
       "0 -0.127511 -0.128051 -0.12783 -0.126704 -0.128321 -0.128459 -0.127847   \n",
       "1 -0.127511 -0.128051 -0.12783 -0.126704 -0.128321 -0.128459 -0.127847   \n",
       "2 -0.127511 -0.128051 -0.12783 -0.126704 -0.128321 -0.128459 -0.127847   \n",
       "3 -0.127511 -0.128051 -0.12783 -0.126704 -0.128321 -0.128459 -0.127847   \n",
       "4 -0.127511 -0.128051 -0.12783 -0.126704 -0.128321 -0.128459 -0.127847   \n",
       "\n",
       "        175       176       177       178       179      180       181  \\\n",
       "0 -0.128524 -0.128728 -0.128378 -0.128541 -0.126547 -0.12801 -0.128647   \n",
       "1 -0.128524 -0.128728 -0.128378 -0.128541 -0.126547 -0.12801 -0.128647   \n",
       "2 -0.128524 -0.128728 -0.128378 -0.128541 -0.126547 -0.12801 -0.128647   \n",
       "3 -0.128524 -0.128728 -0.128378 -0.128541 -0.126547 -0.12801 -0.128647   \n",
       "4 -0.128524 -0.128728 -0.128378 -0.128541 -0.126547 -0.12801 -0.128647   \n",
       "\n",
       "        182       183     184      185       186       187      188       189  \\\n",
       "0 -0.128647 -0.128736 -0.1271 -0.12696 -0.127428 -0.128801 -0.12792 -0.128002   \n",
       "1 -0.128647 -0.128736 -0.1271 -0.12696 -0.127428 -0.128801 -0.12792 -0.128002   \n",
       "2 -0.128647 -0.128736 -0.1271 -0.12696 -0.127428 -0.128801 -0.12792 -0.128002   \n",
       "3 -0.128647 -0.128736 -0.1271 -0.12696 -0.127428 -0.128801 -0.12792 -0.128002   \n",
       "4 -0.128647 -0.128736 -0.1271 -0.12696 -0.127428 -0.128801 -0.12792 -0.128002   \n",
       "\n",
       "        190       191       192       193       194      195      196  \\\n",
       "0 -0.127757 -0.129515 -0.127568 -0.128321 -0.128002 -0.12902 -0.12792   \n",
       "1 -0.127757 -0.129515 -0.127568 -0.128321 -0.128002 -0.12902 -0.12792   \n",
       "2 -0.127757 -0.129515 -0.127568 -0.128321 -0.128002 -0.12902 -0.12792   \n",
       "3 -0.127757 -0.129515 -0.127568 -0.128321 -0.128002 -0.12902 -0.12792   \n",
       "4 -0.127757 -0.129515 -0.127568 -0.128321 -0.128002 -0.12902 -0.12792   \n",
       "\n",
       "       197       198       199       200       201       202       203  \\\n",
       "0 -0.12756 -0.128573 -0.128524 -0.127428 -0.128076 -0.129134 -0.127486   \n",
       "1 -0.12756 -0.128573 -0.128524 -0.127428 -0.128076 -0.129134 -0.127486   \n",
       "2 -0.12756 -0.128573 -0.128524 -0.127428 -0.128076 -0.129134 -0.127486   \n",
       "3 -0.12756 -0.128573 -0.128524 -0.127428 -0.128076 -0.129134 -0.127486   \n",
       "4 -0.12756 -0.128573 -0.128524 -0.127428 -0.128076 -0.129134 -0.127486   \n",
       "\n",
       "       204       205       206       207       208       209       210  \\\n",
       "0 -0.12863 -0.128084  7.792981 -0.128051 -0.126621 -0.127593 -0.128141   \n",
       "1 -0.12863 -0.128084 -0.128321 -0.128051 -0.126621 -0.127593 -0.128141   \n",
       "2 -0.12863 -0.128084 -0.128321 -0.128051 -0.126621 -0.127593 -0.128141   \n",
       "3 -0.12863 -0.128084 -0.128321 -0.128051 -0.126621 -0.127593 -0.128141   \n",
       "4 -0.12863 -0.128084 -0.128321 -0.128051 -0.126621 -0.127593 -0.128141   \n",
       "\n",
       "        211       212       213       214       215       216       217  \\\n",
       "0 -0.127797 -0.126976 -0.127937 -0.127264 -0.128027 -0.127896 -0.127363   \n",
       "1 -0.127797 -0.126976 -0.127937 -0.127264 -0.128027 -0.127896 -0.127363   \n",
       "2 -0.127797 -0.126976 -0.127937 -0.127264 -0.128027 -0.127896 -0.127363   \n",
       "3 -0.127797 -0.126976 -0.127937 -0.127264 -0.128027 -0.127896 -0.127363   \n",
       "4 -0.127797 -0.126976 -0.127937 -0.127264 -0.128027 -0.127896 -0.127363   \n",
       "\n",
       "        218       219       220       221       222       223       224  \\\n",
       "0 -0.126819 -0.128149 -0.128573 -0.127912 -0.127609 -0.127108 -0.126605   \n",
       "1 -0.126819 -0.128149 -0.128573 -0.127912 -0.127609 -0.127108 -0.126605   \n",
       "2 -0.126819 -0.128149 -0.128573 -0.127912 -0.127609 -0.127108 -0.126605   \n",
       "3 -0.126819 -0.128149 -0.128573 -0.127912 -0.127609 -0.127108 -0.126605   \n",
       "4 -0.126819 -0.128149 -0.128573 -0.127912 -0.127609 -0.127108 -0.126605   \n",
       "\n",
       "        225      226       227       228      229       230       231  \\\n",
       "0 -0.128149 -0.12949 -0.128231 -0.127773 -0.12663 -0.128704 -0.128598   \n",
       "1 -0.128149 -0.12949 -0.128231 -0.127773 -0.12663 -0.128704 -0.128598   \n",
       "2 -0.128149 -0.12949 -0.128231 -0.127773 -0.12663 -0.128704 -0.128598   \n",
       "3 -0.128149 -0.12949 -0.128231 -0.127773 -0.12663 -0.128704 -0.128598   \n",
       "4 -0.128149 -0.12949 -0.128231 -0.127773 -0.12663 -0.128704 -0.128598   \n",
       "\n",
       "        232       233       234       235       236       237       238  \\\n",
       "0 -0.127461 -0.128263 -0.127904 -0.128059 -0.128476 -0.126506 -0.127461   \n",
       "1 -0.127461 -0.128263 -0.127904 -0.128059 -0.128476 -0.126506 -0.127461   \n",
       "2 -0.127461 -0.128263 -0.127904 -0.128059 -0.128476 -0.126506 -0.127461   \n",
       "3 -0.127461 -0.128263 -0.127904 -0.128059 -0.128476 -0.126506 -0.127461   \n",
       "4 -0.127461 -0.128263 -0.127904 -0.128059 -0.128476 -0.126506 -0.127461   \n",
       "\n",
       "        239       240       241       242       243       244       245  \n",
       "0 -0.128573 -0.126365 -0.127896 -0.128182 -0.129045 -0.126423 -0.127428  \n",
       "1 -0.128573 -0.126365 -0.127896 -0.128182 -0.129045 -0.126423 -0.127428  \n",
       "2 -0.128573 -0.126365 -0.127896 -0.128182 -0.129045 -0.126423 -0.127428  \n",
       "3 -0.128573 -0.126365 -0.127896 -0.128182 -0.129045 -0.126423 -0.127428  \n",
       "4 -0.128573 -0.126365 -0.127896 -0.128182 -0.129045 -0.126423 -0.127428  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "814eabb1-0be6-4512-92dd-54024c0eb1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '236', '237', '238', '239', '240', '241', '242', '243', '244', '245'],\n",
       "      dtype='object', length=246)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1951e99a-57c4-465f-b5ce-94fe45f45f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '236', '237', '238', '239', '240', '241', '242', '243', '244', '245'],\n",
       "      dtype='object', length=246)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca5cde74-1623-40b7-879e-87cb24b397b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_test = X_test.to_numpy()\n",
    "X_wide_te = wide_preprocessor.fit_transform(X_test)\n",
    "X_tab_te = tab_preprocessor.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5406295-1bcb-4be3-8515-aa756debd783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[     1,  68341, 128605, ..., 686901, 686903, 686905],\n",
       "       [     2,  68342, 128606, ..., 686901, 686903, 686905],\n",
       "       [     3,  68343, 128607, ..., 686901, 686903, 686905],\n",
       "       ...,\n",
       "       [     8,  68348, 128612, ..., 686901, 686903, 686905],\n",
       "       [     9,  68349, 128613, ..., 686901, 686903, 686905],\n",
       "       [    10,  68350, 128614, ..., 686901, 686903, 686905]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_te[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be80ca97-efab-4bda-889c-e572aeb04999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        , ...,  0.55039265,\n",
       "         0.01325731, -0.88405807],\n",
       "       [ 2.        ,  2.        ,  2.        , ...,  0.4294142 ,\n",
       "        -0.2434078 , -1.0068317 ],\n",
       "       [ 3.        ,  3.        ,  1.        , ..., -0.35796436,\n",
       "        -0.91644899,  0.85525809],\n",
       "       ...,\n",
       "       [ 8.        ,  8.        ,  2.        , ..., -0.20433381,\n",
       "        -0.65738129, -0.80560633],\n",
       "       [ 9.        ,  9.        ,  1.        , ..., -0.26027519,\n",
       "        -0.68821454, -1.08294345],\n",
       "       [10.        , 10.        ,  4.        , ..., -0.68056968,\n",
       "        -0.74512913,  0.60969703]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab_te[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c904b869-0b6f-443c-895f-b93ca4f80e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493474, 136)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc2df00a-c9d6-4dc7-9a5f-54f496ec2580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493474, 121)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f17cd1d-c053-4fee-b08f-8f6e3ee0a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = pd.read_feather(datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')\n",
    "\n",
    "# low_card_features = [f for f in X_test.columns if X_test[f].nunique() <= 50000]\n",
    "# high_card_features = [f for f in X_test.columns if X_test[f].nunique() > 50000]\n",
    "\n",
    "# wide_cols_pre = [f for f in X_test.columns if X_test[f].nunique() <= max_card_cat and X_test[f].nunique() > 2]\n",
    "# wide_cols_onehot = [f for f in X_test.columns if X_test[f].nunique() == 2]\n",
    "# cont_cols = high_card_features\n",
    "# embed_cols = [f for f in X_test.columns if X_test[f].nunique() <= max_card_embed and X_test[f].nunique() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c06afef5-79e2-4751-8e79-57037f1cf393",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-68a894bef700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# X_test = X_test.to_numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_wide_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwide_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_tab_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtab_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;34mr\"\"\"Returns the processed dataframe\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mdf_wide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_wide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_wide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_crossed_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_crossed_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/wide_preprocessor.py\u001b[0m in \u001b[0;36m_prepare_wide\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_cc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# # X_test = X_test.to_numpy()\n",
    "# X_wide_te = wide_preprocessor.transform(X_test)\n",
    "# X_tab_te = tab_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9d22a-cce9-4370-8192-13a432bdd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wide_te = wide_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb9552-d4b9-4e83-9cb4-fd22c4d3f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = trainer.predict(X_wide=X_wide_te, X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078f3c6-b2af-412c-8f3d-a5866f13d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8de5b122-a5e4-4ae8-ad5d-bae73d59e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "predict:  24%|       | 116/482 [00:01<00:04, 89.41it/s] \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-c0c80264a3a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_wide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_wide_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_tab_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/training/trainer.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X_wide, X_tab, X_text, X_img, X_test, batch_size)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \"\"\"\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         \u001b[0mpreds_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_wide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/training/trainer.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X_wide, X_tab, X_text, X_img, X_test, batch_size)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     preds = (\n\u001b[0;32m-> 1151\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tabnet\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m                     )\n\u001b[1;32m   1153\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/models/wide_deep.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_deephead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwide_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwide_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     def _build_deephead(\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/models/wide_deep.py\u001b[0m in \u001b[0;36m_forward_deep\u001b[0;34m(self, X, wide_out)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mwide_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mwide_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptabular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"deeptabular\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mwide_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"deeptext\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/models/tab_mlp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthrough\u001b[0m \u001b[0ma\u001b[0m \u001b[0mseries\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdense\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mx_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat_embed_and_cont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx_emb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/models/tab_mlp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_input\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             embed = [\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"emb_layer_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/pytorch_widedeep/models/tab_mlp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_input\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             embed = [\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"emb_layer_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             ]\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/anaconda3/envs/tabular/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "preds_proba = trainer.predict_proba(X_wide=X_wide_te, X_tab=X_tab_te, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed2697c9-926b-4802-945b-a768631748e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|| 936/936 [00:04<00:00, 193.74it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_proba_train = trainer.predict_proba(X_wide=X_wide, X_tab=X_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef18171c-34c0-497e-9d86-4bf38aa8dbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_te[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a89c15b-a44f-4a20-95cd-f5a94c062a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01788539, 0.98211461],\n",
       "       [0.79904032, 0.20095971],\n",
       "       [0.02058744, 0.97941256],\n",
       "       [0.34795648, 0.65204352],\n",
       "       [0.10245168, 0.89754832],\n",
       "       [0.72765213, 0.27234787],\n",
       "       [0.55998504, 0.44001493],\n",
       "       [0.30380297, 0.69619703],\n",
       "       [0.73843539, 0.26156464],\n",
       "       [0.87854004, 0.12145996],\n",
       "       [0.40143102, 0.59856898],\n",
       "       [0.61878359, 0.38121644],\n",
       "       [0.34835207, 0.65164793],\n",
       "       [0.77924246, 0.22075753],\n",
       "       [0.93200088, 0.06799912],\n",
       "       [0.03524351, 0.96475649],\n",
       "       [0.06104219, 0.93895781],\n",
       "       [0.8391974 , 0.16080263],\n",
       "       [0.5203411 , 0.4796589 ],\n",
       "       [0.02382565, 0.97617435]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_proba_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dba64d2b-96a1-4dc9-a837-c27ce7ea5c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_proba[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ea2fe-0f38-4b5b-b9e9-913d8d33e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(preds_proba, datapath/'preds/widedeep_5epochs_bs1024_64x32tabmlp_20210930_probas.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5f215-b585-4f7b-afac-36e49ee28c8f",
   "metadata": {},
   "source": [
    "## Weights and Biases Run Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a928e1-0a18-4c91-b9bb-a32846e39e5b",
   "metadata": {},
   "source": [
    "Below is the configuration for a Weights and Biases (`wandb`) run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5402d98-7bcd-4e46-b85d-6b0129ca6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "config_run = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['stacking-sklearn', 'attempt'],\n",
    "    'notes': \"Trying a fastai tabular MLP model, for ensembling with the GBMs\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638002ad-9266-44d6-8302-ebce2a6f7b06",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30d6f8-d893-43d3-8a4d-41c5756b6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379b0bf-1c3b-4095-b382-ec5a16e3474d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24391812-dce3-4513-bd38-ee95694730e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_valid, y_train, y_valid, model_config, \n",
    "                                              random_state=42,\n",
    "                                              exmodel_config=exmodel_config, \n",
    "                                              config_run=config_run):#, scaler): # passed in via config dict for now\n",
    "    \"\"\"\n",
    "    Basic training function. Note that some of the options passed via the argument are\n",
    "    in fact hard-coded in, to avoid inconveniences.\n",
    "    :param X_train: the training set features\n",
    "    :param X_valid: the validation set features\n",
    "    :param y_train: the training set targets\n",
    "    :param y_valid: the validation set targets\n",
    "    :param random_staKFold: for reproducibility\n",
    "    :param exmodel_config: dict containing configuration details including the library \n",
    "                            (thus model) used, preprocessing, and cross-validation\n",
    "    :param model_config: dict containing hyperparameter specifications for the model\n",
    "    :param config_run: dict containing wandb run configuration (name, etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    # As of 20210920, best CatBoost config is:\n",
    "    best_20210920_catboost_params = {\n",
    "        'iterations': 3493,\n",
    "        'depth': 5,\n",
    "        'learning_rate': 0.09397459954141321,\n",
    "        'random_strength': 43,\n",
    "        'l2_leaf_reg': 26,\n",
    "        'border_count': 239,\n",
    "        'bagging_temperature': 12.532400413798356,\n",
    "        'od_type': 'Iter'\n",
    "    }\n",
    "    \n",
    "    # catboost 20210921 on colab (only 15 trials though)\n",
    "    best_catboost_params = {\n",
    "        'iterations': 3302,\n",
    "        'depth': 5,\n",
    "        'learning_rate': 0.017183208677599107,\n",
    "        'random_strength': 41,\n",
    "        'l2_leaf_reg': 30,\n",
    "        'border_count': 251,\n",
    "        'bagging_temperature': 9.898390369028036, \n",
    "        'od_type': 'IncToDec'\n",
    "    }\n",
    "    \n",
    "    # optuna 20210921\n",
    "    best_xgboost_params = {\n",
    "        'n_estimators': 1119,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.04123392555159452,\n",
    "        'reg_alpha': 4.511876752318655,\n",
    "        'reg_lambda': 4.074347238862406,\n",
    "        'subsample': 0.8408586950521992\n",
    "    }\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"202109_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=exmodel_config)   \n",
    "        \n",
    "    if exmodel_config['library'] == 'xgboost':\n",
    "        model = XGBClassifier(\n",
    "            tree_method=model_config['tree_method'],\n",
    "            random_state=random_state,\n",
    "            n_jobs=model_config['n_jobs'], \n",
    "            verbosity=model_config['verbosity'], \n",
    "            objective=model_config['objective'],\n",
    "            **best_xgboost_params\n",
    "            # #             eval_metric=model_config['eval_metric'],\n",
    "\n",
    "            # comment out the below for a fairly default model\n",
    "#             booster=model_config['booster'],\n",
    "#             max_depth=model_config['max_depth'],\n",
    "#             learning_rate=model_config['learning_rate'], \n",
    "#             subsample=model_config['subsample'],\n",
    "#             reg_alpha=model_config['reg_alpha'],\n",
    "#             reg_lambda=model_config['reg_lambda'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()],\n",
    "#                                     eval_metric=model_config['eval_metric'],\n",
    "                 )\n",
    "\n",
    "\n",
    "    elif exmodel_config['library'] == 'lightgbm':\n",
    "        model = LGBMClassifier(\n",
    "#             boosting_type=model_config['boosting_type'],\n",
    "#             max_depth=model_config['max_depth']\n",
    "            # TODO\n",
    "            random_state=random_state,\n",
    "            n_jobs=model_config['n_jobs'],\n",
    "            objective=model_config['objective'],\n",
    "#             eval_metric=model_config['eval_metric'],\n",
    "            boosting_type=model_config['boosting_type'],\n",
    "            device_type=model_config['device_type'],\n",
    "            \n",
    "            # comment out the below for a basically default model\n",
    "            n_estimators=model_config['n_estimators'],\n",
    "            learning_rate=model_config['learning_rate'],\n",
    "            max_depth=model_config['max_depth'],\n",
    "            reg_alpha=model_config['reg_alpha'],\n",
    "            reg_lambda=model_config['reg_lambda'],\n",
    "            subsample=model_config['subsample'],\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],\n",
    "#                                     eval_metric=model_config['eval_metric'],\n",
    "                 )\n",
    "        \n",
    "    elif exmodel_config['library'] == 'catboost':\n",
    "        print(\"CatBoost, therefore no WandB callback.\")\n",
    "        model = CatBoostClassifier(\n",
    "#             n_estimators=config['n_estimators'],\n",
    "#             learning_rate=config['learning_rate'],\n",
    "#             max_depth=config['max_depth'],\n",
    "            task_type=model_config['task_type'],\n",
    "    #         n_jobs=config['n_jobs'],\n",
    "    #         verbosity=config['verbosity'],\n",
    "    #         subsample=config['subsample'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "            random_state=random_state,\n",
    "            # objective='Logloss', # default, accepts only one\n",
    "#             custom_metrics=model_config['custom_metrics'],\n",
    "    #         bootstrap_type=config['bootstrap_type'],\n",
    "    #         device:config['device']\n",
    "            **best_catboost_params\n",
    "        ) \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "#     y_train_pred = model.predict(X_train)\n",
    "    y_train_pred = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    train_loss = log_loss(y_train, y_train_pred)\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    wandb.log({'train_loss': train_loss, 'train_auc': train_auc})\n",
    "\n",
    "    if exmodel_config['library'] == 'catboost':\n",
    "        print(model.get_all_params())\n",
    "        wandb.log(model.get_all_params())\n",
    "    else:\n",
    "        wandb.log(model.get_params()) # logging model parameters, trying bare-invocation rather than params: model.get_params()\n",
    "    \n",
    "    # trying with predict_proba\n",
    "    y_pred = model.predict_proba(X_valid)[:,1]\n",
    "#     y_pred = model.predict(X_valid)\n",
    "\n",
    "    valid_loss = log_loss(y_valid, y_pred)\n",
    "    valid_auc = roc_auc_score(y_valid, y_pred)\n",
    "    wandb.log({'valid_loss':valid_loss, 'valid_auc':valid_auc})\n",
    "    print(f\"Valid log-loss is {valid_loss}\\nValid AUC is {valid_auc}\")   \n",
    "#     wandb.finish()   \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfa66428-3fb9-410f-9ea5-50785a4bd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model_config, X=X, y=y, start_fold=0, exmodel_config=exmodel_config, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to handle model training process in the context of cross-validation -- via hold-out or via k-fold.\n",
    "    If exmodel_config['cross_val_strategy'] == None, then any kfolds= input is ignored; otherwise, the number specified is used.\n",
    "    \n",
    "    :param kfolds: int specifying number of k-folds to use in cross-validation\n",
    "    :param exmodel_config: dict containing general config including for cross-validation -- `kfold=1` implies hold-out\n",
    "    \"\"\"\n",
    "    if exmodel_config['kfolds'] == 1:\n",
    "        print(\"Proceeding with holdout\")\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=exmodel_config['test_size'], \n",
    "                                                      random_state=random_state,\n",
    "                                                     )\n",
    "        model = train(X_train, X_valid, y_train, y_valid, exmodel_config=exmodel_config, \n",
    "                                                    model_config=model_config,\n",
    "                                                    config_run=config_run)\n",
    "        wandb.finish()\n",
    "        \n",
    "    else:\n",
    "        X, y = X.to_numpy(), y.to_numpy()\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=random_state)\n",
    "        models = {}\n",
    "        model_path = Path(datapath/f\"models/{config_run['name']}_{exmodel_config['kfolds']}folds/\")\n",
    "        (model_path).mkdir(exist_ok=True)\n",
    "        for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "            if fold < start_fold:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"FOLD {fold}\")\n",
    "                print(\"---------------------------------------------------\")\n",
    "                X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "                model = train(X_train, X_valid, y_train, y_valid, exmodel_config=exmodel_config, \n",
    "                                                    model_config=model_config,\n",
    "                                                    config_run=config_run)\n",
    "                wandb.log({'fold': fold})\n",
    "                models[fold] = model\n",
    "                dump(model, Path(model_path/f\"{exmodel_config['library']}_fold{fold}_model.joblib\"))\n",
    "                wandb.finish()\n",
    "        return models\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a437b-f880-4284-8e02-66a398ba6454",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58234814-68d1-4ee4-bedd-d722c18e4fa6",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4085e60e-13cf-4da7-90b9-30306480b4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library = 'xgboost'\n",
    "# exmodel_config['library'] = library\n",
    "# model_config = model_configurator(library)\n",
    "# xgboost_models = cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b780292-f23c-4a3f-be1e-53038c5ae7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for scaler in [StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler]:\n",
    "#     exmodel_config['scaler'] = scaler\n",
    "#     scaler = scaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "#     exmodel_config['library'] = 'lightgbm'\n",
    "#     model_config = model_configurator('lightgbm')\n",
    "#     cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1106b86c-70c1-4722-a86f-6c53a2e32503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library = 'lightgbm'\n",
    "# exmodel_config['library'] = library\n",
    "# model_config = model_configurator(library)\n",
    "# lightgbm_models = cross_validation(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542539d-323d-4a4a-b574-d0bf883179b2",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776a94d-01f3-46c0-9c9c-0d162de17e37",
   "metadata": {},
   "source": [
    "## Via `sklearn.ensemble.StackingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca87f5f-e214-4968-a789-74f83dc7a656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e68e3adf-fd9d-4185-87c4-330ecd768679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost_estimators = [(f'xgboost_fold{fold}', xgboost_models[fold]) for fold in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed3aa865-fec7-417b-9369-677d3b840760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaving this default for first try\n",
    "# final_estimator = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ac5214d-a3ab-4219-8047-822c4385e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacker(estimators:dict, library:str, X=X, y=y): #, load_models:bool=False, load_path:Path=None):\n",
    "    \"\"\"\n",
    "    A wrapper that will take a dict of the form {fold:int : model} and a string representing the library (for file-naming), \n",
    "    then run `sklearn.ensemble.StackingClassifier` with it, and save the stacked model afterward\n",
    "    \"\"\"\n",
    "    estimators_list = [(f'{library}_fold{fold}', estimators[fold]) for fold in range(5)]\n",
    "    blender = StackingClassifier(estimators=estimators_list,\n",
    "                                 cv=5,\n",
    "                                 stack_method='predict_proba',\n",
    "                                 n_jobs=2,\n",
    "                                 passthrough=False,\n",
    "                                 verbose=1\n",
    "                                )\n",
    "    print(f\"Starting fitting at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    blender.fit(X,y)\n",
    "    print(f\"Fitting complete at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    dump(blender, filename=datapath/f\"models/{config_run['name']}_{exmodel_config['kfolds']}folds/{library}_stack.joblib\")\n",
    "    print(f\"Blender model saved at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    return blender\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39f08073-4061-4f03-be9c-1c1fa601b624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhushifang\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">catboost_20210922_112905</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/drhfu0df\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/drhfu0df</a><br/>\n",
       "                Run data is saved locally in <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_112905-drhfu0df</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# might encapsulate this in a new version of the above train function later\n",
    "exmodel_config['ensemble'] = 'stacking'\n",
    "\n",
    "wandb.init(\n",
    "        project=\"202109_Kaggle_tabular_playground\",\n",
    "        save_code=True,\n",
    "        tags=config_run['tags'],\n",
    "        name=config_run['name'],\n",
    "        notes=config_run['notes'],\n",
    "        config=exmodel_config)   \n",
    "\n",
    "random_state = exmodel_config['random_state'] # 42\n",
    "\n",
    "\n",
    "# # optuna 20210921\n",
    "# best_xgboost_params = {\n",
    "#     'n_estimators': 1119,\n",
    "#     'max_depth': 6,\n",
    "#     'learning_rate': 0.04123392555159452,\n",
    "#     'reg_alpha': 4.511876752318655,\n",
    "#     'reg_lambda': 4.074347238862406,\n",
    "#     'subsample': 0.8408586950521992\n",
    "# }\n",
    "\n",
    "# model_config = model_configurator('xgboost')\n",
    "# xgboost_model = XGBClassifier(\n",
    "#             tree_method=model_config['tree_method'],\n",
    "#             random_state=random_state,\n",
    "# #             n_jobs=model_config['n_jobs'], \n",
    "#             verbosity=model_config['verbosity'], \n",
    "#             objective=model_config['objective'],\n",
    "#             **best_xgboost_params\n",
    "#             # #             eval_metric=model_config['eval_metric'],\n",
    "\n",
    "#             # comment out the below for a fairly default model\n",
    "# #             booster=model_config['booster'],\n",
    "# #             max_depth=model_config['max_depth'],\n",
    "# #             learning_rate=model_config['learning_rate'], \n",
    "# #             subsample=model_config['subsample'],\n",
    "# #             reg_alpha=model_config['reg_alpha'],\n",
    "# #             reg_lambda=model_config['reg_lambda'],\n",
    "# #             n_estimators=model_config['n_estimators'],\n",
    "#         )\n",
    "\n",
    "# model_config = model_configurator('lightgbm')\n",
    "# lightgbm_model = LGBMClassifier(\n",
    "#             random_state=random_state,\n",
    "# #             n_jobs=model_config['n_jobs'],\n",
    "#             objective=model_config['objective'],\n",
    "#             boosting_type=model_config['boosting_type'],\n",
    "#             device_type=model_config['device_type'],\n",
    "            \n",
    "#             # comment out the below for a basically default model\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "#             learning_rate=model_config['learning_rate'],\n",
    "#             max_depth=model_config['max_depth'],\n",
    "#             reg_alpha=model_config['reg_alpha'],\n",
    "#             reg_lambda=model_config['reg_lambda'],\n",
    "#             subsample=model_config['subsample'],\n",
    "#         )\n",
    "\n",
    "model_config = model_configurator('catboost', gpu_available=False) # set GPU false to avoid parallel threads blocking GPU\n",
    "\n",
    "# # As of 20210920, best CatBoost config is:\n",
    "# best_20210920_catboost_params = {\n",
    "#     'iterations': 3493,\n",
    "#     'depth': 5,\n",
    "#     'learning_rate': 0.09397459954141321,\n",
    "#     'random_strength': 43,\n",
    "#     'l2_leaf_reg': 26,\n",
    "#     'border_count': 239,\n",
    "#     'bagging_temperature': 12.532400413798356,\n",
    "#     'od_type': 'Iter'\n",
    "# }\n",
    "\n",
    "# catboost 20210921 on colab (only 15 trials though)\n",
    "best_catboost_params = {\n",
    "    'iterations': 3302,\n",
    "    'depth': 5,\n",
    "    'learning_rate': 0.017183208677599107,\n",
    "    'random_strength': 41,\n",
    "    'l2_leaf_reg': 30,\n",
    "    'border_count': 251,\n",
    "    'bagging_temperature': 9.898390369028036, \n",
    "    'od_type': 'IncToDec'\n",
    "}\n",
    "    \n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "            task_type=model_config['task_type'],\n",
    "#             n_estimators=model_config['n_estimators'],\n",
    "            random_state=random_state,\n",
    "            \n",
    "            **best_catboost_params\n",
    "        ) \n",
    "\n",
    "\n",
    "\n",
    "estimators_list = [\n",
    "#     ('xgboost', xgboost_model),\n",
    "#     ('lightgbm', lightgbm_model),\n",
    "    ('catboost', catboost_model)\n",
    "]\n",
    "\n",
    "# wandb.log({'estimators': estimators_list})\n",
    "\n",
    "final_estimator = LogisticRegression(max_iter=1000)\n",
    "exmodel_config['blender_final_estimator'] = str(final_estimator)\n",
    "exmodel_config['blender-passthrough'] = False\n",
    "\n",
    "blender = StackingClassifier(estimators=estimators_list,\n",
    "                             final_estimator=final_estimator,\n",
    "                             cv=5,\n",
    "                             stack_method='predict_proba',\n",
    "                             n_jobs=-1, # 4 is max allowable for CPU\n",
    "                             passthrough=exmodel_config['blender-passthrough'],\n",
    "                             verbose=1\n",
    "                            )\n",
    "\n",
    "\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aca21cef-74f4-4b23-a82e-8c35419dd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'blender-final_estimator': str(blender.final_estimator),\n",
    "#            'blender-final_estimator_params': str(blender.final_estimator.get_params()),\n",
    "           'blender-stack_mdethod': 'predict_proba',\n",
    "           'blender-cv': 5\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e82b11-2324-449a-8b85-58b4cbe9921c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting at 20210922_112912\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting fitting at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "blender.fit(X,y) # unsure of this -- given kwarg cv=5, is it producing the splits? Or do I have to somehow?\n",
    "print(f\"Fitting complete at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1b180-22cf-494b-8987-aec8918d9d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb.log({'xgboost_params':str(blender.estimators[0][1].get_params()),\n",
    "#            'lightgbm_params':str(blender.estimators[1][1].get_params()),\n",
    "# #            'catboost_params':str(blender.estimators[2][1].get_all_params()),\n",
    "#           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8076f3-d1dc-4bd1-80ad-0eb29cc5bd00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = Path(datapath/f\"models/{config_run['name']}/\")\n",
    "(model_path).mkdir(exist_ok=True)\n",
    "dump(blender, filename=model_path/f\"{config_run['name']}_stack.joblib\")\n",
    "print(f\"Blender model saved at {datetime.now().strftime('%Y%m%d_%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b57e6-e4d7-4cb2-aa5d-7757a8d6d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = blender.predict_proba(X)[:,1]\n",
    "train_loss = log_loss(y_pred=train_preds, y_true=y)\n",
    "train_auc = roc_auc_score(y, train_preds)\n",
    "wandb.log({'train_loss': train_loss, 'train_auc': train_auc})\n",
    "print(f\"train_loss is {train_loss}, train_auc is {train_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec8f4d-ef7e-4382-b5c0-a960099a95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c226e-1ef7-4e03-91a1-06fbb73139f0",
   "metadata": {},
   "source": [
    "# Test set preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ec520-259f-44d2-ad33-7b8c22621132",
   "metadata": {},
   "source": [
    "(Here's where encapsulating the transformations in a pipeline would come in handy. But I'll do it manually for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec74e4-ccb8-43b4-b910-3df1542aaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [x for x in test_df.columns if x != 'claim']\n",
    "# X_test = test_df[features] # this is just for naming consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725cd3e-f883-4d20-837a-9f557b2122a9",
   "metadata": {},
   "source": [
    "Now, let's get the features the model was trained on and subset the test set's features accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66e579-7f01-46e5-a47c-580c8f5d678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation polynomial features\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)\n",
    "# X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1817e3a-7d90-4bc2-8c47-e97806f7dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_poly_names = poly.get_feature_names(X_test.columns)\n",
    "# X_poly_names[100:150]\n",
    "# features = pd.read_csv('X_candidates_20210827.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6bc49-d478-4f59-84d2-5e23e3e236db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks = [feature in X_test_poly_names for feature in features]\n",
    "# checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68187e-271a-4df1-ae02-a2bb5d62c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_final = pd.DataFrame(X_test_poly, columns=X_test_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020ad9b-1b05-49b8-b89b-c90362c256d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_final = X_test_final[features[1:]]\n",
    "# X_test_final = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07699efa-37df-4ed9-aaf2-1b77a73f9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['nan_count'] = X_test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd0c2d-7f9a-4fb6-8b4d-c3c51a9ef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = SimpleImputer(strategy='median', add_indicator=True)\n",
    "# X_test_imputed_np = imputer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ef600-e3cd-44ef-ae66-e1d1663ec4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_imputed = pd.DataFrame(X_test_imputed, columns=[str(x) for x in range(X_test_imputed.shape[1])])\n",
    "# X_test_imputed.to_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da840a-caa6-4d76-a542-c1315a593346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = exmodel_config['scaler']()\n",
    "# X_test_imputed_scaled_np = scaler.fit_transform(X_test_imputed)\n",
    "# X_test_imputed_scaled = pd.DataFrame(X_test_imputed_scaled_np, columns=X_test_imputed.columns)\n",
    "# X_test_imputed_scaled.to_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators_StandardScaled.feather')\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=X_poly_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c062597-20aa-4054-accf-4da20a1400bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path = str(datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')\n",
    "wandb.log({'test_set': test_set_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99cf5b-3477-47f4-9391-73e2ff93c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_imputed_scaled = pd.read_feather(path=datapath/'X_test_NaNcounts_imputed-Median-wIndicators_StandardScaled.feather')\n",
    "X_test_imputed_scaled = pd.read_feather(path=datapath/'X_test_NaNcounts_SummaryStats_imputed-Median-wIndicators-StandardScaled.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f347f-872f-4011-9f13-78fad542f36a",
   "metadata": {},
   "source": [
    "## Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773d286-257d-4776-add5-0f724944befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = Path(datapath/\"preds/\")\n",
    "\n",
    "blender_preds = blender.predict_proba(X_test_imputed_scaled)[:,1]\n",
    "dump(blender_preds, preds_path/f\"{config_run['name']}_stack.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719290c6-a10f-4506-8eeb-6a4bbf281b3d",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a464a1c-9ca8-4a07-9cdb-18af399cf95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(datapath/'sample_solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f4b8-2356-4916-a091-45793db784ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[:, 'claim'] = blender_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957ce26-bbf5-4aee-bccd-988f2471db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f6d45-6b2a-4fe8-be77-ecc70cec6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = datapath/'submissions'\n",
    "submission_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ae726f8-0c8e-46ab-bba1-cff095978d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(submission_path/f\"{config_run['name']}_blended.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9672937c-18a0-4eaf-a3f1-54d3a779d672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f227c7b81c0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str(blender.estimators[2][1].get_all_params())\n",
    "# blender.estimators[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4e34083-9f47-4583-b4f3-c10b4f3311d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'leaderboard_auc': ,\n",
    "           'catboost_params': str(best_catboost_params),\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b08b089-292e-43a1-87f1-9d46ac917ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2327090<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_055836-17fcrnlu/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/sf/Dropbox/code_cloud/python_code/kaggle/tabular_playgrounds/sep2021/wandb/run-20210922_055836-17fcrnlu/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>blender-final_estimator</td><td>LogisticRegression(m...</td></tr><tr><td>blender-stack_mdethod</td><td>predict_proba</td></tr><tr><td>blender-cv</td><td>5</td></tr><tr><td>_runtime</td><td>16221</td></tr><tr><td>_timestamp</td><td>1632331738</td></tr><tr><td>_step</td><td>4</td></tr><tr><td>xgboost_params</td><td>{'objective': 'binar...</td></tr><tr><td>lightgbm_params</td><td>{'boosting_type': 'g...</td></tr><tr><td>train_loss</td><td>0.48961</td></tr><tr><td>train_auc</td><td>0.84839</td></tr><tr><td>test_set</td><td>/media/sf/easystore/...</td></tr><tr><td>leaderboard_auc</td><td>0.81671</td></tr><tr><td>catboost_params</td><td>{'iterations': 3302,...</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>blender-cv</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>train_auc</td><td></td></tr><tr><td>leaderboard_auc</td><td></td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">stacking_off-shelf_20210922_055836</strong>: <a href=\"https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/17fcrnlu\" target=\"_blank\">https://wandb.ai/hushifang/202109_Kaggle_tabular_playground/runs/17fcrnlu</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afb61b-8022-4f0f-8996-2adfb3ec640c",
   "metadata": {},
   "source": [
    "## Manual Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1524723c-87c3-410f-863b-9387ef5b59e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.425545</td>\n",
       "      <td>-2.357891</td>\n",
       "      <td>-0.637206</td>\n",
       "      <td>-0.866657</td>\n",
       "      <td>-0.111568</td>\n",
       "      <td>-4.829243</td>\n",
       "      <td>-1.171229</td>\n",
       "      <td>-0.603397</td>\n",
       "      <td>-0.596871</td>\n",
       "      <td>-0.516828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247600</td>\n",
       "      <td>-0.323982</td>\n",
       "      <td>1.223569</td>\n",
       "      <td>0.361863</td>\n",
       "      <td>1.071182</td>\n",
       "      <td>-0.361140</td>\n",
       "      <td>0.082051</td>\n",
       "      <td>-0.746590</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.469668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.032371</td>\n",
       "      <td>-2.435680</td>\n",
       "      <td>-0.488960</td>\n",
       "      <td>0.341193</td>\n",
       "      <td>1.069656</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.537069</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>-0.763516</td>\n",
       "      <td>1.056879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.438373</td>\n",
       "      <td>-2.337605</td>\n",
       "      <td>-0.508914</td>\n",
       "      <td>-0.829607</td>\n",
       "      <td>1.485682</td>\n",
       "      <td>3.592008</td>\n",
       "      <td>-1.189087</td>\n",
       "      <td>-0.339152</td>\n",
       "      <td>-0.735281</td>\n",
       "      <td>-0.529158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602333</td>\n",
       "      <td>1.076218</td>\n",
       "      <td>-0.648438</td>\n",
       "      <td>0.463365</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>-0.157989</td>\n",
       "      <td>0.727338</td>\n",
       "      <td>-0.905498</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>-0.511066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128368</td>\n",
       "      <td>-0.127677</td>\n",
       "      <td>-0.128242</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>-0.127119</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "id                                                                         \n",
       "0   0.425545 -2.357891 -0.637206 -0.866657 -0.111568 -4.829243 -1.171229   \n",
       "1   0.247600 -0.323982  1.223569  0.361863  1.071182 -0.361140  0.082051   \n",
       "2   2.032371 -2.435680 -0.488960  0.341193  1.069656  0.118532  0.537069   \n",
       "3   1.438373 -2.337605 -0.508914 -0.829607  1.485682  3.592008 -1.189087   \n",
       "4   0.602333  1.076218 -0.648438  0.463365  0.275053 -0.157989  0.727338   \n",
       "\n",
       "           7         8         9  ...       227       228       229       230  \\\n",
       "id                                ...                                           \n",
       "0  -0.603397 -0.596871 -0.516828  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "1  -0.746590  0.899454  0.469668  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "2  -0.044075 -0.763516  1.056879  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "3  -0.339152 -0.735281 -0.529158  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "4  -0.905498  0.052478 -0.511066  ... -0.128368 -0.127677 -0.128242 -0.127867   \n",
       "\n",
       "         231       232       233      234       235      236  \n",
       "id                                                            \n",
       "0  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "1  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "2  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "3  -0.127119 -0.127985 -0.128494 -0.12862 -0.127854 -0.12703  \n",
       "4  -0.127119 -0.127985 -0.128494 -0.12862  7.821398 -0.12703  \n",
       "\n",
       "[5 rows x 237 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fe341d69-270a-445e-9a05-287cd9f12c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0a114476-71cb-4ef2-8f1f-2d8b2c837d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 237)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8dd525a7-3254-4e0c-8295-8871ffadd39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# generate probability predictions for the XGBoost model's folds\n",
    "for fold in xgboost_models.keys():\n",
    "#     X1[f\"xgboost_fold{fold}_pred\"] = xgboost_models[fold].predict(X)\n",
    "    X1[f\"xgboost_fold{fold}_pred\"] = xgboost_models[fold].predict_proba(X)[:,1]\n",
    "#     xgboost_preds[fold] = xgboost_models[fold].predict(X_test_imputed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eff33be8-e342-42e8-843c-75e1acd96120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>xgboost_fold0_pred</th>\n",
       "      <th>xgboost_fold1_pred</th>\n",
       "      <th>xgboost_fold2_pred</th>\n",
       "      <th>xgboost_fold3_pred</th>\n",
       "      <th>xgboost_fold4_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.425545</td>\n",
       "      <td>-2.357891</td>\n",
       "      <td>-0.637206</td>\n",
       "      <td>-0.866657</td>\n",
       "      <td>-0.111568</td>\n",
       "      <td>-4.829243</td>\n",
       "      <td>-1.171229</td>\n",
       "      <td>-0.603397</td>\n",
       "      <td>-0.596871</td>\n",
       "      <td>-0.516828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.582566</td>\n",
       "      <td>0.580950</td>\n",
       "      <td>0.576743</td>\n",
       "      <td>0.569523</td>\n",
       "      <td>0.595877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247600</td>\n",
       "      <td>-0.323982</td>\n",
       "      <td>1.223569</td>\n",
       "      <td>0.361863</td>\n",
       "      <td>1.071182</td>\n",
       "      <td>-0.361140</td>\n",
       "      <td>0.082051</td>\n",
       "      <td>-0.746590</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.469668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.152252</td>\n",
       "      <td>0.150803</td>\n",
       "      <td>0.148316</td>\n",
       "      <td>0.155218</td>\n",
       "      <td>0.147297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.032371</td>\n",
       "      <td>-2.435680</td>\n",
       "      <td>-0.488960</td>\n",
       "      <td>0.341193</td>\n",
       "      <td>1.069656</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.537069</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>-0.763516</td>\n",
       "      <td>1.056879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.794083</td>\n",
       "      <td>0.789945</td>\n",
       "      <td>0.788326</td>\n",
       "      <td>0.787177</td>\n",
       "      <td>0.797979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.438373</td>\n",
       "      <td>-2.337605</td>\n",
       "      <td>-0.508914</td>\n",
       "      <td>-0.829607</td>\n",
       "      <td>1.485682</td>\n",
       "      <td>3.592008</td>\n",
       "      <td>-1.189087</td>\n",
       "      <td>-0.339152</td>\n",
       "      <td>-0.735281</td>\n",
       "      <td>-0.529158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>-0.127854</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.774001</td>\n",
       "      <td>0.768510</td>\n",
       "      <td>0.774555</td>\n",
       "      <td>0.782187</td>\n",
       "      <td>0.773245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602333</td>\n",
       "      <td>1.076218</td>\n",
       "      <td>-0.648438</td>\n",
       "      <td>0.463365</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>-0.157989</td>\n",
       "      <td>0.727338</td>\n",
       "      <td>-0.905498</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>-0.511066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>-0.128494</td>\n",
       "      <td>-0.12862</td>\n",
       "      <td>7.821398</td>\n",
       "      <td>-0.12703</td>\n",
       "      <td>0.759366</td>\n",
       "      <td>0.755764</td>\n",
       "      <td>0.763769</td>\n",
       "      <td>0.758034</td>\n",
       "      <td>0.758038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "id                                                                         \n",
       "0   0.425545 -2.357891 -0.637206 -0.866657 -0.111568 -4.829243 -1.171229   \n",
       "1   0.247600 -0.323982  1.223569  0.361863  1.071182 -0.361140  0.082051   \n",
       "2   2.032371 -2.435680 -0.488960  0.341193  1.069656  0.118532  0.537069   \n",
       "3   1.438373 -2.337605 -0.508914 -0.829607  1.485682  3.592008 -1.189087   \n",
       "4   0.602333  1.076218 -0.648438  0.463365  0.275053 -0.157989  0.727338   \n",
       "\n",
       "           7         8         9  ...       232       233      234       235  \\\n",
       "id                                ...                                          \n",
       "0  -0.603397 -0.596871 -0.516828  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "1  -0.746590  0.899454  0.469668  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "2  -0.044075 -0.763516  1.056879  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "3  -0.339152 -0.735281 -0.529158  ... -0.127985 -0.128494 -0.12862 -0.127854   \n",
       "4  -0.905498  0.052478 -0.511066  ... -0.127985 -0.128494 -0.12862  7.821398   \n",
       "\n",
       "        236  xgboost_fold0_pred  xgboost_fold1_pred  xgboost_fold2_pred  \\\n",
       "id                                                                        \n",
       "0  -0.12703            0.582566            0.580950            0.576743   \n",
       "1  -0.12703            0.152252            0.150803            0.148316   \n",
       "2  -0.12703            0.794083            0.789945            0.788326   \n",
       "3  -0.12703            0.774001            0.768510            0.774555   \n",
       "4  -0.12703            0.759366            0.755764            0.763769   \n",
       "\n",
       "    xgboost_fold3_pred  xgboost_fold4_pred  \n",
       "id                                          \n",
       "0             0.569523            0.595877  \n",
       "1             0.155218            0.147297  \n",
       "2             0.787177            0.797979  \n",
       "3             0.782187            0.773245  \n",
       "4             0.758034            0.758038  \n",
       "\n",
       "[5 rows x 242 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac245be1-3a2d-4c84-b83e-7224e4b13194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
