{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1197a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "COLAB = False # will trigger manual installation of packages\n",
    "USE_GPU = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49490936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba53287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"cleanlab_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8510d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler #StandardScaler #, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n",
    "\n",
    "# from BorutaShap import BorutaShap\n",
    "import optuna\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53448c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.classification import LearningWithNoisyLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3cae3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_widedeep import Trainer\n",
    "# from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "# from pytorch_widedeep.models import Wide, TabMlp, WideDeep#, SAINT, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "# from pytorch_widedeep.metrics import Accuracy\n",
    "# from torchmetrics import AUROC\n",
    "# import torch\n",
    "# from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "# from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "599a53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86471292",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/nov2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "#     datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')  \n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath]:\n",
    "        pth.mkdir(exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d72881",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1634e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3940fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    # 'train_source': str(datapath/'X-RobustScaled-pca_mle-umap_embedding_20211107-n_comp10-n_neighbors15-rs42.joblib'),\n",
    "    'train_source': str(datapath/'X_orig.feather'),\n",
    "    'target_source': str(datapath/'y_orig.joblib'),\n",
    "    # 'test_source': str(datapath/'X_test-RobustScaled-pca_mle-umap_embedding_20211107-n_comp10-n_neighbors15-rs42.joblib'),\n",
    "    'test_source': str(datapath/'X_test_orig-no_scaling.feather'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "# X = load(dataset_params['train_source'])\n",
    "X = pd.read_feather(dataset_params['train_source'])\n",
    "y = load(dataset_params['target_source'])\n",
    "# X_test = load(dataset_params['test_source'])\n",
    "# X_test = pd.read_feather(dataset_params['test_source'])\n",
    "\n",
    "dataset_params['feature_count'] = X.shape[1]\n",
    "dataset_params['instance_count'] = X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4116b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6932964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "# in the sweep version, this includes both ex-model parameters and defaults for model parameters\n",
    "exmodel_config = {\n",
    "    \"arch\": 'lightgbm',\n",
    "    # \"type\": 'sweep',\n",
    "    # \"denoising\": \"cleanlab\",\n",
    "    \"level\": 1,\n",
    "    'random_state': SEED,\n",
    "    # 'tuner': \"Optuna\",\n",
    "    'cross_val_strategy': None, # None for holdout, or the relevant sklearn class\n",
    "    'kfolds': 1, # if 1, that means just doing holdout\n",
    "    'test_size': 0.2,\n",
    "    'scaler': str(RobustScaler()),\n",
    "    **dataset_params\n",
    "}\n",
    "\n",
    "wandb_kwargs = {\n",
    "    # wandb config\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'project': '202111_Kaggle_tabular_playground',\n",
    "    'tags': ['experiment'],\n",
    "    'notes': \"Going to try CleanLab with the 'best' params from the previous sweep, but wanting to see what the actual LB score is, and how it compares to the noisy one. Using a default LGBMClassfier with holdout on robust-scaled original dataset.\",\n",
    "    'config': exmodel_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34381df",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"202111_Kaggle_tabular_playground\",\n",
    "    save_code=True,\n",
    "    tags=wandb_config['tags'],\n",
    "    name=wandb_config['name'],\n",
    "    notes=wandb_config['notes'],\n",
    "    config=exmodel_config\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ca3851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "# in the sweep version, this includes both ex-model parameters and defaults for model parameters\n",
    "exmodel_config = {\n",
    "    \"arch\": 'lightgbm',\n",
    "    # \"type\": 'sweep',\n",
    "    # \"denoising\": \"cleanlab\",\n",
    "    \"level\": 1,\n",
    "    'random_state': SEED,\n",
    "    # 'tuner': \"Optuna\",\n",
    "    'cross_val_strategy': None, # None for holdout, or the relevant sklearn class\n",
    "    'kfolds': 1, # if 1, that means just doing holdout\n",
    "    'test_size': 0.2,\n",
    "    'scaler': str(RobustScaler()),\n",
    "    **dataset_params\n",
    "}\n",
    "\n",
    "wandb_config = {\n",
    "    # wandb config\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'project': '202111_Kaggle_tabular_playground',\n",
    "    'tags': ['experiment'],\n",
    "    'notes': \"Going to try CleanLab with the 'best' params from the previous sweep, but wanting to see what the actual LB score is, and how it compares to the noisy one. Using a default LGBMClassfier with holdout on robust-scaled original dataset.\",\n",
    "    'config': exmodel_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad462f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.sdk.wandb_run.Run at 0x7f735419d760>"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/202111_Kaggle_tabular_playground/runs/3sw3r26n\" target=\"_blank\">cleanlab_20211122_145249</a></strong> to <a href=\"https://wandb.ai/hushifang/202111_Kaggle_tabular_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"202111_Kaggle_tabular_playground\",\n",
    "    save_code=True,\n",
    "    tags=wandb_config['tags'],\n",
    "    name=wandb_config['name'],\n",
    "    notes=wandb_config['notes'],\n",
    "    config=exmodel_config\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ede38fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        random_state=42,\n",
    "        #                     device_type='cpu',\n",
    "        #                     n_jobs=-1,\n",
    "        #                 eval_metric='auc',\n",
    "        device_type='gpu',\n",
    "        max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "        gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82d9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f207e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "800d94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-0.27296268,  0.01579322, -0.19031588, ...,  0.50363337,\n",
      "         0.17720313, -0.58369832],\n",
      "       [-0.18106382,  0.2410393 ,  5.30773161, ...,  0.24955035,\n",
      "        -0.48450355, -0.62285755],\n",
      "       [ 6.07300903, -0.85078822,  0.17280319, ..., -0.62764574,\n",
      "        -0.67894852,  0.73726127],\n",
      "       ...,\n",
      "       [ 4.18856991, -0.68824246,  0.21225385, ...,  0.88726112,\n",
      "         6.19955807, -0.09918299],\n",
      "       [-0.300117  ,  0.13080398, -0.54802214, ..., -0.3979886 ,\n",
      "        -0.01474   ,  0.22398866],\n",
      "       [-0.29279129,  0.62179137,  0.07317624, ...,  0.14508993,\n",
      "        12.038684  ,  0.6437292 ]])"
     ]
    }
   ],
   "source": [
    "scaler.fit_transform(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "766c97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40824748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.27745959, -0.79095855,  3.57879036, ...,  0.40431588,\n",
      "         0.30027418,  0.45467042],\n",
      "       [-0.35032922, -0.36662758, -0.83368479, ...,  0.48734142,\n",
      "        -0.59656634,  9.97028335],\n",
      "       [ 2.07990909,  0.20997962, -0.0127639 , ..., -0.80240405,\n",
      "        -0.22697866,  0.23615553],\n",
      "       ...,\n",
      "       [ 0.80633635,  0.39936366,  0.49845399, ...,  0.68680344,\n",
      "        -1.04204429,  0.24502101],\n",
      "       [-0.06863608, -0.27843164, -0.36283929, ...,  0.18702425,\n",
      "         0.57010813, -0.05371796],\n",
      "       [-0.3620719 , -0.30144763, -0.06982129, ..., -0.73907299,\n",
      "        -1.0792864 ,  0.24365673]])"
     ]
    }
   ],
   "source": [
    "scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49ada4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39bdc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d39237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-0.27296268,  0.01579322, -0.19031588, ...,  0.50363337,\n",
      "         0.17720313, -0.58369832],\n",
      "       [-0.18106382,  0.2410393 ,  5.30773161, ...,  0.24955035,\n",
      "        -0.48450355, -0.62285755],\n",
      "       [ 6.07300903, -0.85078822,  0.17280319, ..., -0.62764574,\n",
      "        -0.67894852,  0.73726127],\n",
      "       ...,\n",
      "       [ 4.18856991, -0.68824246,  0.21225385, ...,  0.88726112,\n",
      "         6.19955807, -0.09918299],\n",
      "       [-0.300117  ,  0.13080398, -0.54802214, ..., -0.3979886 ,\n",
      "        -0.01474   ,  0.22398866],\n",
      "       [-0.29279129,  0.62179137,  0.07317624, ...,  0.14508993,\n",
      "        12.038684  ,  0.6437292 ]])"
     ]
    }
   ],
   "source": [
    "scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45a7c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.27745959, -0.79095855,  3.57879036, ...,  0.40431588,\n",
      "         0.30027418,  0.45467042],\n",
      "       [-0.35032922, -0.36662758, -0.83368479, ...,  0.48734142,\n",
      "        -0.59656634,  9.97028335],\n",
      "       [ 2.07990909,  0.20997962, -0.0127639 , ..., -0.80240405,\n",
      "        -0.22697866,  0.23615553],\n",
      "       ...,\n",
      "       [ 0.80633635,  0.39936366,  0.49845399, ...,  0.68680344,\n",
      "        -1.04204429,  0.24502101],\n",
      "       [-0.06863608, -0.27843164, -0.36283929, ...,  0.18702425,\n",
      "         0.57010813, -0.05371796],\n",
      "       [-0.3620719 , -0.30144763, -0.06982129, ..., -0.73907299,\n",
      "        -1.0792864 ,  0.24365673]])"
     ]
    }
   ],
   "source": [
    "scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c987c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(device_type='gpu', gpu_use_dp=False, max_bin=63,\n",
      "               objective='binary', random_state=42)"
     ]
    }
   ],
   "source": [
    "lgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2eab3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7322242958171294"
     ]
    }
   ],
   "source": [
    "preds = lgb_model.predict_proba(X_valid)[:,1]\n",
    "valid_auc = roc_auc_score(y_true=y_valid, y_score=preds)\n",
    "valid_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df58e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'overall_valid_auc': valid_auc,\n",
    "           'model_params': str(model.get_params()),\n",
    "           'model_seed': 42,\n",
    "          })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a81e6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'overall_valid_auc': valid_auc,\n",
    "           'model_params': str(lgb_model.get_params()),\n",
    "           'model_seed': 42,\n",
    "          })\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
