{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e815d3-d755-4fa2-85a5-ea4df4948fcd",
   "metadata": {},
   "source": [
    "Header for notebooks -- customize as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843e3531-f950-4701-9330-07960ae9a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "COLAB = False # will trigger manual installation of packages\n",
    "USE_GPU = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a61aa18-6ef3-41d1-bb75-08d1f766dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d84988-5ddb-40d9-bc12-826012acb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"nb_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf1e65-6447-47b9-88f2-8d02bbc29af0",
   "metadata": {},
   "source": [
    "Now, non-stdlib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee16c791-548b-4616-9c82-ea76001e4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "# from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler #StandardScaler #, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n",
    "\n",
    "# from BorutaShap import BorutaShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed675a2-4ee6-4313-a656-274a01b6e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "# from fastai.callback.wandb import *\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from torchmetrics import AUROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b139207-5ea6-464d-922e-e4d0a398062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep#, SAINT, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "from pytorch_widedeep.metrics import Accuracy\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55072fd2-dcc0-4169-a6f8-54d9b76ab432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71932e1-2e32-4474-acbc-3cbe63ce993e",
   "metadata": {},
   "source": [
    "Now, datapath setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880b9f33-b517-40ef-a3e6-049dcc52a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/nov2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "#     datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')  \n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath]:\n",
    "        pth.mkdir(exist_ok=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe9e0d-1301-4ade-a63d-9f3d298c2255",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1905a9de-cd7c-4a73-a39e-b42a8297785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/envs/tabular-x/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "\n",
    "def set_seed(seed:int, reproducible=True):\n",
    "    random.seed(seed) # set Python (CPU) seed\n",
    "    np.random.seed(seed) # set NumPy (CPU) seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed) # set torch CPU seed\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed) # set torch GPU(s) seed(s)\n",
    "    if reproducible and torch.backends.cudnn.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(seed=SEED, reproducible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba521a1a-8c41-4b9b-becd-36eb62739ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31aa63eb-c511-48a8-8e6f-216fa6336188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643097f-ce64-4fd9-9a9f-5fab4aba6407",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac591aaf-f3bc-4042-a79d-cdc9e3aeb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM (Sharpness-Aware Minimization) -- wraps a conventional optimizer, improves generalization through smoothing\n",
    "# implementation from https://github.com/davda54/sam/blob/main/sam.py\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63616619-94cd-408e-8407-6a5bcf66182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    '''\n",
    "    Resets model weights between folds\n",
    "    - https://www.machinecurve.com/index.php/2021/02/03/how-to-use-k-fold-cross-validation-with-pytorch/\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f\"Reset trainable parameters of layer = {layer}\")\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f63ee2-a167-4111-9b44-bf67c8240603",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60cd4c5f-d848-4e4c-9fa6-097961ac8cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-a0dda623f316>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-a0dda623f316>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    'target_source': str(datapath/'y_orig.joblib'),\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    # 'train_source': str(datapath/'X-RobustScaled-pca_mle-umap_embedding_20211107-n_comp10-n_neighbors15-rs42.joblib'),\n",
    "    'train_source': str(datapath/'X_orig.feather'),\n",
    "    'target_source': str(datapath/'y_orig.joblib'),\n",
    "    # 'test_source': str(datapath/'X_test-RobustScaled-pca_mle-umap_embedding_20211107-n_comp10-n_neighbors15-rs42.joblib'),\n",
    "    'test_source': str(datapath/'X_test_orig-no_scaling.feather'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "# X = load(dataset_params['train_source'])\n",
    "X = pd.read_feather(dataset_params['train_source'])\n",
    "y = load(dataset_params['target_source'])\n",
    "# X_test = load(dataset_params['test_source'])\n",
    "X_test = pd.read_feather(dataset_params['test_source'])\n",
    "\n",
    "dataset_params['feature_count'] = X.shape[1]\n",
    "dataset_params['instance_count'] = X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d647e8af-e065-4ea8-aa6c-5d158791904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "exmodel_config = {\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "#     'random_state': SEED,\n",
    "#     'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "#     'subsample': 1,\n",
    "    'cross_val_strategy': KFold, # None for holdout, or the relevant sklearn class\n",
    "    'kfolds': 5, # if 1, that means just doing holdout\n",
    "    'test_size': 0.2,\n",
    "    **dataset_params\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
