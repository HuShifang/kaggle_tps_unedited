{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f771b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "COLAB = False # will trigger manual installation of packages\n",
    "USE_GPU = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae1f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3199de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"stacking_{datetime.now().strftime('%Y%m%d')}a.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b09c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# general ML tooling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "from sklearn.impute import SimpleImputer #, KNNImputer\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler#, StandardScaler #, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from joblib import dump, load\n",
    "# feature engineering tools\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import featuretools as ft\n",
    "\n",
    "from BorutaShap import BorutaShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bac92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep#, SAINT, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "from pytorch_widedeep.metrics import Accuracy\n",
    "from torchmetrics import AUROC\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b7ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2339c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/nov2021/')\n",
    "    \n",
    "else:\n",
    "    # if on local machine\n",
    "#     datapath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/sep2021/')  \n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/nov2021/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath]:\n",
    "        pth.mkdir(exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0bcf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3633650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_source = datapath/'train.feather'\n",
    "# df = pd.read_feather(path=datapath/'train.feather')\n",
    "# y = np.array(df.target)\n",
    "# dump(y, filename=datapath/'y.joblib')\n",
    "# del df\n",
    "\n",
    "y = load(datapath/'y_corrected.joblib')\n",
    "\n",
    "# df.index.name = 'id'\n",
    "# y_train = df.target\n",
    "# features = [x for x in df.columns if x != 'target']\n",
    "# X_train = df[features]\n",
    "# # X.index.name = 'id'\n",
    "# # y.index.name = 'id'\n",
    "# X = np.array(X_train)\n",
    "# y = np.array(y_train)\n",
    "\n",
    "# del df, X_train, y_train\n",
    "\n",
    "# load the Boruta-filtered green-zone 98 features (based on 200 iterations of the algo)\n",
    "# train_source = '/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/alt_datasets/X_boruta_200iter_filtered_green.joblib'\n",
    "# X = load(train_source)\n",
    "\n",
    "train_source = datapath/'X_orig.feather'\n",
    "X = pd.read_feather(train_source)\n",
    "\n",
    "# exmodel_config['feature_count'] = len(X.columns)\n",
    "exmodel_config['feature_count'] = X.shape[1]\n",
    "exmodel_config['instance_count'] = X.shape[0]\n",
    "exmodel_config['scaler'] = str(RobustScaler())\n",
    "# exmodel_config['feature_generator'] = None\n",
    "# exmodel_config['feature_generator'] = \"Summary statistics\"\n",
    "\n",
    "exmodel_config['train_source'] = str(train_source)\n",
    "# test_source = datapath/'test.feather'\n",
    "# exmodel_config['test_source'] = str(test_source)\n",
    "# X_test = pd.read_feather(path=test_source)\n",
    "# X_test = X_test.iloc[:, 1:]\n",
    "# X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c55e44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmodel_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1239f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_source = datapath/'train.feather'\n",
    "# df = pd.read_feather(path=datapath/'train.feather')\n",
    "# y = np.array(df.target)\n",
    "# dump(y, filename=datapath/'y.joblib')\n",
    "# del df\n",
    "\n",
    "y = load(datapath/'y_corrected.joblib')\n",
    "\n",
    "# df.index.name = 'id'\n",
    "# y_train = df.target\n",
    "# features = [x for x in df.columns if x != 'target']\n",
    "# X_train = df[features]\n",
    "# # X.index.name = 'id'\n",
    "# # y.index.name = 'id'\n",
    "# X = np.array(X_train)\n",
    "# y = np.array(y_train)\n",
    "\n",
    "# del df, X_train, y_train\n",
    "\n",
    "# load the Boruta-filtered green-zone 98 features (based on 200 iterations of the algo)\n",
    "# train_source = '/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/alt_datasets/X_boruta_200iter_filtered_green.joblib'\n",
    "# X = load(train_source)\n",
    "\n",
    "train_source = datapath/'X_orig.feather'\n",
    "X = pd.read_feather(train_source)\n",
    "\n",
    "# exmodel_config['feature_count'] = len(X.columns)\n",
    "exmodel_config['feature_count'] = X.shape[1]\n",
    "exmodel_config['instance_count'] = X.shape[0]\n",
    "exmodel_config['scaler'] = str(RobustScaler())\n",
    "# exmodel_config['feature_generator'] = None\n",
    "# exmodel_config['feature_generator'] = \"Summary statistics\"\n",
    "\n",
    "exmodel_config['train_source'] = str(train_source)\n",
    "# test_source = datapath/'test.feather'\n",
    "# exmodel_config['test_source'] = str(test_source)\n",
    "# X_test = pd.read_feather(path=test_source)\n",
    "# X_test = X_test.iloc[:, 1:]\n",
    "# X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0d0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['preprocessing'],\n",
    "    'notes': \"Running Big Three GBMs with default parameters and a hybrid dataset: a concatenation of 1. the original (RobustScaled) and 2. the original transformed with A. RobustScaler, B. PCA (MLE), C. UMAP (n_neighbors=15, n_components=10)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4234e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optuna 20211124, with corrected dataset and RobustScaler\n",
    "best_xgboost_params = {\n",
    "    'n_estimators': 9872,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.12943882615104757,\n",
    "    'reg_alpha': 4.793236314677738,\n",
    "    'reg_lambda': 0.03427038053813167,\n",
    "    'subsample': 0.5026684329097286,\n",
    "    'min_child_weight': 3.2374430610042664,\n",
    "    'colsample_bytree': 0.9875504456465564,\n",
    "    'gamma': 4.691772640321729\n",
    "}\n",
    "\n",
    "# best as of 20211125, with corrected dataset and RobustScaler\n",
    "best_lightgbm_params = {\n",
    "    'n_estimators': 6986,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.09080435106650955,\n",
    "    'reg_alpha': 19.060739534647425,\n",
    "    'reg_lambda': 0.12865332700612375,\n",
    "    'subsample': 0.5612404690403716,\n",
    "    'boosting_type': 'goss',\n",
    "    'min_child_samples': 17,\n",
    "    'num_leaves': 59,\n",
    "    'colsample_bytree': 0.5125554530181221\n",
    "}\n",
    "\n",
    "# best as of 20211126, with corrected dataset and RobustScaler\n",
    "best_catboost_params = {\n",
    "    'iterations': 17997,\n",
    "    'depth': 4,\n",
    "    'learning_rate': 0.05807421036756052,\n",
    "    'random_strength': 27,\n",
    "    'od_wait': 1664,\n",
    "    'reg_lambda': 57.67864249277457,\n",
    "    'border_count': 275,\n",
    "    'min_child_samples': 10,\n",
    "    'leaf_estimation_iterations': 2\n",
    "}\n",
    "\n",
    "# # 20211021 lv2 on the K-Means 8-cluster, synth dataset\n",
    "# lv2_xgboost_params = {\n",
    "#     'n_estimators': 1534,\n",
    "#     'max_depth': 4,\n",
    "#     'learning_rate': 0.0062941159127744535,\n",
    "#     'reg_alpha': 21.3946930650266,\n",
    "#     'reg_lambda': 0.021003786013817635,\n",
    "#     'subsample': 0.5726680367393964,\n",
    "#     'min_child_weight': 0.07566661785187714,\n",
    "#     'colsample_bytree': 0.7850419523745037,\n",
    "#     'gamma': 4.26660233356059\n",
    "# }\n",
    "\n",
    "# # 20211021 lv2 on the K-Means 8-cluster, synth dataset\n",
    "# lv2_lightgbm_params = {\n",
    "#     'n_estimators': 5776,\n",
    "#     'max_depth': 4,\n",
    "#     'learning_rate': 0.0010172282832994653,\n",
    "#     'reg_alpha': 0.013879765609402173,\n",
    "#     'reg_lambda': 0.002787031048344079,\n",
    "#     'subsample': 0.800000753298926,\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'min_child_samples': 11,\n",
    "#     'num_leaves': 190,\n",
    "#     'colsample_bytree': 0.9976443570341007\n",
    "# }\n",
    "\n",
    "# # 20211021 lv2 on the K-Means 8-cluster, synth dataset\n",
    "# lv2_catboost_params = {\n",
    "#     'iterations': 2000,\n",
    "#     'depth': 6,\n",
    "#     'learning_rate': 0.002984126581340097,\n",
    "#     'random_strength': 0,\n",
    "#     'od_wait': 334,\n",
    "#     'reg_lambda': 33.469738674488084,\n",
    "#     'border_count': 158,\n",
    "#     'min_child_samples': 8,\n",
    "#     'leaf_estimation_iterations': 4\n",
    "# }\n",
    "\n",
    "# # initial, non-default guess -- need to get optuna working (20211010)\n",
    "# # basic_widedeep_tabmlp_params = {\n",
    "    \n",
    "# # }\n",
    "\n",
    "# # basic_widedeep_trainer_params = {\n",
    "# #     optimizers=AdamW()\n",
    "# # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32fc2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.basic import LightGBMError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f50ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(arch:str, X, y, X_test, params:dict={}, start_fold=0, \n",
    "                         exmodel_config=exmodel_config, wandb_config=wandb_config, \n",
    "                         random_state=42, shuffle_kfolds=True, wandb_tracked=True, encode_cats=False):\n",
    "    \"\"\"\n",
    "    Function to handle model training process in the context of cross-validation -- via hold-out or via k-fold.\n",
    "    If exmodel_config['cross_val_strategy'] == None, then any kfolds= input is ignored; otherwise, the number specified is used.\n",
    "    \n",
    "    :param kfolds: int specifying number of k-folds to use in cross-validation\n",
    "    :param exmodel_config: dict containing general config including for cross-validation -- `kfold=1` implies hold-out\n",
    "    \"\"\"\n",
    "#     if exmodel_config['kfolds'] == 1:\n",
    "#         print(\"Proceeding with holdout\")\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                           test_size=0.2, \n",
    "#                                                           random_state=SEED)                 \n",
    "    \n",
    "    # prepare for k-fold cross-validation; random-state here is notebook-wide, not per-model\n",
    "    # shuffle on the initial sets, but not subsequently -- performing the same operation twice means a very different dataset\n",
    "    if shuffle_kfolds:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=SEED)\n",
    "    else:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=False)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(params)\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )   \n",
    "    \n",
    "    # setup for serialization\n",
    "    # runpath = Path(modelpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds/\")\n",
    "    # (runpath).mkdir(exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # initialize lists for out-of-fold preds and ground truth\n",
    "    oof_preds, oof_y = [], []\n",
    "    \n",
    "    # initialize a numpy.ndarray containing the fold-model's preds for test set\n",
    "    test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    # preprocessing\n",
    "    if 'widedeep' in arch: # Rank-Gauss scale, and do WideDeep preprocessing\n",
    "        # preprocessing first\n",
    "\n",
    "        # CONSIDER whether to feed the binned data into the GBMs too\n",
    "        X_bins = np.zeros((X.shape[0],X.shape[1]))\n",
    "        for i in range(X.shape[1]): # assumes X is a pd.DataFrame\n",
    "            X_bins[:,i] = pd.qcut(X.iloc[:,i],X.shape[1],labels=False,duplicates = 'drop')\n",
    "        \n",
    "        X_bins = X_bins.astype(np.int8)     \n",
    "        X_bins = pd.DataFrame(X_bins, index=X.index, columns=[f'rkd_f{col}' for col in range(100)])\n",
    "        \n",
    "        scaler = GaussRankScaler()\n",
    "        X_gauss = scaler.fit_transform(X)\n",
    "        X_gauss = pd.DataFrame(X_gauss, columns=X.columns, index=X.index)\n",
    "        \n",
    "        X_pre = X_gauss.join(X_bins)\n",
    "        \n",
    "        cont_cols = X_pre.iloc[:,:100].columns\n",
    "        wide_cols = X_pre.iloc[:, 100:].columns\n",
    "        \n",
    "        # wide part\n",
    "        wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "        X_wide = wide_preprocessor.fit_transform(X_pre)\n",
    "\n",
    "        # deep part\n",
    "        if \"SAINT\" in arch:\n",
    "            tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=True,embed_cols=wide_cols)\n",
    "        else:\n",
    "            tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=False,embed_cols=wide_cols)\n",
    "        X_tab = tab_preprocessor.fit_transform(X_pre) \n",
    "        \n",
    "        # transforming the test set\n",
    "        X_test_wide = wide_preprocessor.transform(X_test)\n",
    "        X_test_tab = tab_preprocessor.transform(X_test)\n",
    "        \n",
    "    else: # if using a GBM, simply use the RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "#         print(f\"type(train_ids) = {type(train_ids)} and train_ids.shape = {train_ids.shape}\")\n",
    "#         print(f\"type(valid_ids) = {type(valid_ids)} and train_ids.shape = {valid_ids.shape}\")\n",
    "        if fold < start_fold: # skip folds that are already trained\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"FOLD {fold}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "            if 'widedeep' in arch: # handle wide and deep tabs in parallel\n",
    "                X_train_wide, X_valid_wide = X_wide[train_ids, :], X_wide[valid_ids, :]\n",
    "                X_train_tab, X_valid_tab = X_tab[train_ids, :], X_tab[valid_ids, :]\n",
    "#                 print(f\"X_train_wide.shape = {X_train_wide.shape}\")\n",
    "#                 print(f\"X_train_tab.shape = {X_train_tab.shape}\")\n",
    "#                 print(f\"X_test_wide.shape = {X_test_wide.shape}\")\n",
    "#                 print(f\"X_test_tab.shape = {X_test_tab.shape}\")\n",
    "            else: # handle datasets for GBMs\n",
    "                if isinstance(X, np.ndarray):\n",
    "                    X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                else:\n",
    "                    X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:] # bc need pandas.DataFrames for ce\n",
    "                \n",
    "                # scaling\n",
    "                # scaler = RobustScaler()\n",
    "                # X_train = scaler.fit_transform(X_train)\n",
    "                # X_valid = scaler.transform(X_valid)    \n",
    "                \n",
    "                # if encode_cats:\n",
    "                #     encoder = ce.WOEEncoder(cols=categoricals)\n",
    "                #     encoder.fit(X_train,y_train)\n",
    "                #     X_train = encoder.transform(X_train)\n",
    "                #     X_valid = encoder.transform(X_valid)\n",
    "                # # exmodel_config['feature_count'] = len(X.columns)\n",
    "                #     wandb.log({\n",
    "                #         'feature_count': X_train.shape[1],\n",
    "                #         'instance_count': X_train.shape[0],\n",
    "                #         'encoder': str(encoder)\n",
    "                #     })\n",
    "#                 exmodel_config['instance_count'] = X_train.shape[0]\n",
    "#                 exmodel_config['encoder'] = str(encoder)\n",
    "#                     X_test = encoder.transform(X_test)\n",
    "#                 y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "            \n",
    "        \n",
    "        # define models\n",
    "        if arch == 'xgboost':\n",
    "            model = XGBClassifier(\n",
    "                booster='gbtree',\n",
    "                tree_method='gpu_hist',\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1, \n",
    "                verbosity=1, \n",
    "                objective='binary:logistic',\n",
    "                **params)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "        elif arch == 'lightgbm':\n",
    "            try:\n",
    "                model = LGBMClassifier(\n",
    "                    objective='binary',\n",
    "                    random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "    #                 eval_metric='auc',\n",
    "                    device_type='gpu',\n",
    "                    max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "                    gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "                    **params)\n",
    "                \n",
    "                if wandb_tracked:\n",
    "                    model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "#             except LightGBMError:\n",
    "#                 model = LGBMClassifier(\n",
    "#                     objective='binary',\n",
    "#                     random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "#     #                 eval_metric='auc',\n",
    "#     #                 device_type='gpu',\n",
    "#     #                 max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     #                 gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "#                     **params)\n",
    "                \n",
    "#                 if wandb_tracked:\n",
    "#                     model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "#                 else:\n",
    "#                     model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "            \n",
    "        elif arch == 'catboost':\n",
    "            model = CatBoostClassifier(\n",
    "                task_type='GPU',\n",
    "                silent=True,\n",
    "                random_state=random_state,\n",
    "                **params) \n",
    "        \n",
    "            model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "            \n",
    "        elif 'widedeep' in arch: # only coding for TabMlp right now\n",
    "#             X_train = pd.DataFrame(X_train, columns=[f\"f{x}\" for x in range(X_train.shape[1])])\n",
    "#             X_valid = pd.DataFrame(X_valid, columns=[f\"f{x}\" for x in range(X_valid.shape[1])])\n",
    "#             X_test = pd.DataFrame(X_test, columns=[f\"f{x}\" for x in range(X_test.shape[1])])\n",
    "            \n",
    "            wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "            deeptabular = TabMlp(\n",
    "                mlp_hidden_dims=[64,32],\n",
    "                column_idx=tab_preprocessor.column_idx,\n",
    "            #     embed_input=tab_preprocessor.embeddings_input,\n",
    "                continuous_cols=cont_cols,\n",
    "            )\n",
    "            \n",
    "            # model instantiation and training\n",
    "            model = WideDeep(wide=wide, deeptabular=deeptabular)\n",
    "            \n",
    "            \n",
    "            n_epochs = 300\n",
    "\n",
    "            # pytorch hyperparams\n",
    "            wide_opt = AdamW(model.wide.parameters(), lr=0.1)\n",
    "            deep_opt = AdamW(model.deeptabular.parameters(), lr=0.1)\n",
    "            \n",
    "            wide_sch = OneCycleLR(optimizer=wide_opt, max_lr=0.01, steps_per_epoch=X_train_wide.shape[0], epochs=n_epochs)\n",
    "            deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_train_tab.shape[0], epochs=n_epochs)\n",
    "            \n",
    "            optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "            lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "            \n",
    "            \n",
    "            callbacks = [\n",
    "                LRHistory(n_epochs=n_epochs), \n",
    "            ]\n",
    "            \n",
    "            # trainer\n",
    "            trainer = Trainer(model=model, \n",
    "                              objective='binary', \n",
    "                              metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                              seed=random_state, \n",
    "                              optimizers=optimizers,\n",
    "                              callbacks=callbacks\n",
    "                             )\n",
    "            \n",
    "#             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "            trainer.fit( # this is where problem is beginning\n",
    "                X_wide=X_train_wide,\n",
    "                X_tab=X_train_tab,\n",
    "                target=y_train,\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=1024, # default value is 32\n",
    "#                 val_split=0.2, # no need for this\n",
    "            )\n",
    "            \n",
    "            y_valid_preds = trainer.predict_proba(X_wide=X_valid_wide, X_tab=X_valid_tab, batch_size=1024)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            \n",
    "            # test set inference\n",
    "            fold_test_preds = trainer.predict_proba(X_wide=X_test_wide, X_tab=X_test_tab, batch_size=1024)[:,1]\n",
    "            test_preds += fold_test_preds\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "#         valid_loss = log_loss(y_valid, y_pred)\n",
    "        # give the valid AUC score, for edification\n",
    "        fold_valid_auc = roc_auc_score(y_valid, y_valid_preds)\n",
    "        if wandb_tracked:\n",
    "            wandb.log({f'fold{fold}_valid_roc_auc': fold_valid_auc})\n",
    "        print(f\"Valid AUC for fold {fold} is {fold_valid_auc}\")   \n",
    "        # dump(model, Path(runpath/f\"{arch}_fold{fold}_rs{random_state}_model.joblib\"))\n",
    "\n",
    "    model_valid_auc = roc_auc_score(oof_y, oof_preds)\n",
    "    print(f\"Valid AUC score for {arch} model is {model_valid_auc}\")\n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_valid_auc': model_valid_auc,\n",
    "                   'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()),\n",
    "                   'model_seed': random_state,\n",
    "                  })\n",
    "        wandb.finish()\n",
    "    \n",
    "    # finalize test preds\n",
    "    test_preds /= exmodel_config['kfolds']\n",
    "    \n",
    "    # save OOF preds and test-set preds\n",
    "#     if 'widedeep' in arch:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "#     else:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "    if not (datapath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\").is_file():\n",
    "        dump(oof_y, predpath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\")\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         if 'widedeep' in arch:\n",
    "#         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "#                    'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()), \n",
    "#         #                    'model_params': str(model.get_params()),\n",
    "#         })\n",
    "# #         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "# # #                    'model_params': str(model.get_params()),\n",
    "# #                   })\n",
    "#         wandb.finish()\n",
    "    return oof_preds, test_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2f433f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = ['xgboost', 'lightgbm', 'catboost']#, 'widedeep-tabmlp', 'widedeep-saint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "362dd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seeds = [42]#, 1983, 550, 1701, 2063]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a842e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lv1, test_lv1 = pd.DataFrame(), pd.DataFrame() # initialize dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c43b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_params = {\n",
    "    'xgboost': best_xgboost_params,\n",
    "    'lightgbm': best_lightgbm_params,\n",
    "    'catboost': best_catboost_params\n",
    "}\n",
    "#         'n_estimators': 8784,\n",
    "#         'depth': 9,\n",
    "#         'learning_rate': 0.004167178645277267,\n",
    "#         'reg_alpha': 0.007249923752866805,\n",
    "#         'reg_lambda': 0.08945255185214125,\n",
    "#         'subsample': 0.7288417897178108,\n",
    "#         'min_child_weight': 3.9187138542139577,\n",
    "#         'colsample_bytree': 0.5284325948533055,\n",
    "#         'gamma': 3.0265775282730822}\n",
    "#     'lightgbm': { # thru trial 38 cross-validated on RobustScaled orig dataset, as of 202111031440\n",
    "#         'n_estimators': 5108,\n",
    "#         'max_depth': 4,\n",
    "#         'learning_rate': 0.01253791570387513,\n",
    "#         'reg_alpha': 0.015194423057424834,\n",
    "#         'reg_lambda': 10.289397982794664,\n",
    "#         'subsample': 0.996318668039871,\n",
    "#         'boosting_type': 'goss',\n",
    "#         'min_child_samples': 18,\n",
    "#         'num_leaves': 218,\n",
    "#         'colsample_bytree': 0.580388444330496 },\n",
    "#     'catboost': { # thur trial 45 cross-validated on RobustScaled orig dataset, as of 202111041011\n",
    "#         'iterations': 29222,\n",
    "#         'depth': 7,\n",
    "#         'learning_rate': 0.0067277390824230605,\n",
    "#         'random_strength': 1,\n",
    "#         'od_wait': 1989,\n",
    "#         'reg_lambda': 51.436909447809484,\n",
    "#         'border_count': 239,\n",
    "#         'min_child_samples': 11,\n",
    "#         'leaf_estimation_iterations': 2}\n",
    "#     # 'widedeep-tabmlp': #todo,\n",
    "#     # 'widedeep-saint': #todo,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74032e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in architectures:\n",
    "    for model_seed in model_seeds:\n",
    "        # update exmodel_config here\n",
    "        oof_pred, test_pred = cross_validate_model(arch=arch, X=X, y=y, X_test=X_test, \n",
    "                                         wandb_config=wandb_config,\n",
    "                                         random_state=model_seed,\n",
    "                                         params=lv1_params[arch],\n",
    "                                         exmodel_config=exmodel_config, \n",
    "                                         wandb_tracked=True\n",
    "                                        )\n",
    "        oof_lv1[f'{arch}{model_seed}'] = oof_pred\n",
    "        test_lv1[f'{arch}{model_seed}'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e43c99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(arch:str, X, y, X_test, params:dict={}, start_fold=0, \n",
    "                         exmodel_config=exmodel_config, wandb_config=wandb_config, \n",
    "                         random_state=42, shuffle_kfolds=True, wandb_tracked=True, encode_cats=False):\n",
    "    \"\"\"\n",
    "    Function to handle model training process in the context of cross-validation -- via hold-out or via k-fold.\n",
    "    If exmodel_config['cross_val_strategy'] == None, then any kfolds= input is ignored; otherwise, the number specified is used.\n",
    "    \n",
    "    :param kfolds: int specifying number of k-folds to use in cross-validation\n",
    "    :param exmodel_config: dict containing general config including for cross-validation -- `kfold=1` implies hold-out\n",
    "    \"\"\"\n",
    "#     if exmodel_config['kfolds'] == 1:\n",
    "#         print(\"Proceeding with holdout\")\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                           test_size=0.2, \n",
    "#                                                           random_state=SEED)                 \n",
    "    \n",
    "    # prepare for k-fold cross-validation; random-state here is notebook-wide, not per-model\n",
    "    # shuffle on the initial sets, but not subsequently -- performing the same operation twice means a very different dataset\n",
    "    if shuffle_kfolds:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=SEED)\n",
    "    else:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=False)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(params)\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )   \n",
    "    \n",
    "    # setup for serialization\n",
    "    # runpath = Path(modelpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds/\")\n",
    "    # (runpath).mkdir(exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # initialize lists for out-of-fold preds and ground truth\n",
    "    oof_preds, oof_y = [], []\n",
    "    \n",
    "    # initialize a numpy.ndarray containing the fold-model's preds for test set\n",
    "    test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    # preprocessing\n",
    "    if 'widedeep' in arch: # Rank-Gauss scale, and do WideDeep preprocessing\n",
    "        # preprocessing first\n",
    "\n",
    "        # CONSIDER whether to feed the binned data into the GBMs too\n",
    "        X_bins = np.zeros((X.shape[0],X.shape[1]))\n",
    "        for i in range(X.shape[1]): # assumes X is a pd.DataFrame\n",
    "            X_bins[:,i] = pd.qcut(X.iloc[:,i],X.shape[1],labels=False,duplicates = 'drop')\n",
    "        \n",
    "        X_bins = X_bins.astype(np.int8)     \n",
    "        X_bins = pd.DataFrame(X_bins, index=X.index, columns=[f'rkd_f{col}' for col in range(100)])\n",
    "        \n",
    "        scaler = GaussRankScaler()\n",
    "        X_gauss = scaler.fit_transform(X)\n",
    "        X_gauss = pd.DataFrame(X_gauss, columns=X.columns, index=X.index)\n",
    "        \n",
    "        X_pre = X_gauss.join(X_bins)\n",
    "        \n",
    "        cont_cols = X_pre.iloc[:,:100].columns\n",
    "        wide_cols = X_pre.iloc[:, 100:].columns\n",
    "        \n",
    "        # wide part\n",
    "        wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "        X_wide = wide_preprocessor.fit_transform(X_pre)\n",
    "\n",
    "        # deep part\n",
    "        if \"SAINT\" in arch:\n",
    "            tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=True,embed_cols=wide_cols)\n",
    "        else:\n",
    "            tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=False,embed_cols=wide_cols)\n",
    "        X_tab = tab_preprocessor.fit_transform(X_pre) \n",
    "        \n",
    "        # transforming the test set\n",
    "        X_test_wide = wide_preprocessor.transform(X_test)\n",
    "        X_test_tab = tab_preprocessor.transform(X_test)\n",
    "        \n",
    "    else: # if using a GBM, simply use the RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "#         print(f\"type(train_ids) = {type(train_ids)} and train_ids.shape = {train_ids.shape}\")\n",
    "#         print(f\"type(valid_ids) = {type(valid_ids)} and train_ids.shape = {valid_ids.shape}\")\n",
    "        if fold < start_fold: # skip folds that are already trained\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"FOLD {fold}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "            if 'widedeep' in arch: # handle wide and deep tabs in parallel\n",
    "                X_train_wide, X_valid_wide = X_wide[train_ids, :], X_wide[valid_ids, :]\n",
    "                X_train_tab, X_valid_tab = X_tab[train_ids, :], X_tab[valid_ids, :]\n",
    "#                 print(f\"X_train_wide.shape = {X_train_wide.shape}\")\n",
    "#                 print(f\"X_train_tab.shape = {X_train_tab.shape}\")\n",
    "#                 print(f\"X_test_wide.shape = {X_test_wide.shape}\")\n",
    "#                 print(f\"X_test_tab.shape = {X_test_tab.shape}\")\n",
    "            else: # handle datasets for GBMs\n",
    "                if isinstance(X, np.ndarray):\n",
    "                    X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                else:\n",
    "                    X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:] # bc need pandas.DataFrames for ce\n",
    "                \n",
    "                # scaling\n",
    "                # scaler = RobustScaler()\n",
    "                # X_train = scaler.fit_transform(X_train)\n",
    "                # X_valid = scaler.transform(X_valid)    \n",
    "                \n",
    "                # if encode_cats:\n",
    "                #     encoder = ce.WOEEncoder(cols=categoricals)\n",
    "                #     encoder.fit(X_train,y_train)\n",
    "                #     X_train = encoder.transform(X_train)\n",
    "                #     X_valid = encoder.transform(X_valid)\n",
    "                # # exmodel_config['feature_count'] = len(X.columns)\n",
    "                #     wandb.log({\n",
    "                #         'feature_count': X_train.shape[1],\n",
    "                #         'instance_count': X_train.shape[0],\n",
    "                #         'encoder': str(encoder)\n",
    "                #     })\n",
    "#                 exmodel_config['instance_count'] = X_train.shape[0]\n",
    "#                 exmodel_config['encoder'] = str(encoder)\n",
    "#                     X_test = encoder.transform(X_test)\n",
    "#                 y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "            \n",
    "        \n",
    "        # define models\n",
    "        if arch == 'xgboost':\n",
    "            model = XGBClassifier(\n",
    "                booster='gbtree',\n",
    "                tree_method='gpu_hist',\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1, \n",
    "                verbosity=1, \n",
    "                objective='binary:logistic',\n",
    "                **params)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "        elif arch == 'lightgbm':\n",
    "            try:\n",
    "                model = LGBMClassifier(\n",
    "                    objective='binary',\n",
    "                    random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "    #                 eval_metric='auc',\n",
    "                    device_type='gpu',\n",
    "                    max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "                    gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "                    **params)\n",
    "                \n",
    "                if wandb_tracked:\n",
    "                    model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "#             except LightGBMError:\n",
    "#                 model = LGBMClassifier(\n",
    "#                     objective='binary',\n",
    "#                     random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "#     #                 eval_metric='auc',\n",
    "#     #                 device_type='gpu',\n",
    "#     #                 max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     #                 gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "#                     **params)\n",
    "                \n",
    "#                 if wandb_tracked:\n",
    "#                     model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "#                 else:\n",
    "#                     model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "            \n",
    "        elif arch == 'catboost':\n",
    "            model = CatBoostClassifier(\n",
    "                task_type='GPU',\n",
    "                silent=True,\n",
    "                random_state=random_state,\n",
    "                **params) \n",
    "        \n",
    "            model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "            \n",
    "        elif 'widedeep' in arch: # only coding for TabMlp right now\n",
    "#             X_train = pd.DataFrame(X_train, columns=[f\"f{x}\" for x in range(X_train.shape[1])])\n",
    "#             X_valid = pd.DataFrame(X_valid, columns=[f\"f{x}\" for x in range(X_valid.shape[1])])\n",
    "#             X_test = pd.DataFrame(X_test, columns=[f\"f{x}\" for x in range(X_test.shape[1])])\n",
    "            \n",
    "            wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "            deeptabular = TabMlp(\n",
    "                mlp_hidden_dims=[64,32],\n",
    "                column_idx=tab_preprocessor.column_idx,\n",
    "            #     embed_input=tab_preprocessor.embeddings_input,\n",
    "                continuous_cols=cont_cols,\n",
    "            )\n",
    "            \n",
    "            # model instantiation and training\n",
    "            model = WideDeep(wide=wide, deeptabular=deeptabular)\n",
    "            \n",
    "            \n",
    "            n_epochs = 300\n",
    "\n",
    "            # pytorch hyperparams\n",
    "            wide_opt = AdamW(model.wide.parameters(), lr=0.1)\n",
    "            deep_opt = AdamW(model.deeptabular.parameters(), lr=0.1)\n",
    "            \n",
    "            wide_sch = OneCycleLR(optimizer=wide_opt, max_lr=0.01, steps_per_epoch=X_train_wide.shape[0], epochs=n_epochs)\n",
    "            deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_train_tab.shape[0], epochs=n_epochs)\n",
    "            \n",
    "            optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "            lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "            \n",
    "            \n",
    "            callbacks = [\n",
    "                LRHistory(n_epochs=n_epochs), \n",
    "            ]\n",
    "            \n",
    "            # trainer\n",
    "            trainer = Trainer(model=model, \n",
    "                              objective='binary', \n",
    "                              metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                              seed=random_state, \n",
    "                              optimizers=optimizers,\n",
    "                              callbacks=callbacks\n",
    "                             )\n",
    "            \n",
    "#             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "            trainer.fit( # this is where problem is beginning\n",
    "                X_wide=X_train_wide,\n",
    "                X_tab=X_train_tab,\n",
    "                target=y_train,\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=1024, # default value is 32\n",
    "#                 val_split=0.2, # no need for this\n",
    "            )\n",
    "            \n",
    "            y_valid_preds = trainer.predict_proba(X_wide=X_valid_wide, X_tab=X_valid_tab, batch_size=1024)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            \n",
    "            # test set inference\n",
    "            fold_test_preds = trainer.predict_proba(X_wide=X_test_wide, X_tab=X_test_tab, batch_size=1024)[:,1]\n",
    "            test_preds += fold_test_preds\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "#         valid_loss = log_loss(y_valid, y_pred)\n",
    "        # give the valid AUC score, for edification\n",
    "        fold_valid_auc = roc_auc_score(y_valid, y_valid_preds)\n",
    "        if wandb_tracked:\n",
    "            wandb.log({f'fold{fold}_valid_roc_auc': fold_valid_auc})\n",
    "        print(f\"Valid AUC for fold {fold} is {fold_valid_auc}\")   \n",
    "        # dump(model, Path(runpath/f\"{arch}_fold{fold}_rs{random_state}_model.joblib\"))\n",
    "\n",
    "    model_valid_auc = roc_auc_score(oof_y, oof_preds)\n",
    "    print(f\"Valid AUC score for {arch} model is {model_valid_auc}\")\n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_valid_auc': model_valid_auc,\n",
    "                   'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()),\n",
    "                   'model_seed': random_state,\n",
    "                  })\n",
    "        wandb.finish()\n",
    "    \n",
    "    # finalize test preds\n",
    "    test_preds /= exmodel_config['kfolds']\n",
    "    \n",
    "    # save OOF preds and test-set preds\n",
    "#     if 'widedeep' in arch:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "#     else:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "    if not (datapath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\").is_file():\n",
    "        dump(oof_y, predpath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\")\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         if 'widedeep' in arch:\n",
    "#         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "#                    'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()), \n",
    "#         #                    'model_params': str(model.get_params()),\n",
    "#         })\n",
    "# #         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "# # #                    'model_params': str(model.get_params()),\n",
    "# #                   })\n",
    "#         wandb.finish()\n",
    "    return oof_preds, test_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99da57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.basic import LightGBMError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03ec798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(arch:str, X, y, X_test, params:dict={}, start_fold=0, \n",
    "                         exmodel_config=exmodel_config, wandb_config=wandb_config, \n",
    "                         random_state=42, shuffle_kfolds=True, wandb_tracked=True, encode_cats=False):\n",
    "    \"\"\"\n",
    "    Function to handle model training process in the context of cross-validation -- via hold-out or via k-fold.\n",
    "    If exmodel_config['cross_val_strategy'] == None, then any kfolds= input is ignored; otherwise, the number specified is used.\n",
    "    \n",
    "    :param kfolds: int specifying number of k-folds to use in cross-validation\n",
    "    :param exmodel_config: dict containing general config including for cross-validation -- `kfold=1` implies hold-out\n",
    "    \"\"\"\n",
    "#     if exmodel_config['kfolds'] == 1:\n",
    "#         print(\"Proceeding with holdout\")\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                           test_size=0.2, \n",
    "#                                                           random_state=SEED)                 \n",
    "    \n",
    "    # prepare for k-fold cross-validation; random-state here is notebook-wide, not per-model\n",
    "    # shuffle on the initial sets, but not subsequently -- performing the same operation twice means a very different dataset\n",
    "    if shuffle_kfolds:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=SEED)\n",
    "    else:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=False)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(params)\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )   \n",
    "    \n",
    "    # setup for serialization\n",
    "    # runpath = Path(modelpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds/\")\n",
    "    # (runpath).mkdir(exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # initialize lists for out-of-fold preds and ground truth\n",
    "    oof_preds, oof_y = [], []\n",
    "    \n",
    "    # initialize a numpy.ndarray containing the fold-model's preds for test set\n",
    "    test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    # preprocessing\n",
    "    if 'widedeep' in arch: # Rank-Gauss scale, and do WideDeep preprocessing\n",
    "        # preprocessing first\n",
    "\n",
    "        # CONSIDER whether to feed the binned data into the GBMs too\n",
    "        X_bins = np.zeros((X.shape[0],X.shape[1]))\n",
    "        for i in range(X.shape[1]): # assumes X is a pd.DataFrame\n",
    "            X_bins[:,i] = pd.qcut(X.iloc[:,i],X.shape[1],labels=False,duplicates = 'drop')\n",
    "        \n",
    "        X_bins = X_bins.astype(np.int8)     \n",
    "        X_bins = pd.DataFrame(X_bins, index=X.index, columns=[f'rkd_f{col}' for col in range(100)])\n",
    "        \n",
    "        scaler = GaussRankScaler()\n",
    "        X_gauss = scaler.fit_transform(X)\n",
    "        X_gauss = pd.DataFrame(X_gauss, columns=X.columns, index=X.index)\n",
    "        \n",
    "        X_pre = X_gauss.join(X_bins)\n",
    "        \n",
    "        cont_cols = X_pre.iloc[:,:100].columns\n",
    "        wide_cols = X_pre.iloc[:, 100:].columns\n",
    "        \n",
    "        # wide part\n",
    "        wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "        X_wide = wide_preprocessor.fit_transform(X_pre)\n",
    "\n",
    "        # deep part\n",
    "        if \"SAINT\" in arch:\n",
    "            tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=True,embed_cols=wide_cols)\n",
    "        else:\n",
    "            tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=False,embed_cols=wide_cols)\n",
    "        X_tab = tab_preprocessor.fit_transform(X_pre) \n",
    "        \n",
    "        # transforming the test set\n",
    "        X_test_wide = wide_preprocessor.transform(X_test)\n",
    "        X_test_tab = tab_preprocessor.transform(X_test)\n",
    "        \n",
    "    else: # if using a GBM, simply use the RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "#         print(f\"type(train_ids) = {type(train_ids)} and train_ids.shape = {train_ids.shape}\")\n",
    "#         print(f\"type(valid_ids) = {type(valid_ids)} and train_ids.shape = {valid_ids.shape}\")\n",
    "        if fold < start_fold: # skip folds that are already trained\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"FOLD {fold}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "            if 'widedeep' in arch: # handle wide and deep tabs in parallel\n",
    "                X_train_wide, X_valid_wide = X_wide[train_ids, :], X_wide[valid_ids, :]\n",
    "                X_train_tab, X_valid_tab = X_tab[train_ids, :], X_tab[valid_ids, :]\n",
    "#                 print(f\"X_train_wide.shape = {X_train_wide.shape}\")\n",
    "#                 print(f\"X_train_tab.shape = {X_train_tab.shape}\")\n",
    "#                 print(f\"X_test_wide.shape = {X_test_wide.shape}\")\n",
    "#                 print(f\"X_test_tab.shape = {X_test_tab.shape}\")\n",
    "            else: # handle datasets for GBMs\n",
    "                if isinstance(X, np.ndarray):\n",
    "                    X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                else:\n",
    "                    X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:] # bc need pandas.DataFrames for ce\n",
    "                \n",
    "                # scaling\n",
    "                # scaler = RobustScaler()\n",
    "                # X_train = scaler.fit_transform(X_train)\n",
    "                # X_valid = scaler.transform(X_valid)    \n",
    "                \n",
    "                # if encode_cats:\n",
    "                #     encoder = ce.WOEEncoder(cols=categoricals)\n",
    "                #     encoder.fit(X_train,y_train)\n",
    "                #     X_train = encoder.transform(X_train)\n",
    "                #     X_valid = encoder.transform(X_valid)\n",
    "                # # exmodel_config['feature_count'] = len(X.columns)\n",
    "                #     wandb.log({\n",
    "                #         'feature_count': X_train.shape[1],\n",
    "                #         'instance_count': X_train.shape[0],\n",
    "                #         'encoder': str(encoder)\n",
    "                #     })\n",
    "#                 exmodel_config['instance_count'] = X_train.shape[0]\n",
    "#                 exmodel_config['encoder'] = str(encoder)\n",
    "#                     X_test = encoder.transform(X_test)\n",
    "#                 y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "            \n",
    "        \n",
    "        # define models\n",
    "        if arch == 'xgboost':\n",
    "            model = XGBClassifier(\n",
    "                booster='gbtree',\n",
    "                tree_method='gpu_hist',\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1, \n",
    "                verbosity=1, \n",
    "                objective='binary:logistic',\n",
    "                **params)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "        elif arch == 'lightgbm':\n",
    "            try:\n",
    "                model = LGBMClassifier(\n",
    "                    objective='binary',\n",
    "                    random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "    #                 eval_metric='auc',\n",
    "                    device_type='gpu',\n",
    "                    max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "                    gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "                    **params)\n",
    "                \n",
    "                if wandb_tracked:\n",
    "                    model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "#             except LightGBMError:\n",
    "#                 model = LGBMClassifier(\n",
    "#                     objective='binary',\n",
    "#                     random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "#     #                 eval_metric='auc',\n",
    "#     #                 device_type='gpu',\n",
    "#     #                 max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     #                 gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "#                     **params)\n",
    "                \n",
    "#                 if wandb_tracked:\n",
    "#                     model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "#                 else:\n",
    "#                     model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "            \n",
    "        elif arch == 'catboost':\n",
    "            model = CatBoostClassifier(\n",
    "                task_type='GPU',\n",
    "                silent=True,\n",
    "                random_state=random_state,\n",
    "                **params) \n",
    "        \n",
    "            model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "            \n",
    "        elif 'widedeep' in arch: # only coding for TabMlp right now\n",
    "#             X_train = pd.DataFrame(X_train, columns=[f\"f{x}\" for x in range(X_train.shape[1])])\n",
    "#             X_valid = pd.DataFrame(X_valid, columns=[f\"f{x}\" for x in range(X_valid.shape[1])])\n",
    "#             X_test = pd.DataFrame(X_test, columns=[f\"f{x}\" for x in range(X_test.shape[1])])\n",
    "            \n",
    "            wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "            deeptabular = TabMlp(\n",
    "                mlp_hidden_dims=[64,32],\n",
    "                column_idx=tab_preprocessor.column_idx,\n",
    "            #     embed_input=tab_preprocessor.embeddings_input,\n",
    "                continuous_cols=cont_cols,\n",
    "            )\n",
    "            \n",
    "            # model instantiation and training\n",
    "            model = WideDeep(wide=wide, deeptabular=deeptabular)\n",
    "            \n",
    "            \n",
    "            n_epochs = 300\n",
    "\n",
    "            # pytorch hyperparams\n",
    "            wide_opt = AdamW(model.wide.parameters(), lr=0.1)\n",
    "            deep_opt = AdamW(model.deeptabular.parameters(), lr=0.1)\n",
    "            \n",
    "            wide_sch = OneCycleLR(optimizer=wide_opt, max_lr=0.01, steps_per_epoch=X_train_wide.shape[0], epochs=n_epochs)\n",
    "            deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_train_tab.shape[0], epochs=n_epochs)\n",
    "            \n",
    "            optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "            lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "            \n",
    "            \n",
    "            callbacks = [\n",
    "                LRHistory(n_epochs=n_epochs), \n",
    "            ]\n",
    "            \n",
    "            # trainer\n",
    "            trainer = Trainer(model=model, \n",
    "                              objective='binary', \n",
    "                              metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                              seed=random_state, \n",
    "                              optimizers=optimizers,\n",
    "                              callbacks=callbacks\n",
    "                             )\n",
    "            \n",
    "#             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "            trainer.fit( # this is where problem is beginning\n",
    "                X_wide=X_train_wide,\n",
    "                X_tab=X_train_tab,\n",
    "                target=y_train,\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=1024, # default value is 32\n",
    "#                 val_split=0.2, # no need for this\n",
    "            )\n",
    "            \n",
    "            y_valid_preds = trainer.predict_proba(X_wide=X_valid_wide, X_tab=X_valid_tab, batch_size=1024)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            \n",
    "            # test set inference\n",
    "            fold_test_preds = trainer.predict_proba(X_wide=X_test_wide, X_tab=X_test_tab, batch_size=1024)[:,1]\n",
    "            test_preds += fold_test_preds\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "#         valid_loss = log_loss(y_valid, y_pred)\n",
    "        # give the valid AUC score, for edification\n",
    "        fold_valid_auc = roc_auc_score(y_valid, y_valid_preds)\n",
    "        if wandb_tracked:\n",
    "            wandb.log({f'fold{fold}_valid_roc_auc': fold_valid_auc})\n",
    "        print(f\"Valid AUC for fold {fold} is {fold_valid_auc}\")   \n",
    "        # dump(model, Path(runpath/f\"{arch}_fold{fold}_rs{random_state}_model.joblib\"))\n",
    "\n",
    "    model_valid_auc = roc_auc_score(oof_y, oof_preds)\n",
    "    print(f\"Valid AUC score for {arch} model is {model_valid_auc}\")\n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_valid_auc': model_valid_auc,\n",
    "                   'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()),\n",
    "                   'model_seed': random_state,\n",
    "                  })\n",
    "        wandb.finish()\n",
    "    \n",
    "    # finalize test preds\n",
    "    test_preds /= exmodel_config['kfolds']\n",
    "    \n",
    "    # save OOF preds and test-set preds\n",
    "#     if 'widedeep' in arch:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "#     else:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "    if not (datapath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\").is_file():\n",
    "        dump(oof_y, predpath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\")\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         if 'widedeep' in arch:\n",
    "#         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "#                    'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()), \n",
    "#         #                    'model_params': str(model.get_params()),\n",
    "#         })\n",
    "# #         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "# # #                    'model_params': str(model.get_params()),\n",
    "# #                   })\n",
    "#         wandb.finish()\n",
    "    return oof_preds, test_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caadf6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(arch:str, X, y, X_test, params:dict={}, start_fold=0, \n",
    "                         exmodel_config=exmodel_config, wandb_config=wandb_config, \n",
    "                         random_state=42, shuffle_kfolds=True, wandb_tracked=True, encode_cats=False):\n",
    "    \"\"\"\n",
    "    Function to handle model training process in the context of cross-validation -- via hold-out or via k-fold.\n",
    "    If exmodel_config['cross_val_strategy'] == None, then any kfolds= input is ignored; otherwise, the number specified is used.\n",
    "    \n",
    "    :param kfolds: int specifying number of k-folds to use in cross-validation\n",
    "    :param exmodel_config: dict containing general config including for cross-validation -- `kfold=1` implies hold-out\n",
    "    \"\"\"\n",
    "#     if exmodel_config['kfolds'] == 1:\n",
    "#         print(\"Proceeding with holdout\")\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "#                                                           test_size=0.2, \n",
    "#                                                           random_state=SEED)                 \n",
    "    \n",
    "    # prepare for k-fold cross-validation; random-state here is notebook-wide, not per-model\n",
    "    # shuffle on the initial sets, but not subsequently -- performing the same operation twice means a very different dataset\n",
    "    if shuffle_kfolds:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=True, random_state=SEED)\n",
    "    else:\n",
    "        kfold = exmodel_config['cross_val_strategy'](n_splits=exmodel_config['kfolds'], shuffle=False)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        exmodel_config['arch'] = arch\n",
    "        exmodel_config[f'{arch}_params'] = str(params)\n",
    "        wandb.init(\n",
    "            project=\"202111_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )   \n",
    "    \n",
    "    # setup for serialization\n",
    "    # runpath = Path(modelpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds/\")\n",
    "    # (runpath).mkdir(exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # initialize lists for out-of-fold preds and ground truth\n",
    "    oof_preds, oof_y = [], []\n",
    "    \n",
    "    # initialize a numpy.ndarray containing the fold-model's preds for test set\n",
    "    test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    # preprocessing\n",
    "    if 'widedeep' in arch: # Rank-Gauss scale, and do WideDeep preprocessing\n",
    "        # preprocessing first\n",
    "\n",
    "        # CONSIDER whether to feed the binned data into the GBMs too\n",
    "        X_bins = np.zeros((X.shape[0],X.shape[1]))\n",
    "        for i in range(X.shape[1]): # assumes X is a pd.DataFrame\n",
    "            X_bins[:,i] = pd.qcut(X.iloc[:,i],X.shape[1],labels=False,duplicates = 'drop')\n",
    "        \n",
    "        X_bins = X_bins.astype(np.int8)     \n",
    "        X_bins = pd.DataFrame(X_bins, index=X.index, columns=[f'rkd_f{col}' for col in range(100)])\n",
    "        \n",
    "        scaler = GaussRankScaler()\n",
    "        X_gauss = scaler.fit_transform(X)\n",
    "        X_gauss = pd.DataFrame(X_gauss, columns=X.columns, index=X.index)\n",
    "        \n",
    "        X_pre = X_gauss.join(X_bins)\n",
    "        \n",
    "        cont_cols = X_pre.iloc[:,:100].columns\n",
    "        wide_cols = X_pre.iloc[:, 100:].columns\n",
    "        \n",
    "        # wide part\n",
    "        wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "        X_wide = wide_preprocessor.fit_transform(X_pre)\n",
    "\n",
    "        # deep part\n",
    "        if \"SAINT\" in arch:\n",
    "            tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=True,embed_cols=wide_cols)\n",
    "        else:\n",
    "            tab_preprocessor = TabPreprocessor(continuous_cols=cont_cols, scale=False, for_transformer=False,embed_cols=wide_cols)\n",
    "        X_tab = tab_preprocessor.fit_transform(X_pre) \n",
    "        \n",
    "        # transforming the test set\n",
    "        X_test_wide = wide_preprocessor.transform(X_test)\n",
    "        X_test_tab = tab_preprocessor.transform(X_test)\n",
    "        \n",
    "    else: # if using a GBM, simply use the RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X,y)):\n",
    "#         print(f\"type(train_ids) = {type(train_ids)} and train_ids.shape = {train_ids.shape}\")\n",
    "#         print(f\"type(valid_ids) = {type(valid_ids)} and train_ids.shape = {valid_ids.shape}\")\n",
    "        if fold < start_fold: # skip folds that are already trained\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"FOLD {fold}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            y_train, y_valid = y[train_ids], y[valid_ids] # y will be an np.ndarray already; handling will be same regardless of model\n",
    "            if 'widedeep' in arch: # handle wide and deep tabs in parallel\n",
    "                X_train_wide, X_valid_wide = X_wide[train_ids, :], X_wide[valid_ids, :]\n",
    "                X_train_tab, X_valid_tab = X_tab[train_ids, :], X_tab[valid_ids, :]\n",
    "#                 print(f\"X_train_wide.shape = {X_train_wide.shape}\")\n",
    "#                 print(f\"X_train_tab.shape = {X_train_tab.shape}\")\n",
    "#                 print(f\"X_test_wide.shape = {X_test_wide.shape}\")\n",
    "#                 print(f\"X_test_tab.shape = {X_test_tab.shape}\")\n",
    "            else: # handle datasets for GBMs\n",
    "                if isinstance(X, np.ndarray):\n",
    "                    X_train, X_valid = X[train_ids], X[valid_ids]\n",
    "                else:\n",
    "                    X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:] # bc need pandas.DataFrames for ce\n",
    "                \n",
    "                # scaling\n",
    "                # scaler = RobustScaler()\n",
    "                # X_train = scaler.fit_transform(X_train)\n",
    "                # X_valid = scaler.transform(X_valid)    \n",
    "                \n",
    "                # if encode_cats:\n",
    "                #     encoder = ce.WOEEncoder(cols=categoricals)\n",
    "                #     encoder.fit(X_train,y_train)\n",
    "                #     X_train = encoder.transform(X_train)\n",
    "                #     X_valid = encoder.transform(X_valid)\n",
    "                # # exmodel_config['feature_count'] = len(X.columns)\n",
    "                #     wandb.log({\n",
    "                #         'feature_count': X_train.shape[1],\n",
    "                #         'instance_count': X_train.shape[0],\n",
    "                #         'encoder': str(encoder)\n",
    "                #     })\n",
    "#                 exmodel_config['instance_count'] = X_train.shape[0]\n",
    "#                 exmodel_config['encoder'] = str(encoder)\n",
    "#                     X_test = encoder.transform(X_test)\n",
    "#                 y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "            \n",
    "        \n",
    "        # define models\n",
    "        if arch == 'xgboost':\n",
    "            model = XGBClassifier(\n",
    "                booster='gbtree',\n",
    "                tree_method='gpu_hist',\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1, \n",
    "                verbosity=1, \n",
    "                objective='binary:logistic',\n",
    "                **params)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "        elif arch == 'lightgbm':\n",
    "            # try:\n",
    "            model = LGBMClassifier(\n",
    "                objective='binary',\n",
    "                random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "#                 eval_metric='auc',\n",
    "                device_type='gpu',\n",
    "                max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "                gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "                **params)\n",
    "\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "#             except LightGBMError:\n",
    "#                 model = LGBMClassifier(\n",
    "#                     objective='binary',\n",
    "#                     random_state=random_state,\n",
    "#                     device_type='cpu',\n",
    "#                     n_jobs=-1,\n",
    "#     #                 eval_metric='auc',\n",
    "#     #                 device_type='gpu',\n",
    "#     #                 max_bin=63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     #                 gpu_use_dp=False, # forces use of single precision rather than double for better perf, esp on consumer Nvidia chips\n",
    "#                     **params)\n",
    "                \n",
    "#                 if wandb_tracked:\n",
    "#                     model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()],)\n",
    "#                 else:\n",
    "#                     model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "\n",
    "            \n",
    "        elif arch == 'catboost':\n",
    "            model = CatBoostClassifier(\n",
    "                task_type='GPU',\n",
    "                silent=True,\n",
    "                random_state=random_state,\n",
    "                **params) \n",
    "        \n",
    "            model.fit(X_train, y_train)\n",
    "            y_valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            # add the fold's predictions to the model's test-set predictions (will divide later)\n",
    "            test_preds += model.predict_proba(X_test)[:,1]\n",
    "            \n",
    "        elif 'widedeep' in arch: # only coding for TabMlp right now\n",
    "#             X_train = pd.DataFrame(X_train, columns=[f\"f{x}\" for x in range(X_train.shape[1])])\n",
    "#             X_valid = pd.DataFrame(X_valid, columns=[f\"f{x}\" for x in range(X_valid.shape[1])])\n",
    "#             X_test = pd.DataFrame(X_test, columns=[f\"f{x}\" for x in range(X_test.shape[1])])\n",
    "            \n",
    "            wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "            deeptabular = TabMlp(\n",
    "                mlp_hidden_dims=[64,32],\n",
    "                column_idx=tab_preprocessor.column_idx,\n",
    "            #     embed_input=tab_preprocessor.embeddings_input,\n",
    "                continuous_cols=cont_cols,\n",
    "            )\n",
    "            \n",
    "            # model instantiation and training\n",
    "            model = WideDeep(wide=wide, deeptabular=deeptabular)\n",
    "            \n",
    "            \n",
    "            n_epochs = 300\n",
    "\n",
    "            # pytorch hyperparams\n",
    "            wide_opt = AdamW(model.wide.parameters(), lr=0.1)\n",
    "            deep_opt = AdamW(model.deeptabular.parameters(), lr=0.1)\n",
    "            \n",
    "            wide_sch = OneCycleLR(optimizer=wide_opt, max_lr=0.01, steps_per_epoch=X_train_wide.shape[0], epochs=n_epochs)\n",
    "            deep_sch = OneCycleLR(optimizer=deep_opt, max_lr=0.01, steps_per_epoch=X_train_tab.shape[0], epochs=n_epochs)\n",
    "            \n",
    "            optimizers = {'wide': wide_opt, 'deeptabular': deep_opt }\n",
    "            lr_schedulers = {'wide': wide_sch, 'deeptabular': deep_sch }\n",
    "            \n",
    "            \n",
    "            callbacks = [\n",
    "                LRHistory(n_epochs=n_epochs), \n",
    "            ]\n",
    "            \n",
    "            # trainer\n",
    "            trainer = Trainer(model=model, \n",
    "                              objective='binary', \n",
    "                              metrics=[Accuracy], # with AUROC got TypeError: '>' not supported between instances of 'NoneType' and 'int' \n",
    "                              seed=random_state, \n",
    "                              optimizers=optimizers,\n",
    "                              callbacks=callbacks\n",
    "                             )\n",
    "            \n",
    "#             print(f\"type(X_train_wide) is {type(X_train_wide)} and type(X_train_tab) is {type(X_train_tab)}\")\n",
    "            trainer.fit( # this is where problem is beginning\n",
    "                X_wide=X_train_wide,\n",
    "                X_tab=X_train_tab,\n",
    "                target=y_train,\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=1024, # default value is 32\n",
    "#                 val_split=0.2, # no need for this\n",
    "            )\n",
    "            \n",
    "            y_valid_preds = trainer.predict_proba(X_wide=X_valid_wide, X_tab=X_valid_tab, batch_size=1024)[:,1]\n",
    "            \n",
    "            # add the fold-model's OOF preds and ground truths to the out-of-loop lists\n",
    "            oof_preds.extend(y_valid_preds)\n",
    "            oof_y.extend(y_valid)\n",
    "            \n",
    "            \n",
    "            # test set inference\n",
    "            fold_test_preds = trainer.predict_proba(X_wide=X_test_wide, X_tab=X_test_tab, batch_size=1024)[:,1]\n",
    "            test_preds += fold_test_preds\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "#         valid_loss = log_loss(y_valid, y_pred)\n",
    "        # give the valid AUC score, for edification\n",
    "        fold_valid_auc = roc_auc_score(y_valid, y_valid_preds)\n",
    "        if wandb_tracked:\n",
    "            wandb.log({f'fold{fold}_valid_roc_auc': fold_valid_auc})\n",
    "        print(f\"Valid AUC for fold {fold} is {fold_valid_auc}\")   \n",
    "        # dump(model, Path(runpath/f\"{arch}_fold{fold}_rs{random_state}_model.joblib\"))\n",
    "\n",
    "    model_valid_auc = roc_auc_score(oof_y, oof_preds)\n",
    "    print(f\"Valid AUC score for {arch} model is {model_valid_auc}\")\n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_valid_auc': model_valid_auc,\n",
    "                   'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()),\n",
    "                   'model_seed': random_state,\n",
    "                  })\n",
    "        wandb.finish()\n",
    "    \n",
    "    # finalize test preds\n",
    "    test_preds /= exmodel_config['kfolds']\n",
    "    \n",
    "    # save OOF preds and test-set preds\n",
    "#     if 'widedeep' in arch:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_{n_epochs}epochs-per-fold_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "#     else:\n",
    "#         dump(oof_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_oof_preds.joblib\"))\n",
    "#         dump(test_preds, Path(predpath/f\"{wandb_config['name']}_{arch}_{exmodel_config['kfolds']}folds_rs{random_state}_test_preds.joblib\"))\n",
    "    \n",
    "    if not (datapath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\").is_file():\n",
    "        dump(oof_y, predpath/f\"{exmodel_config['kfolds']}folds_rs{SEED}_oof_y.joblib\")\n",
    "    \n",
    "#     if wandb_tracked:\n",
    "# #         if 'widedeep' in arch:\n",
    "#         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "#                    'model_params': str(model.parameters()) if 'widedeep' in arch else str(model.get_params()), \n",
    "#         #                    'model_params': str(model.get_params()),\n",
    "#         })\n",
    "# #         wandb.log({'model_valid_auc': model_valid_auc,\n",
    "# #                    'oof_preds': oof_preds,\n",
    "# #                    'test_preds': test_preds,\n",
    "# # #                    'model_params': str(model.get_params()),\n",
    "# #                   })\n",
    "#         wandb.finish()\n",
    "    return oof_preds, test_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da27fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in architectures:\n",
    "    for model_seed in model_seeds:\n",
    "        # update exmodel_config here\n",
    "        oof_pred, test_pred = cross_validate_model(arch=arch, X=X, y=y, X_test=X_test, \n",
    "                                         wandb_config=wandb_config,\n",
    "                                         random_state=model_seed,\n",
    "                                         params=lv1_params[arch],\n",
    "                                         exmodel_config=exmodel_config, \n",
    "                                         wandb_tracked=True\n",
    "                                        )\n",
    "        oof_lv1[f'{arch}{model_seed}'] = oof_pred\n",
    "        test_lv1[f'{arch}{model_seed}'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b00ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_source = datapath/'train.feather'\n",
    "# df = pd.read_feather(path=datapath/'train.feather')\n",
    "# y = np.array(df.target)\n",
    "# dump(y, filename=datapath/'y.joblib')\n",
    "# del df\n",
    "\n",
    "y = load(datapath/'y_corrected.joblib')\n",
    "\n",
    "# df.index.name = 'id'\n",
    "# y_train = df.target\n",
    "# features = [x for x in df.columns if x != 'target']\n",
    "# X_train = df[features]\n",
    "# # X.index.name = 'id'\n",
    "# # y.index.name = 'id'\n",
    "# X = np.array(X_train)\n",
    "# y = np.array(y_train)\n",
    "\n",
    "# del df, X_train, y_train\n",
    "\n",
    "# load the Boruta-filtered green-zone 98 features (based on 200 iterations of the algo)\n",
    "# train_source = '/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/alt_datasets/X_boruta_200iter_filtered_green.joblib'\n",
    "# X = load(train_source)\n",
    "\n",
    "train_source = datapath/'X_orig.feather'\n",
    "X = pd.read_feather(train_source)\n",
    "\n",
    "# exmodel_config['feature_count'] = len(X.columns)\n",
    "exmodel_config['feature_count'] = X.shape[1]\n",
    "exmodel_config['instance_count'] = X.shape[0]\n",
    "exmodel_config['scaler'] = str(RobustScaler())\n",
    "# exmodel_config['feature_generator'] = None\n",
    "# exmodel_config['feature_generator'] = \"Summary statistics\"\n",
    "\n",
    "exmodel_config['train_source'] = str(train_source)\n",
    "test_source = datapath/'test.feather'\n",
    "exmodel_config['test_source'] = str(test_source)\n",
    "X_test = pd.read_feather(path=test_source)\n",
    "# X_test = X_test.iloc[:, 1:]\n",
    "# X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8b2798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in architectures:\n",
    "    for model_seed in model_seeds:\n",
    "        # update exmodel_config here\n",
    "        oof_pred, test_pred = cross_validate_model(arch=arch, X=X, y=y, X_test=X_test, \n",
    "                                         wandb_config=wandb_config,\n",
    "                                         random_state=model_seed,\n",
    "                                         params=lv1_params[arch],\n",
    "                                         exmodel_config=exmodel_config, \n",
    "                                         wandb_tracked=True\n",
    "                                        )\n",
    "        oof_lv1[f'{arch}{model_seed}'] = oof_pred\n",
    "        test_lv1[f'{arch}{model_seed}'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4325feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "exmodel_config = {\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "#     'random_state': SEED,\n",
    "#     'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "#     'subsample': 1,\n",
    "    'cross_val_strategy': KFold, # None for holdout, or the relevant sklearn class\n",
    "    'kfolds': 5, # if 1, that means just doing holdout\n",
    "    'test_size': 0.2,\n",
    "    **dataset_params\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a7fc802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-config for preprocessing and cross-validation, but NOT for model parameters\n",
    "exmodel_config = {\n",
    "#     \"feature_selector\": SelectKBest,\n",
    "#     \"k_best\": 80,\n",
    "#     \"feature_selection_scoring\": f_regression,\n",
    "#     'random_state': SEED,\n",
    "#     'feature_generation': ['NaN_counts', 'SummaryStats', 'NaN_OneHots'],\n",
    "#     'subsample': 1,\n",
    "    'cross_val_strategy': KFold, # None for holdout, or the relevant sklearn class\n",
    "    'kfolds': 5, # if 1, that means just doing holdout\n",
    "    'test_size': 0.2,\n",
    "    # **dataset_params\n",
    "#     'features_created': False,\n",
    "#     'feature_creator': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e8286c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in architectures:\n",
    "    for model_seed in model_seeds:\n",
    "        # update exmodel_config here\n",
    "        oof_pred, test_pred = cross_validate_model(arch=arch, X=X, y=y, X_test=X_test, \n",
    "                                         wandb_config=wandb_config,\n",
    "                                         random_state=model_seed,\n",
    "                                         params=lv1_params[arch],\n",
    "                                         exmodel_config=exmodel_config, \n",
    "                                         wandb_tracked=True\n",
    "                                        )\n",
    "        oof_lv1[f'{arch}{model_seed}'] = oof_pred\n",
    "        test_lv1[f'{arch}{model_seed}'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c01026eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n",
      "       'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n",
      "       'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n",
      "       'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40',\n",
      "       'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n",
      "       'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n",
      "       'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n",
      "       'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n",
      "       'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n",
      "       'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'],\n",
      "      dtype='object')"
     ]
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12b53789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 100)"
     ]
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ed005c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n",
      "       'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n",
      "       'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n",
      "       'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40',\n",
      "       'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n",
      "       'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n",
      "       'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n",
      "       'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n",
      "       'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n",
      "       'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'],\n",
      "      dtype='object')"
     ]
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0095d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8',\n",
      "       ...\n",
      "       'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'],\n",
      "      dtype='object', length=101)"
     ]
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65fff658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_source = datapath/'train.feather'\n",
    "# df = pd.read_feather(path=datapath/'train.feather')\n",
    "# y = np.array(df.target)\n",
    "# dump(y, filename=datapath/'y.joblib')\n",
    "# del df\n",
    "\n",
    "y = load(datapath/'y_corrected.joblib')\n",
    "\n",
    "# df.index.name = 'id'\n",
    "# y_train = df.target\n",
    "# features = [x for x in df.columns if x != 'target']\n",
    "# X_train = df[features]\n",
    "# # X.index.name = 'id'\n",
    "# # y.index.name = 'id'\n",
    "# X = np.array(X_train)\n",
    "# y = np.array(y_train)\n",
    "\n",
    "# del df, X_train, y_train\n",
    "\n",
    "# load the Boruta-filtered green-zone 98 features (based on 200 iterations of the algo)\n",
    "# train_source = '/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/alt_datasets/X_boruta_200iter_filtered_green.joblib'\n",
    "# X = load(train_source)\n",
    "\n",
    "train_source = datapath/'X_orig.feather'\n",
    "X = pd.read_feather(train_source)\n",
    "\n",
    "# exmodel_config['feature_count'] = len(X.columns)\n",
    "exmodel_config['feature_count'] = X.shape[1]\n",
    "exmodel_config['instance_count'] = X.shape[0]\n",
    "exmodel_config['scaler'] = str(RobustScaler())\n",
    "# exmodel_config['feature_generator'] = None\n",
    "# exmodel_config['feature_generator'] = \"Summary statistics\"\n",
    "\n",
    "exmodel_config['train_source'] = str(train_source)\n",
    "test_source = datapath/'test.feather'\n",
    "exmodel_config['test_source'] = str(test_source)\n",
    "X_test = pd.read_feather(path=test_source)\n",
    "X_test = X_test.iloc[:, 1:]\n",
    "# X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67f6d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n",
      "       'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n",
      "       'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n",
      "       'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40',\n",
      "       'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n",
      "       'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n",
      "       'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n",
      "       'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n",
      "       'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n",
      "       'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'],\n",
      "      dtype='object')"
     ]
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c73c2618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n",
      "       'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n",
      "       'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n",
      "       'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40',\n",
      "       'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n",
      "       'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n",
      "       'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n",
      "       'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n",
      "       'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n",
      "       'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'],\n",
      "      dtype='object')"
     ]
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0cdc962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/202111_Kaggle_tabular_playground/runs/3blrghp7\" target=\"_blank\">stacking_20211130a_120455</a></strong> to <a href=\"https://wandb.ai/hushifang/202111_Kaggle_tabular_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for arch in architectures:\n",
    "    for model_seed in model_seeds:\n",
    "        # update exmodel_config here\n",
    "        oof_pred, test_pred = cross_validate_model(arch=arch, X=X, y=y, X_test=X_test, \n",
    "                                         wandb_config=wandb_config,\n",
    "                                         random_state=model_seed,\n",
    "                                         params=lv1_params[arch],\n",
    "                                         exmodel_config=exmodel_config, \n",
    "                                         wandb_tracked=True\n",
    "                                        )\n",
    "        oof_lv1[f'{arch}{model_seed}'] = oof_pred\n",
    "        test_lv1[f'{arch}{model_seed}'] = test_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
