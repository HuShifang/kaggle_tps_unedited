{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d092083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "# if '/sf/' in pwd:\n",
    "#     COLAB, SAGE = False, False\n",
    "# elif 'google.colab' in str(get_ipython()):\n",
    "#     COLAB, SAGE = True, False # do colab-specific installs later\n",
    "# else:\n",
    "#     COLAB, SAGE = False, True\n",
    "    \n",
    "CONTEXT = 'local' # or 'colab', 'sage', 'kaggle'\n",
    "USE_GPU = True \n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e572817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests # for telegram notifications\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936f70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model selection\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# normalization\n",
    "# from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\n",
    "# from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "# feature generation\n",
    "# import category_encoders as ce\n",
    "\n",
    "# models\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop, LBFGS\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CyclicLR, OneCycleLR, StepLR, CosineAnnealingLR\n",
    "# from pytorch_widedeep import Trainer\n",
    "# from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "# from pytorch_widedeep.models import Wide, TabMlp, WideDeep, SAINT#, TabTransformer, TabNet, TabFastFormer, TabResnet\n",
    "# from pytorch_widedeep.metrics import Accuracy\n",
    "# from pytorch_widedeep.callbacks import EarlyStopping, LRHistory, ModelCheckpoint\n",
    "\n",
    "# feature reduction\n",
    "# from sklearn.decomposition import PCA\n",
    "# from umap import UMAP\n",
    "\n",
    "# clustering\n",
    "# from sklearn.cluster import DBSCAN, KMeans\n",
    "# import hdbscan\n",
    "\n",
    "# feature selection\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "# import featuretools as ft\n",
    "# from BorutaShap import BorutaShap\n",
    "# from boruta import BorutaPy\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"nb_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdfb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time series\n",
    "# import tsfresh\n",
    "\n",
    "# import darts\n",
    "# from darts import TimeSeries\n",
    "# from darts.models import ExponentialSmoothing, AutoARIMA, ARIMA, Prophet, RandomForest, RegressionEnsembleModel, RegressionModel, TFTModel, TCNModel, TransformerModel, NBEATSModel\n",
    "import holidays\n",
    "import dateutil.easter as easter\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee334d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTEXT == 'colab':\n",
    "    # mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # handling datapath\n",
    "    # datapath = Path('/content/drive/MyDrive/kaggle/tabular_playgrounds/dec2021/')\n",
    "    root = Path('') # TODO\n",
    "\n",
    "elif CONTEXT == 'sage':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "elif CONTEXT == 'kaggle':\n",
    "    root = Path('') # TODO\n",
    "    \n",
    "else: # if on local machine\n",
    "    root = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/jan2022/')\n",
    "    datapath = root/'datasets'\n",
    "    # edapath = root/'EDA'\n",
    "    # modelpath = Path('/media/sf/easystore/kaggle_data/tabular_playgrounds/oct2021/models/')\n",
    "    predpath = root/'preds'\n",
    "    subpath = root/'submissions'\n",
    "    studypath = root/'studies'\n",
    "    \n",
    "    for pth in [datapath, predpath, subpath, studypath]:\n",
    "        pth.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea4aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Function to seed everything but the models\n",
    "def seed_everything(seed, pytorch=True, reproducible=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if pytorch:\n",
    "        torch.manual_seed(seed) # set torch CPU seed\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed) # set torch GPU(s) seed(s)\n",
    "        if reproducible and torch.backends.cudnn.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cd3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Function to reduce memory usage by downcasting datatypes in a Pandas DataFrame when possible.\n",
    "    \n",
    "    h/t to Bryan Arnold (https://www.kaggle.com/puremath86/label-correction-experiments-tps-nov-21)\n",
    "    \"\"\"\n",
    "    \n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "066690cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_api_token = 'your_api_token' # for Galileo (jupyter_watcher_bot) on Telegram\n",
    "tg_chat_id = 'your_chat_id'\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_tg_message(text='Cell execution completed.'):  \n",
    "    \"\"\"\n",
    "    h/t Ivan Dembicki Jr. for the base version \n",
    "    (https://medium.com/@ivan.dembicki.jr/notifications-in-jupyter-notebook-with-telegram-f2e892c55173)\n",
    "    \"\"\"\n",
    "    requests.post('https://api.telegram.org/' +  'bot{}/sendMessage'.format(tg_api_token),\n",
    "                  params=dict(chat_id=tg_chat_id, text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d5d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(y_true, y_pred):\n",
    "    '''\n",
    "    h/t Jean-François Puget (@CPMP) -- see https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\n",
    "    '''\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6fa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/282735\n",
    "def better_than_median(inputs, axis):\n",
    "    \"\"\"Compute the mean of the predictions if there are no outliers,\n",
    "    or the median if there are outliers.\n",
    "\n",
    "    Parameter: inputs = ndarray of shape (n_samples, n_folds)\"\"\"\n",
    "    spread = inputs.max(axis=axis) - inputs.min(axis=axis) \n",
    "    spread_lim = 0.45\n",
    "    print(f\"Inliers:  {(spread < spread_lim).sum():7} -> compute mean\")\n",
    "    print(f\"Outliers: {(spread >= spread_lim).sum():7} -> compute median\")\n",
    "    print(f\"Total:    {len(inputs):7}\")\n",
    "    return np.where(spread < spread_lim,\n",
    "                    np.mean(inputs, axis=axis),\n",
    "                    np.median(inputs, axis=axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62a8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series\n",
    "def plot_periodogram(ts, detrend='linear', ax=None):\n",
    "    from scipy.signal import periodogram\n",
    "    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n",
    "    freqencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend=detrend,\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.step(freqencies, spectrum, color=\"purple\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Annual (1)\",\n",
    "            \"Semiannual (2)\",\n",
    "            \"Quarterly (4)\",\n",
    "            \"Bimonthly (6)\",\n",
    "            \"Monthly (12)\",\n",
    "            \"Biweekly (26)\",\n",
    "            \"Weekly (52)\",\n",
    "            \"Semiweekly (104)\",\n",
    "        ],\n",
    "        rotation=30,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Periodogram\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6365cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series\n",
    "def fourier_features(index, freq, order):\n",
    "    time = np.arange(len(index), dtype=np.float32)\n",
    "    k = 2 * np.pi * (1 / freq) * time\n",
    "    features = {}\n",
    "    for i in range(1, order + 1):\n",
    "        features.update({\n",
    "            f\"sin_{freq}_{i}\": np.sin(i * k),\n",
    "            f\"cos_{freq}_{i}\": np.cos(i * k),\n",
    "        })\n",
    "    return pd.DataFrame(features, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b50cc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_params will initially include either trivial class instances or loaded, precomputed artifacts\n",
    "dataset_params = {\n",
    "    'train_source': str(datapath/'train.csv'),\n",
    "    'target_source': str(datapath/'train.csv'),\n",
    "    'test_source': str(datapath/'test.csv'),\n",
    "    # 'scaler': str(RobustScaler()),\n",
    "    # 'pca': str(load(datapath/'pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "    # 'umap': str(load(datapath/'umap_reducer-20211107-n_comp10-n_neighbors15-rs42-pca_mle-RobustScaled_orig_trainset.joblib')),\n",
    "}   \n",
    "\n",
    "# referring back to the already-entered attributes, specify how the pipeline was sequenced\n",
    "# dataset_params['preprocessing_pipeline'] = str([dataset_params['scaler'], dataset_params['pca'], dataset_params['umap']]) # ACTUALLY this is unwieldy\n",
    "# dataset_params['preprocessing_pipeline'] = '[scaler, pca, umap]' # more fragile, but also more readable\n",
    "\n",
    "# now, load the datasets and generate more metadata from them\n",
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "test_df = pd.read_csv(datapath/'test.csv')\n",
    "orig_train_df = train_df.copy()\n",
    "orig_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e5790ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model\n",
    "for df in [train_df, test_df]:\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "# for convenience later\n",
    "countries = ['Sweden', 'Finland', 'Norway']\n",
    "stores = ['KaggleMart', 'KaggleRama']\n",
    "products = ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60231667",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "# all_df.columns\n",
    "print(len(all_df) == len(train_df) + len(test_df))\n",
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98445249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gdp_data(df):\n",
    "    gdp_df = pd.read_csv(datapath/'GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n",
    "    gdp_df.set_index('year', inplace=True)\n",
    "    def get_gdp(row):\n",
    "        country = 'GDP_' + row.country\n",
    "        return gdp_df.loc[row.date.year, country]\n",
    "\n",
    "    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b30a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_exponent = 1.2121103201489674 # see https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model for an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afce9604",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = add_gdp_data(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109b2654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      row_id       date  country       store         product  num_sold  \\\n",
      "0          0 2015-01-01  Finland  KaggleMart      Kaggle Mug     329.0   \n",
      "1          1 2015-01-01  Finland  KaggleMart      Kaggle Hat     520.0   \n",
      "2          2 2015-01-01  Finland  KaggleMart  Kaggle Sticker     146.0   \n",
      "3          3 2015-01-01  Finland  KaggleRama      Kaggle Mug     572.0   \n",
      "4          4 2015-01-01  Finland  KaggleRama      Kaggle Hat     911.0   \n",
      "...      ...        ...      ...         ...             ...       ...   \n",
      "6565   32863 2019-12-31   Sweden  KaggleMart      Kaggle Hat       NaN   \n",
      "6566   32864 2019-12-31   Sweden  KaggleMart  Kaggle Sticker       NaN   \n",
      "6567   32865 2019-12-31   Sweden  KaggleRama      Kaggle Mug       NaN   \n",
      "6568   32866 2019-12-31   Sweden  KaggleRama      Kaggle Hat       NaN   \n",
      "6569   32867 2019-12-31   Sweden  KaggleRama  Kaggle Sticker       NaN   \n",
      "\n",
      "           gdp  \n",
      "0     5.461456  \n",
      "1     5.461456  \n",
      "2     5.461456  \n",
      "3     5.461456  \n",
      "4     5.461456  \n",
      "...        ...  \n",
      "6565  6.282042  \n",
      "6566  6.282042  \n",
      "6567  6.282042  \n",
      "6568  6.282042  \n",
      "6569  6.282042  \n",
      "\n",
      "[32868 rows x 7 columns]"
     ]
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f15eddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [holidays.Finland, holidays.Sweden, holidays.Norway]:\n",
    "#     print(c)\n",
    "    for h in c(years = [2019], observed=True).items():\n",
    "#         print(h)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f7e40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_engineering(df):\n",
    "    '''\n",
    "    Function inspired by / borrowing from @teckmengwong and @ambrosm to create time features that will\n",
    "    capture seasonality.\n",
    "    '''\n",
    "    \n",
    "#     df[YEAR] = df[DATE].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "#     df['week'] = df['date'].dt.week # not used by Teck Meng Wong\n",
    "#     df['day'] = df['date'].dt.day # not used by Teck Meng Wong\n",
    "#     df['day_of_year'] = df['date'].dt.dayofyear # not used by Teck Meng Wong\n",
    "#     df['day_of_month'] = df['date'].dt.days_in_month # not used by Teck Meng Wong\n",
    "#     df['day_of_week'] = df['date'].dt.dayofweek # not used by Teck Meng Wong\n",
    "#    df['weekday'] = df['date'].dt.weekday # not used by Teck Meng Wong\n",
    "    # Teck Meng Wong mapped the integers to first-letters in triplets\n",
    "    # I'm leaving it as integers, where winter=1, spring=2, summer=3, fall=4\n",
    "    df['season'] = ((df['date'].dt.month % 12 + 3) // 3) #.map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n",
    "#     df['month'] = df['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "\n",
    "    df['wd4'] = df['date'].dt.weekday == 4\n",
    "    df['wd56'] = df['date'].dt.weekday >= 5\n",
    "#     df['wd6'] = df['date'].dt.weekday >= 6\n",
    "#     df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), 'day_of_year'] += 1 # fix for leap years\n",
    "    \n",
    "    # 21 days cyclic for lunar\n",
    "    dayofyear = df.date.dt.dayofyear # for convenience\n",
    "    \n",
    "    # here he's creating Fourier features\n",
    "    for k in range(1, 32, 4):\n",
    "        df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
    "        df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
    "        df[f'finland_sin{k}'] = np.where(df['country'] == 'Finland', df[f'sin{k}'], 0)\n",
    "        df[f'finland_cos{k}'] = np.where(df['country'] == 'Finland', df[f'cos{k}'], 0)\n",
    "        df[f'norway_sin{k}'] = np.where(df['country'] == 'Norway', df[f'sin{k}'], 0)\n",
    "        df[f'norway_cos{k}'] = np.where(df['country'] == 'Norway', df[f'cos{k}'], 0)\n",
    "        df[f'store_sin{k}'] = np.where(df['store'] == 'KaggleMart', df[f'sin{k}'], 0)\n",
    "        df[f'store_cos{k}'] = np.where(df['store'] == 'KaggleMart', df[f'cos{k}'], 0)\n",
    "        df[f'mug_sin{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'sin{k}'], 0)\n",
    "        df[f'mug_cos{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'cos{k}'], 0)\n",
    "        df[f'sticker_sin{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'sin{k}'], 0)\n",
    "        df[f'sticker_cos{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'cos{k}'], 0)\n",
    "    \n",
    "#     df[f'semiweekly_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 14)\n",
    "#     df[f'semiweekly_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 14)\n",
    "#     df[f'lunar_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 21)\n",
    "#     df[f'lunar_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 21)\n",
    "    df[f'season_sin'] = np.sin(dayofyear / 365 * 2 * math.pi * 91.5)\n",
    "    df[f'season_cos'] = np.cos(dayofyear / 365 * 2 * math.pi * 91.5)\n",
    "#     df = pd.concat([df, pd.DataFrame({f'fin{ptr[1]}':\n",
    "#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Finland')\n",
    "#                                       for ptr in holidays.Finland(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "#     df = pd.concat([df, pd.DataFrame({f'nor{ptr[1]}':\n",
    "#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Norway')\n",
    "#                                       for ptr in holidays.Norway(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "#     df = pd.concat([df, pd.DataFrame({f'swe{ptr[1]}':\n",
    "#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Sweden')\n",
    "#                                       for ptr in holidays.Sweden(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n",
    "\n",
    "    # End of year\n",
    "    # Dec - teckmengwong\n",
    "    for d in range(24, 32):\n",
    "        df[f\"dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d)\n",
    "    # I'm unsure of the logic of only doing this for Norway\n",
    "    for d in range(24, 32):\n",
    "        df[f\"n-dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "    \n",
    "    # not sure why he's using different date ranges for each country here\n",
    "    # Jan - teckmengwong\n",
    "    for d in range(1, 14):\n",
    "        df[f\"f-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "    for d in range(1, 10):\n",
    "        df[f\"n-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "    for d in range(1, 15):\n",
    "        df[f\"s-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "    \n",
    "    \n",
    "    # May - tekcmengwong\n",
    "    for d in list(range(1, 10)): # May Day and after, I guess\n",
    "        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d)\n",
    "    for d in list(range(19, 26)):\n",
    "        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n",
    "    # June \n",
    "    for d in list(range(8, 14)):\n",
    "        df[f\"june{d}\"] = (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n",
    "    \n",
    "    #Swedish Rock Concert - teckmengwong\n",
    "    #Jun 3, 2015 – Jun 6, 2015\n",
    "    #Jun 8, 2016 – Jun 11, 2016\n",
    "    #Jun 7, 2017 – Jun 10, 2017\n",
    "    #Jun 6, 2018 – Jun 10, 2018\n",
    "    #Jun 5, 2019 – Jun 8, 2019\n",
    "    swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),\n",
    "                                         2016: pd.Timestamp(('2016-06-11')),\n",
    "                                         2017: pd.Timestamp(('2017-06-10')),\n",
    "                                         2018: pd.Timestamp(('2018-06-10')),\n",
    "                                         2019: pd.Timestamp(('2019-06-8'))})\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n",
    "                                      (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n",
    "                                      for d in list(range(-3, 3))})], axis=1)\n",
    "\n",
    "    \n",
    "    # Last Wednesday of June - teckmengwong\n",
    "    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n",
    "                                         2016: pd.Timestamp(('2016-06-29')),\n",
    "                                         2017: pd.Timestamp(('2017-06-28')),\n",
    "                                         2018: pd.Timestamp(('2018-06-27')),\n",
    "                                         2019: pd.Timestamp(('2019-06-26'))})\n",
    "    for d in list(range(-4, 6)):\n",
    "        df[f\"wed_june{d}\"] = (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n",
    "        \n",
    "    # First Sunday of November - teckmengwong\n",
    "    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n",
    "                                         2016: pd.Timestamp(('2016-11-6')),\n",
    "                                         2017: pd.Timestamp(('2017-11-5')),\n",
    "                                         2018: pd.Timestamp(('2018-11-4')),\n",
    "                                         2019: pd.Timestamp(('2019-11-3'))})\n",
    "    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n",
    "                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n",
    "                                      for d in list(range(0, 9))})], axis=1)\n",
    "    \n",
    "    # First half of December (Independence Day of Finland, 6th of December) -teckmengwong\n",
    "    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n",
    "                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n",
    "                                      for d in list(range(6, 14))})], axis=1)\n",
    "    \n",
    "    # Easter -teckmengwong\n",
    "    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n",
    "    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n",
    "                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n",
    "                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38d1bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_all_df = temporal_engineering(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f4d3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      row_id       date  country       store         product  num_sold  \\\n",
      "0          0 2015-01-01  Finland  KaggleMart      Kaggle Mug     329.0   \n",
      "1          1 2015-01-01  Finland  KaggleMart      Kaggle Hat     520.0   \n",
      "2          2 2015-01-01  Finland  KaggleMart  Kaggle Sticker     146.0   \n",
      "3          3 2015-01-01  Finland  KaggleRama      Kaggle Mug     572.0   \n",
      "4          4 2015-01-01  Finland  KaggleRama      Kaggle Hat     911.0   \n",
      "...      ...        ...      ...         ...             ...       ...   \n",
      "6565   32863 2019-12-31   Sweden  KaggleMart      Kaggle Hat       NaN   \n",
      "6566   32864 2019-12-31   Sweden  KaggleMart  Kaggle Sticker       NaN   \n",
      "6567   32865 2019-12-31   Sweden  KaggleRama      Kaggle Mug       NaN   \n",
      "6568   32866 2019-12-31   Sweden  KaggleRama      Kaggle Hat       NaN   \n",
      "6569   32867 2019-12-31   Sweden  KaggleRama  Kaggle Sticker       NaN   \n",
      "\n",
      "           gdp  month  season    wd4  ...  easter47  easter50  easter51  \\\n",
      "0     5.461456      1       1  False  ...     False     False     False   \n",
      "1     5.461456      1       1  False  ...     False     False     False   \n",
      "2     5.461456      1       1  False  ...     False     False     False   \n",
      "3     5.461456      1       1  False  ...     False     False     False   \n",
      "4     5.461456      1       1  False  ...     False     False     False   \n",
      "...        ...    ...     ...    ...  ...       ...       ...       ...   \n",
      "6565  6.282042     12       1  False  ...     False     False     False   \n",
      "6566  6.282042     12       1  False  ...     False     False     False   \n",
      "6567  6.282042     12       1  False  ...     False     False     False   \n",
      "6568  6.282042     12       1  False  ...     False     False     False   \n",
      "6569  6.282042     12       1  False  ...     False     False     False   \n",
      "\n",
      "      easter52  easter53  easter54  easter55  easter56  easter57  easter58  \n",
      "0        False     False     False     False     False     False     False  \n",
      "1        False     False     False     False     False     False     False  \n",
      "2        False     False     False     False     False     False     False  \n",
      "3        False     False     False     False     False     False     False  \n",
      "4        False     False     False     False     False     False     False  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "6565     False     False     False     False     False     False     False  \n",
      "6566     False     False     False     False     False     False     False  \n",
      "6567     False     False     False     False     False     False     False  \n",
      "6568     False     False     False     False     False     False     False  \n",
      "6569     False     False     False     False     False     False     False  \n",
      "\n",
      "[32868 rows x 246 columns]"
     ]
    }
   ],
   "source": [
    "temporal_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b90e73b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [temporal_all_df]:\n",
    "    df['target'] = np.log(df['num_sold'] / df['gdp']**gdp_exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b32c9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_all_df['target'] = np.log(encoded_all_df['num_sold'] / (encoded_all_df['gdp']**gdp_exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88a51f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      row_id       date  country       store         product  num_sold  \\\n",
      "0          0 2015-01-01  Finland  KaggleMart      Kaggle Mug     329.0   \n",
      "1          1 2015-01-01  Finland  KaggleMart      Kaggle Hat     520.0   \n",
      "2          2 2015-01-01  Finland  KaggleMart  Kaggle Sticker     146.0   \n",
      "3          3 2015-01-01  Finland  KaggleRama      Kaggle Mug     572.0   \n",
      "4          4 2015-01-01  Finland  KaggleRama      Kaggle Hat     911.0   \n",
      "...      ...        ...      ...         ...             ...       ...   \n",
      "6565   32863 2019-12-31   Sweden  KaggleMart      Kaggle Hat       NaN   \n",
      "6566   32864 2019-12-31   Sweden  KaggleMart  Kaggle Sticker       NaN   \n",
      "6567   32865 2019-12-31   Sweden  KaggleRama      Kaggle Mug       NaN   \n",
      "6568   32866 2019-12-31   Sweden  KaggleRama      Kaggle Hat       NaN   \n",
      "6569   32867 2019-12-31   Sweden  KaggleRama  Kaggle Sticker       NaN   \n",
      "\n",
      "           gdp  month  season    wd4  ...  easter50  easter51  easter52  \\\n",
      "0     5.461456      1       1  False  ...     False     False     False   \n",
      "1     5.461456      1       1  False  ...     False     False     False   \n",
      "2     5.461456      1       1  False  ...     False     False     False   \n",
      "3     5.461456      1       1  False  ...     False     False     False   \n",
      "4     5.461456      1       1  False  ...     False     False     False   \n",
      "...        ...    ...     ...    ...  ...       ...       ...       ...   \n",
      "6565  6.282042     12       1  False  ...     False     False     False   \n",
      "6566  6.282042     12       1  False  ...     False     False     False   \n",
      "6567  6.282042     12       1  False  ...     False     False     False   \n",
      "6568  6.282042     12       1  False  ...     False     False     False   \n",
      "6569  6.282042     12       1  False  ...     False     False     False   \n",
      "\n",
      "      easter53  easter54  easter55  easter56  easter57  easter58    target  \n",
      "0        False     False     False     False     False     False  3.738239  \n",
      "1        False     False     False     False     False     False  4.196010  \n",
      "2        False     False     False     False     False     False  2.925788  \n",
      "3        False     False     False     False     False     False  4.291321  \n",
      "4        False     False     False     False     False     False  4.756724  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "6565     False     False     False     False     False     False       NaN  \n",
      "6566     False     False     False     False     False     False       NaN  \n",
      "6567     False     False     False     False     False     False       NaN  \n",
      "6568     False     False     False     False     False     False       NaN  \n",
      "6569     False     False     False     False     False     False       NaN  \n",
      "\n",
      "[32868 rows x 247 columns]"
     ]
    }
   ],
   "source": [
    "temporal_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90b3ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    features = ['country', 'product', 'store']\n",
    "    le_dict = {feature: LabelEncoder().fit(orig_train_df[feature]) for feature in features}\n",
    "    enc_df = df.copy()\n",
    "    for feature in features:\n",
    "        enc_df[feature] = le_dict[feature].transform(df[feature])\n",
    "    return le_dict, enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5100a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in le_dict.keys():\n",
    "#     print(f\"Values for key {key} are {le_dict[key].inverse_transform(range(len(le_dict[key].values())))}\")#\"\n",
    "# print(le_dict['country'].inverse_transform([0,1,2]))\n",
    "# print(le_dict['product'].inverse_transform([0,1,2]))\n",
    "# print(le_dict['store'].inverse_transform([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11ae4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_all_df = label_encoder(temporal_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7dfba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = encoded_all_df.drop(columns=['num_sold', 'row_id'])\n",
    "all_df = temporal_all_df.drop(columns=['row_id']) # writing over the previous version of `all_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1b6a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_df = all_df[:len(orig_train_df)] # training and validation sets -- still not encoded\n",
    "test_df = all_df[len(orig_train_df):] # still not encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2a7cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = encoded_all_df.iloc[np.where(encoded_all_df['date'] < '2019-01-01'), :]\n",
    "# test_df = encoded_all_df[[np.where(encoded_all_df['date'] > '2018-12-31')]]\n",
    "\n",
    "# encoded_tv_df = encoded_all_df.drop(columns=['row_id'])[:len(orig_train_df)]\n",
    "# encoded_test_df = encoded_all_df.drop(columns=['row_id'])[len(orig_train_df):]\n",
    "\n",
    "# valid_df = tv_df[tv_df['date'] > '2017-12-31']\n",
    "# train_df = tv_df[tv_df['date'] <= '2017-12-31']\n",
    "\n",
    "# train_and_valid_residual_df = train_and_valid_df.drop(columns=['date'])\n",
    "# test_residual_df = test_df.drop(columns=['date'])\n",
    "\n",
    "# len(valid_df) + len(train_df) == len(tv_df)\n",
    "\n",
    "# encoded_tv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b692d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, HuberRegressor, LinearRegression, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet\n",
    "# earth? wouldn't install via pip on my machine at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df374a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cca1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_trainset = load(predpath/'20220121_prophet_baseline_trainset.joblib')\n",
    "\n",
    "# neural_trainset = load(predpath/'20220121_neuralprophet_baseline_trainset.joblib')\n",
    "# neural_test_preds = load(predpath/'20220121_neuralprophet_baseline_testset.joblib')\n",
    "\n",
    "# ridge_tv_preds = load(predpath/'20210121_ridge_baseline_trainset_preds.joblib')\n",
    "# ridge_test_preds = load(predpath/'20220121_ridge_testset_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77e4b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_tv_preds = neural_trainset['prophet_forecast']\n",
    "# prophet_tv_preds = prophet_trainset['prophet_forecast']\n",
    "\n",
    "# neural_train_preds = neural_tv_preds[:train_length]\n",
    "# neural_valid_preds = neural_tv_preds[train_length:]\n",
    "\n",
    "# prophet_train_preds = prophet_tv_preds[:train_length]\n",
    "# prophet_valid_preds = prophet_tv_preds[train_length:]\n",
    "\n",
    "# train_length = len(neural_trainset[neural_trainset['date'] <= '2017-12-31'])\n",
    "\n",
    "# ridge_train_preds = ridge_tv_preds[:train_length]\n",
    "# ridge_valid_preds = ridge_tv_preds[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2395a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_linear_df = train_df.copy()\n",
    "# valid_linear_df = valid_df.copy()\n",
    "# test_linear_df = test_df.copy()\n",
    "# tv_linear_df = tv_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6bbcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "#     'holidays':holidays_train, # will add this in-function\n",
    "    'n_changepoints':10,\n",
    "    'changepoint_range':0.4,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_prior_scale':25,\n",
    "    'holidays_prior_scale':100,\n",
    "    'changepoint_prior_scale':0.01,\n",
    "    'interval_width':0.5,\n",
    "    'uncertainty_samples':False\n",
    "}\n",
    "\n",
    "neuralprophet_kwargs = {\n",
    "    'growth':'linear',\n",
    "    'n_changepoints':10,\n",
    "    'changepoints_range':0.4,\n",
    "    'trend_reg':1,\n",
    "    'trend_reg_threshold':False,\n",
    "    'yearly_seasonality':True,\n",
    "    'weekly_seasonality':True,\n",
    "    'daily_seasonality':False,\n",
    "    'seasonality_mode':'additive',\n",
    "    'seasonality_reg':1,\n",
    "    'n_forecasts':365,\n",
    "    'normalize':'off'\n",
    "}\n",
    "\n",
    "# for pytorch / skorch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tcn_kwargs = {\n",
    "#     'module': estimator, # will be handled at-call\n",
    "    'criterion': nn.MSELoss, # consider enhancement here\n",
    "    \"lr\": 0.01, # default is 0.01\n",
    "    'optimizer':Adam,\n",
    "    'max_epochs':10, # default is 10\n",
    "    'device':device,\n",
    "}\n",
    "\n",
    "# model_params['hyperparams'] = str(neuralprophet_kwargs)\n",
    "# model_params['holiday_source'] = 'Prophet builtin for each country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac32ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "#     'eval_metric': ['mae', 'mape', 'rmse'],\n",
    "    'learning_rate': .09,\n",
    "    'max_depth': 0,\n",
    "    'subsample': .15,\n",
    "#     'sampling_method': 'gradient_based',\n",
    "#     'seed': 42,\n",
    "#     'grow_policy': 'lossguide',\n",
    "    'max_leaves': 255,\n",
    "    'lambda': 100,\n",
    "#     'n_estimators': 3000,\n",
    "#     'objective': 'reg:squarederror',\n",
    "    'n_estimators': 50,\n",
    "#     'verbose': True,\n",
    "}\n",
    "\n",
    "\n",
    "lightgbm_params = {\n",
    "    'objective': 'mse',\n",
    "    'random_state': 42,\n",
    "    'device_type': 'cpu',\n",
    "    'n_jobs': -1,\n",
    "#                 eval_metric='auc',\n",
    "#     'device_type': 'gpu',\n",
    "#     'max_bin': 63, # 15 might be even better for GPU perf, but depends on dataset -- see https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\n",
    "#     'gpu_use_dp': False,\n",
    "    'max_depth': 0,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': .15,\n",
    "    'n_estimators': 1500,\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'task_type':'GPU',\n",
    "    'silent':True,\n",
    "    'random_state':42,\n",
    "}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3be7cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        # this is the first convolutional layer; note that it foregoes padding irrespective of argument\n",
    "        self.conv1 = weight_norm(nn.Conv2d(n_inputs, n_outputs, (1, kernel_size),\n",
    "                                           stride=stride, padding=0, dilation=dilation))\n",
    "        # the padding is then added after the first conv layer\n",
    "        self.pad = torch.nn.ZeroPad2d((padding, 0, 0, 0))\n",
    "        # this is a very standard choice\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # the second convolutional layer in the block is identical to the first, but now padding has been added to the input\n",
    "        self.conv2 = weight_norm(nn.Conv2d(n_outputs, n_outputs, (1, kernel_size),\n",
    "                                           stride=stride, padding=0, dilation=dilation))\n",
    "        \n",
    "        # this simply strings together the above architectural elements, for convenience I guess\n",
    "        self.net = nn.Sequential(self.pad, self.conv1, self.relu, self.dropout,\n",
    "                                 self.pad, self.conv2, self.relu, self.dropout)\n",
    "        \n",
    "        # if the n_outputs is nonzero, this adds on a final convlutional layer to ensure that we get the desired number of outputs\n",
    "        self.downsample = nn.Conv1d(\n",
    "            n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # this initializes the weights as specified in the separate weight initialization method, below\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # this method initializes the weights for the Conv1D and Conv2D layers, plus the Downsample layer (if it's used)\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # note the nice one-liner here, to add in the requisite number of dimensions both inbound to the NN and outbound\n",
    "        out = self.net(x.unsqueeze(2)).squeeze(2)\n",
    "        # is this a residual, then?\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d53fbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b8ffc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TCNModel, self).__init__()\n",
    "        self.tcn = TemporalConvNet(\n",
    "            128, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.decoder = nn.Linear(num_channels[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.dropout(self.tcn(x)[:, :, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac42735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_folds = [\n",
    "    ('2015-01-01', '2018-01-01'),\n",
    "    ('2018-01-01', '2019-01-01'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "819ff719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_tv_df = tv_df_encoded.copy() # encoded_tv_df.copy()\n",
    "# prophet_test_df = test_df_encoded.copy() # encoded_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e66ceb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in ['country', 'product', 'store']:\n",
    "#     prophet_tv_df[feature] = orig_train_df[feature]\n",
    "#     prophet_test_df[feature] = orig_test_df[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ea6ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_tv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "615e5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_enc = le_dict['country'].transform(countries)\n",
    "# stores_enc = le_dict['store'].transform(stores)\n",
    "# products_enc = le_dict['product'].transform(products)\n",
    "\n",
    "# countries, countries_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66d1b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralprophet_trainer(model_kwargs=neuralprophet_kwargs, countries=countries, stores=stores, products=products, folds=prophet_folds, \n",
    "                          tv_df=tv_df, test_df=test_df,\n",
    "#                           df_train=tv_df, df_test=test_df, \n",
    "                          target='num_sold', wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    # no label encoding here -- but test it with too\n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', target: 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', target: 'y'})\n",
    "\n",
    "#                     model = Prophet(**prophet_kwargs)\n",
    "                    model = NeuralProphet(**model_kwargs)\n",
    "\n",
    "                    model = model.add_country_holidays(country_name=country) # uses FacebookProphet or NeuralProphet API to add holidays\n",
    "                    print(train.columns)\n",
    "                    model.fit(train, freq='D') # neuralprophet\n",
    "                    # prophet\n",
    "#                     train_predictions = model.predict(train[['ds']])['yhat']\n",
    "#                     val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    # neuralprophet\n",
    "                    train_predictions = model.predict(train)['yhat1']\n",
    "                    val_predictions = model.predict(val)['yhat1']\n",
    "                    df_train.loc[train_idx, 'neuralprophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'neuralprophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test['y'] = np.nan\n",
    "                    test_predictions = model.predict(test)['yhat1']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'neuralprophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "#     train_\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train['neuralprophet_forecast'], df_test['neuralprophet_forecast']#, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cd05880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_trainer(prophet_kwargs=prophet_kwargs, countries=countries, stores=stores, products=products, folds=prophet_folds, \n",
    "                    tv_df=tv_df, test_df=test_df,\n",
    "#                           df_train=tv_df, df_test=test_df, \n",
    "                    target='num_sold', wandb_tracked=False):\n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (df_train['date'] >= start) &\\\n",
    "                                (df_train['date'] < end) &\\\n",
    "                                (df_train['country'] == country) &\\\n",
    "                                (df_train['store'] == store) &\\\n",
    "                                (df_train['product'] == product)\n",
    "                    \n",
    "#                     print(train_idx)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = df_train.loc[train_idx, ['date', target]].reset_index(drop=True)\n",
    "#                     print(train.shape)\n",
    "\n",
    "                    val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (df_train['date'] < folds[fold + 1][1]) &\\\n",
    "                              (df_train['country'] == country) &\\\n",
    "                              (df_train['store'] == store) &\\\n",
    "                              (df_train['product'] == product)\n",
    "\n",
    "                    val = df_train.loc[val_idx, ['date', target]].reset_index(drop=True)\n",
    "\n",
    "                    # rename the columns for standardization (this seems conventional)\n",
    "                    train = train.rename(columns={'date': 'ds', target: 'y'})\n",
    "                    val = val.rename(columns={'date': 'ds', target: 'y'})\n",
    "\n",
    "                    model = Prophet(**prophet_kwargs)\n",
    "\n",
    "                    model.add_country_holidays(country_name=country) # uses FacebookProphet API to add holidays\n",
    "                    model.fit(train)\n",
    "        \n",
    "                    train_predictions = model.predict(train[['ds']])['yhat']\n",
    "                    val_predictions = model.predict(val[['ds']])['yhat']\n",
    "                    df_train.loc[train_idx, 'prophet_forecast'] = train_predictions.values\n",
    "                    df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n",
    "\n",
    "                    train_score = SMAPE(train['y'].values, train_predictions.values)\n",
    "                    val_score = SMAPE(val['y'].values, val_predictions.values)\n",
    "            \n",
    "                    if wandb_tracked:\n",
    "                        wandb.log({f\"{(country,store,product)}_valid_smape\": val_score})\n",
    "            \n",
    "                    train_smape += train_score\n",
    "                    val_smape += val_score\n",
    "            \n",
    "                    print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n",
    "                    print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n",
    "\n",
    "                    test_idx = (df_test['country'] == country) &\\\n",
    "                               (df_test['store'] == store) &\\\n",
    "                               (df_test['product'] == product)\n",
    "                    test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n",
    "                    \n",
    "                    test = test.rename(columns={'date': 'ds'})\n",
    "                    test_predictions = model.predict(test[['ds']])['yhat']\n",
    "                    \n",
    "                    \n",
    "                    df_test.loc[test_idx, 'prophet_forecast'] = test_predictions.values\n",
    "    \n",
    "    train_smape /= (3*2*3)\n",
    "    val_smape /= (3*2*3)\n",
    "    \n",
    "    if wandb_tracked:\n",
    "        wandb.log({'overall_train_smape': train_smape, 'overall_valid_smape': val_smape})\n",
    "        wandb.finish()\n",
    "    return df_train['prophet_forecast'], df_test['prophet_forecast']#, train_smape, val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb3261c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_trainer(estimator, model_kwargs={}, tv_df=tv_df, test_df=test_df, #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                    folds=prophet_folds, countries=countries, stores=stores, products=products, target='target',\n",
    "#                     by_combo=True, \n",
    "                    model_type=None, # None -> fully scikit-learn compatible; alternatives are 'skorch' or 'gbm'\n",
    "                    wandb_tracked=False):\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    df_train = tv_df.copy()\n",
    "    df_test = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, tv_df = label_encoder(df_train) # should leave broader scope's tv_df alone\n",
    "    _, test_df = label_encoder(df_test) # should leave broader scope's test_df alone\n",
    "    del df_train, df_test\n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "    train_smape = 0\n",
    "    val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "    )\n",
    "    \n",
    "    # drop whichever version of the dependent variable is not being used\n",
    "#     for df in [tv_df, test_df]:\n",
    "    if target == 'num_sold': \n",
    "        tv_df = tv_df.drop(columns=['target'])\n",
    "        test_df = test_df.drop(columns=['target'])\n",
    "    else:\n",
    "        tv_df = tv_df.drop(columns=['num_sold'])\n",
    "        test_df = test_df.drop(columns=['num_sold'])\n",
    "            \n",
    "#     print(\"'num_sold' in test_df.columns == \", 'num_sold' in test_df.columns)\n",
    "    \n",
    "    # handling each combination of country, store, and product separately\n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for fold, (start, end) in enumerate(folds):\n",
    "                    # Skip iteration if it's the last fold\n",
    "                    if fold == len(folds) - 1:\n",
    "                        continue\n",
    "\n",
    "                    # put only those rows in that are in the training window and have the correct country, store, and product\n",
    "                    train_idx = (tv_df['date'] >= start) &\\\n",
    "                                (tv_df['date'] < end) &\\\n",
    "                                (tv_df['country'] == country) &\\\n",
    "                                (tv_df['store'] == store) &\\\n",
    "                                (tv_df['product'] == product)\n",
    "\n",
    "#                     print(train_idx)\n",
    "\n",
    "                    # redefine the training set in the local (holdout) sense\n",
    "                    train = tv_df.loc[train_idx, :].reset_index(drop=True)\n",
    "#                         print(train.shape)\n",
    "\n",
    "                    val_idx = (tv_df['date'] >= folds[fold + 1][0]) &\\\n",
    "                              (tv_df['date'] < folds[fold + 1][1]) &\\\n",
    "                              (tv_df['country'] == country) &\\\n",
    "                              (tv_df['store'] == store) &\\\n",
    "                              (tv_df['product'] == product)\n",
    "\n",
    "                    val = tv_df.loc[val_idx, :].reset_index(drop=True)\n",
    "\n",
    "                    test_idx = (test_df['country'] == country) &\\\n",
    "                               (test_df['store'] == store) &\\\n",
    "                               (test_df['product'] == product)\n",
    "                    test = test_df.loc[test_idx, :].reset_index(drop=True)\n",
    "\n",
    "                    # with the training and validation sets sorted out, make them integers for model fitting\n",
    "                    for df in [train, val, test]:\n",
    "                        df['date'] = df['date'].map(dt.datetime.toordinal)\n",
    "                    if 'model_forecast' in train.columns:\n",
    "                        X = train.drop(columns=[target, 'model_forecast'])\n",
    "                        X_valid = val.drop(columns=[target, 'model_forecast'])\n",
    "                        X_test = test.drop(columns=[target, 'model_forecast'])\n",
    "                    else:\n",
    "                        X = train.drop(columns=[target])\n",
    "                        X_valid = val.drop(columns=[target])\n",
    "                        X_test = test.drop(columns=[target])\n",
    "\n",
    "                    y = train[target]\n",
    "                    y_valid = val[target]\n",
    "\n",
    "\n",
    "#                         print(type(X), type(y))\n",
    "#                         print(f\"X has {X.isna().any().sum()} NaNs\")\n",
    "#                         print(f\"y has {y.isna().sum()} NaNs\")\n",
    "#                     print(X_test.info())\n",
    "#                     print(y_valid.dtype)\n",
    "    \n",
    "                    if model_type == 'skorch':\n",
    "#                         for df in [X, X_valid, X_test]:\n",
    "# #                             df['date'] = df['date'].apply(dt.datetime.toordinal)\n",
    "#                             df = torch.tensor(df.to_numpy(dtype=np.float32))\n",
    "#                         for target in [y, y_valid]:\n",
    "#                             target = torch.tensor(np.array(target))\n",
    "# #                             target = target.reshape(-1,1)\n",
    "#                             target = target.unsqueeze(0)\n",
    "                        X = torch.tensor(X.to_numpy(dtype=np.float32))\n",
    "                        X_valid = torch.tensor(X_valid.to_numpy(dtype=np.float32))\n",
    "                        X_test = torch.tensor(X_test.to_numpy(dtype=np.float32))\n",
    "            \n",
    "                        y = torch.tensor(np.array(y)).reshape(-1,1)\n",
    "                        y_valid = torch.tensor(np.array(y)).reshape(-1,1)\n",
    "    \n",
    "                        tcn_kwargs = {\n",
    "                            'num_channels': X_valid.shape[0] * X_valid.shape[1]\n",
    "                        }\n",
    "                        print(type(y), type(y_valid))\n",
    "#                         y = y.reshape(-1,1)\n",
    "#                         y_valid = y_valid.reshape(-1,1)\n",
    "                        # create the Datasets\n",
    "                \n",
    "                        # create the DataLoaders\n",
    "\n",
    "                        # instantiate the wrapper\n",
    "                        model = NeuralNetRegressor(\n",
    "                            module=estimator(**tcn_kwargs),\n",
    "                            **model_kwargs\n",
    "                        )\n",
    "#                     elif model_type=='gbm':\n",
    "                        \n",
    "                    else:\n",
    "                        model = estimator(**model_kwargs)\n",
    "\n",
    "                    model.fit(X,y)\n",
    "\n",
    "                    model_train_preds = model.predict(X)\n",
    "                    model_valid_preds = model.predict(X_valid)\n",
    "                    model_test_preds = model.predict(X_test)\n",
    "\n",
    "                    tv_df.loc[train_idx, 'model_forecast'] = model_train_preds#.values\n",
    "                    tv_df.loc[val_idx, 'model_forecast'] =  model_valid_preds#.values\n",
    "                    test_df.loc[test_idx, 'model_forecast'] = model_test_preds#.values\n",
    "\n",
    "\n",
    "    # reverse the dependent variable transform if appropriate\n",
    "    if target == 'target':\n",
    "#             model_tv_preds = np.multiply(np.exp(model_tv_preds), tv_df['gdp']**gdp_exponent)\n",
    "        tv_df['model_forecast'] = np.exp(tv_df['model_forecast']) * tv_df['gdp']**gdp_exponent\n",
    "#             output_tv_df['model_forecast'] = np.exp(output_tv_df['model_forecast']) * output_tv_df['gdp']**gdp_exponent\n",
    "\n",
    "#             model_test_preds = np.multiply(np.exp(model_test_preds), test_df['gdp']**gdp_exponent)\n",
    "        test_df['model_forecast'] = np.exp(test_df['model_forecast']) * test_df['gdp']**gdp_exponent\n",
    "#             output_test_df['model_forecast'] = np.exp(output_test_df['model_forecast']) * output_test_df['gdp']**gdp_exponent\n",
    "#             model_test_preds = np.exp(model_test_preds) * test_df['gdp']**gdp_exponent\n",
    "        \n",
    "#         tv_df['model_forecast'] = model_tv_preds\n",
    "#         test_df['model_forecast'] = model_test_preds\n",
    "#     return output_tv_df, output_test_df\n",
    "    return tv_df['model_forecast'], test_df['model_forecast']\n",
    "#     return tv_df['model_forecast'], test_df['model_forecast']\n",
    "#     return model_tv_preds, model_test_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "200989b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1cc07ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_trainer(arch:str, model_kwargs={}, exmodel_config={}, tv_df=tv_df, test_df=test_df, #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                countries=countries, stores=stores, products=products, \n",
    "                target='target', wandb_tracked=True, random_state=42):\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    X = tv_df.copy()\n",
    "#     X_test = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, X = label_encoder(X) # should leave broader scope's tv_df alone\n",
    "#     _, X_test = label_encoder(X_test) # should leave broader scope's test_df alone\n",
    "#     del df_train, df_test\n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "#     train_smape = 0\n",
    "#     val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "        )\n",
    "    \n",
    "    # drop whichever version of the dependent variable is not being used\n",
    "#     for df in [tv_df, test_df]:\n",
    "    y = X[target]\n",
    "#     for df in [X, X_test]:\n",
    "#         df = df.drop(columns=['num_sold', 'target'])\n",
    "    X = X.drop(columns=['num_sold', 'target'])\n",
    "#     X = X.drop(columns)\n",
    "#     if target == 'num_sold': \n",
    "#         y = X['num_sold']\n",
    "#         X = X.drop(columns=['target'])\n",
    "#         X_test = X_test.drop(columns=['target'])\n",
    "#     else:\n",
    "#         X = X.drop(columns=['num_sold'])\n",
    "#         X_test = X_test.drop(columns=['num_sold'])\n",
    "    \n",
    "    kfold = GroupKFold(n_splits=4)\n",
    "    oof_preds = pd.Series(0, index=tv_df.index)\n",
    "#     oof_preds, oof_y = [], []\n",
    "    \n",
    "#     test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X, groups=X.date.dt.year)):\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        # remove dates \n",
    "#         for df in [X, X_test]:\n",
    "#             df = df.drop(columns=['date'])\n",
    "        if 'date' in X.columns:\n",
    "            X = X.drop(columns=['date'])\n",
    "#             X_test = X_test.drop(columns=['date'])#, 'num_sold'])\n",
    "        \n",
    "        y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "        X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:]\n",
    "        \n",
    "        if arch == 'xgboost':\n",
    "            model = XGBRegressor(\n",
    "                tree_method='gpu_hist',\n",
    "                predictor= 'gpu_predictor',\n",
    "                eval_metric=['mae', 'mape'],\n",
    "                sampling_method='gradient_based',\n",
    "                seed=42,\n",
    "                grow_policy='lossguide',\n",
    "                objective='reg:squarederror',\n",
    "                **model_kwargs)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        elif arch == 'lightgbm':\n",
    "            model = LGBRegressor(**model_kwargs)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        elif arch == 'catboost':\n",
    "            model = CatBoostRegressor(\n",
    "                task_type='GPU',\n",
    "                verbose= False,\n",
    "                random_state=42,\n",
    "                **model_kwargs)\n",
    "            \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_preds = model.predict(X_valid)\n",
    "        \n",
    "#         oof_preds.extend(y_valid_preds)\n",
    "#         oof_y.extend(y_valid)\n",
    "        oof_preds[valid_ids] = y_valid_preds\n",
    "                \n",
    "    if target == 'target':\n",
    "        oof_preds = np.exp(oof_preds) * tv_df['gdp']**gdp_exponent\n",
    "#         oof_y = np.exp(tv_df[target]) * tv_df['gdp']**gdp_exponent\n",
    "#         test_preds = np.exp(test_preds) * test_df['gdp']**gdp_exponent\n",
    "\n",
    "#     return oof_preds, test_preds\n",
    "    smape = SMAPE(y_pred=oof_preds, y_true=tv_df['num_sold'])\n",
    "#     print(\"Lengths of oof_preds and tv_df[target] are same? \", len(oof_preds) == len(tv_df[target]))\n",
    "#     print(oof_preds[:10])\n",
    "#     print(tv_df[target][:10])\n",
    "    if wandb_tracked:\n",
    "        wandb.log({\n",
    "            'arch': arch,\n",
    "            'SMAPE': smape,\n",
    "            'model_params': str(model_kwargs),\n",
    "            'model_seed': random_state\n",
    "        })\n",
    "        wandb.finish()\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb420de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# smape = gbm_trainer(arch='xgboost', model_kwargs=xgboost_params, wandb_tracked=False)\n",
    "# smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be445b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# tracking \n",
    "import wandb\n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.lightgbm import wandb_callback\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = f\"optuna_forecasting_{datetime.now().strftime('%Y%m%d')}.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd138854",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmodel_config = {\n",
    "    'cross-validation': 'GroupKFold(n_split=4)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea02c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb config:\n",
    "wandb_config = {\n",
    "    'name': f\"{os.environ['WANDB_NOTEBOOK_NAME'][:-6]}_{datetime.now().strftime('%H%M%S')}\", # just removes the .ipynb extension, leaving the notebook filename's stem\n",
    "    'tags': ['study', 'gbms'],\n",
    "    'notes': \"Optuna study of forecasting methods (including the time-stripped GBMs)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57ed1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ffb7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally from https://www.kaggle.com/satorushibata/optimize-catboost-hyperparameter-with-optuna-gpu\n",
    "def objective(trial, arch=arch):#, tune_fold=tune_fold):\n",
    "    \"\"\"\n",
    "    Wrapper around cross_validation_trainer to test different model hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if arch == 'catboost':\n",
    "        model_params = {\n",
    "            'iterations' : trial.suggest_int('iterations', 2000, 30000),                         \n",
    "            'depth' : trial.suggest_int('depth', 3, 10),                                       \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.5),               \n",
    "            'random_strength': trial.suggest_int('random_strength', 0, 100), \n",
    "    #         'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n",
    "    #         'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['MVC', 'Bernoulli']),#, 'Poisson']),\n",
    "            'od_wait': trial.suggest_int('od_wait', 20, 2000),\n",
    "            'reg_lambda': trial.suggest_uniform('reg_lambda', 2, 70), # aka l2_leaf_reg\n",
    "            'border_count': trial.suggest_int('border_count', 50, 275),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 20), # aka min_data_in_leaf\n",
    "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 5),\n",
    "#             'task_type':'GPU',\n",
    "#             'verbose': False,\n",
    "# #             'silent':True,\n",
    "#             'random_state':42,\n",
    "            # 'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
    "    #         'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "    #         'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "            # 'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "    #         'max_leaves': trial.suggest_int('max_leaves', 32, 128)\n",
    "        }\n",
    "        \n",
    "    elif arch == 'lightgbm':\n",
    "        pass # todo -- fill in tomorrow\n",
    "        \n",
    "    elif arch == 'xgboost':\n",
    "        model_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 10000), # was 900-4500 for CPU\n",
    "            'max_depth' : trial.suggest_int('depth', 3, 10),                                       \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.3),               \n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 50),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 30),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
    "    #         'booster': trial.suggest_categorical('boosting_type', ['gbtree', 'dart']),\n",
    "            'min_child_weight': trial.suggest_uniform('min_child_weight', 0.001, 12),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "            'gamma': trial.suggest_uniform('gamma', 0.1, 10),\n",
    "#             'tree_method': 'gpu_hist',\n",
    "#             'predictor': 'gpu_predictor',\n",
    "#             'eval_metric': ['mae', 'mape'],\n",
    "#             'sampling_method': 'gradient_based',\n",
    "#             'seed': 42,\n",
    "#             'grow_policy': 'lossguide',\n",
    "#             'max_leaves': 255,\n",
    "#             'lambda': 100,\n",
    "#     'n_estimators': 3000,\n",
    "#             'objective': 'reg:squarederror',\n",
    "#             'n_estimators': 500,\n",
    "#     'verbose': True,\n",
    "            \n",
    "        } \n",
    "    \n",
    "    return gbm_trainer(arch=arch, model_kwargs=model_params, wandb_tracked=False)#, telegram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68a60df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82752bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b8038c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d737235",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "study = optuna.create_study(direction = \"minimize\", \n",
    "                            sampler = TPESampler(seed=int(SEED)), \n",
    "                            study_name=f\"{arch}_study-{start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c590c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "for x in range(1, 500):\n",
    "    study.optimize(objective, n_trials = 1, callbacks = [wandbc], show_progress_bar=False)#, catch=(xgboost.core.XGBoostError,)) \n",
    "    dump(study, filename=studypath/f\"optuna_{arch}_study-{start_time}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb218320",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'best_params': str(study.best_trial.params),\n",
    "#            'trials_in_run': len(study.trials),\n",
    "           'trials_in_study': len(study.trials)\n",
    "          })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53825c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 4207,\n",
      " 'depth': 5,\n",
      " 'learning_rate': 0.05378597302351865,\n",
      " 'reg_alpha': 0.0067949392113948815,\n",
      " 'reg_lambda': 0.04865823628931899,\n",
      " 'subsample': 0.212875760245356,\n",
      " 'min_child_weight': 6.997692447967251,\n",
      " 'colsample_bytree': 0.9824893256584818,\n",
      " 'gamma': 0.10395228539921328}"
     ]
    }
   ],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76448ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e4e96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CatBoost Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "692e67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'catboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "619290b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04bd2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally from https://www.kaggle.com/satorushibata/optimize-catboost-hyperparameter-with-optuna-gpu\n",
    "def objective(trial, arch=arch):#, tune_fold=tune_fold):\n",
    "    \"\"\"\n",
    "    Wrapper around cross_validation_trainer to test different model hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if arch == 'catboost':\n",
    "        model_params = {\n",
    "            'iterations' : trial.suggest_int('iterations', 2000, 30000),                         \n",
    "            'depth' : trial.suggest_int('depth', 3, 10),                                       \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.5),               \n",
    "            'random_strength': trial.suggest_int('random_strength', 0, 100), \n",
    "    #         'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n",
    "    #         'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['MVC', 'Bernoulli']),#, 'Poisson']),\n",
    "            'od_wait': trial.suggest_int('od_wait', 20, 2000),\n",
    "            'reg_lambda': trial.suggest_uniform('reg_lambda', 2, 70), # aka l2_leaf_reg\n",
    "            'border_count': trial.suggest_int('border_count', 50, 275),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 20), # aka min_data_in_leaf\n",
    "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 5),\n",
    "#             'task_type':'GPU',\n",
    "#             'verbose': False,\n",
    "# #             'silent':True,\n",
    "#             'random_state':42,\n",
    "            # 'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
    "    #         'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "    #         'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "            # 'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "    #         'max_leaves': trial.suggest_int('max_leaves', 32, 128)\n",
    "        }\n",
    "        \n",
    "    elif arch == 'lightgbm':\n",
    "        pass # todo -- fill in tomorrow\n",
    "        \n",
    "    elif arch == 'xgboost':\n",
    "        model_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 10000), # was 900-4500 for CPU\n",
    "            'max_depth' : trial.suggest_int('depth', 3, 10),                                       \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.3),               \n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 50),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 30),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
    "    #         'booster': trial.suggest_categorical('boosting_type', ['gbtree', 'dart']),\n",
    "            'min_child_weight': trial.suggest_uniform('min_child_weight', 0.001, 12),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "            'gamma': trial.suggest_uniform('gamma', 0.1, 10),\n",
    "#             'tree_method': 'gpu_hist',\n",
    "#             'predictor': 'gpu_predictor',\n",
    "#             'eval_metric': ['mae', 'mape'],\n",
    "#             'sampling_method': 'gradient_based',\n",
    "#             'seed': 42,\n",
    "#             'grow_policy': 'lossguide',\n",
    "#             'max_leaves': 255,\n",
    "#             'lambda': 100,\n",
    "#     'n_estimators': 3000,\n",
    "#             'objective': 'reg:squarederror',\n",
    "#             'n_estimators': 500,\n",
    "#     'verbose': True,\n",
    "            \n",
    "        } \n",
    "    \n",
    "    return gbm_trainer(arch=arch, model_kwargs=model_params, wandb_tracked=False)#, telegram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6b39548",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "study = optuna.create_study(direction = \"minimize\", \n",
    "                            sampler = TPESampler(seed=int(SEED)), \n",
    "                            study_name=f\"{arch}_study-{start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5628b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for x in range(1, 500):\n",
    "    study.optimize(objective, n_trials = 1, callbacks = [wandbc], show_progress_bar=False)#, catch=(xgboost.core.XGBoostError,)) \n",
    "    dump(study, filename=studypath/f\"optuna_{arch}_study-{start_time}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a3b9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'best_params': str(study.best_trial.params),\n",
    "#            'trials_in_run': len(study.trials),\n",
    "           'trials_in_study': len(study.trials)\n",
    "          })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48a12f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iterations': 10529,\n",
      " 'depth': 3,\n",
      " 'learning_rate': 0.07026263205443048,\n",
      " 'random_strength': 44,\n",
      " 'od_wait': 261,\n",
      " 'reg_lambda': 35.672029887566374,\n",
      " 'border_count': 57,\n",
      " 'min_child_samples': 19,\n",
      " 'leaf_estimation_iterations': 2}"
     ]
    }
   ],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15335618",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for x in range(1, 500):\n",
    "    study.optimize(objective, n_trials = 1, callbacks = [wandbc], show_progress_bar=False)#, catch=(xgboost.core.XGBoostError,)) \n",
    "    dump(study, filename=studypath/f\"optuna_{arch}_study-{start_time}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e1db81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'best_params': str(study.best_trial.params),\n",
    "#            'trials_in_run': len(study.trials),\n",
    "           'trials_in_study': len(study.trials)\n",
    "          })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bacd73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'best_params': str(study.best_trial.params),\n",
    "#            'trials_in_run': len(study.trials),\n",
    "           'trials_in_study': len(study.trials)\n",
    "          })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2aa7995",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = load(studypath/'optuna_catboost_study-20220127082356.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f49e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "020d625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iterations': 10529,\n",
      " 'depth': 3,\n",
      " 'learning_rate': 0.07026263205443048,\n",
      " 'random_strength': 44,\n",
      " 'od_wait': 261,\n",
      " 'reg_lambda': 35.672029887566374,\n",
      " 'border_count': 57,\n",
      " 'min_child_samples': 19,\n",
      " 'leaf_estimation_iterations': 2}"
     ]
    }
   ],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9974652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_trainer(arch:str, model_kwargs={}, exmodel_config={}, tv_df=tv_df, test_df=test_df, #X=X, y=y, X_valid=X_valid, y_valid=y_valid, X_test=X_test, \n",
    "                countries=countries, stores=stores, products=products, \n",
    "                target='target', wandb_tracked=True, random_state=42):\n",
    "    \n",
    "    # create local versions of the dataframes, to avoid mutation\n",
    "    X = tv_df.copy()\n",
    "#     X_test = test_df.copy()\n",
    "    \n",
    "    # apply label encoding (which Scikit-Learn models require, but *Prophets don't)\n",
    "    le_dict, X = label_encoder(X) # should leave broader scope's tv_df alone\n",
    "#     _, X_test = label_encoder(X_test) # should leave broader scope's test_df alone\n",
    "#     del df_train, df_test\n",
    "    \n",
    "    # encode the lists of countries, stores, and products\n",
    "    countries = le_dict['country'].transform(countries)\n",
    "    stores = le_dict['store'].transform(stores)\n",
    "    products = le_dict['product'].transform(products)\n",
    "    \n",
    "#     train_smape = 0\n",
    "#     val_smape = 0\n",
    "    \n",
    "    if wandb_tracked:\n",
    "#         exmodel_config['arch'] = arch\n",
    "#         exmodel_config[f'{arch}_params'] = str(model_params)\n",
    "        wandb.init(\n",
    "            project=\"202201_Kaggle_tabular_playground\",\n",
    "            save_code=True,\n",
    "            tags=wandb_config['tags'],\n",
    "            name=wandb_config['name'],\n",
    "            notes=wandb_config['notes'],\n",
    "            config=exmodel_config\n",
    "        )\n",
    "    \n",
    "    # drop whichever version of the dependent variable is not being used\n",
    "#     for df in [tv_df, test_df]:\n",
    "    y = X[target]\n",
    "#     for df in [X, X_test]:\n",
    "#         df = df.drop(columns=['num_sold', 'target'])\n",
    "    X = X.drop(columns=['num_sold', 'target'])\n",
    "#     X = X.drop(columns)\n",
    "#     if target == 'num_sold': \n",
    "#         y = X['num_sold']\n",
    "#         X = X.drop(columns=['target'])\n",
    "#         X_test = X_test.drop(columns=['target'])\n",
    "#     else:\n",
    "#         X = X.drop(columns=['num_sold'])\n",
    "#         X_test = X_test.drop(columns=['num_sold'])\n",
    "    \n",
    "    kfold = GroupKFold(n_splits=4)\n",
    "    oof_preds = pd.Series(0, index=tv_df.index)\n",
    "#     oof_preds, oof_y = [], []\n",
    "    \n",
    "#     test_preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(X, groups=X.date.dt.year)):\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        # remove dates \n",
    "#         for df in [X, X_test]:\n",
    "#             df = df.drop(columns=['date'])\n",
    "        if 'date' in X.columns:\n",
    "            X = X.drop(columns=['date'])\n",
    "#             X_test = X_test.drop(columns=['date'])#, 'num_sold'])\n",
    "        \n",
    "        y_train, y_valid = y[train_ids], y[valid_ids]\n",
    "        X_train, X_valid = X.iloc[train_ids,:], X.iloc[valid_ids,:]\n",
    "        \n",
    "        if arch == 'xgboost':\n",
    "            model = XGBRegressor(\n",
    "                tree_method='gpu_hist',\n",
    "                predictor= 'gpu_predictor',\n",
    "                eval_metric=['mae', 'mape'],\n",
    "                sampling_method='gradient_based',\n",
    "                seed=42,\n",
    "                grow_policy='lossguide',\n",
    "                objective='reg:squarederror',\n",
    "                **model_kwargs)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.xgboost.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        elif arch == 'lightgbm':\n",
    "            model = LGBMRegressor(**model_kwargs)\n",
    "            if wandb_tracked:\n",
    "                model.fit(X_train, y_train, callbacks=[wandb.lightgbm.wandb_callback()])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        elif arch == 'catboost':\n",
    "            model = CatBoostRegressor(\n",
    "                task_type='GPU',\n",
    "                verbose= False,\n",
    "                random_state=42,\n",
    "                **model_kwargs)\n",
    "            \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_preds = model.predict(X_valid)\n",
    "        \n",
    "#         oof_preds.extend(y_valid_preds)\n",
    "#         oof_y.extend(y_valid)\n",
    "        oof_preds[valid_ids] = y_valid_preds\n",
    "                \n",
    "    if target == 'target':\n",
    "        oof_preds = np.exp(oof_preds) * tv_df['gdp']**gdp_exponent\n",
    "#         oof_y = np.exp(tv_df[target]) * tv_df['gdp']**gdp_exponent\n",
    "#         test_preds = np.exp(test_preds) * test_df['gdp']**gdp_exponent\n",
    "\n",
    "#     return oof_preds, test_preds\n",
    "    smape = SMAPE(y_pred=oof_preds, y_true=tv_df['num_sold'])\n",
    "#     print(\"Lengths of oof_preds and tv_df[target] are same? \", len(oof_preds) == len(tv_df[target]))\n",
    "#     print(oof_preds[:10])\n",
    "#     print(tv_df[target][:10])\n",
    "    if wandb_tracked:\n",
    "        wandb.log({\n",
    "            'arch': arch,\n",
    "            'SMAPE': smape,\n",
    "            'model_params': str(model_kwargs),\n",
    "            'model_seed': random_state\n",
    "        })\n",
    "        wandb.finish()\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d49da25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 4207,\n",
      " 'learning_rate': 0.05378597302351865,\n",
      " 'reg_alpha': 0.0067949392113948815,\n",
      " 'reg_lambda': 0.04865823628931899,\n",
      " 'subsample': 0.212875760245356,\n",
      " 'min_child_weight': 6.997692447967251,\n",
      " 'colsample_bytree': 0.9824893256584818,\n",
      " 'gamma': 0.10395228539921328,\n",
      " 'max_depth': 5}"
     ]
    }
   ],
   "source": [
    "best_xgboost_params = load(studypath/'optuna_xgboost_study-20220126213551.joblib').best_trial.params\n",
    "best_xgboost_params['max_depth'] = best_xgboost_params['depth']\n",
    "del best_xgboost_params['depth']\n",
    "best_xgboost_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0ebb5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hushifang/202201_Kaggle_tabular_playground/runs/rlamov2s\" target=\"_blank\">optuna_forecasting_20220126_213545</a></strong> to <a href=\"https://wandb.ai/hushifang/202201_Kaggle_tabular_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbm_trainer(arch='xgboost', model_kwargs=best_xgboost_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
