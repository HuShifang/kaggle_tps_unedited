{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So above, you have a two-item, rank-1 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, you have an embedding that will contain 1 distinct entry and have a size-4 vector. \n",
    "    - You might think of this as describing how a single word maps to four other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1799, -0.1115, -1.1305,  1.4224],\n",
       "        [ 0.1799, -0.1115, -1.1305,  1.4224]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.LongTensor([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the entirety of the embedding is the 1x4 vector `[ 0.1799, -0.1115, -1.1305,  1.4224]`. When you pass in a two-item, rank-1 tensor with 0s, it returns the vector at index 0 of the embedding twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.9557e-01, -6.8639e-01,  1.2521e-01,  1.0239e-04, -1.2211e+00],\n",
       "        [-7.5383e-01, -4.4092e-01, -1.2775e+00,  9.2140e-01,  9.1968e-01],\n",
       "        [-1.0855e-01,  7.3577e-01, -2.1150e-01,  4.4531e-01, -2.2939e+00],\n",
       "        [-6.9557e-01, -6.8639e-01,  1.2521e-01,  1.0239e-04, -1.2211e+00]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.LongTensor([0,1,2,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass as a lookup-list a tensor like so, you'll get another tensor containing the items index 0, 1, and 2, and then 0 again, from the randomly generated embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4102]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(200, 1)\n",
    "embedding(torch.LongTensor([199]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look up the length-1, rank-1 item with index 199 in the 200-item embedding table like above. But if you try to look up the item at index 200, you'll have a bad time, because there is no such item -- you would have had to pass `201` as the first argument to `nn.Embedding` to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a9b3115a32a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/anaconda3/envs/tabular-x/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "embedding(torch.LongTensor([200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([[1,2,3,4,5], [6,7,8,9,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1566, -1.0602,  1.6966, -0.4835,  0.1418],\n",
       "         [-0.9794, -1.3156, -0.3406,  1.7615, -0.7826],\n",
       "         [ 1.0896, -0.8429, -0.1067,  0.2787, -0.8359],\n",
       "         [ 1.8928, -0.4486, -0.7178, -0.4341, -0.0655],\n",
       "         [ 1.9307,  0.7189, -0.6727, -1.2131, -1.0864]],\n",
       "\n",
       "        [[-0.6851, -0.5309, -0.1807,  1.1145, -0.1963],\n",
       "         [-0.0842, -0.1603,  0.4753, -0.5600, -0.3373],\n",
       "         [ 1.2985, -0.0272,  0.4433,  0.5025,  0.1340],\n",
       "         [ 0.5890,  0.0357,  1.9312,  0.3142,  0.2670],\n",
       "         [ 0.6460,  0.0627, -0.1356,  0.8055,  1.2612]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(11,5)\n",
    "embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7285,  0.0778, -0.6906, -0.0529,  1.1316],\n",
       "         [ 1.3995, -0.8523,  0.2807, -2.6537, -1.0933],\n",
       "         [-0.0457, -0.0781,  3.8702,  0.3604,  0.0418]],\n",
       "\n",
       "        [[ 1.6352,  1.7193, -0.3228, -0.5870, -0.5407],\n",
       "         [-0.6589,  0.9963, -0.2757,  0.4144, -0.6856],\n",
       "         [-0.2298, -0.1375,  0.6844, -0.3869,  0.1136]],\n",
       "\n",
       "        [[ 0.7640, -1.6889, -1.1883, -0.4289, -0.6095],\n",
       "         [ 1.2039,  1.9125,  0.1949,  1.2838,  1.1462],\n",
       "         [-0.7507, -0.8444,  0.8797, -0.2231, -0.6771]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.LongTensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(x.shape)\n",
    "embedding = nn.Embedding(10,5)\n",
    "embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.5321, -0.4328,  1.5068, -0.4174, -0.9384],\n",
       "          [ 0.5735,  1.0236, -0.7790,  0.3301,  0.3866],\n",
       "          [-0.4201, -1.3785,  0.9226,  1.1392,  0.9528]],\n",
       "\n",
       "         [[-0.3818, -0.4232,  1.1798,  0.9941,  0.0115],\n",
       "          [ 2.1712, -0.5416, -1.1216, -0.8111,  0.8903],\n",
       "          [-0.8523, -0.6758, -0.3092, -0.8634, -1.0632]],\n",
       "\n",
       "         [[-0.2722, -1.5317,  0.8587,  0.3555,  0.4001],\n",
       "          [-0.5981,  0.3267, -1.3805,  1.7650,  1.6163],\n",
       "          [-0.4526,  0.3024, -1.1152, -1.2691,  0.7107]]],\n",
       "\n",
       "\n",
       "        [[[-0.2419,  0.1115,  0.7022, -0.8589,  0.9142],\n",
       "          [-0.4763,  1.2867, -0.5896, -1.1737, -1.9199],\n",
       "          [ 2.1772,  0.3886,  1.3080, -0.0141, -1.2014]],\n",
       "\n",
       "         [[-0.9749,  0.5032, -1.4533,  0.6109,  0.0718],\n",
       "          [ 0.2362,  0.9619, -1.3447, -0.5027,  0.0739],\n",
       "          [-0.2146, -2.2915,  0.5675,  1.0459, -1.5595]],\n",
       "\n",
       "         [[ 0.4355, -1.8622,  1.0779,  1.0574, -0.3239],\n",
       "          [-1.4404, -1.2105, -0.4782,  1.7985,  0.5934],\n",
       "          [ 1.8132,  1.7913, -3.1021,  0.7971, -0.4364]]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.LongTensor([[[1,2,3],[4,5,6],[7,8,9]],\n",
    "                      [[10,11,12],[13,14,15],[16,17,18]]])\n",
    "print(x.shape)\n",
    "embedding = nn.Embedding(19,5)\n",
    "embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635]],\n",
       "\n",
       "          [[-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973]],\n",
       "\n",
       "          [[-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503]]],\n",
       "\n",
       "\n",
       "         [[[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503]],\n",
       "\n",
       "          [[ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757]],\n",
       "\n",
       "          [[-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958]],\n",
       "\n",
       "          [[ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470]]],\n",
       "\n",
       "\n",
       "         [[[-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331]],\n",
       "\n",
       "          [[ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503]],\n",
       "\n",
       "          [[-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255]],\n",
       "\n",
       "          [[-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757]],\n",
       "\n",
       "          [[-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470]],\n",
       "\n",
       "          [[ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062]]],\n",
       "\n",
       "\n",
       "         [[[-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958]],\n",
       "\n",
       "          [[ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470]]],\n",
       "\n",
       "\n",
       "         [[[ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958]],\n",
       "\n",
       "          [[ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757]],\n",
       "\n",
       "          [[ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757]],\n",
       "\n",
       "          [[-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958]]],\n",
       "\n",
       "\n",
       "         [[[-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757]],\n",
       "\n",
       "          [[ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757]],\n",
       "\n",
       "          [[ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062]],\n",
       "\n",
       "          [[-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [-1.2213,  2.5747,  0.1518,  0.7915, -0.9062],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503]]],\n",
       "\n",
       "\n",
       "         [[[-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [ 0.1575,  1.1389,  0.3828,  0.0385, -1.2757],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [-1.1996,  1.1203, -2.0749, -0.6699,  0.9470],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [-1.6401, -0.6372,  2.3019, -0.8344, -0.0973],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [ 1.8128,  0.7059,  1.3513, -0.7154,  0.5635]],\n",
       "\n",
       "          [[ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 0.1310,  0.8138, -1.6016, -2.4117, -0.4255],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958]],\n",
       "\n",
       "          [[-1.0844,  0.4452,  1.8177,  1.6652,  0.0757],\n",
       "           [ 1.4438,  1.4082, -0.8478, -1.6787, -0.8331],\n",
       "           [-1.2930,  1.0422,  1.3527,  0.3613, -3.5503],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958],\n",
       "           [ 0.5062,  0.9103, -1.9796,  0.4233,  0.3958]]]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(10, (3,3,4,5))\n",
    "print(x.shape)\n",
    "embedding = nn.Embedding(10,5)\n",
    "embedding(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
